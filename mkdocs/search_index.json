{
    "docs": [
        {
            "location": "/index.html",
            "text": "Introduction\n\n\nWelcome to the \nCloudbreak\n documentation!\n\n\nCloudbreak simplifies the provisioning, management, and monitoring of on-demand HDP clusters in virtual and cloud environments. It leverages cloud infrastructure to create host instances, and uses Apache Ambari via Ambari blueprints to provision and manage HDP clusters. \n\n\nCloudbreak allows you to create clusters using the Cloudbreak web UI, Cloudbreak Shell, and Cloudbreak REST API. Clusters can be launched on public cloud infrastructure platforms \nMicrosoft Azure\n, \nAmazon Web Services (AWS)\n, and \nGoogle Cloud Platform (GCP)\n, and on the private cloud infrastructure platform \nOpenStack\n.\n\n\n    \n\n\nUse Cases\n\n\nCloudbreak allows you to create, manage, and monitor your clusters on your chosen cloud platform:\n\n\n\n\nQuickly create a cluster using one of the default cluster blueprints and infrastructure settings.  \n\n\nCreate a cluster based on the requirements of your workloads and provision infrastructure based on your IT requirements.\n\n\nSecure your cluster by enabling Kerberos and HA. \n\n\nAutomate cluster creation using the Cloudbreak shell.  \n\n\nDevelop your application using Cloudbreak API. \n\n\n\n\nArchitecture\n\n\nRefer to \nArchitecture\n.\n\n\nGet Started\n\n\nTo get started with Cloudbreak:\n\n\n\n\nSelect the \ncloud platform\n on which you would like to launch Cloudbreak.   \n\n\nSelect the \ndeployment option\n that you would like to use. \n\n\nLaunch Cloudbreak\n. \n\n\n\n\nSelect Cloud Platform\n\n\nYou can deploy and use Cloudbreak on the following cloud platforms:\n\n\n\n\nAmazon Web Services (AWS)\n\n\nMicrosoft Azure\n\n\nGoogle Cloud Platform (GCP)\n\n\nOpenStack\n\n\n\n\nSelect Deployment Option\n\n\nThere are two basic deployment options:\n\n\n\n\n\n\n\n\nDeployment option\n\n\nWhen to use\n\n\n\n\n\n\n\n\n\n\nInstantiate one of the pre-built cloud images\n\n\nThis is the recommended basic deployment option.\n The cloud images include Cloudbreak deployer pre-installed on a CentOS VM.\n\n\n\n\n\n\nInstall the Cloudbreak deployer on your own VM\n\n\nThis is an advanced deployment option.\n \nSelect this option if you have custom VM requirements. The supported operating systems are RHEL, CentOS, and Oracle Linux 7 (64-bit).\n\n\n\n\n\n\n\n\nLaunch Cloudbreak\n\n\n(Option 1) You can launch Cloudbreak from one of the pre-built images:  \n\n\n\n\nLaunch on AWS\n  \n\n\nLaunch on Azure\n  \n\n\nLaunch on GCP\n   \n\n\nLaunch on OpenStack\n    \n\n\n\n\n(Option 2) Or you can launch Cloudbreak \non your own VM\n on one of these cloud platforms. This is an \nadvanced\n deployment option that you should only use if you have custom VM requirements. \n\n\nIn general, the steps include meeting the prerequisites, launching Cloudbreak on a VM, and creating Cloudbreak credential. After performing these steps, you can create a cluster based on one of the default blueprints or upload your own blueprint and then create a cluster. \n\n\n\n    \nNote\n\n    \nThe Cloudbreak software runs in your cloud environment. You are responsible for cloud infrastructure related charges while running Cloudbreak and the clusters being managed by Cloudbreak.",
            "title": "Introduction"
        },
        {
            "location": "/index.html#introduction",
            "text": "Welcome to the  Cloudbreak  documentation!  Cloudbreak simplifies the provisioning, management, and monitoring of on-demand HDP clusters in virtual and cloud environments. It leverages cloud infrastructure to create host instances, and uses Apache Ambari via Ambari blueprints to provision and manage HDP clusters.   Cloudbreak allows you to create clusters using the Cloudbreak web UI, Cloudbreak Shell, and Cloudbreak REST API. Clusters can be launched on public cloud infrastructure platforms  Microsoft Azure ,  Amazon Web Services (AWS) , and  Google Cloud Platform (GCP) , and on the private cloud infrastructure platform  OpenStack .",
            "title": "Introduction"
        },
        {
            "location": "/index.html#use-cases",
            "text": "Cloudbreak allows you to create, manage, and monitor your clusters on your chosen cloud platform:   Quickly create a cluster using one of the default cluster blueprints and infrastructure settings.    Create a cluster based on the requirements of your workloads and provision infrastructure based on your IT requirements.  Secure your cluster by enabling Kerberos and HA.   Automate cluster creation using the Cloudbreak shell.    Develop your application using Cloudbreak API.",
            "title": "Use Cases"
        },
        {
            "location": "/index.html#architecture",
            "text": "Refer to  Architecture .",
            "title": "Architecture"
        },
        {
            "location": "/index.html#get-started",
            "text": "To get started with Cloudbreak:   Select the  cloud platform  on which you would like to launch Cloudbreak.     Select the  deployment option  that you would like to use.   Launch Cloudbreak .",
            "title": "Get Started"
        },
        {
            "location": "/index.html#select-cloud-platform",
            "text": "You can deploy and use Cloudbreak on the following cloud platforms:   Amazon Web Services (AWS)  Microsoft Azure  Google Cloud Platform (GCP)  OpenStack",
            "title": "Select Cloud Platform"
        },
        {
            "location": "/index.html#select-deployment-option",
            "text": "There are two basic deployment options:     Deployment option  When to use      Instantiate one of the pre-built cloud images  This is the recommended basic deployment option.  The cloud images include Cloudbreak deployer pre-installed on a CentOS VM.    Install the Cloudbreak deployer on your own VM  This is an advanced deployment option.   Select this option if you have custom VM requirements. The supported operating systems are RHEL, CentOS, and Oracle Linux 7 (64-bit).",
            "title": "Select Deployment Option"
        },
        {
            "location": "/index.html#launch-cloudbreak",
            "text": "(Option 1) You can launch Cloudbreak from one of the pre-built images:     Launch on AWS     Launch on Azure     Launch on GCP      Launch on OpenStack        (Option 2) Or you can launch Cloudbreak  on your own VM  on one of these cloud platforms. This is an  advanced  deployment option that you should only use if you have custom VM requirements.   In general, the steps include meeting the prerequisites, launching Cloudbreak on a VM, and creating Cloudbreak credential. After performing these steps, you can create a cluster based on one of the default blueprints or upload your own blueprint and then create a cluster.   \n     Note \n     The Cloudbreak software runs in your cloud environment. You are responsible for cloud infrastructure related charges while running Cloudbreak and the clusters being managed by Cloudbreak.",
            "title": "Launch Cloudbreak"
        },
        {
            "location": "/architecture/index.html",
            "text": "Architecture\n\n\nCloudbreak deployer\n installs Cloudbreak components on your AWS VM. Once these components are deployed, you can use \nCloudbreak application\n to create, manage, and monitor clusters. \n\n\nCloudbreak Deployer Architecture\n\n\nCloudbreak deployer includes the following components:\n\n\n\n\n\n\n\n\nComponent\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nCloudbreak Application\n\n\nCloudbreak application is built on the foundation of cloud provider APIs and Apache Ambari.\n\n\n\n\n\n\nUluwatu\n\n\nThis is Cloudbreak web UI, which can be used to create, manage, and monitor clusters.\n\n\n\n\n\n\nCloudbreak Shell\n\n\nThis is Cloudbreak's command line tool, which can be used to create, manage, and monitor clusters.\n\n\n\n\n\n\nIdentity\n\n\nThis is Cloudbreak's OAuth identity server implementation, which utilizes UAA.\n\n\n\n\n\n\nSultans\n\n\nThis is Cloudbreak's user management system.\n\n\n\n\n\n\nPeriscope\n\n\nThis is Cloudbreak's autoscaling application, which is responsible for automatically increasing or decreasing the capacity of the cluster when your pre-defined conditions are met.\n\n\n\n\n\n\n\n\n\n\nThese component names are used in Cloudbreak logs, so for troubleshooting purposes it is useful to know what they refer to.\n\n\n\n\nCloudbreak Application Architecture\n\n\nThe Cloudbreak application is a web application that communicates with the cloud provider account to create cloud resources on your behalf. Once the cloud resources are in place, Cloudbreak uses Apache Ambari to deploy and configure the cluster on cloud VMs. Once your cluster is deployed, you can use Cloudbreak to scale the cluster.\n\n\nCloudbreak application is built on the foundation of cloud provider APIs and Apache Ambari:\n\n\n\n\n\n\nCloudbreak uses \nApache Ambari\n to provision, manage, and monitor HDP clusters. \n\n\nAmbari \nblueprints\n are a declarative definition of a cluster. With a blueprint, you can specify stack, component layout, and configurations to materialize an HDP cluster instance via Ambari REST API, without having to use the Ambari cluster install wizard. \n\n\n\n\n\n\nCloudbreak uses \ncloud provider APIs\n to create cloud resources required for the HDP clusters. \n\n\nYou can define these resources (networks, security groups, VMs and storage, and so on) in the create a cluster wizard in the Cloudbreak web UI. Resources are only provisioned once you create the cluster.  \n\n\n\n\n\n\nThe use of blueprints is illustrated in the following image:\n\n\n \n\n\nAmbari Blueprints\n\n\nAmbari blueprints\n are a declarative definition of a cluster. With a blueprint, you can specify stack, component layout, and configurations to materialize an HDP cluster instance via Ambari REST API, without having to use the Ambari cluster install wizard. \n\n\nTo learn more about Ambari blueprints, refer to \nApache documentation\n.\n\n\nCloud Infrastructure\n\n\nCloudbreak runs on the \ncloud infrastructure\n within your cloud provider account. Cloud infrastructure consists of the virtual networks, virtual machines, storage, and other resources on which your clusters run. When creating a cluster, you define cloud these infrastructure components that will be provisioned. The exact names for infrastructure elements vary depending on your cloud provider, as presented in the table below:\n\n\n\n\n\n\n\n\nConfiguration\n\n\nDescription\n\n\nAWS\n\n\nAzure\n\n\nGCP\n\n\nOpenStack\n\n\n\n\n\n\n\n\n\n\nNetworks\n\n\nVirtual networks\n provide the networking infrastructure (network, subnet, Internet gateway, and so on) in which your clusters run. You can create new virtual networks or reuse existing virtual networks for your clusters. \nIn addition, \nsecurity groups\n include rules which define inbound traffic allowed to the instances in your cluster.\n\n\nAmazon VPC\n\n\nVirtual network\n\n\nVirtual Private Cloud\n\n\nNetworks\n\n\n\n\n\n\nVMs and Storage\n\n\nYou can select VM types and their attached storage, including storage type, size, count, and encryption settings.\n\n\nAmazon EC2\n\n\nVirtual machines\n\n\nCompute Engine\n\n\nInstances\n\n\n\n\n\n\n\n\n\n    \nNote\n\n    \nThe Cloudbreak software runs in your cloud environment. You are responsible for cloud infrastructure related charges while running Cloudbreak and the clusters being managed by Cloudbreak.\n\n\n\n\n\nCloudbreak Credential\n\n\nCloudbreak credential\n allows Cloudbreak to authenticate with the cloud provider and create resources on your behalf. This is typically done via assigning a specific IAM role to Cloudbreak which allows Cloudbreak to perform certain actions within your cloud provider account.\n\n\nAfter launching Cloudbreak, you must create a Cloudbreak credntial and only after you complete that step you can start creating clusters.",
            "title": "Architecture"
        },
        {
            "location": "/architecture/index.html#architecture",
            "text": "Cloudbreak deployer  installs Cloudbreak components on your AWS VM. Once these components are deployed, you can use  Cloudbreak application  to create, manage, and monitor clusters.",
            "title": "Architecture"
        },
        {
            "location": "/architecture/index.html#cloudbreak-deployer-architecture",
            "text": "Cloudbreak deployer includes the following components:     Component  Description      Cloudbreak Application  Cloudbreak application is built on the foundation of cloud provider APIs and Apache Ambari.    Uluwatu  This is Cloudbreak web UI, which can be used to create, manage, and monitor clusters.    Cloudbreak Shell  This is Cloudbreak's command line tool, which can be used to create, manage, and monitor clusters.    Identity  This is Cloudbreak's OAuth identity server implementation, which utilizes UAA.    Sultans  This is Cloudbreak's user management system.    Periscope  This is Cloudbreak's autoscaling application, which is responsible for automatically increasing or decreasing the capacity of the cluster when your pre-defined conditions are met.      These component names are used in Cloudbreak logs, so for troubleshooting purposes it is useful to know what they refer to.",
            "title": "Cloudbreak Deployer Architecture"
        },
        {
            "location": "/architecture/index.html#cloudbreak-application-architecture",
            "text": "The Cloudbreak application is a web application that communicates with the cloud provider account to create cloud resources on your behalf. Once the cloud resources are in place, Cloudbreak uses Apache Ambari to deploy and configure the cluster on cloud VMs. Once your cluster is deployed, you can use Cloudbreak to scale the cluster.  Cloudbreak application is built on the foundation of cloud provider APIs and Apache Ambari:    Cloudbreak uses  Apache Ambari  to provision, manage, and monitor HDP clusters.   Ambari  blueprints  are a declarative definition of a cluster. With a blueprint, you can specify stack, component layout, and configurations to materialize an HDP cluster instance via Ambari REST API, without having to use the Ambari cluster install wizard.     Cloudbreak uses  cloud provider APIs  to create cloud resources required for the HDP clusters.   You can define these resources (networks, security groups, VMs and storage, and so on) in the create a cluster wizard in the Cloudbreak web UI. Resources are only provisioned once you create the cluster.      The use of blueprints is illustrated in the following image:",
            "title": "Cloudbreak Application Architecture"
        },
        {
            "location": "/architecture/index.html#ambari-blueprints",
            "text": "Ambari blueprints  are a declarative definition of a cluster. With a blueprint, you can specify stack, component layout, and configurations to materialize an HDP cluster instance via Ambari REST API, without having to use the Ambari cluster install wizard.   To learn more about Ambari blueprints, refer to  Apache documentation .",
            "title": "Ambari Blueprints"
        },
        {
            "location": "/architecture/index.html#cloud-infrastructure",
            "text": "Cloudbreak runs on the  cloud infrastructure  within your cloud provider account. Cloud infrastructure consists of the virtual networks, virtual machines, storage, and other resources on which your clusters run. When creating a cluster, you define cloud these infrastructure components that will be provisioned. The exact names for infrastructure elements vary depending on your cloud provider, as presented in the table below:     Configuration  Description  AWS  Azure  GCP  OpenStack      Networks  Virtual networks  provide the networking infrastructure (network, subnet, Internet gateway, and so on) in which your clusters run. You can create new virtual networks or reuse existing virtual networks for your clusters.  In addition,  security groups  include rules which define inbound traffic allowed to the instances in your cluster.  Amazon VPC  Virtual network  Virtual Private Cloud  Networks    VMs and Storage  You can select VM types and their attached storage, including storage type, size, count, and encryption settings.  Amazon EC2  Virtual machines  Compute Engine  Instances     \n     Note \n     The Cloudbreak software runs in your cloud environment. You are responsible for cloud infrastructure related charges while running Cloudbreak and the clusters being managed by Cloudbreak.",
            "title": "Cloud Infrastructure"
        },
        {
            "location": "/architecture/index.html#cloudbreak-credential",
            "text": "Cloudbreak credential  allows Cloudbreak to authenticate with the cloud provider and create resources on your behalf. This is typically done via assigning a specific IAM role to Cloudbreak which allows Cloudbreak to perform certain actions within your cloud provider account.  After launching Cloudbreak, you must create a Cloudbreak credntial and only after you complete that step you can start creating clusters.",
            "title": "Cloudbreak Credential"
        },
        {
            "location": "/aws-launch/index.html",
            "text": "Launching Cloudbreak on AWS\n\n\nBefore launching Cloudbreak on AWS, review and meet the prerequisites. Next, launch a VM using a Cloudbreak Amazon Machine Image (AMI), SSH to the VM, and start Cloudbreak. Once Cloudbreak is running, log in to the Cloudbreak UI and create a Cloudbreak credential. \n\n\nMeet the Prerequisites\n\n\nBefore launching Cloudbreak on AWS, you must meet the following prerequisites.\n\n\nAWS Account\n\n\nIn order to launch Cloudbreak on Azure, you must log in to your AWS account. If you don't have an account, you can create one at \nhttps://aws.amazon.com/\n.\n\n\nAWS Region\n\n\nDecide in which AWS region you would like to launch Cloudbreak. The following AWS regions are supported: \n\n\n\n\n\n\n\n\nRegion Name\n\n\nRegion\n\n\n\n\n\n\n\n\n\n\nEU (Ireland)\n\n\neu-west-1\n\n\n\n\n\n\nEU (Frankfurt)\n\n\neu-central-1\n\n\n\n\n\n\nUS East (N. Virginia)\n\n\nus-east-1\n\n\n\n\n\n\nUS West (N. California)\n\n\nus-west-1\n\n\n\n\n\n\nUS West (Oregon)\n\n\nus-west-2\n\n\n\n\n\n\nSouth America (S\u00e3o Paulo)\n\n\nsa-east-1\n\n\n\n\n\n\nAsia Pacific (Tokyo)\n\n\nap-northeast-1\n\n\n\n\n\n\nAsia Pacific (Singapore)\n\n\nap-southeast-1\n\n\n\n\n\n\nAsia Pacific (Sydney)\n\n\nap-southeast-2\n\n\n\n\n\n\n\n\nClusters created via Cloudbreak can be in the same or different region as Cloudbreak; when you launch a cluster, you select the region in which to launch it.\n\n\nFor detailed information about AWS regions, refer to \nAWS documentation\n. \n\n\nSSH Key Pair\n\n\nImport an existing key pair or generate a new key pair in the AWS region which you are planning to use for launching Cloudbreak and clusters. You can access this option by navigating to the \nEC2 Console\n and selecting \nNETWORK AND SECURITY\n > \nKey Pairs\n from the left pane. Refer to \nAWS documentation\n for detailed instructions on how to create a key pair in a selected region.\n\n\nYou will need this SSH key pair to SSH to the Cloudbreak instance and start Cloudbreak. \n\n\nAuthentication\n\n\nBefore you can start using Cloudbreak for provisioning clusters, you must create IAM roles that  Cloudbreak to connect to your AWS account and create resources on your behalf. There are two ways to do this: \n\n\n\n\n\n\nKey-based\n: This is a simpler option which does not require additional configuration at this point. It requires that you provide your AWS access key and secret key pair in the Cloudbreak web UI later. All you need to do now is check your AWS account and ensure that you can access this key pair.\n\n\n\n\n\n\nRole-based\n: This requires that you or your AWS admin create two IAM roles: one to allow Cloudbreak to assume AWS roles (\"AssumeRole\" policy) and the second one to provide Cloudbreak with the capabilities required for cluster creation (\"cb-policy\" policy).\n\n\n\n\n\n\nOption 1: Key-based Authentication\n\n\nThis option requires your AWS access key and secret key pair. Cloudbreak will use these keys to launch the resources. You must provide the access and secret keys later in the Cloudbreak web UI later when creating a credential. \n\n\nIf you choose this option, all you need to do at this point is check your AWS account and make sure that you can access this key pair. You can generate new access and secret keys from the \nIAM Console\n > \nUsers\n. Next, select a user and click on the \nSecurity credentials\n tab:\n\n\n \n\n\nOption 2: Role-based Authentication\n\n\nThis requires that you create two IAM roles: one to grant Cloudbreak access to allow Cloudbreak to assume AWS roles (using the \"AssumeRole\" policy) and the second one to provide Cloudbreak with the capabilities required for cluster creation (using the \"cb-policy\" policy).\n\n\nThe following table provides contextual information about the two roles required: \n\n\n\n\n\n\n\n\nRole\n\n\nPurpose\n\n\nOverview of Steps\n\n\nConfiguration\n\n\n\n\n\n\n\n\n\n\nCloudbreakRole\n\n\nAllows Cloudbreak to assume other IAM roles - specifically the CredentialRole.\n\n\nCreate a role called \"CloudbreakRole\" and attach the \"AssumeRole\" policy. The \"AssumeRole\" policy definition is provided below.\nAlternatively, you can generate the \"CredentialRole\" role later once your VM is running and once you have \nconfigured AWS CLI\n on your VM by running the \ncbd aws generate-role\n command. This command creates a role with the name \"cbreak-deployer\" (equivalent to the \"CredentialRole\"). To customize the name of the role, add \nexport AWS_ROLE_NAME=my-cloudbreak-role-name\n (where \"my-cloudbreak-role-name\" is your custom role name) as a new line to your Profile.\n\n\nOption 1: When launching your Cloudbreak VM, during \nStep 3: Configure Instance Details\n > \nIAM\n, you will attach the \"CloudbreakRole\" IAM role to the VM.\n Option 2: Alternatively, instead of attaching the \"CloudbreakRole\" role during the VM launch, you can assign the \"CloudbreakRole\" to an IAM user and then add the access and security key of that user to your 'Profile'.\n\n\n\n\n\n\nCredentialRole\n\n\nAllows Cloudbreak to create AWS resources required for clusters.\n\n\nCreate a new IAM role called \"CredentialRole\" and attach the \"cb-policy\" policy to it. The \"cb-policy\" policy definition is provided below.\n When creating this role using the AWS Console, make sure that that it is a role for cross-account access and that the trust-relation is set up as follows: 'Account ID' is your own 12-digit AWS account ID and 'External ID' is \u201cprovision-ambari\u201d.\n\n\nOnce you log in to the Cloudbreak UI and are ready to create clusters, you will use this role to create the Cloudbreak Credential.\n\n\n\n\n\n\n\n\nYou can create these roles in the \nIAM console\n, on the \nRoles\n page via the \nCreate Role\n option. Detailed steps are provided below. \n\n\nCreate CloudbreakRole\n\n\n\n\n\n\nNavigate to the \nIAM console\n > \nRoles\n and click \nCreate Role\n.\n\n\n \n\n\n\n\n\n\nIn the \"Create Role\" wizard, select \nAWS service\n role type and then select any service. \n\n\n \n\n\n\n\n\n\nWhen done, click \nNext: Permissions\n to navigate to the next page in the wizard.\n\n\n\n\n\n\nClick \nCreate policy\n.\n\n\n\n\n\n\n\n\nClick \nSelect\n next to \"Create Your Own Policy\".\n\n\n  \n\n\n\n\n\n\nIn the \nPolicy Name\n field, enter \"AssumeRole\" and in the \nPolicy Document\n paste the policy declaration as provided in the previous step.\n\n\n  \n\n\n\n\n\n\nWhen done, click \nCreate Policy\n.\n\n\n\n\n\n\nClick \nRefresh\n. Next, find the \"AsumeRole\" policy that you just created and select it by checking the box.\n\n\n \n\n\n\n\n\n\nWhen done, click \nNext: Review\n.\n\n\n\n\n\n\nIn the \nRoles name\n field, enter \"CloudbreakRole\". \n\n\n \n\n\n\n\n\n\nWhen done, click \nCreate role\n to finish the role creation process.\n\n\n\n\n\n\nCreate CredentialRole\n\n\n\n\n\n\nNavigate to the \nIAM console\n > \nRoles\n and click \nCreate Role\n.\n\n\n \n\n\n\n\n\n\nIn the \"Create Role\" wizard, select \nAnother AWS account\n role type. Next, provide the following:\n\n\n\n\nIn the \nAccount ID\n field, enter your AWS account ID.\n\n\nUnder \nOptions\n, check \nRequire external ID\n.\n\n\nIn the \nExternal ID\n, enter \"provision-ambari\".\n\n\n\n\n \n\n\n\n\n\n\nWhen done, click \nNext: Permissions\n to navigate to the next page in the wizard.\n\n\n\n\n\n\nClick \nCreate policy\n.\n\n\n\n\n\n\n\n\nClick \nSelect\n next to \"Create Your Own Policy\".\n\n\n \n\n\n\n\n\n\nCopy the \"cb-policy\" policy definition (see below).   \n\n\n\n\n\n\nIn the \nPolicy Name\n field, enter \"cb-policy\" and in the \nPolicy Document\n paste the policy declaration as provided below.\n\n\n  \n\n\n\n\n\n\nWhen done, click \nCreate Policy\n.\n\n\n\n\n\n\nClick \nRefresh\n. Next, find the \"cb-policy\" that you just created and select it by checking the box.\n\n\n \n\n\n\n\n\n\nWhen done, click \nNext: Review\n.\n\n\n\n\n\n\nIn the \nRoles name\n field, enter \"CredentialRole\". \n\n\n \n\n\n\n\n\n\nWhen done, click \nCreate role\n to finish the role creation process.\n\n\n\n\n\n\nPolicy Definitions\n\n\nThe \"AssumeRole\" policy definition: \n\n\n{\n\u2002\u2002\"Version\": \"2012-10-17\",\n\u2002\u2002\"Statement\": [\n\u2002\u2002\u2002\u2002{\n\u2002\u2002\u2002\u2002\u2002\u2002\"Sid\": \"Stmt1400068149000\",\n\u2002\u2002\u2002\u2002\u2002\u2002\"Effect\": \"Allow\",\n\u2002\u2002\u2002\u2002\u2002\u2002\"Action\": [\n\u2002\u2002\u2002\u2002\u2002\u2002\u2002\u2002\"sts:AssumeRole\"\n\u2002\u2002\u2002\u2002\u2002\u2002],\n\u2002\u2002\u2002\u2002\u2002\u2002\"Resource\": [\n\u2002\u2002\u2002\u2002\u2002\u2002\u2002\u2002\"*\"\n\u2002\u2002\u2002\u2002\u2002\u2002]\n\u2002\u2002\u2002\u2002}\n\u2002\u2002]\n}\n\n\n\nThe \"cb-policy\" policy definition: \n\n\n{\n  \"Version\": \"2012-10-17\",\n  \"Statement\": [\n    {\n      \"Effect\": \"Allow\",\n      \"Action\": [ \"cloudformation:*\" ],\n      \"Resource\": [ \"*\" ]\n    },\n    {\n      \"Effect\": \"Allow\",\n      \"Action\": [ \"ec2:*\" ],\n      \"Resource\": [ \"*\" ]\n    },\n    {\n      \"Effect\": \"Allow\",\n      \"Action\": [ \"iam:PassRole\" ],\n      \"Resource\": [ \"*\" ]\n    },\n    {\n      \"Effect\": \"Allow\",\n      \"Action\": [ \"autoscaling:*\" ],\n      \"Resource\": [ \"*\" ]\n    }\n  ]\n}\n\n\n\nFor more information about IAM, refer to \nUsing Instance Profiles\n and \nUsing an IAM Role to Grant Permissions to Applications Running on Amazon EC2 Instances\n.    \n\n\nLaunch the VM\n\n\nNow that you've met the prerequisites, you can launch the Cloudbreak deployer VM available as a Community AMI.\n\n\n\n\n\n\nIn the AWS Management Console, navigate to the EC2 Console.  \n\n\n\n\n\n\nIn the top right corner, select the region in which you want to launch Cloudbreak.  \n\n\n \n\n\n\n\n\n\nFrom the left pane, select \nINSTANCES\n > \nInstances\n.  \n\n\n\n\n\n\nClick on \nLaunch Instance\n.\n\n\n\n\n\n\nIn \nStep 1: Choose an Amazon Machine Image (AMI)\n page, from the left pane, select \nCommunity AMIs\n. \n\n\n \n\n\n\n\n\n\nIn the search box, enter the image name. The following Cloudbreak deplouer images are available:\n\n\n\n\n\n\n\n\nRegion\n\n\nImage Name\n\n\n\n\n\n\n\n\n\n\neu-west-1\n\n\nami-79c63f00\n\n\n\n\n\n\nsa-east-1\n\n\nami-1af18176\n\n\n\n\n\n\nus-east-1\n\n\nami-e9bdbc92\n\n\n\n\n\n\nus-west-1\n\n\nami-2dd1e44d\n\n\n\n\n\n\nus-west-2\n\n\nami-ae6c85d6\n\n\n\n\n\n\neu-central-1\n\n\nami-bc18b3d3\n\n\n\n\n\n\nap-northeast-1\n\n\nami-f2fb0494\n\n\n\n\n\n\nap-southeast-1\n\n\nami-47036324\n\n\n\n\n\n\nap-southeast-2\n\n\nami-370b1154\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTO-DO: This table should be automatically generated.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nClick \nSelect\n.  \n\n\n\n\nThe steps listed below only mention required parameters. You may optionally review and adjust additional parameters. \n\n\n\n\n\n\n\n\nIn \nStep2: Choose Instance Type\n, choose an instance type. The minimum instance type which is suitable for Cloudbreak is \nm3.large\n. Minimum requirements are 8GB RAM, 10GB disk, 2 cores. Click \nNext\n.\n\n\n   \n\n\n\n\n\n\n(Perform this step only if you are using role-based authorization) In \nStep 3: Configure Instance Details\n > \nIAM\n, select the \"CloudbreakRole\" IAM role which you \ncreated earlier\n.\n\n\n\n\n\n\nIn \nStep 6: Configure Security Group\n, open the following ports: 22 (for access via SSH) and 443 (for access via HTTPS). Click \nReview and Launch\n.\n\n\n \n\n\n\n\n\n\nIn \nStep 7: Review Instance Launch\n, review the information carefully and then click \nLaunch\n. \n\n\n\n\n\n\nWhen prompted select an existing key pair or create a new one. Next, acknowledge that you have access to the private key file and click \nLaunch Instance\n. \n\n\n  \n\n\n\n\n\n\nClick on the instance ID to navigate to the \nInstances\n view in your EC2 console. \n\n\n  \n\n\n\n\n\n\nSSH to the VM\n\n\nNow that your VM is ready, access it via SSH: \n\n\n\n\nUse the private key from the key pair that you selected when launching the instance. \n\n\nThe SSH user is called \"cloudbreak\".\n\n\nYou can obtain the host IP from the EC2 console > \nInstances\n view by selecting the instance, selecting the \nDescription\n tab, and copying the value of the \nPublic DNS (IPv4)\n\n or \nIPv4 Public IP\n parameter.\n\n\n\n\nLaunch Cloudbreak Deployer\n\n\nAfter accessing the VM via SSH: \n\n\n\n\n\n\nNavigate to the cloudbreak-deployment directory:\n\n\ncd /var/lib/cloudbreak-deployment/\n\n\nThis directory contains configuration files and the supporting binaries for Cloudbreak deployer.\n\n\n\n\n\n\nInitialize your profile by creating a new file called \nProfile\n and adding the following content:\n\n\nexport UAA_DEFAULT_SECRET=MY-SECRET\nexport UAA_DEFAULT_USER_PW=MY-PASSWORD\n  \n\n\nFor example: \n\n\nexport UAA_DEFAULT_SECRET=MySecret123\nexport UAA_DEFAULT_USER_PW=MySecurePassword123\n \n\n\n\n\nYou will need to provide the password when logging in to the Cloudbreak web UI and when using the Cloudbreak Shell. The secret will be used by Cloudbreak for authentication.  \n\n\n\n\n\n\n\n\nStart the Cloudbreak application by using the following command:\n\n\ncbd start\n\n\nThis will start the Docker containers and initialize the application. The first time you start the Coudbreak app, the process will take longer than usual due to the download of all the necessary docker images.\n\n\nThe \ncbd start\n command includes the \ncbd generate\n command which applies the following steps:\n\n\n\n\nCreates the \ndocker-compose.yml\n file, which describes the configuration of all the Docker containers needed for the Cloudbreak deployment.\n\n\nCreates the \nuaa.yml\n file, which holds the configuration of the identity server used to authenticate users with Cloudbreak.\n\n\n\n\n\n\nOnce the \ncbd start\n has finished, it returns the \"Uluwatu (Cloudbreak UI) url\" which you can later paste in your browser and log in to Cloudbreak web UI. \n\n\n\n\n\n\n\n\nCheck Cloudbreak deployer version and health: \n\n\ncbd doctor\n\n\n\n\n\n\nNext, check Cloudbreak Application logs: \n\n\ncbd logs cloudbreak\n\n\nYou should see a message like this in the log: \nStarted CloudbreakApplication in 36.823 seconds.\n Cloudbreak normally takes less than a minute to start.\n\n\n\n\n\n\nAccess Cloudbreak UI\n\n\n\n\n\n\nYou can log into the Cloudbreak application at \nhttps://IPv4_Public_IP>/\n or \nhttps://Public_DNS\n. For example \nhttps://34.212.141.253\n or \nhttps://ec2-34-212-141-253.us-west-2.compute.amazonaws.com\n. \n\n\n\n\n\n\nConfirm the security exception to proceed to the Cloudbreak web UI.\n\n\nThe first time you access Cloudbreak UI, Cloudbreak will automatically generate a self-signed certificate, due to which your browser will warn you about an untrusted connection and will ask you to confirm a security exception.\n\n\n\n\n\n\nLog in to the Cloudbreak web UI: \n\n\n\n\nThe default username is \nadmin@example.com\n but you should sign up with your own email address.\n\n\nThe password is the value of the \nUAA_DEFAULT_USER_PW\n variable that you configured in your \nProfile\n file when \nlaunching Cloudbreak deployer\n.\n\n\n\n\n  \n\n\n\n\n\n\nCreate Cloudbreak Credential\n\n\nAs part of the prerequisites, you had two options to \nauthorize Cloudbreak\n to create resources on your behalf: key-based or role-based authorization. \n\n\nDepending on your earlier choice, you must configure a key-based or role-based credential. Without this credential, you will not be able to create clusters via Cloudbreak. \n\n\nCreate Key-Based Credential\n\n\nTo perform these steps, you must know your access and secret key as well as your public SSH key. If needed, you can generate new access and secret keys from the \nIAM Console\n > \nUsers\n. Next, select a user and click on the \nSecurity credentials\n tab. \n\n\n  \n\n\n\n\n\n\nIn the Cloudbreak web UI, open the \nmanage credentials\n pane. \n\n\n\n\n\n\nClick \n+create credential\n. \n\n\n\n\n\n\nProvide the following information:\n\n\n\n\n\n\n\n\nParameter\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nSelect Credential Type\n\n\nSelect \nKey Based\n.\n\n\n\n\n\n\nName\n\n\nEnter a name for your credential.\n\n\n\n\n\n\nDescription\n\n\n(Optional) Enter a description.\n\n\n\n\n\n\nAccess Key\n\n\nPaste your access key.\n\n\n\n\n\n\nSecret Access Key\n\n\nPaste your secret key.\n\n\n\n\n\n\nSSH Public Key\n\n\nPaste your SSH public key.\n\n\n\n\n\n\nSelect Platform\n\n\n(Optional) Select a platform (if previously configured).\n\n\n\n\n\n\nPublic In Account\n\n\n(Optional) If you check this, other users added to your Cloudbreak instance will be able to use this credential to create clusters.\n\n\n\n\n\n\n\n\n\n\n\n\nClick \n+create credential\n.\n\n\n\n\n\n\nYour credential should now be displayed in the \nmanage credentials\n tab.\n\n\nCongratulations! You've successfully launched and configured Cloudbreak. Now you can use Cloudbreak to \ncreate clusters\n. \n\n\n\n\n\n\nCreate Role-Based Credential\n\n\nTo perform these steps, you must know the \nIAM Role ARN\n corresponding to the \"CredentialRole\" (configured as a \nprerequisite\n). You must also have your public SSH key. \n\n\n  \n\n\n\n\n\n\nIn the Cloudbreak web UI, open the \nmanage credentials\n pane. \n\n\n\n\n\n\nClick \n+create credential\n. \n\n\n\n\n\n\nProvide the following information:\n\n\n\n\n\n\n\n\nParameter\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nSelect Credential Type\n\n\nSelect \nRole Based\n.\n\n\n\n\n\n\nName\n\n\nEnter a name for your credential.\n\n\n\n\n\n\nDescription\n\n\n(Optional) Enter a description.\n\n\n\n\n\n\nIAM Role ARN\n\n\nPaste the \nIAM Role ARN\n corresponding to the \"CredentialRole\" that you created earlier. For example \narn:aws:iam::315627065446:role/CredentialRole\n\n\n\n\n\n\nSSH Public Key\n\n\nPaste your SSH public key.\n\n\n\n\n\n\nSelect Platform\n\n\n(Optional) Select a platform (if previously configured).\n\n\n\n\n\n\nPublic In Account\n\n\n(Optional) If you check this, other users added to your Cloudbreak instance will be able to use this credential to create clusters.\n\n\n\n\n\n\n\n\n\n\n\n\nClick \n+create credential\n.\n\n\n\n\n\n\nYour credential should now be displayed in the \nmanage credentials\n tab.\n\n\nCongratulations! You've successfully launched and configured Cloudbreak.\n\n\n\n\n\n\n\n\nNext: Create a Cluster",
            "title": "Launch on AWS"
        },
        {
            "location": "/aws-launch/index.html#launching-cloudbreak-on-aws",
            "text": "Before launching Cloudbreak on AWS, review and meet the prerequisites. Next, launch a VM using a Cloudbreak Amazon Machine Image (AMI), SSH to the VM, and start Cloudbreak. Once Cloudbreak is running, log in to the Cloudbreak UI and create a Cloudbreak credential.",
            "title": "Launching Cloudbreak on AWS"
        },
        {
            "location": "/aws-launch/index.html#meet-the-prerequisites",
            "text": "Before launching Cloudbreak on AWS, you must meet the following prerequisites.",
            "title": "Meet the Prerequisites"
        },
        {
            "location": "/aws-launch/index.html#aws-account",
            "text": "In order to launch Cloudbreak on Azure, you must log in to your AWS account. If you don't have an account, you can create one at  https://aws.amazon.com/ .",
            "title": "AWS Account"
        },
        {
            "location": "/aws-launch/index.html#aws-region",
            "text": "Decide in which AWS region you would like to launch Cloudbreak. The following AWS regions are supported:      Region Name  Region      EU (Ireland)  eu-west-1    EU (Frankfurt)  eu-central-1    US East (N. Virginia)  us-east-1    US West (N. California)  us-west-1    US West (Oregon)  us-west-2    South America (S\u00e3o Paulo)  sa-east-1    Asia Pacific (Tokyo)  ap-northeast-1    Asia Pacific (Singapore)  ap-southeast-1    Asia Pacific (Sydney)  ap-southeast-2     Clusters created via Cloudbreak can be in the same or different region as Cloudbreak; when you launch a cluster, you select the region in which to launch it.  For detailed information about AWS regions, refer to  AWS documentation .",
            "title": "AWS Region"
        },
        {
            "location": "/aws-launch/index.html#ssh-key-pair",
            "text": "Import an existing key pair or generate a new key pair in the AWS region which you are planning to use for launching Cloudbreak and clusters. You can access this option by navigating to the  EC2 Console  and selecting  NETWORK AND SECURITY  >  Key Pairs  from the left pane. Refer to  AWS documentation  for detailed instructions on how to create a key pair in a selected region.  You will need this SSH key pair to SSH to the Cloudbreak instance and start Cloudbreak.",
            "title": "SSH Key Pair"
        },
        {
            "location": "/aws-launch/index.html#authentication",
            "text": "Before you can start using Cloudbreak for provisioning clusters, you must create IAM roles that  Cloudbreak to connect to your AWS account and create resources on your behalf. There are two ways to do this:     Key-based : This is a simpler option which does not require additional configuration at this point. It requires that you provide your AWS access key and secret key pair in the Cloudbreak web UI later. All you need to do now is check your AWS account and ensure that you can access this key pair.    Role-based : This requires that you or your AWS admin create two IAM roles: one to allow Cloudbreak to assume AWS roles (\"AssumeRole\" policy) and the second one to provide Cloudbreak with the capabilities required for cluster creation (\"cb-policy\" policy).",
            "title": "Authentication"
        },
        {
            "location": "/aws-launch/index.html#option-1-key-based-authentication",
            "text": "This option requires your AWS access key and secret key pair. Cloudbreak will use these keys to launch the resources. You must provide the access and secret keys later in the Cloudbreak web UI later when creating a credential.   If you choose this option, all you need to do at this point is check your AWS account and make sure that you can access this key pair. You can generate new access and secret keys from the  IAM Console  >  Users . Next, select a user and click on the  Security credentials  tab:",
            "title": "Option 1: Key-based Authentication"
        },
        {
            "location": "/aws-launch/index.html#option-2-role-based-authentication",
            "text": "This requires that you create two IAM roles: one to grant Cloudbreak access to allow Cloudbreak to assume AWS roles (using the \"AssumeRole\" policy) and the second one to provide Cloudbreak with the capabilities required for cluster creation (using the \"cb-policy\" policy).  The following table provides contextual information about the two roles required:      Role  Purpose  Overview of Steps  Configuration      CloudbreakRole  Allows Cloudbreak to assume other IAM roles - specifically the CredentialRole.  Create a role called \"CloudbreakRole\" and attach the \"AssumeRole\" policy. The \"AssumeRole\" policy definition is provided below. Alternatively, you can generate the \"CredentialRole\" role later once your VM is running and once you have  configured AWS CLI  on your VM by running the  cbd aws generate-role  command. This command creates a role with the name \"cbreak-deployer\" (equivalent to the \"CredentialRole\"). To customize the name of the role, add  export AWS_ROLE_NAME=my-cloudbreak-role-name  (where \"my-cloudbreak-role-name\" is your custom role name) as a new line to your Profile.  Option 1: When launching your Cloudbreak VM, during  Step 3: Configure Instance Details  >  IAM , you will attach the \"CloudbreakRole\" IAM role to the VM.  Option 2: Alternatively, instead of attaching the \"CloudbreakRole\" role during the VM launch, you can assign the \"CloudbreakRole\" to an IAM user and then add the access and security key of that user to your 'Profile'.    CredentialRole  Allows Cloudbreak to create AWS resources required for clusters.  Create a new IAM role called \"CredentialRole\" and attach the \"cb-policy\" policy to it. The \"cb-policy\" policy definition is provided below.  When creating this role using the AWS Console, make sure that that it is a role for cross-account access and that the trust-relation is set up as follows: 'Account ID' is your own 12-digit AWS account ID and 'External ID' is \u201cprovision-ambari\u201d.  Once you log in to the Cloudbreak UI and are ready to create clusters, you will use this role to create the Cloudbreak Credential.     You can create these roles in the  IAM console , on the  Roles  page via the  Create Role  option. Detailed steps are provided below.   Create CloudbreakRole    Navigate to the  IAM console  >  Roles  and click  Create Role .       In the \"Create Role\" wizard, select  AWS service  role type and then select any service.        When done, click  Next: Permissions  to navigate to the next page in the wizard.    Click  Create policy .     Click  Select  next to \"Create Your Own Policy\".        In the  Policy Name  field, enter \"AssumeRole\" and in the  Policy Document  paste the policy declaration as provided in the previous step.        When done, click  Create Policy .    Click  Refresh . Next, find the \"AsumeRole\" policy that you just created and select it by checking the box.       When done, click  Next: Review .    In the  Roles name  field, enter \"CloudbreakRole\".        When done, click  Create role  to finish the role creation process.    Create CredentialRole    Navigate to the  IAM console  >  Roles  and click  Create Role .       In the \"Create Role\" wizard, select  Another AWS account  role type. Next, provide the following:   In the  Account ID  field, enter your AWS account ID.  Under  Options , check  Require external ID .  In the  External ID , enter \"provision-ambari\".        When done, click  Next: Permissions  to navigate to the next page in the wizard.    Click  Create policy .     Click  Select  next to \"Create Your Own Policy\".       Copy the \"cb-policy\" policy definition (see below).       In the  Policy Name  field, enter \"cb-policy\" and in the  Policy Document  paste the policy declaration as provided below.        When done, click  Create Policy .    Click  Refresh . Next, find the \"cb-policy\" that you just created and select it by checking the box.       When done, click  Next: Review .    In the  Roles name  field, enter \"CredentialRole\".        When done, click  Create role  to finish the role creation process.    Policy Definitions  The \"AssumeRole\" policy definition:   {\n\u2002\u2002\"Version\": \"2012-10-17\",\n\u2002\u2002\"Statement\": [\n\u2002\u2002\u2002\u2002{\n\u2002\u2002\u2002\u2002\u2002\u2002\"Sid\": \"Stmt1400068149000\",\n\u2002\u2002\u2002\u2002\u2002\u2002\"Effect\": \"Allow\",\n\u2002\u2002\u2002\u2002\u2002\u2002\"Action\": [\n\u2002\u2002\u2002\u2002\u2002\u2002\u2002\u2002\"sts:AssumeRole\"\n\u2002\u2002\u2002\u2002\u2002\u2002],\n\u2002\u2002\u2002\u2002\u2002\u2002\"Resource\": [\n\u2002\u2002\u2002\u2002\u2002\u2002\u2002\u2002\"*\"\n\u2002\u2002\u2002\u2002\u2002\u2002]\n\u2002\u2002\u2002\u2002}\n\u2002\u2002]\n}  The \"cb-policy\" policy definition:   {\n  \"Version\": \"2012-10-17\",\n  \"Statement\": [\n    {\n      \"Effect\": \"Allow\",\n      \"Action\": [ \"cloudformation:*\" ],\n      \"Resource\": [ \"*\" ]\n    },\n    {\n      \"Effect\": \"Allow\",\n      \"Action\": [ \"ec2:*\" ],\n      \"Resource\": [ \"*\" ]\n    },\n    {\n      \"Effect\": \"Allow\",\n      \"Action\": [ \"iam:PassRole\" ],\n      \"Resource\": [ \"*\" ]\n    },\n    {\n      \"Effect\": \"Allow\",\n      \"Action\": [ \"autoscaling:*\" ],\n      \"Resource\": [ \"*\" ]\n    }\n  ]\n}  For more information about IAM, refer to  Using Instance Profiles  and  Using an IAM Role to Grant Permissions to Applications Running on Amazon EC2 Instances .",
            "title": "Option 2: Role-based Authentication"
        },
        {
            "location": "/aws-launch/index.html#launch-the-vm",
            "text": "Now that you've met the prerequisites, you can launch the Cloudbreak deployer VM available as a Community AMI.    In the AWS Management Console, navigate to the EC2 Console.      In the top right corner, select the region in which you want to launch Cloudbreak.         From the left pane, select  INSTANCES  >  Instances .      Click on  Launch Instance .    In  Step 1: Choose an Amazon Machine Image (AMI)  page, from the left pane, select  Community AMIs .        In the search box, enter the image name. The following Cloudbreak deplouer images are available:     Region  Image Name      eu-west-1  ami-79c63f00    sa-east-1  ami-1af18176    us-east-1  ami-e9bdbc92    us-west-1  ami-2dd1e44d    us-west-2  ami-ae6c85d6    eu-central-1  ami-bc18b3d3    ap-northeast-1  ami-f2fb0494    ap-southeast-1  ami-47036324    ap-southeast-2  ami-370b1154         TO-DO: This table should be automatically generated.        Click  Select .     The steps listed below only mention required parameters. You may optionally review and adjust additional parameters.      In  Step2: Choose Instance Type , choose an instance type. The minimum instance type which is suitable for Cloudbreak is  m3.large . Minimum requirements are 8GB RAM, 10GB disk, 2 cores. Click  Next .         (Perform this step only if you are using role-based authorization) In  Step 3: Configure Instance Details  >  IAM , select the \"CloudbreakRole\" IAM role which you  created earlier .    In  Step 6: Configure Security Group , open the following ports: 22 (for access via SSH) and 443 (for access via HTTPS). Click  Review and Launch .       In  Step 7: Review Instance Launch , review the information carefully and then click  Launch .     When prompted select an existing key pair or create a new one. Next, acknowledge that you have access to the private key file and click  Launch Instance .         Click on the instance ID to navigate to the  Instances  view in your EC2 console.",
            "title": "Launch the VM"
        },
        {
            "location": "/aws-launch/index.html#ssh-to-the-vm",
            "text": "Now that your VM is ready, access it via SSH:    Use the private key from the key pair that you selected when launching the instance.   The SSH user is called \"cloudbreak\".  You can obtain the host IP from the EC2 console >  Instances  view by selecting the instance, selecting the  Description  tab, and copying the value of the  Public DNS (IPv4)  or  IPv4 Public IP  parameter.",
            "title": "SSH to the VM"
        },
        {
            "location": "/aws-launch/index.html#launch-cloudbreak-deployer",
            "text": "After accessing the VM via SSH:     Navigate to the cloudbreak-deployment directory:  cd /var/lib/cloudbreak-deployment/  This directory contains configuration files and the supporting binaries for Cloudbreak deployer.    Initialize your profile by creating a new file called  Profile  and adding the following content:  export UAA_DEFAULT_SECRET=MY-SECRET\nexport UAA_DEFAULT_USER_PW=MY-PASSWORD     For example:   export UAA_DEFAULT_SECRET=MySecret123\nexport UAA_DEFAULT_USER_PW=MySecurePassword123     You will need to provide the password when logging in to the Cloudbreak web UI and when using the Cloudbreak Shell. The secret will be used by Cloudbreak for authentication.       Start the Cloudbreak application by using the following command:  cbd start  This will start the Docker containers and initialize the application. The first time you start the Coudbreak app, the process will take longer than usual due to the download of all the necessary docker images.  The  cbd start  command includes the  cbd generate  command which applies the following steps:   Creates the  docker-compose.yml  file, which describes the configuration of all the Docker containers needed for the Cloudbreak deployment.  Creates the  uaa.yml  file, which holds the configuration of the identity server used to authenticate users with Cloudbreak.    Once the  cbd start  has finished, it returns the \"Uluwatu (Cloudbreak UI) url\" which you can later paste in your browser and log in to Cloudbreak web UI.      Check Cloudbreak deployer version and health:   cbd doctor    Next, check Cloudbreak Application logs:   cbd logs cloudbreak  You should see a message like this in the log:  Started CloudbreakApplication in 36.823 seconds.  Cloudbreak normally takes less than a minute to start.",
            "title": "Launch Cloudbreak Deployer"
        },
        {
            "location": "/aws-launch/index.html#access-cloudbreak-ui",
            "text": "You can log into the Cloudbreak application at  https://IPv4_Public_IP>/  or  https://Public_DNS . For example  https://34.212.141.253  or  https://ec2-34-212-141-253.us-west-2.compute.amazonaws.com .     Confirm the security exception to proceed to the Cloudbreak web UI.  The first time you access Cloudbreak UI, Cloudbreak will automatically generate a self-signed certificate, due to which your browser will warn you about an untrusted connection and will ask you to confirm a security exception.    Log in to the Cloudbreak web UI:    The default username is  admin@example.com  but you should sign up with your own email address.  The password is the value of the  UAA_DEFAULT_USER_PW  variable that you configured in your  Profile  file when  launching Cloudbreak deployer .",
            "title": "Access Cloudbreak UI"
        },
        {
            "location": "/aws-launch/index.html#create-cloudbreak-credential",
            "text": "As part of the prerequisites, you had two options to  authorize Cloudbreak  to create resources on your behalf: key-based or role-based authorization.   Depending on your earlier choice, you must configure a key-based or role-based credential. Without this credential, you will not be able to create clusters via Cloudbreak.",
            "title": "Create Cloudbreak Credential"
        },
        {
            "location": "/aws-launch/index.html#create-key-based-credential",
            "text": "To perform these steps, you must know your access and secret key as well as your public SSH key. If needed, you can generate new access and secret keys from the  IAM Console  >  Users . Next, select a user and click on the  Security credentials  tab.         In the Cloudbreak web UI, open the  manage credentials  pane.     Click  +create credential .     Provide the following information:     Parameter  Description      Select Credential Type  Select  Key Based .    Name  Enter a name for your credential.    Description  (Optional) Enter a description.    Access Key  Paste your access key.    Secret Access Key  Paste your secret key.    SSH Public Key  Paste your SSH public key.    Select Platform  (Optional) Select a platform (if previously configured).    Public In Account  (Optional) If you check this, other users added to your Cloudbreak instance will be able to use this credential to create clusters.       Click  +create credential .    Your credential should now be displayed in the  manage credentials  tab.  Congratulations! You've successfully launched and configured Cloudbreak. Now you can use Cloudbreak to  create clusters .",
            "title": "Create Key-Based Credential"
        },
        {
            "location": "/aws-launch/index.html#create-role-based-credential",
            "text": "To perform these steps, you must know the  IAM Role ARN  corresponding to the \"CredentialRole\" (configured as a  prerequisite ). You must also have your public SSH key.         In the Cloudbreak web UI, open the  manage credentials  pane.     Click  +create credential .     Provide the following information:     Parameter  Description      Select Credential Type  Select  Role Based .    Name  Enter a name for your credential.    Description  (Optional) Enter a description.    IAM Role ARN  Paste the  IAM Role ARN  corresponding to the \"CredentialRole\" that you created earlier. For example  arn:aws:iam::315627065446:role/CredentialRole    SSH Public Key  Paste your SSH public key.    Select Platform  (Optional) Select a platform (if previously configured).    Public In Account  (Optional) If you check this, other users added to your Cloudbreak instance will be able to use this credential to create clusters.       Click  +create credential .    Your credential should now be displayed in the  manage credentials  tab.  Congratulations! You've successfully launched and configured Cloudbreak.     Next: Create a Cluster",
            "title": "Create Role-Based Credential"
        },
        {
            "location": "/aws-create/index.html",
            "text": "Create a Cluster on AWS\n\n\nTo create a cluster via CLoudbreak UI:\n\n\n\n\n\n\nLog in to the Cloudbreak UI.\n\n\n\n\n\n\nIn the top right corner, select the credential that you want to use to create a cluster:\n\n\n  \n\n\n\n\n\n\nClick \n+create cluster\n and the \nCreate cluster\n form is displayed.\n\n\n\n\n\n\nOn the \nConfigure Cluster\n page, provide the following parameters:\n\n\n\n\nTo view advanced options, click \nShow Advanced Options\n. To learn about advanced options, refer to \nAdvanced Options\n.\n\n\n\n\n\n\n\n\n\n\nParameter\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nCluster Name\n\n\nEnter a name for your cluster. The name must be between 5 and 40 characters, must start with a letter, must only include lowercase letters, numbers, and hyphens.\n\n\n\n\n\n\nTags\n\n\n(Optional) You can optionally add tags, which will help you find your cluster-related resources, such as VMs, in your cloud provider account.\n\n\n\n\n\n\nRegion\n\n\nSelect the region in which you would like to launch your cluster.\n\n\n\n\n\n\nSend Email When Cluster is Ready\n\n\n(Optional) Check this to receive an email each time the cluster status changes.\n\n\n\n\n\n\nPublic In Account\n\n\n(Optional) If this option is checked, all the users belonging to your account will be able to use this cluster, but they cannot delete it.\n\n\n\n\n\n\n\n\n\n\nBy default, Ambari Username and Ambari Password are set to \nadmin\n. You can override it in the \"\nConfigure Cluster\n\" tab.\n\n\n\n\n\n\n\n\nOn the \nSet up Network and Security\n page, provide the following parameters:\n\n\n\n\n\n\n\n\nParameter\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nNetwork\n\n\nSelect the virtual network in which you would like your cluster to be provisioned. You can define custom network configurations or use default network configurations.\n\n\n\n\n\n\nEnable Knox Gateway\n\n\n(Optional) Select this option to enable secure access to Ambari web UI and other cluster UIs via Knox gateway.\n\n\n\n\n\n\nEnable Kerberos Security\n\n\n(Optional) Select this option to enable Kerberos for your cluster. You will have an option to create a new kerberos or use an existing one. For more information refer to Kerberos \ndocumentation\n.\n\n\n\n\n\n\n\n\n\n\n\n\nOn the \nChoose Blueprint\n page, select the blueprint that you would like to use for your cluster. You can either choose one of the pre-configured blueprints, or add your own in the \nmanage blueprints\n tab.\n\n\nFor each host group you must provide the following:\n\n\n\n\n\n\n\n\nParameter\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nGroup Size\n\n\nEnter a number defining how many nodes to create per host group. Default is 1. The \"Group Size\" for that host group on which Ambari Server is installed must be set to \"1\".\n\n\n\n\n\n\nTemplate\n\n\nIf you have previously created a template for VMs and storage, you can select it here. If you don't make a selection, default will be used.\n\n\n\n\n\n\nSecurity Group\n\n\nIf you have previously created a template for a security group, you can select it here. If you don't make a selection, default will be used.\n\n\n\n\n\n\nAmbari Server\n\n\nYou must select one node for Ambari Server. The \"Group Size\" for that host group must be set to \"1\".\n\n\n\n\n\n\nRecipes\n\n\nYou can select a previously added recipe (custom script) to be executed on all nodes of the host group. Refer to \nRecipes\n.\n\n\n\n\n\n\n\n\n\n\n\n\nClick on \nReview and Launch\n and then \n+create and start cluster\n.\n\n\n\n\n\n\nYou will be redirected to the Cloudbreak dashboard, and a new tile representing your cluster will appear at the top of the page.\n\n\n\n\n\n\nAdvanced Options\n\n\nClick on \nShow Advanced Options\n to enter additional configuration options.\n\n\nConfigure Cluster\n\n\nYou can optionally configure the following advanced parameters:\n\n\n\n\n\n\n\n\nParameter\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nAmbari Username\n\n\nYou can log in to the Ambari UI using this username. By default, this is set to \nadmin\n.\n\n\n\n\n\n\nAmbari Password\n\n\nYou can log in to the Ambari UI using this password. By default, this is set to \nadmin\n.\n\n\n\n\n\n\nProvision Cluster\n\n\nSALT\n is pre-selected to provision your cluster.\n\n\n\n\n\n\nUse dedicated instances\n\n\nSelect this to use dedicated instances (i.e. EC2 instances that run in a virtual private cloud (VPC) on hardware that's dedicated to a single customer). For more information about dedicated instances, refer to \nAWS documentation\n.\n\n\n\n\n\n\nEnable Lifetime Management\n\n\nCheck this option if you would like your cluster to be automatically terminated after a specific amount of time (defined as \"Time to Live\" in minuter) has passed.\n\n\n\n\n\n\nFlex Subscription\n\n\nThis option will appear if you have configured your deployment for a \nFlex Subscription\n.\n\n\n\n\n\n\n\n\nChoose Blueprint\n\n\nAfter selecting a blueprint, you can optionally configure the following advanced parameters:\n\n\n\n\n\n\n\n\nParameter\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nConfig Recommendation Strategy (Stack Advisor)\n\n\nSelect how configuration recommendations generated by stack advisor will be applied. Select one of \nALWAYS_APPLY: Configuration recommendations will be applied automatically.\nALWAYS_APPLY_DONT_OVERRIDE_CUSTOM_VALUES: Configuration recommendations will be applied automatically, but they will be ignored for custom configurations.\nNEVER_APPLY: Configuration recommendations will be ignored.\nONLY_STACK_DEFAULTS_APPLY: Configuration recommendations will be applied only on the default configurations for all included services.\n\n\n\n\n\n\nValidate Blueprint\n\n\nSelect to validate the blueprint.\n\n\n\n\n\n\nInstance Profile\n\n\nThis option allows you to optionally create or reuse an existing instance profile for your cluster VMs. An instance profile is a container for an IAM role that you can use to pass role information to an EC2 instance when the instance starts. You can use this option to allow access to AWS resources such as Amazon S3 object storage. The following choices are available:\nDisable Instance Profile attaching by default\nCreate Instance Profile and attach to the instance\nDefine existing Instance Profile and attach to the instances\n For more information about instance profile, refer to \nAWS documentation\n.\n\n\n\n\n\n\n\n\nChoose Failure Action\n\n\nYou can optionally select what to do if cluster creation fails or if there aren't enough instances available to create all requested nodes:\n\n\n\n\n\n\n\n\nParameter\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nFailure Action\n\n\nSelect one of: \ndo NOT rollback resources\n (default) or \nrollback resources\n. \nBy default, if creating a cluster fails, the Azure resources that were created up to that point will not be rolled back. This means that they will remain accessible for troubleshooting and you will need to to delete them manually.\n\n\n\n\n\n\nMinimum Cluster Size\n\n\nThis defines the provisioning strategy in case the cloud provider cannot allocate all the requested nodes. Select \nbest effort\n or \nexact\n.\n\n\n\n\n\n\n\n\nConfigure Ambari Repos\n\n\nYou can optionally configure a different version of Ambari than the default by providing the following information:\n\n\n\n\n\n\n\n\nParameter\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nAmbari Version\n\n\nEnter Ambari version.\n\n\n\n\n\n\nAmbari Repo URL\n\n\nEnter Ambari repo URL.\n\n\n\n\n\n\nAmbari Repo Gpg Key URL\n\n\nEnter gpgkey URL.\n\n\n\n\n\n\n\n\nConfigure HDP Repos\n\n\nYou can optionally configure a different version of HDP than the default by providing the following information:\n\n\n\n\n\n\n\n\nParameter\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nStack\n\n\nEnter stack name.\n\n\n\n\n\n\nVersion\n\n\nEnter stack version.\n\n\n\n\n\n\nStack Repo ID\n\n\nEnter stack repo ID.\n\n\n\n\n\n\nBase URL\n\n\nEner stack repo base URL.\n\n\n\n\n\n\nUtils Repo ID\n\n\nEnter Utils repo ID.\n\n\n\n\n\n\nUtils Base URL\n\n\nEnter Utils repo base URL.\n\n\n\n\n\n\nVerify\n\n\nSelect to verify the repo information.\n\n\n\n\n\n\n\n\nConfigure Ambari Database\n\n\nBy default, Ambari stores data on an embedded database, which is sufficient for ephemeral or test clusters. However, as Ambari and Cloudbreak don't perform backups of this database, it is insufficient for long-running production clusters, and you may need to configure a remote database for Ambari and Cloudbreak.\n\n\n\n\n\n\n\n\nParameter\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nVendor\n\n\nSelect database vendor from the list.\n\n\n\n\n\n\nHost\n\n\nEnter database host IP.\n\n\n\n\n\n\nPort\n\n\nEnter port number.\n\n\n\n\n\n\nName\n\n\nEnter database name.\n\n\n\n\n\n\nUser Name\n\n\nEnter database user name.\n\n\n\n\n\n\nPassword\n\n\nEnter database password.\n\n\n\n\n\n\n\n\n\n\nNext: Access Cloud Data",
            "title": "Create a Cluster"
        },
        {
            "location": "/aws-create/index.html#create-a-cluster-on-aws",
            "text": "To create a cluster via CLoudbreak UI:    Log in to the Cloudbreak UI.    In the top right corner, select the credential that you want to use to create a cluster:        Click  +create cluster  and the  Create cluster  form is displayed.    On the  Configure Cluster  page, provide the following parameters:   To view advanced options, click  Show Advanced Options . To learn about advanced options, refer to  Advanced Options .      Parameter  Description      Cluster Name  Enter a name for your cluster. The name must be between 5 and 40 characters, must start with a letter, must only include lowercase letters, numbers, and hyphens.    Tags  (Optional) You can optionally add tags, which will help you find your cluster-related resources, such as VMs, in your cloud provider account.    Region  Select the region in which you would like to launch your cluster.    Send Email When Cluster is Ready  (Optional) Check this to receive an email each time the cluster status changes.    Public In Account  (Optional) If this option is checked, all the users belonging to your account will be able to use this cluster, but they cannot delete it.      By default, Ambari Username and Ambari Password are set to  admin . You can override it in the \" Configure Cluster \" tab.     On the  Set up Network and Security  page, provide the following parameters:     Parameter  Description      Network  Select the virtual network in which you would like your cluster to be provisioned. You can define custom network configurations or use default network configurations.    Enable Knox Gateway  (Optional) Select this option to enable secure access to Ambari web UI and other cluster UIs via Knox gateway.    Enable Kerberos Security  (Optional) Select this option to enable Kerberos for your cluster. You will have an option to create a new kerberos or use an existing one. For more information refer to Kerberos  documentation .       On the  Choose Blueprint  page, select the blueprint that you would like to use for your cluster. You can either choose one of the pre-configured blueprints, or add your own in the  manage blueprints  tab.  For each host group you must provide the following:     Parameter  Description      Group Size  Enter a number defining how many nodes to create per host group. Default is 1. The \"Group Size\" for that host group on which Ambari Server is installed must be set to \"1\".    Template  If you have previously created a template for VMs and storage, you can select it here. If you don't make a selection, default will be used.    Security Group  If you have previously created a template for a security group, you can select it here. If you don't make a selection, default will be used.    Ambari Server  You must select one node for Ambari Server. The \"Group Size\" for that host group must be set to \"1\".    Recipes  You can select a previously added recipe (custom script) to be executed on all nodes of the host group. Refer to  Recipes .       Click on  Review and Launch  and then  +create and start cluster .    You will be redirected to the Cloudbreak dashboard, and a new tile representing your cluster will appear at the top of the page.",
            "title": "Create a Cluster on AWS"
        },
        {
            "location": "/aws-create/index.html#advanced-options",
            "text": "Click on  Show Advanced Options  to enter additional configuration options.",
            "title": "Advanced Options"
        },
        {
            "location": "/aws-create/index.html#configure-cluster",
            "text": "You can optionally configure the following advanced parameters:     Parameter  Description      Ambari Username  You can log in to the Ambari UI using this username. By default, this is set to  admin .    Ambari Password  You can log in to the Ambari UI using this password. By default, this is set to  admin .    Provision Cluster  SALT  is pre-selected to provision your cluster.    Use dedicated instances  Select this to use dedicated instances (i.e. EC2 instances that run in a virtual private cloud (VPC) on hardware that's dedicated to a single customer). For more information about dedicated instances, refer to  AWS documentation .    Enable Lifetime Management  Check this option if you would like your cluster to be automatically terminated after a specific amount of time (defined as \"Time to Live\" in minuter) has passed.    Flex Subscription  This option will appear if you have configured your deployment for a  Flex Subscription .",
            "title": "Configure Cluster"
        },
        {
            "location": "/aws-create/index.html#choose-blueprint",
            "text": "After selecting a blueprint, you can optionally configure the following advanced parameters:     Parameter  Description      Config Recommendation Strategy (Stack Advisor)  Select how configuration recommendations generated by stack advisor will be applied. Select one of  ALWAYS_APPLY: Configuration recommendations will be applied automatically. ALWAYS_APPLY_DONT_OVERRIDE_CUSTOM_VALUES: Configuration recommendations will be applied automatically, but they will be ignored for custom configurations. NEVER_APPLY: Configuration recommendations will be ignored. ONLY_STACK_DEFAULTS_APPLY: Configuration recommendations will be applied only on the default configurations for all included services.    Validate Blueprint  Select to validate the blueprint.    Instance Profile  This option allows you to optionally create or reuse an existing instance profile for your cluster VMs. An instance profile is a container for an IAM role that you can use to pass role information to an EC2 instance when the instance starts. You can use this option to allow access to AWS resources such as Amazon S3 object storage. The following choices are available: Disable Instance Profile attaching by default Create Instance Profile and attach to the instance Define existing Instance Profile and attach to the instances  For more information about instance profile, refer to  AWS documentation .",
            "title": "Choose Blueprint"
        },
        {
            "location": "/aws-create/index.html#choose-failure-action",
            "text": "You can optionally select what to do if cluster creation fails or if there aren't enough instances available to create all requested nodes:     Parameter  Description      Failure Action  Select one of:  do NOT rollback resources  (default) or  rollback resources .  By default, if creating a cluster fails, the Azure resources that were created up to that point will not be rolled back. This means that they will remain accessible for troubleshooting and you will need to to delete them manually.    Minimum Cluster Size  This defines the provisioning strategy in case the cloud provider cannot allocate all the requested nodes. Select  best effort  or  exact .",
            "title": "Choose Failure Action"
        },
        {
            "location": "/aws-create/index.html#configure-ambari-repos",
            "text": "You can optionally configure a different version of Ambari than the default by providing the following information:     Parameter  Description      Ambari Version  Enter Ambari version.    Ambari Repo URL  Enter Ambari repo URL.    Ambari Repo Gpg Key URL  Enter gpgkey URL.",
            "title": "Configure Ambari Repos"
        },
        {
            "location": "/aws-create/index.html#configure-hdp-repos",
            "text": "You can optionally configure a different version of HDP than the default by providing the following information:     Parameter  Description      Stack  Enter stack name.    Version  Enter stack version.    Stack Repo ID  Enter stack repo ID.    Base URL  Ener stack repo base URL.    Utils Repo ID  Enter Utils repo ID.    Utils Base URL  Enter Utils repo base URL.    Verify  Select to verify the repo information.",
            "title": "Configure HDP Repos"
        },
        {
            "location": "/aws-create/index.html#configure-ambari-database",
            "text": "By default, Ambari stores data on an embedded database, which is sufficient for ephemeral or test clusters. However, as Ambari and Cloudbreak don't perform backups of this database, it is insufficient for long-running production clusters, and you may need to configure a remote database for Ambari and Cloudbreak.     Parameter  Description      Vendor  Select database vendor from the list.    Host  Enter database host IP.    Port  Enter port number.    Name  Enter database name.    User Name  Enter database user name.    Password  Enter database password.      Next: Access Cloud Data",
            "title": "Configure Ambari Database"
        },
        {
            "location": "/aws-data/index.html",
            "text": "Accessing Data on S3\n\n\nPrerequisites\n\n\nTo use S3 storage, you must have one or more S3 buckets on your AWS account. For instructions on how to create a bucket on S3, refer to \nAWS documentation\n.\n\n\nConfiguring Access to S3\n\n\nAmazon S3 is not supported as a default file system, but access to data in S3 from your cluster VMs can be automatically configured through attaching an instance profile allowing access to S3. You can optionally create or attach an existing instance profile during \ncluster creation\n, on the \nChoose Blueprint\n page. \n\n\nAccess Path\n\n\nAmazon S3 access path syntax is:\n\n\ns3a://bucket/dir/file\n\n\n\nFor example, to access a file called \"mytestfile\" in a directory called \"mytestdir\", which is stored in a bucket called \"mytestbucket\", the URL is:\n\n\ns3a://mytestbucket/mytestdir/mytestfile\n\n\n\nThe following FileSystem shell commands demonstrate access to a bucket named \"mytestbucket\": \n\n\nhadoop fs -ls s3a://mytestbucket/\n\nhadoop fs -mkdir s3a://mytestbucket/testDir\n\nhadoop fs -put testFile s3a://mytestbucket/testFile\n\nhadoop fs -cat s3a://mytestbucket/testFile\ntest file content\n\n\n\nLearn More\n\n\nFor more information about configuring the S3 connector and working with data stored on S3, refer to \nCloud Data Access\n documentation.\n\n\n\n\nNext: Access Cluster",
            "title": "Access Data on S3"
        },
        {
            "location": "/aws-data/index.html#accessing-data-on-s3",
            "text": "",
            "title": "Accessing Data on S3"
        },
        {
            "location": "/aws-data/index.html#prerequisites",
            "text": "To use S3 storage, you must have one or more S3 buckets on your AWS account. For instructions on how to create a bucket on S3, refer to  AWS documentation .",
            "title": "Prerequisites"
        },
        {
            "location": "/aws-data/index.html#configuring-access-to-s3",
            "text": "Amazon S3 is not supported as a default file system, but access to data in S3 from your cluster VMs can be automatically configured through attaching an instance profile allowing access to S3. You can optionally create or attach an existing instance profile during  cluster creation , on the  Choose Blueprint  page.",
            "title": "Configuring Access to S3"
        },
        {
            "location": "/aws-data/index.html#access-path",
            "text": "Amazon S3 access path syntax is:  s3a://bucket/dir/file  For example, to access a file called \"mytestfile\" in a directory called \"mytestdir\", which is stored in a bucket called \"mytestbucket\", the URL is:  s3a://mytestbucket/mytestdir/mytestfile  The following FileSystem shell commands demonstrate access to a bucket named \"mytestbucket\":   hadoop fs -ls s3a://mytestbucket/\n\nhadoop fs -mkdir s3a://mytestbucket/testDir\n\nhadoop fs -put testFile s3a://mytestbucket/testFile\n\nhadoop fs -cat s3a://mytestbucket/testFile\ntest file content",
            "title": "Access Path"
        },
        {
            "location": "/aws-data/index.html#learn-more",
            "text": "For more information about configuring the S3 connector and working with data stored on S3, refer to  Cloud Data Access  documentation.   Next: Access Cluster",
            "title": "Learn More"
        },
        {
            "location": "/azure-launch/index.html",
            "text": "Launching Cloudbreak on Azure\n\n\nBefore launching Cloudbreak on Azure, review and meet the prerequisites. Next, launch Cloudbreak using one of the two available methods. Once Cloudbreak is running, log in to the Cloudbreak UI and create a Cloudbreak credential. \n\n\nMeet the Prerequisites\n\n\nBefore launching Cloudbreak on Azure, you must meet the following prerequisites.\n\n\nAzure Account\n\n\nIn order to launch Cloubdreak on the Azure Marketplace, log in to your existing Microsoft Azure account. If you don't have an account, you can set it up at \nhttps://azure.microsoft.com\n.\n\n\nAzure Roles\n\n\nIn order to provision clusters on Azure, Cloudbreak must be able to assume a sufficient Azure role (\"Owner\" or \"Contributor\") via Cloudbreak credential: \n\n\n\n\n\n\nYour account must have the \"\nOwner\n\" role in the subscription in order to \ncreate a Cloudbreak credential\n using the interactive credential method. \n\n\n\n\n\n\nYour account must have the \"\nContributor\n\" role (or higher) in the subscription in order to \ncreate a Cloudbreak credential\n using the app-based credential method. \n\n\n\n\n\n\nTo check the roles that your subscription has, in your Azure account navigate to \nSubscriptions\n. \n\n\nAzure Region\n\n\nDecide in which Azure region you would like to launch Cloudbreak. You can launch Cloudbreak and provision your clusters in all regions \nsupported by Microsoft Azure\n. \n\n\nClusters created via Cloudbreak can be in the same or different region as Cloudbreak; when you launch a cluster, you select the region in which to launch it.\n\n\nSSH Key Pair\n\n\nWhen launching Cloudbreak, you will be required to provide your public SSH key. If needed, you can generate a new SSH keypair:\n\n\n\n\nOn MacOS and Linux using \nssh-keygen -t rsa -b 4096 -C \"your_email@example.com\"\n  \n\n\nOn Windows using \nputtgen\n\n\n\n\nAzure Data Lake Store (Optional)\n\n\nIf you want to use \nAzure Data Lake Store\n with your cluster, you must create an\nAzure Data Lake Store account in the same subscription as you use for launching Cloudbreak and clusters. In the cluster creation phase, you will specify the created account name only, and access to the Azure Data Lake Store will be\nconfigured automatically.\n\n\nYou may also review the \nMicrosoft Azure Documentation\n for instructions on how these configuration steps\nare manually done from the Azure portal. After the cluster is deployed, you must define which parts of the ADLS store this cluster\nwill have access and test access to ADLS. Refer to \nAccessing Data\n for more information.\n\n\nFor further information, refer to \nMicrosoft Azure documentation\n.\n\n\nLaunch Cloudbreak\n\n\nYou have two options to launch Cloudbreak on Azure:\n\n\n\n\nLaunch via Azure Resource Manager Template\n\n\nLaunch from Azure Marketplace (Technical Preview)\n\n\n\n\n(Option 1) Launch via Azure Resource Manager Template\n\n\n\n\n\n\nLog in to your \nAzure Portal\n.\n\n\n\n\n\n\nClick here to get started with Cloudbreak installation using the Azure Resource Manager template:\n\n\n \n\n\n\n\n\n\nThe template for installing Cloudbreak will appear. On the \nBasics\n page, provide the following basic parameters:   \n\n\nBASICS\n\n\n\n\n\n\n\n\nParameter\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nSubscription\n\n\n(Required) Select which existing subscription you want to use.\n\n\n\n\n\n\nResource group\n\n\n(Required) Select an existing resource group or create a new one by selecting \nCreate new\n and entering a name for your new resource group. Cloudbreak resources will later be accessible in that chosen resource group.\n\n\n\n\n\n\nLocation\n\n\n(Required) Select an Azure region in which you want to deploy Cloudbreak.\n\n\n\n\n\n\n\n\nSETTINGS\n\n\n\n\n\n\n\n\nParameter\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nBase Url\n\n\nThis is the URL to the page where the template is stored.\n\n\n\n\n\n\nLocation\n\n\nThis is an internal parameter. Do not change it.\n\n\n\n\n\n\nVM Size\n\n\nSelect virtual machine instance type to use for the Cloudbreak controller. The minimum instance type suitable for Cloudbreak is \nD2\n.\n\n\n\n\n\n\nAdmin Username\n\n\nCreate an admin login that you will use to log in to the Cloudbreak UI. Must be a valid email address.\n\n\n\n\n\n\nAdmin User Password\n\n\n(Required) Password for the admin login. Must be at least 8 characters containing letters, numbers, and symbols.\n\n\n\n\n\n\nUsername\n\n\nEnter an admin username for the virtual machine. You will use it to SSH to the VM.\n\n\n\n\n\n\nSmartSense\n\n\nSelect whether you want to use SmartSense telemetry. Default is \"false\" (not using SmartSense telemetry).\n\n\n\n\n\n\nRemote Location\n\n\nEnter a valid \nCIDR IP\n or use one of the default tags. Default value is \nInternet\n which allows access from all IP addresses. Examples: \n10.0.0.0/24 will allow access from 10.0.0.0 through 10.0.0.255\n'Internet' will allow access from all. This is not a secure option but you can use it it you are just getting started and are not planning to have the instance on for a longer period. \n(Advanced) 'VirtualNetwork' will allow access from the address space of the Virtual Network.\n (Advanced) 'AzureLoadBalancer' will allow access from the address space of the load balancer.\nFor more information, refer to the \nAzure documentation\n.\n\n\n\n\n\n\nSsh Key\n\n\n(Required) Paste your SSH public key.\nYou can use \npbcopy\n to quickly copy it. For example: \npbcopy < /Users/homedir/.ssh/id_rsa.pub\n\n\n\n\n\n\nVnet New Or Existing\n\n\nBy default, Cloudbreak is launched in a new VNet called \ncbdeployerVnet\n and a new subnet called \ncbdeployerSubnet\n; if needed, you can customize the settings for the new VNet using available VNet and Subnet parameters.\n\n\n\n\n\n\nVnet Name\n\n\nProvide the name for a new Vnet. Default is \n`cbdeployerVnet\n.\n\n\n\n\n\n\nVnet Subnet Name\n\n\nProvide a name for a new subnet. Default is \ncbdeployerSubnet\n.\n\n\n\n\n\n\nVnet Address Prefix\n\n\nProvide a CIDR for the virtual network. Default is \n10.0.0.0/16\n.\n\n\n\n\n\n\nVnet Subnet Address Prefix\n\n\nProvide a CIDR for the subnet. Default is \n10.0.0.0/24\n.\n\n\n\n\n\n\nVnet RG Name\n\n\nThe name of the resource group in which the Vnet is located. If creating a new Vnet, enter the same resource group name as provided in the \nResource group\n field in the \nBASICS\n section.\n\n\n\n\n\n\n\n\n\n\n\n\nReview terms of use and check \"I agree to the terms and conditions stated above\". \n\n\n\n\n\n\nClick \nPurchase\n.\n\n\n\n\n\n\nProceed to the next step: \nExplore Newly Created Resources\n\n\n\n\n\n\n(Option 2) Launch from Azure Marketplace\n\n\n\n\nThis feature is Technical Preview.  \n\n\n\n\n\n\n\n\nLog in to your \nAzure Portal\n.\n\n\n\n\n\n\nFrom the services menu, select \n.\n\n\n\n\n\n\nIn the search box, enter \"Cloudbreak\":  \n\n\n  \n\n\n\n\n\n\nSelect \n.\n\n    The information about the Cloudbreak for Hortonworks Data Platform will be displayed.\n    In the bottom of the page, you will see a dropdown for selecting a deployment model, with the \nResource Manager\n deployment model pre-selected. This is the only deployment model available for this offering.\n\n\n\n\n\n\nClick \nCreate\n.   \n\n\n\n\n\n\nThe template for installing Cloudbreak will appear. On the \nBasics\n page, provide the following basic parameters:\n\n\n\n\n\n\n\n\nParameter\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nAdministrator email address\n\n\nCreate an admin login that you will use to log in to the Cloudbreak UI. Must be a valid email address.\n\n\n\n\n\n\nAdministrator user password\n\n\nPassword for the admin login. Must be at least 8 characters containing letters, numbers, and symbols.\n\n\n\n\n\n\nConfirm password\n\n\nConfirm the password for the admin login.\n\n\n\n\n\n\nVM Username\n\n\nEnter an admin username for the virtual machine. You will use it to SSH to the VM.\n\n\n\n\n\n\nVM SSH public key\n\n\nPaste your SSH public key.\nYou can use \npbcopy\n to quickly copy it. For example: \npbcopy < /Users/homedir/.ssh/id_rsa.pub\n\n\n\n\n\n\nSubscription\n\n\nSelect which existing subscription you want to use.\n\n\n\n\n\n\nResource group\n\n\nOnly \nCreate new\n is supported. Select \nCreate new\n to create a new resource group and enter a name for your new resource group. Cloudbreak resources will later be accessible in that chosen resource group.\n\n\n\n\n\n\nLocation\n\n\nSelect an Azure region in which you want to deploy Cloudbreak.\n\n\n\n\n\n\n\n\n\n\n\n\nOnce done, click \nOK\n.\n\n\n\n\n\n\nOn the \nAdvanced Settings\n page, provide the following advanced parameters:\n\n\n\n\n\n\n\n\nParameter\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nController Instance Type\n\n\nSelect virtual machine instance type to use for the Cloudbreak. The minimum instance type suitable for Cloudbreak is \nD2\n.\n\n\n\n\n\n\nAllow connections to the cloud controller from this address range or \ndefault tag\n\n\nEnter a valid \nCIDR IP\n or use one of the default tags such as  \nInternet\n. For example: \n10.0.0.0/24 will allow access from 10.0.0.0 through 10.0.0.255\n'Internet' will allow access from all. This is not a secure option but you can use it it you are just getting started and are not planning to have the instance on for a longer period. \n(Advanced) 'VirtualNetwork' will allow access from the address space of the Virtual Network.\n (Advanced) 'AzureLoadBalancer' will allow access from the address space of the load balancer.\nFor more information, refer to the \nAzure documentation\n.\n\n\n\n\n\n\nEnable SmartSense\n\n\n(Optional) Select whether to enable SmartSense telemetry. Default is \nI have read and opt-in to SmartSense telemetry\n. SmartSense provides product telemetry and usage information to Hortonworks. For more information, refer to the \nSmartSense\n terms.\n\n\n\n\n\n\nVirtual network\n\n\nCreate a new Vnet (default) or select an existing Vnet.\n\n\n\n\n\n\nSubnets\n\n\nIf you created a new Vnet, create subnets within it. If selected an existing Vnet, select exiting subnets.\n\n\n\n\n\n\n\n\n\n\n\n\nOnce done, click \nOK\n.\n\n\n\n\n\n\nOn the \nSummary\n page, validate the information that you provided.\n    Before proceeding to the next page, you have an option to \nDownload template and parameters\n. You can use them to launch Cloudbreak via CLI. Once done, click \nOK\n.\n\n\n\n\n\n\nReview terms of use and click \nPurchase\n.\n\n\n\n\n\n\nProceed to the next step: \nExplore Newly Created Resources\n\n\n\n\n\n\nExplore Newly Created Resources\n\n\n\n\nThis step is optional.  \n\n\n\n\nWhile the deployment is in progress, you can optionally navigate to the newly created resource group and see what Azure resources are being created.\n\n\n\n\n\n\nFrom the left pane, select \n.\n\n\n\n\n\n\nFind the the resource group that you just created and select it to view details.\n\n\n\n\n\n\nThe following resources should have been created in your resource group:\n\n\n\n\n\n\nIf you chose to use an existing virtual network, the virtual network will not be added to the resource group. \n\n\n\n\n\n\nVirtual network\n (VNet) securely connects Azure resources to each other.    \n\n\nNetwork security group\n (NSG) defines inbound and outbound security rules, which control network traffic flow.  \n\n\nVirtual machine\n runs Cloudbreak.   \n\n\nPublic IP address\n is assigned to your VM so that it can communicate with other Azure resources.  \n\n\nNetwork interface\n (NIC) attached to the VM provides the interconnection between the VM and the underlying software network.    \n\n\nBlob storage container\n is created to store Cloudbreak Deployer OS disk's data.  \n\n\n\n\n\n\n\n\nYou can click on each entry to view details of the resource. For example, click on \n to view details, including Cloudbreak IP address.\n\n\n\n\n\n\nOnce your deployment is ready, the status will change from \"Deploying\" to \"Success\".\n\n\n\n\n\n\nAccess Cloudbreak UI\n\n\n\n\n\n\nWhen your deployment succeeds, you will receive a notification in the top-right corner. You can click on the link provided to navigate to the resource group created earlier.\n\n\n\n\nThis only works right after deployment. At other times, you can find your resource group by selecting \nResource Groups\n from the service menu and then finding your resource group by name.\n\n\n\n\n\n\n\n\nOnce you've navigated to your resource group, click on \nDeployments\n and then click on \nhortonworks.cloudbreal-for-hortonworks-data-platf-...\n:\n\n\n \n\n\n\n\n\n\nFrom \nOutputs\n, you can copy the link by clicking on the \n icon:\n\n\n   \n\n\n\n\n\n\nPaste the link in your browser's address bar.\n\n\nThe first time you access Cloudbreak UI, Cloudbreak will automatically generate a self-signed certificate, due to which your browser will warn you about an untrusted connection and will ask you to confirm a security exception. You can safely proceed to the website. \n\n\n\n\n\n\n\n\nBrowser\n\n\nSteps\n\n\n\n\n\n\n\n\n\n\nFirefox\n\n\nClick \nAdvanced\n > Click \nAdd Exception...\n > Click \nConfirm Security Exception\n\n\n\n\n\n\nSafari\n\n\nClick \nContinue\n\n\n\n\n\n\nChrome\n\n\nClick \nAdvanced\n > Click \nProceed...\n\n\n\n\n\n\n\n\n\n\n\n\nNow you should be able to access Cloudbreak UI and log in with the \nAdmin email address\n and \nAdmin password\n that you created when launching Cloudbreak:\n\n\n\n\nThe last task that you need to perform before you can use Cloudbreak is to \ncreate a cloudbreak credential\n.         \n\n\n\n\n\n\nCreate Cloudbreak Credential\n\n\nThe first time you log in to the Cloudbreak UI, you should see \nmanage credentials\n tab open, informing you that to use Cloudbreak you first need to create a credential associated with your Azure subscription. Cloudbreak works by connecting your Azure account through this credential, and then uses it to create resources on your behalf.\n\n\nBefore you can start provisioning cluster using Cloudbreak, you must create a Cloudbreak credential. There are two ways to do this:\n\n\n\n\n\n\nInteractive\n: This is the recommended simpler method. The advantage of using this method is that the app and service principal creation and role assignment is fully automated, so the only input that you need to provide is the name and the SSH key. To configure an interactive credential, refer to \nCreate an Interactive Credential\n.  \n\n\n\n\n\n\nApp-based\n: This is the more complex method. The advantage of the app-based credential creation is that it allows you to create a credential without logging in to the Azure account, as long as you have been given all the information. In addition to providing your \ntenant_id\n and \nsubscription_id\n, you must provide information for your previously created Azure AD application, including its password. To configure an app based credential, refer to \nCreate an App Based Credential\n. Alternatively, when using this method, you use a utility called \nazure-cli-tools\n. The utility supports app creation and role assignment. It is available at \nhttps://github.com/sequenceiq/azure-cli-tools/blob/master/cli_tools\n.\n\n\n\n\n\n\nCreate an Interactive Credential\n\n\n\n\n\n\nOpen the \nmanage credentials\n pane:\n\n\n\n\n\n\n\n\nClick \nNext\n.\n\n\n\n\n\n\nOn the \nConfigure credentials\n page, provide the following parameters:\n\n\n\n\n\n\n\n\nParameter\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nSelect Credential Type\n\n\nThe \nInteractive\n credential type should be pre-selected.\n\n\n\n\n\n\nName\n\n\nEnter a name for your credential.\n\n\n\n\n\n\nDescription\n\n\n(Optional) Enter a description.\n\n\n\n\n\n\nSSH Public Key\n\n\nThe SSH key that you provided when launching Cloudbreak should be pre-entered.\n\n\n\n\n\n\nSelect Azure role type\n\n\nYou have the following options:\n\"Use existing Contributor role\" (default): If you select this option, Cloudbreak will use the \"\nContributor\n\" role to create resources. This requires no further input.\n\"Reuse existing custom role\": If you select this option and enter the name of an existing role, Cloudbreak will use this role to create resources.\n\"Let Cloudbreak create a custom role\": If you select this option and enter a name for the new role, the role will be created. When choosing role name, make sure that there is no existing role with the name chosen. For information on creating custom roles, refer to \nAzure\n documentation. \nIf using a custom role, make sure that it includes the necessary Action set for Cloudbreak to be able to manage clusters: \nMicrosoft.Compute/*\n, \nMicrosoft.Network/*\n, \nMicrosoft.Storage/*\n, \nMicrosoft.Resources/*\n.\n\n\n\n\n\n\nSelect Platform\n\n\n(Optional advanced option) Select a platform (if previously configured).\n\n\n\n\n\n\nPublic In Account\n\n\n(Optional) If you check this, other users added to your Cloudbreak instance will be able to use this credential to create clusters.\n\n\n\n\n\n\n\n\n\n\n\n\nClick \nNext\n.\n\n\n\n\n\n\nOn the \nFinish\n page, click on the \n icon next to the randomly generated code to copy it.\n\n\n\n\n\n\nClick \nAzure login\n and a new \nDevice login\n page will open in a new browser tab:\n\n\n  \n\n\n\n\n\n\nNext, paste the code in field on the  \nDevice login\n page and click \nContinue\n.\n\n\n\n\n\n\nConfirm your account by selecting it:\n\n\n\n\n\n\n\n\nA confirmation page will appear, confirming that you have signed in to the Microsoft Azure Cross-platform Command Line Interface application on your device. You may now close this window.\n\n\n\n\n\n\nYour credential should now be displayed in the \nmanage credentials\n tab.\n\n\nCongratulations! You've successfully launched and configured Cloudbreak. Now you can use Cloudbreak to \ncreate clusters\n.\n\n\n\n\n\n\nCreate an App Based Credential\n\n\n\n\n\n\nOn Azure Portal, navigate to the \nActive Directory\n > \nApp Registrations\n and register a new application. For more information, refer to \nCreate an Azure AD Application\n.\n\n\n\n\n\n\n\n\nNavigate to the \nSubscriptions\n, choose \nAccess control (IAM)\n. Click \nAdd\n and then assign the \"Contributor\" role to your newly created application by selecting \"Contributor\" under \nRole\n and your app name under \nSelect\n:\n\n\n   \n\n\n\n\n\n\nIn the Cloudbreak web UI, open the \nmanage credentials\n pane:\n\n\n\n\n\n\n\n\nClick \nNext\n.\n\n\n\n\n\n\nOn the \nConfigure credential\n page, provide the following parameters:\n\n\n\n\n\n\n\n\nParameter\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nSelect Credential Type\n\n\nSelect \nApp based\n.\n\n\n\n\n\n\nName\n\n\nEnter a name for your credential.\n\n\n\n\n\n\nDescription\n\n\n(Optional) Enter a description.\n\n\n\n\n\n\nSubscription Id\n\n\nCopy and paste the Subscription ID from your \nSubscriptions\n.\n\n\n\n\n\n\nApp Id\n\n\nCopy and paste the Application ID from your \nAzure Active Directory\n > \nApp Registrations\n > your app registration's \nSettings\n > \nProperties\n.\n\n\n\n\n\n\nPassword\n\n\nThis is your application key. You can generate it from your from your \nAzure Active Directory\n app registration's \nSettings\n > \nKeys\n.\n\n\n\n\n\n\nApp Owner Tenant Id\n\n\nCopy and paste your Directory ID from your \nActive Directory\n > \nProperties\n.\n\n\n\n\n\n\nSSH Public Key\n\n\nThe SSH key that you provided when launching Cloudbreak should be pre-entered.\n\n\n\n\n\n\nSelect Azure role type\n\n\nYou have the following options:\n\"Use existing Contributor role\" (default): If you select this option, Cloudbreak will use the \"\nContributor\n\" role to create resources. This requires no further input.\n\"Reuse existing custom role\": If you select this option and enter the name of an existing role, Cloudbreak will use this role to create resources.\n\"Let Cloudbreak create a custom role\": If you select this option and enter a name for the new role, the role will be created. When choosing role name, make sure that there is no existing role with the name chosen. For information on creating custom roles, refer to \nAzure\n documentation. \nIf using a custom role, make sure that it includes the necessary Action set for Cloudbreak to be able to manage clusters: \nMicrosoft.Compute/*\n, \nMicrosoft.Network/*\n, \nMicrosoft.Storage/*\n, \nMicrosoft.Resources/*\n.\n\n\n\n\n\n\nSelect Platform\n\n\n(Optional advanced option) Select a platform (if previously configured).\n\n\n\n\n\n\nPublic In Account\n\n\n(Optional) If you check this, other users added to your Cloudbreak instance will be able to use this credential to create clusters.\n\n\n\n\n\n\n\n\n\n\nIf you are having trouble locating the parameters, refer to this \nAzure\n documentation. \n\n\n\n\n\n\n\n\nClick \nNext\n and then \nFinish\n.\n\n\n\n\n\n\nYour credential should now be displayed at the top of the page in the \nmanage credentials\n tab.\n\n\nCongratulations! You've successfully launched and configured Cloudbreak. Now you can use Cloudbreak to \ncreate clusters\n.\n\n\n\n\n\n\n\n\nNext: Define Infrastructure Templates",
            "title": "Launch on Azure"
        },
        {
            "location": "/azure-launch/index.html#launching-cloudbreak-on-azure",
            "text": "Before launching Cloudbreak on Azure, review and meet the prerequisites. Next, launch Cloudbreak using one of the two available methods. Once Cloudbreak is running, log in to the Cloudbreak UI and create a Cloudbreak credential.",
            "title": "Launching Cloudbreak on Azure"
        },
        {
            "location": "/azure-launch/index.html#meet-the-prerequisites",
            "text": "Before launching Cloudbreak on Azure, you must meet the following prerequisites.",
            "title": "Meet the Prerequisites"
        },
        {
            "location": "/azure-launch/index.html#azure-account",
            "text": "In order to launch Cloubdreak on the Azure Marketplace, log in to your existing Microsoft Azure account. If you don't have an account, you can set it up at  https://azure.microsoft.com .",
            "title": "Azure Account"
        },
        {
            "location": "/azure-launch/index.html#azure-roles",
            "text": "In order to provision clusters on Azure, Cloudbreak must be able to assume a sufficient Azure role (\"Owner\" or \"Contributor\") via Cloudbreak credential:     Your account must have the \" Owner \" role in the subscription in order to  create a Cloudbreak credential  using the interactive credential method.     Your account must have the \" Contributor \" role (or higher) in the subscription in order to  create a Cloudbreak credential  using the app-based credential method.     To check the roles that your subscription has, in your Azure account navigate to  Subscriptions .",
            "title": "Azure Roles"
        },
        {
            "location": "/azure-launch/index.html#azure-region",
            "text": "Decide in which Azure region you would like to launch Cloudbreak. You can launch Cloudbreak and provision your clusters in all regions  supported by Microsoft Azure .   Clusters created via Cloudbreak can be in the same or different region as Cloudbreak; when you launch a cluster, you select the region in which to launch it.",
            "title": "Azure Region"
        },
        {
            "location": "/azure-launch/index.html#ssh-key-pair",
            "text": "When launching Cloudbreak, you will be required to provide your public SSH key. If needed, you can generate a new SSH keypair:   On MacOS and Linux using  ssh-keygen -t rsa -b 4096 -C \"your_email@example.com\"     On Windows using  puttgen",
            "title": "SSH Key Pair"
        },
        {
            "location": "/azure-launch/index.html#azure-data-lake-store-optional",
            "text": "If you want to use  Azure Data Lake Store  with your cluster, you must create an\nAzure Data Lake Store account in the same subscription as you use for launching Cloudbreak and clusters. In the cluster creation phase, you will specify the created account name only, and access to the Azure Data Lake Store will be\nconfigured automatically.  You may also review the  Microsoft Azure Documentation  for instructions on how these configuration steps\nare manually done from the Azure portal. After the cluster is deployed, you must define which parts of the ADLS store this cluster\nwill have access and test access to ADLS. Refer to  Accessing Data  for more information.  For further information, refer to  Microsoft Azure documentation .",
            "title": "Azure Data Lake Store (Optional)"
        },
        {
            "location": "/azure-launch/index.html#launch-cloudbreak",
            "text": "You have two options to launch Cloudbreak on Azure:   Launch via Azure Resource Manager Template  Launch from Azure Marketplace (Technical Preview)",
            "title": "Launch Cloudbreak"
        },
        {
            "location": "/azure-launch/index.html#option-1-launch-via-azure-resource-manager-template",
            "text": "Log in to your  Azure Portal .    Click here to get started with Cloudbreak installation using the Azure Resource Manager template:       The template for installing Cloudbreak will appear. On the  Basics  page, provide the following basic parameters:     BASICS     Parameter  Description      Subscription  (Required) Select which existing subscription you want to use.    Resource group  (Required) Select an existing resource group or create a new one by selecting  Create new  and entering a name for your new resource group. Cloudbreak resources will later be accessible in that chosen resource group.    Location  (Required) Select an Azure region in which you want to deploy Cloudbreak.     SETTINGS     Parameter  Description      Base Url  This is the URL to the page where the template is stored.    Location  This is an internal parameter. Do not change it.    VM Size  Select virtual machine instance type to use for the Cloudbreak controller. The minimum instance type suitable for Cloudbreak is  D2 .    Admin Username  Create an admin login that you will use to log in to the Cloudbreak UI. Must be a valid email address.    Admin User Password  (Required) Password for the admin login. Must be at least 8 characters containing letters, numbers, and symbols.    Username  Enter an admin username for the virtual machine. You will use it to SSH to the VM.    SmartSense  Select whether you want to use SmartSense telemetry. Default is \"false\" (not using SmartSense telemetry).    Remote Location  Enter a valid  CIDR IP  or use one of the default tags. Default value is  Internet  which allows access from all IP addresses. Examples:  10.0.0.0/24 will allow access from 10.0.0.0 through 10.0.0.255 'Internet' will allow access from all. This is not a secure option but you can use it it you are just getting started and are not planning to have the instance on for a longer period.  (Advanced) 'VirtualNetwork' will allow access from the address space of the Virtual Network.  (Advanced) 'AzureLoadBalancer' will allow access from the address space of the load balancer. For more information, refer to the  Azure documentation .    Ssh Key  (Required) Paste your SSH public key. You can use  pbcopy  to quickly copy it. For example:  pbcopy < /Users/homedir/.ssh/id_rsa.pub    Vnet New Or Existing  By default, Cloudbreak is launched in a new VNet called  cbdeployerVnet  and a new subnet called  cbdeployerSubnet ; if needed, you can customize the settings for the new VNet using available VNet and Subnet parameters.    Vnet Name  Provide the name for a new Vnet. Default is  `cbdeployerVnet .    Vnet Subnet Name  Provide a name for a new subnet. Default is  cbdeployerSubnet .    Vnet Address Prefix  Provide a CIDR for the virtual network. Default is  10.0.0.0/16 .    Vnet Subnet Address Prefix  Provide a CIDR for the subnet. Default is  10.0.0.0/24 .    Vnet RG Name  The name of the resource group in which the Vnet is located. If creating a new Vnet, enter the same resource group name as provided in the  Resource group  field in the  BASICS  section.       Review terms of use and check \"I agree to the terms and conditions stated above\".     Click  Purchase .    Proceed to the next step:  Explore Newly Created Resources",
            "title": "(Option 1) Launch via Azure Resource Manager Template"
        },
        {
            "location": "/azure-launch/index.html#option-2-launch-from-azure-marketplace",
            "text": "This feature is Technical Preview.       Log in to your  Azure Portal .    From the services menu, select  .    In the search box, enter \"Cloudbreak\":          Select  . \n    The information about the Cloudbreak for Hortonworks Data Platform will be displayed.\n    In the bottom of the page, you will see a dropdown for selecting a deployment model, with the  Resource Manager  deployment model pre-selected. This is the only deployment model available for this offering.    Click  Create .       The template for installing Cloudbreak will appear. On the  Basics  page, provide the following basic parameters:     Parameter  Description      Administrator email address  Create an admin login that you will use to log in to the Cloudbreak UI. Must be a valid email address.    Administrator user password  Password for the admin login. Must be at least 8 characters containing letters, numbers, and symbols.    Confirm password  Confirm the password for the admin login.    VM Username  Enter an admin username for the virtual machine. You will use it to SSH to the VM.    VM SSH public key  Paste your SSH public key. You can use  pbcopy  to quickly copy it. For example:  pbcopy < /Users/homedir/.ssh/id_rsa.pub    Subscription  Select which existing subscription you want to use.    Resource group  Only  Create new  is supported. Select  Create new  to create a new resource group and enter a name for your new resource group. Cloudbreak resources will later be accessible in that chosen resource group.    Location  Select an Azure region in which you want to deploy Cloudbreak.       Once done, click  OK .    On the  Advanced Settings  page, provide the following advanced parameters:     Parameter  Description      Controller Instance Type  Select virtual machine instance type to use for the Cloudbreak. The minimum instance type suitable for Cloudbreak is  D2 .    Allow connections to the cloud controller from this address range or  default tag  Enter a valid  CIDR IP  or use one of the default tags such as   Internet . For example:  10.0.0.0/24 will allow access from 10.0.0.0 through 10.0.0.255 'Internet' will allow access from all. This is not a secure option but you can use it it you are just getting started and are not planning to have the instance on for a longer period.  (Advanced) 'VirtualNetwork' will allow access from the address space of the Virtual Network.  (Advanced) 'AzureLoadBalancer' will allow access from the address space of the load balancer. For more information, refer to the  Azure documentation .    Enable SmartSense  (Optional) Select whether to enable SmartSense telemetry. Default is  I have read and opt-in to SmartSense telemetry . SmartSense provides product telemetry and usage information to Hortonworks. For more information, refer to the  SmartSense  terms.    Virtual network  Create a new Vnet (default) or select an existing Vnet.    Subnets  If you created a new Vnet, create subnets within it. If selected an existing Vnet, select exiting subnets.       Once done, click  OK .    On the  Summary  page, validate the information that you provided.\n    Before proceeding to the next page, you have an option to  Download template and parameters . You can use them to launch Cloudbreak via CLI. Once done, click  OK .    Review terms of use and click  Purchase .    Proceed to the next step:  Explore Newly Created Resources",
            "title": "(Option 2) Launch from Azure Marketplace"
        },
        {
            "location": "/azure-launch/index.html#explore-newly-created-resources",
            "text": "This step is optional.     While the deployment is in progress, you can optionally navigate to the newly created resource group and see what Azure resources are being created.    From the left pane, select  .    Find the the resource group that you just created and select it to view details.    The following resources should have been created in your resource group:    If you chose to use an existing virtual network, the virtual network will not be added to the resource group.     Virtual network  (VNet) securely connects Azure resources to each other.      Network security group  (NSG) defines inbound and outbound security rules, which control network traffic flow.    Virtual machine  runs Cloudbreak.     Public IP address  is assigned to your VM so that it can communicate with other Azure resources.    Network interface  (NIC) attached to the VM provides the interconnection between the VM and the underlying software network.      Blob storage container  is created to store Cloudbreak Deployer OS disk's data.       You can click on each entry to view details of the resource. For example, click on   to view details, including Cloudbreak IP address.    Once your deployment is ready, the status will change from \"Deploying\" to \"Success\".",
            "title": "Explore Newly Created Resources"
        },
        {
            "location": "/azure-launch/index.html#access-cloudbreak-ui",
            "text": "When your deployment succeeds, you will receive a notification in the top-right corner. You can click on the link provided to navigate to the resource group created earlier.   This only works right after deployment. At other times, you can find your resource group by selecting  Resource Groups  from the service menu and then finding your resource group by name.     Once you've navigated to your resource group, click on  Deployments  and then click on  hortonworks.cloudbreal-for-hortonworks-data-platf-... :       From  Outputs , you can copy the link by clicking on the   icon:         Paste the link in your browser's address bar.  The first time you access Cloudbreak UI, Cloudbreak will automatically generate a self-signed certificate, due to which your browser will warn you about an untrusted connection and will ask you to confirm a security exception. You can safely proceed to the website.      Browser  Steps      Firefox  Click  Advanced  > Click  Add Exception...  > Click  Confirm Security Exception    Safari  Click  Continue    Chrome  Click  Advanced  > Click  Proceed...       Now you should be able to access Cloudbreak UI and log in with the  Admin email address  and  Admin password  that you created when launching Cloudbreak:   The last task that you need to perform before you can use Cloudbreak is to  create a cloudbreak credential .",
            "title": "Access Cloudbreak UI"
        },
        {
            "location": "/azure-launch/index.html#create-cloudbreak-credential",
            "text": "The first time you log in to the Cloudbreak UI, you should see  manage credentials  tab open, informing you that to use Cloudbreak you first need to create a credential associated with your Azure subscription. Cloudbreak works by connecting your Azure account through this credential, and then uses it to create resources on your behalf.  Before you can start provisioning cluster using Cloudbreak, you must create a Cloudbreak credential. There are two ways to do this:    Interactive : This is the recommended simpler method. The advantage of using this method is that the app and service principal creation and role assignment is fully automated, so the only input that you need to provide is the name and the SSH key. To configure an interactive credential, refer to  Create an Interactive Credential .      App-based : This is the more complex method. The advantage of the app-based credential creation is that it allows you to create a credential without logging in to the Azure account, as long as you have been given all the information. In addition to providing your  tenant_id  and  subscription_id , you must provide information for your previously created Azure AD application, including its password. To configure an app based credential, refer to  Create an App Based Credential . Alternatively, when using this method, you use a utility called  azure-cli-tools . The utility supports app creation and role assignment. It is available at  https://github.com/sequenceiq/azure-cli-tools/blob/master/cli_tools .",
            "title": "Create Cloudbreak Credential"
        },
        {
            "location": "/azure-launch/index.html#create-an-interactive-credential",
            "text": "Open the  manage credentials  pane:     Click  Next .    On the  Configure credentials  page, provide the following parameters:     Parameter  Description      Select Credential Type  The  Interactive  credential type should be pre-selected.    Name  Enter a name for your credential.    Description  (Optional) Enter a description.    SSH Public Key  The SSH key that you provided when launching Cloudbreak should be pre-entered.    Select Azure role type  You have the following options: \"Use existing Contributor role\" (default): If you select this option, Cloudbreak will use the \" Contributor \" role to create resources. This requires no further input. \"Reuse existing custom role\": If you select this option and enter the name of an existing role, Cloudbreak will use this role to create resources. \"Let Cloudbreak create a custom role\": If you select this option and enter a name for the new role, the role will be created. When choosing role name, make sure that there is no existing role with the name chosen. For information on creating custom roles, refer to  Azure  documentation.  If using a custom role, make sure that it includes the necessary Action set for Cloudbreak to be able to manage clusters:  Microsoft.Compute/* ,  Microsoft.Network/* ,  Microsoft.Storage/* ,  Microsoft.Resources/* .    Select Platform  (Optional advanced option) Select a platform (if previously configured).    Public In Account  (Optional) If you check this, other users added to your Cloudbreak instance will be able to use this credential to create clusters.       Click  Next .    On the  Finish  page, click on the   icon next to the randomly generated code to copy it.    Click  Azure login  and a new  Device login  page will open in a new browser tab:        Next, paste the code in field on the   Device login  page and click  Continue .    Confirm your account by selecting it:     A confirmation page will appear, confirming that you have signed in to the Microsoft Azure Cross-platform Command Line Interface application on your device. You may now close this window.    Your credential should now be displayed in the  manage credentials  tab.  Congratulations! You've successfully launched and configured Cloudbreak. Now you can use Cloudbreak to  create clusters .",
            "title": "Create an Interactive Credential"
        },
        {
            "location": "/azure-launch/index.html#create-an-app-based-credential",
            "text": "On Azure Portal, navigate to the  Active Directory  >  App Registrations  and register a new application. For more information, refer to  Create an Azure AD Application .     Navigate to the  Subscriptions , choose  Access control (IAM) . Click  Add  and then assign the \"Contributor\" role to your newly created application by selecting \"Contributor\" under  Role  and your app name under  Select :         In the Cloudbreak web UI, open the  manage credentials  pane:     Click  Next .    On the  Configure credential  page, provide the following parameters:     Parameter  Description      Select Credential Type  Select  App based .    Name  Enter a name for your credential.    Description  (Optional) Enter a description.    Subscription Id  Copy and paste the Subscription ID from your  Subscriptions .    App Id  Copy and paste the Application ID from your  Azure Active Directory  >  App Registrations  > your app registration's  Settings  >  Properties .    Password  This is your application key. You can generate it from your from your  Azure Active Directory  app registration's  Settings  >  Keys .    App Owner Tenant Id  Copy and paste your Directory ID from your  Active Directory  >  Properties .    SSH Public Key  The SSH key that you provided when launching Cloudbreak should be pre-entered.    Select Azure role type  You have the following options: \"Use existing Contributor role\" (default): If you select this option, Cloudbreak will use the \" Contributor \" role to create resources. This requires no further input. \"Reuse existing custom role\": If you select this option and enter the name of an existing role, Cloudbreak will use this role to create resources. \"Let Cloudbreak create a custom role\": If you select this option and enter a name for the new role, the role will be created. When choosing role name, make sure that there is no existing role with the name chosen. For information on creating custom roles, refer to  Azure  documentation.  If using a custom role, make sure that it includes the necessary Action set for Cloudbreak to be able to manage clusters:  Microsoft.Compute/* ,  Microsoft.Network/* ,  Microsoft.Storage/* ,  Microsoft.Resources/* .    Select Platform  (Optional advanced option) Select a platform (if previously configured).    Public In Account  (Optional) If you check this, other users added to your Cloudbreak instance will be able to use this credential to create clusters.      If you are having trouble locating the parameters, refer to this  Azure  documentation.      Click  Next  and then  Finish .    Your credential should now be displayed at the top of the page in the  manage credentials  tab.  Congratulations! You've successfully launched and configured Cloudbreak. Now you can use Cloudbreak to  create clusters .     Next: Define Infrastructure Templates",
            "title": "Create an App Based Credential"
        },
        {
            "location": "/azure-create/index.html",
            "text": "Create a Cluster on Azure\n\n\nTo create a cluster via CLoudbreak UI:\n\n\n\n\n\n\nLog in to the Cloudbreak UI.\n\n\n\n\n\n\nIn the top right corner, select the credential that you want to use to create a cluster:\n\n\n  \n\n\n\n\n\n\nClick \n+create cluster\n and the \nCreate cluster\n form is displayed.\n\n\n\n\n\n\nOn the \nConfigure Cluster\n page, provide the following parameters:\n\n\n\n\nTo view advanced options, click \nShow Advanced Options\n. To learn about advanced options, refer to \nAdvanced Options\n.\n\n\n\n\n\n\n\n\n\n\nParameter\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nCluster Name\n\n\nEnter a name for your cluster. The name must be between 5 and 40 characters, must start with a letter, must only include lowercase letters, numbers, and hyphens.\n\n\n\n\n\n\nTags\n\n\n(Optional) You can optionally add tags, which will help you find your cluster-related resources, such as VMs, in your cloud provider account.\n\n\n\n\n\n\nRegion\n\n\nSelect the region in which you would like to launch your cluster.\n\n\n\n\n\n\nSend Email When Cluster is Ready\n\n\n(Optional) Check this to receive an email each time the cluster status changes.\n\n\n\n\n\n\nPublic In Account\n\n\n(Optional) If this option is checked, all the users belonging to your account will be able to use this cluster, but they cannot delete it.\n\n\n\n\n\n\n\n\n\n\nBy default, Ambari Username and Ambari Password are set to \nadmin\n. You can override it in the \"\nConfigure Cluster\n\" tab.\n\n\n\n\n\n\n\n\nOn the \nSet up Network and Security\n page, provide the following parameters:\n\n\n\n\n\n\n\n\nParameter\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nNetwork\n\n\nSelect the virtual network in which you would like your cluster to be provisioned. You can define custom network configurations or use default network configurations.\n\n\n\n\n\n\nEnable Knox Gateway\n\n\n(Optional) Select this option to enable secure access to Ambari web UI and other cluster UIs via Knox gateway.\n\n\n\n\n\n\nEnable Kerberos Security\n\n\n(Optional) Select this option to enable Kerberos for your cluster. You will have an option to create a new kerberos or use an existing one. For more information refer to Kerberos \ndocumentation\n.\n\n\n\n\n\n\n\n\n\n\n\n\nOn the \nChoose Blueprint\n page, select the blueprint that you would like to use for your cluster. You can either choose one of the pre-configured blueprints, or add your own in the \nmanage blueprints\n tab.\n\n\nFor each host group you must provide the following:\n\n\n\n\n\n\n\n\nParameter\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nGroup Size\n\n\nEnter a number defining how many nodes to create per host group. Default is 1. The \"Group Size\" for that host group on which Ambari Server is installed must be set to \"1\".\n\n\n\n\n\n\nTemplate\n\n\nIf you have previously created a template for VMs and storage, you can select it here. If you don't make a selection, default will be used.\n\n\n\n\n\n\nSecurity Group\n\n\nIf you have previously created a template for a security group, you can select it here. If you don't make a selection, default will be used.\n\n\n\n\n\n\nAmbari Server\n\n\nYou must select one node for Ambari Server. The \"Group Size\" for that host group must be set to \"1\".\n\n\n\n\n\n\nRecipes\n\n\nYou can select a previously added recipe (custom script) to be executed on all nodes of the host group. Refer to \nRecipes\n.\n\n\n\n\n\n\n\n\n\n\n\n\nOn the \nAdd File System\n page, select to use one of the following filesystems:\n\n\n\n\nLocal HDFS\n: No external storage outside of HDFS will be used\n\n\nWindows Azure Data Lake Storage\n: If you select this option, you must provide \nData Lake Store account name\n.  \n\n\nYou must configure access control for Cloudbreak's service principal manually after cluster installation. You can obtain the service principal ID from the Cloudbreak UI. For more information, refer to \nAzure documentation\n.  \n\n\n\n\n\n\n\n\nWindows Azure Blob Storage\n: If you select this option, you must provide:\n\n\n\n\n\n\n\n\nParameter\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nStorage Account Name\n\n\nEnter your account name.\n\n\n\n\n\n\nStorage Account Access Key\n\n\nEnter your access key.\n\n\n\n\n\n\nUse File System As Default\n\n\nSelect this option if you want to make WASB your default file system, instead of HDFS.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nClick on \nReview and Launch\n and then \n+create and start cluster\n.\n\n\n\n\n\n\nYou will be redirected to the Cloudbreak dashboard, and a new tile representing your cluster will appear at the top of the page.\n\n\n\n\n\n\nAdvanced Options\n\n\nClick on \nShow Advanced Options\n to enter additional configuration options.\n\n\nConfigure Cluster\n\n\nYou can optionally configure the following advanced parameters:\n\n\n\n\n\n\n\n\nParameter\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nAmbari Username\n\n\nYou can log in to the Ambari UI using this username. By default, this is set to \nadmin\n.\n\n\n\n\n\n\nAmbari Password\n\n\nYou can log in to the Ambari UI using this password. By default, this is set to \nadmin\n.\n\n\n\n\n\n\nProvision Cluster\n\n\nSALT\n is pre-selected to provision your cluster.\n\n\n\n\n\n\nEnable availability sets\n\n\nAzure implements the concept of \navailability sets\n to support fault tolerance for VM's. You can enable availability sets by using the checkbox, and then add the desired availability sets by providing a name and the desired fault domain count (2 or 3). Next, these defined availability sets can be assigned to specific host groups in the \nChoose Blueprint\n tab. For more information about availability sets, refer to \nAvailability Sets\n.\n\n\n\n\n\n\nEnable Lifetime Management\n\n\nCheck this option if you would like your cluster to be automatically terminated after a specific amount of time (defined as \"Time to Live\" in minuter) has passed.\n\n\n\n\n\n\nFlex Subscription\n\n\nThis option will appear if you have configured your deployment for a \nFlex Subscription\n.\n\n\n\n\n\n\n\n\nChoose Blueprint\n\n\nAfter selecting a blueprint, you can optionally configure the following advanced parameters:\n\n\n\n\n\n\n\n\nParameter\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nConfig Recommendation Strategy (Stack Advisor)\n\n\nSelect how configuration recommendations generated by stack advisor will be applied. Select one of \nALWAYS_APPLY: Configuration recommendations will be applied automatically.\nALWAYS_APPLY_DONT_OVERRIDE_CUSTOM_VALUES: Configuration recommendations will be applied automatically, but they will be ignored for custom configurations.\nNEVER_APPLY: Configuration recommendations will be ignored.\nONLY_STACK_DEFAULTS_APPLY: Configuration recommendations will be applied only on the default configurations for all included services.\n\n\n\n\n\n\nValidate Blueprint\n\n\nSelect to validate the blueprint.\n\n\n\n\n\n\n\n\nAdd File System\n\n\nAfter selecting the filesystem, you can optionally configure the following advanced parameters:\n\n\n\n\n\n\n\n\nParameter\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nAttached Storage Type\n\n\nSelect \nsingle storage for all VMs\n or \nseparate storage for every VM\n. Selecting single storage means that your whole cluster's OS disks will be placed in one storage account. Using separate storage for every VM will deploy as many storage accounts as the number of nodes in your cluster, avoiding the IOPS limit of a particular storage account.\n\n\n\n\n\n\nPersistent Storage Name\n\n\nEnter a name for the persistent storage directory. Default is \ncbstore\n.\n\n\n\n\n\n\n\n\nChoose Failure Action\n\n\nYou can optionally select what to do if cluster creation fails or if there aren't enough instances available to create all requested nodes:\n\n\n\n\n\n\n\n\nParameter\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nFailure Action\n\n\nSelect one of: \ndo NOT rollback resources\n (default) or \nrollback resources\n. \nBy default, if creating a cluster fails, the Azure resources that were created up to that point will not be rolled back. This means that they will remain accessible for troubleshooting and you will need to to delete them manually.\n\n\n\n\n\n\nMinimum Cluster Size\n\n\nThis defines the provisioning strategy in case the cloud provider cannot allocate all the requested nodes. Select \nbest effort\n or \nexact\n.\n\n\n\n\n\n\n\n\nConfigure Ambari Repos\n\n\nYou can optionally configure a different version of Ambari than the default by providing the following information:\n\n\n\n\n\n\n\n\nParameter\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nAmbari Version\n\n\nEnter Ambari version.\n\n\n\n\n\n\nAmbari Repo URL\n\n\nEnter Ambari repo URL.\n\n\n\n\n\n\nAmbari Repo Gpg Key URL\n\n\nEnter gpgkey URL.\n\n\n\n\n\n\n\n\nConfigure HDP Repos\n\n\nYou can optionally configure a different version of HDP than the default by providing the following information:\n\n\n\n\n\n\n\n\nParameter\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nStack\n\n\nEnter stack name.\n\n\n\n\n\n\nVersion\n\n\nEnter stack version.\n\n\n\n\n\n\nStack Repo ID\n\n\nEnter stack repo ID.\n\n\n\n\n\n\nBase URL\n\n\nEner stack repo base URL.\n\n\n\n\n\n\nUtils Repo ID\n\n\nEnter Utils repo ID.\n\n\n\n\n\n\nUtils Base URL\n\n\nEnter Utils repo base URL.\n\n\n\n\n\n\nVerify\n\n\nSelect to verify the repo information.\n\n\n\n\n\n\n\n\nConfigure Ambari Database\n\n\nBy default, Ambari stores data on an embedded database, which is sufficient for ephemeral or test clusters. However, as Ambari and Cloudbreak don't perform backups of this database, it is insufficient for long-running production clusters, and you may need to configure a remote database for Ambari and Cloudbreak.\n\n\n\n\n\n\n\n\nParameter\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nVendor\n\n\nSelect database vendor from the list.\n\n\n\n\n\n\nHost\n\n\nEnter database host IP.\n\n\n\n\n\n\nPort\n\n\nEnter port number.\n\n\n\n\n\n\nName\n\n\nEnter database name.\n\n\n\n\n\n\nUser Name\n\n\nEnter database user name.\n\n\n\n\n\n\nPassword\n\n\nEnter database password.\n\n\n\n\n\n\n\n\nAvailability Sets\n\n\nTo support fault tolerance for VMs, Azure introduced the concept of \navailability sets\n. This allows two or more VMs to be mapped to multiple fault domains, each of which defines a group of virtual machines that share a common power source and a network switch. When adding VMs to an availability set, Azure automatically assigns each VM a fault domain. This SLA includes guarantees that during OS Patching in Azure or during maintenance operations, at least one VM belonging to a given fault domain will be available.\n\n\nIn Cloudbreak UI, availability sets can be configured during cluster creation:\n\n\n\n\n\n\nEnable availability sets using the checkbox on the \nConfigure Cluster\n page. \n\n\n\n\n\n\nAdd the desired availability sets by providing a name and the desired fault domain count (2 or 3).\n\n\n\n\n\n\nThe sets defined here can be assigned to the host groups on the \nChoose Blueprint\n page. One availability set can be assigned to only one host group, so you should define in advance as many availability sets as needed for your host groups. The assignment of fault domains is automated by Azure, so there is no option for this in Cloudbreak UI.\n\n\n\n\nIMPORTANT\n: The availability sets should only be used when there is a group of two or more application-tier VMs. Single instances placed in an availability set are not subject to Azure\u2019s SLA, and you will not receive warnings of planned maintenance events. \n\n\n\n\n\n\n\n\nAfter the deployment is finished, you can check the layout of the VMs inside an availability set on Azure Portal. You will find the \"Availability set\" resources corresponding to the host groups inside the deployment's resource group. \n\n\n\n\n\n\n\n\nNext: Access Cloud Data",
            "title": "Create a Cluster"
        },
        {
            "location": "/azure-create/index.html#create-a-cluster-on-azure",
            "text": "To create a cluster via CLoudbreak UI:    Log in to the Cloudbreak UI.    In the top right corner, select the credential that you want to use to create a cluster:        Click  +create cluster  and the  Create cluster  form is displayed.    On the  Configure Cluster  page, provide the following parameters:   To view advanced options, click  Show Advanced Options . To learn about advanced options, refer to  Advanced Options .      Parameter  Description      Cluster Name  Enter a name for your cluster. The name must be between 5 and 40 characters, must start with a letter, must only include lowercase letters, numbers, and hyphens.    Tags  (Optional) You can optionally add tags, which will help you find your cluster-related resources, such as VMs, in your cloud provider account.    Region  Select the region in which you would like to launch your cluster.    Send Email When Cluster is Ready  (Optional) Check this to receive an email each time the cluster status changes.    Public In Account  (Optional) If this option is checked, all the users belonging to your account will be able to use this cluster, but they cannot delete it.      By default, Ambari Username and Ambari Password are set to  admin . You can override it in the \" Configure Cluster \" tab.     On the  Set up Network and Security  page, provide the following parameters:     Parameter  Description      Network  Select the virtual network in which you would like your cluster to be provisioned. You can define custom network configurations or use default network configurations.    Enable Knox Gateway  (Optional) Select this option to enable secure access to Ambari web UI and other cluster UIs via Knox gateway.    Enable Kerberos Security  (Optional) Select this option to enable Kerberos for your cluster. You will have an option to create a new kerberos or use an existing one. For more information refer to Kerberos  documentation .       On the  Choose Blueprint  page, select the blueprint that you would like to use for your cluster. You can either choose one of the pre-configured blueprints, or add your own in the  manage blueprints  tab.  For each host group you must provide the following:     Parameter  Description      Group Size  Enter a number defining how many nodes to create per host group. Default is 1. The \"Group Size\" for that host group on which Ambari Server is installed must be set to \"1\".    Template  If you have previously created a template for VMs and storage, you can select it here. If you don't make a selection, default will be used.    Security Group  If you have previously created a template for a security group, you can select it here. If you don't make a selection, default will be used.    Ambari Server  You must select one node for Ambari Server. The \"Group Size\" for that host group must be set to \"1\".    Recipes  You can select a previously added recipe (custom script) to be executed on all nodes of the host group. Refer to  Recipes .       On the  Add File System  page, select to use one of the following filesystems:   Local HDFS : No external storage outside of HDFS will be used  Windows Azure Data Lake Storage : If you select this option, you must provide  Data Lake Store account name .    You must configure access control for Cloudbreak's service principal manually after cluster installation. You can obtain the service principal ID from the Cloudbreak UI. For more information, refer to  Azure documentation .       Windows Azure Blob Storage : If you select this option, you must provide:     Parameter  Description      Storage Account Name  Enter your account name.    Storage Account Access Key  Enter your access key.    Use File System As Default  Select this option if you want to make WASB your default file system, instead of HDFS.         Click on  Review and Launch  and then  +create and start cluster .    You will be redirected to the Cloudbreak dashboard, and a new tile representing your cluster will appear at the top of the page.",
            "title": "Create a Cluster on Azure"
        },
        {
            "location": "/azure-create/index.html#advanced-options",
            "text": "Click on  Show Advanced Options  to enter additional configuration options.",
            "title": "Advanced Options"
        },
        {
            "location": "/azure-create/index.html#configure-cluster",
            "text": "You can optionally configure the following advanced parameters:     Parameter  Description      Ambari Username  You can log in to the Ambari UI using this username. By default, this is set to  admin .    Ambari Password  You can log in to the Ambari UI using this password. By default, this is set to  admin .    Provision Cluster  SALT  is pre-selected to provision your cluster.    Enable availability sets  Azure implements the concept of  availability sets  to support fault tolerance for VM's. You can enable availability sets by using the checkbox, and then add the desired availability sets by providing a name and the desired fault domain count (2 or 3). Next, these defined availability sets can be assigned to specific host groups in the  Choose Blueprint  tab. For more information about availability sets, refer to  Availability Sets .    Enable Lifetime Management  Check this option if you would like your cluster to be automatically terminated after a specific amount of time (defined as \"Time to Live\" in minuter) has passed.    Flex Subscription  This option will appear if you have configured your deployment for a  Flex Subscription .",
            "title": "Configure Cluster"
        },
        {
            "location": "/azure-create/index.html#choose-blueprint",
            "text": "After selecting a blueprint, you can optionally configure the following advanced parameters:     Parameter  Description      Config Recommendation Strategy (Stack Advisor)  Select how configuration recommendations generated by stack advisor will be applied. Select one of  ALWAYS_APPLY: Configuration recommendations will be applied automatically. ALWAYS_APPLY_DONT_OVERRIDE_CUSTOM_VALUES: Configuration recommendations will be applied automatically, but they will be ignored for custom configurations. NEVER_APPLY: Configuration recommendations will be ignored. ONLY_STACK_DEFAULTS_APPLY: Configuration recommendations will be applied only on the default configurations for all included services.    Validate Blueprint  Select to validate the blueprint.",
            "title": "Choose Blueprint"
        },
        {
            "location": "/azure-create/index.html#add-file-system",
            "text": "After selecting the filesystem, you can optionally configure the following advanced parameters:     Parameter  Description      Attached Storage Type  Select  single storage for all VMs  or  separate storage for every VM . Selecting single storage means that your whole cluster's OS disks will be placed in one storage account. Using separate storage for every VM will deploy as many storage accounts as the number of nodes in your cluster, avoiding the IOPS limit of a particular storage account.    Persistent Storage Name  Enter a name for the persistent storage directory. Default is  cbstore .",
            "title": "Add File System"
        },
        {
            "location": "/azure-create/index.html#choose-failure-action",
            "text": "You can optionally select what to do if cluster creation fails or if there aren't enough instances available to create all requested nodes:     Parameter  Description      Failure Action  Select one of:  do NOT rollback resources  (default) or  rollback resources .  By default, if creating a cluster fails, the Azure resources that were created up to that point will not be rolled back. This means that they will remain accessible for troubleshooting and you will need to to delete them manually.    Minimum Cluster Size  This defines the provisioning strategy in case the cloud provider cannot allocate all the requested nodes. Select  best effort  or  exact .",
            "title": "Choose Failure Action"
        },
        {
            "location": "/azure-create/index.html#configure-ambari-repos",
            "text": "You can optionally configure a different version of Ambari than the default by providing the following information:     Parameter  Description      Ambari Version  Enter Ambari version.    Ambari Repo URL  Enter Ambari repo URL.    Ambari Repo Gpg Key URL  Enter gpgkey URL.",
            "title": "Configure Ambari Repos"
        },
        {
            "location": "/azure-create/index.html#configure-hdp-repos",
            "text": "You can optionally configure a different version of HDP than the default by providing the following information:     Parameter  Description      Stack  Enter stack name.    Version  Enter stack version.    Stack Repo ID  Enter stack repo ID.    Base URL  Ener stack repo base URL.    Utils Repo ID  Enter Utils repo ID.    Utils Base URL  Enter Utils repo base URL.    Verify  Select to verify the repo information.",
            "title": "Configure HDP Repos"
        },
        {
            "location": "/azure-create/index.html#configure-ambari-database",
            "text": "By default, Ambari stores data on an embedded database, which is sufficient for ephemeral or test clusters. However, as Ambari and Cloudbreak don't perform backups of this database, it is insufficient for long-running production clusters, and you may need to configure a remote database for Ambari and Cloudbreak.     Parameter  Description      Vendor  Select database vendor from the list.    Host  Enter database host IP.    Port  Enter port number.    Name  Enter database name.    User Name  Enter database user name.    Password  Enter database password.",
            "title": "Configure Ambari Database"
        },
        {
            "location": "/azure-create/index.html#availability-sets",
            "text": "To support fault tolerance for VMs, Azure introduced the concept of  availability sets . This allows two or more VMs to be mapped to multiple fault domains, each of which defines a group of virtual machines that share a common power source and a network switch. When adding VMs to an availability set, Azure automatically assigns each VM a fault domain. This SLA includes guarantees that during OS Patching in Azure or during maintenance operations, at least one VM belonging to a given fault domain will be available.  In Cloudbreak UI, availability sets can be configured during cluster creation:    Enable availability sets using the checkbox on the  Configure Cluster  page.     Add the desired availability sets by providing a name and the desired fault domain count (2 or 3).    The sets defined here can be assigned to the host groups on the  Choose Blueprint  page. One availability set can be assigned to only one host group, so you should define in advance as many availability sets as needed for your host groups. The assignment of fault domains is automated by Azure, so there is no option for this in Cloudbreak UI.   IMPORTANT : The availability sets should only be used when there is a group of two or more application-tier VMs. Single instances placed in an availability set are not subject to Azure\u2019s SLA, and you will not receive warnings of planned maintenance events.      After the deployment is finished, you can check the layout of the VMs inside an availability set on Azure Portal. You will find the \"Availability set\" resources corresponding to the host groups inside the deployment's resource group.      Next: Access Cloud Data",
            "title": "Availability Sets"
        },
        {
            "location": "/azure-data/index.html",
            "text": "Accessing Data on Azure\n\n\nHortonworks Data Platform (HDP) supports reading and writing both block blobs and page blobs\nfrom/to \nWindows Azure Storage Blob (WASB)\n object store, as well as reading and writing files stored in an\n\nAzure Data Lake Storage (ADLS)\n account. This allows you to:\n\n\n\n\nPersist data using cloud storage services beyond the lifetime of your HDP clusters.  \n\n\nLoad data in Hadoop ecosystem applications directly from Azure storage services, without first importing or uploading data from external resources to HDFS.  \n\n\nUse other applications (not necessarily in your Hadoop ecosystem) to manipulate the data stored in Azure storage services beyond the lifetime of your HDP clusters.  \n\n\nShare data between multiple HDP clusters fast and easily by pointing to the same Azure data sets. \n\n\nMove or copy data between different Azure storage services or between Azure storage services and HDFS to facilitate different scenarios for big data analytics workloads.  \n\n\nBack up unlimited archive data at any scale from HDP cluster to fully managed, durable, and highly available Azure storage services.   \n\n\n\n\nAccessing Data in ADLS\n\n\nAzure Data Lake Store (ADLS)\n is an enterprise-wide hyper-scale repository for big data analytic workloads.\n\n\nPrerequisites\n\n\nIf you want to use \nAzure Data Lake Store\n to store your data, you must enable Azure subscription for Data Lake Store, and then create an Azure Data Lake Store \nstorage account\n.\n\n\nConfiguring Access to ADLS\n\n\nADLS is not supported as a default file system, but access to data in ADLS is automatically configured if you select ADLS during \ncluster creation\n, on the \nAdd File System\n page. After the \ncluster is deployed\n, you must:\n\n\n\n\n\n\nDefine which parts of the ADLS store this cluster will have access by adding the client credentials for the cluster to the data access control for the ADLS account.\n \n\n\nCloudbreak automated the configuration of the cluster with ADLS with the exception of this last step. This last configuration option should not be automated, since you need to select which files in ADLS the cluster should access. For more information, review the \ndocumentation\n on the Microsoft Azure Portal. \n\n\n\n\nIn the Cloudbreak UI, navigate to the \nManage Credentials\n section, select the credential used to deploy your cluster, and copy the \"App Id\u201d associated with it.   \n\n\nNavigate to the Azure Portal > \nData Lake Store\n and select your account.\n\n\nSelect the \nData Explorer\n tab at the top and select the folder that you want the cluster to access. Select the root folder for full access. \n\n\nSelect the \nAccess\n tab at the top and then click \nAdd\n.\n\n\nClick \n+Add\n and paste the App Id (copied in step 1) in the search box to find client certificate name for the cluster.  (Alternatively, you can look up the App Name in the \nAzure Active Directory\n > \nApp Registrations\n).\n\n\nChoose the appropriate permissions. Note that if you do select the root folder, you need to provide \u201cexecute\u201d access to all parent directories. For more information, refer to the \nAzure documentation\n. \n\n\n\n\n\n\n\n\nTest access to ADLS.\n Review the next sections for information on how to access data in ADLS from the cluster once it is deployed, for example from the command line of the cluster name node.\n\n\n\n\n\n\nAccess Path\n\n\nADLS access path syntax is:\n\n\nadl://\naccount_name\n.azuredatalakestore.net/\ndir/file\n\n\n\nFor example, the following Hadoop FileSystem shell commands demonstrate access to a storage account named \"myaccount\":\n\n\nhadoop fs -mkdir adl://myaccount.azuredatalakestore.net/testdir\n\n\n\nhadoop fs -put testfile adl://myaccount.azuredatalakestore.net/testdir/testfile\n\n\n\nTo use DistCp against ADLS, use the following syntax:\n\nhadoop distcp\n    [-D hadoop.security.credential.provider.path=localjceks://file/home/user/adls.jceks]\n    hdfs://\nnamenode_hostname\n:9001/user/foo/007020615\n    adl://\nmyaccount\n.azuredatalakestore.net/testDir/\n\n\nAccessing Data in WASB\n\n\nWindows Azure Storage Blob (WASB) is an object store service available on Azure.\n\n\nPrerequisites\n\n\nIf you want to use Windows Azure Storage Blob to store your data, you must enable Azure subscription for Blob Storage, and then create a \nstorage account\n.  \n\n\nConfiguring Access to WASB\n\n\nIn order to access data stored in your Azure blob storage account, you must configure your storage account access key in \ncore-site.xml\n. The configuration property that you must use is \nfs.azure.account.key.<account name>.blob.core.windows.net\n and the value is the access key. \n\n\nFor example the following property should be used for a storage account called \"testaccount\": \n\n\n<property>\n  <name>fs.azure.account.key.testaccount.blob.core.windows.net</name>\n  <value>TESTACCOUNT-ACCESS-KEY</value>\n</property>\n\n\n\n\nYou can obtain your access key from the Access keys in your storage account settings.\n\n\nAlternatively, it is possible, although not recommended or supported, to configure \nfs.defaultFS\n to use a wasb or wasbs URL. This causes all bare paths, such as /testDir/testFile to resolve automatically to that file system.\n\n\nAccess Path\n\n\nWASB access path syntax is:\n\n\nwasb://\ncontainer_name\n@\nstorage_account_name\n.blob.core.windows.net/\ndir/file\n\n\n\nFor example, to access a file called \"testfile\" located in a directory called \"testdir\", stored in the container called \"testcontainer\" on the account called \"hortonworks\", the URL is:\n\n\nwasb://testcontainer@hortonworks.blob.core.windows.net/testdir/testfile\n\n\n\nYou can also use \"wasbs\" prefix to utilize SSL-encrypted HTTPS access:\n\n\nwasbs://\n@\n.blob.core.windows.net/dir/file\n\n\n\nThe following Hadoop FileSystem shell commands demonstrate access to a storage account named \"myaccount\" and a container named \"mycontainer\":\n\n\nhadoop fs -ls wasb://mycontainer@myaccount.blob.core.windows.net/\n\nhadoop fs -mkdir wasb://mycontainer@myaccount.blob.core.windows.net/testDir\n\nhadoop fs -put testFile wasb://mycontainer@myaccount.blob.core.windows.net/testDir/testFile\n\nhadoop fs -cat wasb://mycontainer@myaccount.blob.core.windows.net/testDir/testFile\ntest file content\n\n\n\nLearn More\n\n\nFor more information about configuring the ADLS and WASB connectors and working with data stored in ADLS and WASB, refer to \nCloud Data Access\n documentation.\n\n\n\n\nNext: Access Cluster",
            "title": "Access Data on Azure"
        },
        {
            "location": "/azure-data/index.html#accessing-data-on-azure",
            "text": "Hortonworks Data Platform (HDP) supports reading and writing both block blobs and page blobs\nfrom/to  Windows Azure Storage Blob (WASB)  object store, as well as reading and writing files stored in an Azure Data Lake Storage (ADLS)  account. This allows you to:   Persist data using cloud storage services beyond the lifetime of your HDP clusters.    Load data in Hadoop ecosystem applications directly from Azure storage services, without first importing or uploading data from external resources to HDFS.    Use other applications (not necessarily in your Hadoop ecosystem) to manipulate the data stored in Azure storage services beyond the lifetime of your HDP clusters.    Share data between multiple HDP clusters fast and easily by pointing to the same Azure data sets.   Move or copy data between different Azure storage services or between Azure storage services and HDFS to facilitate different scenarios for big data analytics workloads.    Back up unlimited archive data at any scale from HDP cluster to fully managed, durable, and highly available Azure storage services.",
            "title": "Accessing Data on Azure"
        },
        {
            "location": "/azure-data/index.html#accessing-data-in-adls",
            "text": "Azure Data Lake Store (ADLS)  is an enterprise-wide hyper-scale repository for big data analytic workloads.",
            "title": "Accessing Data in ADLS"
        },
        {
            "location": "/azure-data/index.html#prerequisites",
            "text": "If you want to use  Azure Data Lake Store  to store your data, you must enable Azure subscription for Data Lake Store, and then create an Azure Data Lake Store  storage account .",
            "title": "Prerequisites"
        },
        {
            "location": "/azure-data/index.html#configuring-access-to-adls",
            "text": "ADLS is not supported as a default file system, but access to data in ADLS is automatically configured if you select ADLS during  cluster creation , on the  Add File System  page. After the  cluster is deployed , you must:    Define which parts of the ADLS store this cluster will have access by adding the client credentials for the cluster to the data access control for the ADLS account.    Cloudbreak automated the configuration of the cluster with ADLS with the exception of this last step. This last configuration option should not be automated, since you need to select which files in ADLS the cluster should access. For more information, review the  documentation  on the Microsoft Azure Portal.    In the Cloudbreak UI, navigate to the  Manage Credentials  section, select the credential used to deploy your cluster, and copy the \"App Id\u201d associated with it.     Navigate to the Azure Portal >  Data Lake Store  and select your account.  Select the  Data Explorer  tab at the top and select the folder that you want the cluster to access. Select the root folder for full access.   Select the  Access  tab at the top and then click  Add .  Click  +Add  and paste the App Id (copied in step 1) in the search box to find client certificate name for the cluster.  (Alternatively, you can look up the App Name in the  Azure Active Directory  >  App Registrations ).  Choose the appropriate permissions. Note that if you do select the root folder, you need to provide \u201cexecute\u201d access to all parent directories. For more information, refer to the  Azure documentation .      Test access to ADLS.  Review the next sections for information on how to access data in ADLS from the cluster once it is deployed, for example from the command line of the cluster name node.",
            "title": "Configuring Access to ADLS"
        },
        {
            "location": "/azure-data/index.html#access-path",
            "text": "ADLS access path syntax is:  adl:// account_name .azuredatalakestore.net/ dir/file  For example, the following Hadoop FileSystem shell commands demonstrate access to a storage account named \"myaccount\":  hadoop fs -mkdir adl://myaccount.azuredatalakestore.net/testdir  hadoop fs -put testfile adl://myaccount.azuredatalakestore.net/testdir/testfile  To use DistCp against ADLS, use the following syntax: hadoop distcp\n    [-D hadoop.security.credential.provider.path=localjceks://file/home/user/adls.jceks]\n    hdfs:// namenode_hostname :9001/user/foo/007020615\n    adl:// myaccount .azuredatalakestore.net/testDir/",
            "title": "Access Path"
        },
        {
            "location": "/azure-data/index.html#accessing-data-in-wasb",
            "text": "Windows Azure Storage Blob (WASB) is an object store service available on Azure.",
            "title": "Accessing Data in WASB"
        },
        {
            "location": "/azure-data/index.html#prerequisites_1",
            "text": "If you want to use Windows Azure Storage Blob to store your data, you must enable Azure subscription for Blob Storage, and then create a  storage account .",
            "title": "Prerequisites"
        },
        {
            "location": "/azure-data/index.html#configuring-access-to-wasb",
            "text": "In order to access data stored in your Azure blob storage account, you must configure your storage account access key in  core-site.xml . The configuration property that you must use is  fs.azure.account.key.<account name>.blob.core.windows.net  and the value is the access key.   For example the following property should be used for a storage account called \"testaccount\":   <property>\n  <name>fs.azure.account.key.testaccount.blob.core.windows.net</name>\n  <value>TESTACCOUNT-ACCESS-KEY</value>\n</property>  You can obtain your access key from the Access keys in your storage account settings.  Alternatively, it is possible, although not recommended or supported, to configure  fs.defaultFS  to use a wasb or wasbs URL. This causes all bare paths, such as /testDir/testFile to resolve automatically to that file system.",
            "title": "Configuring Access to WASB"
        },
        {
            "location": "/azure-data/index.html#access-path_1",
            "text": "WASB access path syntax is:  wasb:// container_name @ storage_account_name .blob.core.windows.net/ dir/file  For example, to access a file called \"testfile\" located in a directory called \"testdir\", stored in the container called \"testcontainer\" on the account called \"hortonworks\", the URL is:  wasb://testcontainer@hortonworks.blob.core.windows.net/testdir/testfile  You can also use \"wasbs\" prefix to utilize SSL-encrypted HTTPS access:  wasbs:// @ .blob.core.windows.net/dir/file  The following Hadoop FileSystem shell commands demonstrate access to a storage account named \"myaccount\" and a container named \"mycontainer\":  hadoop fs -ls wasb://mycontainer@myaccount.blob.core.windows.net/\n\nhadoop fs -mkdir wasb://mycontainer@myaccount.blob.core.windows.net/testDir\n\nhadoop fs -put testFile wasb://mycontainer@myaccount.blob.core.windows.net/testDir/testFile\n\nhadoop fs -cat wasb://mycontainer@myaccount.blob.core.windows.net/testDir/testFile\ntest file content",
            "title": "Access Path"
        },
        {
            "location": "/azure-data/index.html#learn-more",
            "text": "For more information about configuring the ADLS and WASB connectors and working with data stored in ADLS and WASB, refer to  Cloud Data Access  documentation.   Next: Access Cluster",
            "title": "Learn More"
        },
        {
            "location": "/gcp-launch/index.html",
            "text": "Launching Cloudbreak on GCP\n\n\nBefore launching Cloudbreak on Google Cloud, review and meet the prerequisites. Next, import Cloudbreak image, launch a VM, SSH to the VM, and start Cloudbreak. Once Cloudbreak is running, log in to the Cloudbreak UI and create a Cloudbreak credential. \n\n\nMeet the Prerequisites\n\n\nBefore launching Cloudbreak on GCP, you must meet the following prerequisites.\n\n\nGCP Account\n\n\nIn order to launch Cloudbreak on GCP, you must log in to your GCP account. If you don't have an account, you can create one at \nhttps://console.cloud.google.com\n.\n\n\nOnce you log in to your GCP account, you must either create a project or use an existing project. \n\n\nService Account\n\n\nIn order to launch clusters on GCP via Cloudbreak, you must have a Service Account that Cloudbreak can use to create resources. In addition, you must also have a P12 key associated with the account. If you need to create these, refer to \nGCP documentation\n on how to create a service account and generate a P12 key. \n\n\nOnce you have the service account that you want to use for Cloudbreak, make sure that your service account fulfills one of the following APIs are enabled for your service account:\n\n\n\n\nCompute Image User   \n\n\nCompute Instance Admin (v1)  \n\n\nCompute Network Admin  \n\n\nCompute Security Admin  \n\n\n\n\nA user with an \"Owner\" role can assign roles or access rules to service accounts from \nIAM & Admin\n > \nIAM\n. For example:\n\n\n \n\n\nVPC Network\n\n\nWhen launching Cloudbreak, you will be required to select an existing network in which Cloudbreak can be placed. The following ports must be open on the security group: 22 (SSH) and 443 (HTTPS). You may use the \ndefault\n network as long as the aforementioned ports are open. \n\n\nYou can manage networks under \nNetworking\n > \nVPC Networks\n. To edit ports, click on the network name and then click on \nAdd firewall rules\n.\n\n\nRegion and Zone\n\n\nDecide in which region and zone you would like to launch Cloudbreak. You can launch Cloudbreak and provision your clusters in all regions \nsupported by GCP\n.  \n\n\nClusters created via Cloudbreak can be in the same or different region as Cloudbreak; when you launch a cluster, you select the region in which to launch it. \n\n\nLaunch the VM\n\n\n\n\n\n\nLog in to Google Cloud Platform.\n\n\n\n\n\n\nOpen the \nGoogle Cloud Shell\n by clicking on the  \n icon in the top-right corner:\n\n\n \n\n\n\n\n\n\nImport the Cloudbreak deployer image by executing the following command: \n\n\ngcloud compute images create cloudbreak-deployer-1164-2017-08-29 --source-uri gs://sequenceiqimage/cloudbreak-deployer-1164-2017-08-29.tar.gz\n\n\n\n\n\n\n\n\n\n\nTO-DO: This should be generated automatically. \n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIn the GCP UI, from the \nProducts and services\n menu, select \nCompute Engine\n > \nImages\n.\n\n\n\n\n\n\nIn the search bar, type the name of the Cloudbreak deployer image that you imported earlier.\n\n\n\n\n\n\nSelect the image and then select \nCreate Instance\n:  \n\n\n  \n\n\n\n\n\n\nYou will be redirected to \nVM instances\n > \nCreate an instance\n form. Provide the following parameters for your VM:\n\n\n  \n\n\n\n\n\n\n\n\nParameter\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nName\n\n\nEnter a name for the VM.\n\n\n\n\n\n\nZone\n\n\nSelect the zone in which to launch the VM.\n\n\n\n\n\n\nMachine type\n\n\nThe minimum instance type suitable for Cloudbreak is \nn1-standard-2\n. The minimum requirements are 4GB RAM, 10GB disk, 2 cores.\n\n\n\n\n\n\nBoot disk\n\n\nVerify that the Cloudbreak deployer disk which you imported earlier is pre-selected.\n\n\n\n\n\n\n\n\n\n\n\n\nClick on \nManagement, disks, networking, SSH keys\n to view the options.\n\n\n\n\n\n\nUnder \nNetworking\n > \nNetwork interfaces\n, select the network in which you want to launch Cloudbreak. \n\n\n\n\n\n\nUnder \nSSH Keys\n, check \nBlock project-wise SSH keys\n and paste your public SSH key.\n\n\n\n\n\n\nClick \nCreate\n. \n\n\n\n\n\n\nSSH to the VM\n\n\nNow that your VM is ready, access it via SSH: \n\n\n\n\nUse a private key matching the public key that you added to your  project.\n\n\nThe SSH user is called \"cloudbreak\".\n\n\nYou can obtain the VM's IP address from \nCompute Engine\n > \nVM Instances\n, the \nExternal IP\n column.\n\n\n\n\nLaunch Cloudbreak Deployer\n\n\nAfter accessing the VM via SSH: \n\n\n\n\n\n\nNavigate to the cloudbreak-deployment directory:\n\n\ncd /var/lib/cloudbreak-deployment/\n\n\nThis directory contains configuration files and the supporting binaries for Cloudbreak deployer.\n\n\n\n\n\n\nInitialize your profile by creating a new file called \nProfile\n and adding the following content:\n\n\nexport UAA_DEFAULT_SECRET=MY-SECRET\nexport UAA_DEFAULT_USER_PW=MY-PASSWORDP\n  \n\n\nFor example: \n\n\nexport UAA_DEFAULT_SECRET=MySecret123\nexport UAA_DEFAULT_USER_PW=MySecurePassword123\n \n\n\n\n\nYou will need to provide the password when logging in to the Cloudbreak web UI and when using the Cloudbreak Shell. The secret will be used by Cloudbreak for authentication.  \n\n\n\n\n\n\n\n\nStart the Cloudbreak application by using the following command:\n\n\ncbd start\n\n\nThis will start the Docker containers and initialize the application. The first time you start the Coudbreak app, the process will take longer than usual due to the download of all the necessary docker images.\n\n\nThe \ncbd start\n command includes the \ncbd generate\n command which applies the following steps:\n\n\n\n\nCreates the \ndocker-compose.yml\n file, which describes the configuration of all the Docker containers needed for the Cloudbreak deployment.\n\n\nCreates the \nuaa.yml\n file, which holds the configuration of the identity server used to authenticate users with Cloudbreak.\n\n\n\n\n\n\nOnce the \ncbd start\n has finished, it returns the \"Uluwatu (Cloudbreak UI) url\" which you can later paste in your browser and log in to Cloudbreak web UI.\n\n\n\n\n\n\n\n\nCheck Cloudbreak deployer version and health: \n\n\ncbd doctor\n\n\n\n\n\n\nNext, check Cloudbreak Application logs: \n\n\ncbd logs cloudbreak\n\n\nYou should see a message like this in the log: \nStarted CloudbreakApplication in 36.823 seconds.\n Cloudbreak normally takes less than a minute to start.\n\n\n\n\n\n\nAccess Cloudbreak UI\n\n\n\n\n\n\nYou can log into the Cloudbreak application at  \nhttps://IP_Address\n. For example \nhttps://34.212.141.253\n.  You can obtain the VM's IP address from \nCompute Engine\n > \nVM Instances\n, the \nExternal IP\n column.\n\n\n\n\n\n\nConfirm the security exception to proceed to the Cloudbreak web UI.\n\n\nThe first time you access Cloudbreak UI, Cloudbreak will automatically generate a self-signed certificate, due to which your browser will warn you about an untrusted connection and will ask you to confirm a security exception.\n\n\n\n\n\n\nLog in to the Cloudbreak web UI: \n\n\n\n\nThe default username is \nadmin@example.com\n but you should sign up with your own email address.    \n\n\nThe password is the value of the \nUAA_DEFAULT_USER_PW\n variable that you configured in your \nProfile\n file when \nlaunching Cloudbreak deployer\n.\n\n\n\n\n  \n\n\n\n\n\n\nCreate Cloudbreak Credential\n\n\nCloudbreak works by connecting your GCP account through this credential, and then uses it to create resources on your behalf. Before you can start provisioning cluster using Cloudbreak, you must create a Cloudbreak credential.\n\n\n\n\n\n\nIn the Cloudbreak web UI, open the \nmanage credentials\n pane. \n\n\n\n\n\n\nClick \n+create credential\n. \n\n\n\n\n\n\nProvide the following information:\n\n\n\n\n\n\n\n\nParameter\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nName\n\n\nEnter a name for your credential.\n\n\n\n\n\n\nDescription\n\n\n(Optional) Enter a description.\n\n\n\n\n\n\nProject Id\n\n\nEnter the project ID. You can obtain it from your GCP account by clicking on the name of your project at the top of the page and copying the \nID\n.\n\n\n\n\n\n\nService Account Email Address\n\n\n\"Service account ID\" value for your service account created in prerequisites. You can find it on GCP at \nIAM & Admin\n > \nService accounts\n.\n\n\n\n\n\n\nService Account Private (p12) Key\n\n\nPaste the P12 key that you created in the prerequisites when creating a service account.\n\n\n\n\n\n\nSSH Public Key\n\n\nPaste your SSH public key.\n\n\n\n\n\n\nSelect Platform\n\n\n(Optional) Select a platform (if previously configured).\n\n\n\n\n\n\nPublic In Account\n\n\n(Optional) If you check this, other users added to your Cloudbreak instance will be able to use this credential to create clusters.\n\n\n\n\n\n\n\n\n\n\n\n\nClick \n+create credential\n.\n\n\n\n\n\n\nYour credential should now be displayed at the top of the page and in the \nmanage credentials\n tab.\n\n\nCongratulations! You've successfully launched and configured Cloudbreak. Now you can use Cloudbreak to \ncreate clusters\n. \n\n\n\n\n\n\n\n\nNext: Create a Cluster",
            "title": "Launch on GCP"
        },
        {
            "location": "/gcp-launch/index.html#launching-cloudbreak-on-gcp",
            "text": "Before launching Cloudbreak on Google Cloud, review and meet the prerequisites. Next, import Cloudbreak image, launch a VM, SSH to the VM, and start Cloudbreak. Once Cloudbreak is running, log in to the Cloudbreak UI and create a Cloudbreak credential.",
            "title": "Launching Cloudbreak on GCP"
        },
        {
            "location": "/gcp-launch/index.html#meet-the-prerequisites",
            "text": "Before launching Cloudbreak on GCP, you must meet the following prerequisites.",
            "title": "Meet the Prerequisites"
        },
        {
            "location": "/gcp-launch/index.html#gcp-account",
            "text": "In order to launch Cloudbreak on GCP, you must log in to your GCP account. If you don't have an account, you can create one at  https://console.cloud.google.com .  Once you log in to your GCP account, you must either create a project or use an existing project.",
            "title": "GCP Account"
        },
        {
            "location": "/gcp-launch/index.html#service-account",
            "text": "In order to launch clusters on GCP via Cloudbreak, you must have a Service Account that Cloudbreak can use to create resources. In addition, you must also have a P12 key associated with the account. If you need to create these, refer to  GCP documentation  on how to create a service account and generate a P12 key.   Once you have the service account that you want to use for Cloudbreak, make sure that your service account fulfills one of the following APIs are enabled for your service account:   Compute Image User     Compute Instance Admin (v1)    Compute Network Admin    Compute Security Admin     A user with an \"Owner\" role can assign roles or access rules to service accounts from  IAM & Admin  >  IAM . For example:",
            "title": "Service Account"
        },
        {
            "location": "/gcp-launch/index.html#vpc-network",
            "text": "When launching Cloudbreak, you will be required to select an existing network in which Cloudbreak can be placed. The following ports must be open on the security group: 22 (SSH) and 443 (HTTPS). You may use the  default  network as long as the aforementioned ports are open.   You can manage networks under  Networking  >  VPC Networks . To edit ports, click on the network name and then click on  Add firewall rules .",
            "title": "VPC Network"
        },
        {
            "location": "/gcp-launch/index.html#region-and-zone",
            "text": "Decide in which region and zone you would like to launch Cloudbreak. You can launch Cloudbreak and provision your clusters in all regions  supported by GCP .    Clusters created via Cloudbreak can be in the same or different region as Cloudbreak; when you launch a cluster, you select the region in which to launch it.",
            "title": "Region and Zone"
        },
        {
            "location": "/gcp-launch/index.html#launch-the-vm",
            "text": "Log in to Google Cloud Platform.    Open the  Google Cloud Shell  by clicking on the    icon in the top-right corner:       Import the Cloudbreak deployer image by executing the following command:   gcloud compute images create cloudbreak-deployer-1164-2017-08-29 --source-uri gs://sequenceiqimage/cloudbreak-deployer-1164-2017-08-29.tar.gz      TO-DO: This should be generated automatically.         In the GCP UI, from the  Products and services  menu, select  Compute Engine  >  Images .    In the search bar, type the name of the Cloudbreak deployer image that you imported earlier.    Select the image and then select  Create Instance :          You will be redirected to  VM instances  >  Create an instance  form. Provide the following parameters for your VM:         Parameter  Description      Name  Enter a name for the VM.    Zone  Select the zone in which to launch the VM.    Machine type  The minimum instance type suitable for Cloudbreak is  n1-standard-2 . The minimum requirements are 4GB RAM, 10GB disk, 2 cores.    Boot disk  Verify that the Cloudbreak deployer disk which you imported earlier is pre-selected.       Click on  Management, disks, networking, SSH keys  to view the options.    Under  Networking  >  Network interfaces , select the network in which you want to launch Cloudbreak.     Under  SSH Keys , check  Block project-wise SSH keys  and paste your public SSH key.    Click  Create .",
            "title": "Launch the VM"
        },
        {
            "location": "/gcp-launch/index.html#ssh-to-the-vm",
            "text": "Now that your VM is ready, access it via SSH:    Use a private key matching the public key that you added to your  project.  The SSH user is called \"cloudbreak\".  You can obtain the VM's IP address from  Compute Engine  >  VM Instances , the  External IP  column.",
            "title": "SSH to the VM"
        },
        {
            "location": "/gcp-launch/index.html#launch-cloudbreak-deployer",
            "text": "After accessing the VM via SSH:     Navigate to the cloudbreak-deployment directory:  cd /var/lib/cloudbreak-deployment/  This directory contains configuration files and the supporting binaries for Cloudbreak deployer.    Initialize your profile by creating a new file called  Profile  and adding the following content:  export UAA_DEFAULT_SECRET=MY-SECRET\nexport UAA_DEFAULT_USER_PW=MY-PASSWORDP     For example:   export UAA_DEFAULT_SECRET=MySecret123\nexport UAA_DEFAULT_USER_PW=MySecurePassword123     You will need to provide the password when logging in to the Cloudbreak web UI and when using the Cloudbreak Shell. The secret will be used by Cloudbreak for authentication.       Start the Cloudbreak application by using the following command:  cbd start  This will start the Docker containers and initialize the application. The first time you start the Coudbreak app, the process will take longer than usual due to the download of all the necessary docker images.  The  cbd start  command includes the  cbd generate  command which applies the following steps:   Creates the  docker-compose.yml  file, which describes the configuration of all the Docker containers needed for the Cloudbreak deployment.  Creates the  uaa.yml  file, which holds the configuration of the identity server used to authenticate users with Cloudbreak.    Once the  cbd start  has finished, it returns the \"Uluwatu (Cloudbreak UI) url\" which you can later paste in your browser and log in to Cloudbreak web UI.     Check Cloudbreak deployer version and health:   cbd doctor    Next, check Cloudbreak Application logs:   cbd logs cloudbreak  You should see a message like this in the log:  Started CloudbreakApplication in 36.823 seconds.  Cloudbreak normally takes less than a minute to start.",
            "title": "Launch Cloudbreak Deployer"
        },
        {
            "location": "/gcp-launch/index.html#access-cloudbreak-ui",
            "text": "You can log into the Cloudbreak application at   https://IP_Address . For example  https://34.212.141.253 .  You can obtain the VM's IP address from  Compute Engine  >  VM Instances , the  External IP  column.    Confirm the security exception to proceed to the Cloudbreak web UI.  The first time you access Cloudbreak UI, Cloudbreak will automatically generate a self-signed certificate, due to which your browser will warn you about an untrusted connection and will ask you to confirm a security exception.    Log in to the Cloudbreak web UI:    The default username is  admin@example.com  but you should sign up with your own email address.      The password is the value of the  UAA_DEFAULT_USER_PW  variable that you configured in your  Profile  file when  launching Cloudbreak deployer .",
            "title": "Access Cloudbreak UI"
        },
        {
            "location": "/gcp-launch/index.html#create-cloudbreak-credential",
            "text": "Cloudbreak works by connecting your GCP account through this credential, and then uses it to create resources on your behalf. Before you can start provisioning cluster using Cloudbreak, you must create a Cloudbreak credential.    In the Cloudbreak web UI, open the  manage credentials  pane.     Click  +create credential .     Provide the following information:     Parameter  Description      Name  Enter a name for your credential.    Description  (Optional) Enter a description.    Project Id  Enter the project ID. You can obtain it from your GCP account by clicking on the name of your project at the top of the page and copying the  ID .    Service Account Email Address  \"Service account ID\" value for your service account created in prerequisites. You can find it on GCP at  IAM & Admin  >  Service accounts .    Service Account Private (p12) Key  Paste the P12 key that you created in the prerequisites when creating a service account.    SSH Public Key  Paste your SSH public key.    Select Platform  (Optional) Select a platform (if previously configured).    Public In Account  (Optional) If you check this, other users added to your Cloudbreak instance will be able to use this credential to create clusters.       Click  +create credential .    Your credential should now be displayed at the top of the page and in the  manage credentials  tab.  Congratulations! You've successfully launched and configured Cloudbreak. Now you can use Cloudbreak to  create clusters .      Next: Create a Cluster",
            "title": "Create Cloudbreak Credential"
        },
        {
            "location": "/gcp-create/index.html",
            "text": "Create a Cluster on GCP\n\n\nTo create a cluster via CLoudbreak UI:\n\n\n\n\n\n\nLog in to the Cloudbreak UI.\n\n\n\n\n\n\nIn the top right corner, select the credential that you want to use to create a cluster:\n\n\n  \n\n\n\n\n\n\nClick \n+create cluster\n and the \nCreate cluster\n form is displayed.\n\n\n\n\n\n\nOn the \nConfigure Cluster\n page, provide the following parameters:\n\n\n\n\nTo view advanced options, click \nShow Advanced Options\n. To learn about advanced options, refer to \nAdvanced Options\n.\n\n\n\n\n\n\n\n\n\n\nParameter\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nCluster Name\n\n\nEnter a name for your cluster. The name must be between 5 and 40 characters, must start with a letter, must only include lowercase letters, numbers, and hyphens.\n\n\n\n\n\n\nTags\n\n\n(Optional) You can optionally add tags, which will help you find your cluster-related resources, such as VMs, in your cloud provider account.\n\n\n\n\n\n\nRegion\n\n\nSelect the region in which you would like to launch your cluster.\n\n\n\n\n\n\nAvailability Zone\n\n\nSelect the availability zone in which you would like to launch your cluster.\n\n\n\n\n\n\nSend Email When Cluster is Ready\n\n\n(Optional) Check this to receive an email each time the cluster status changes.\n\n\n\n\n\n\nPublic In Account\n\n\n(Optional) If this option is checked, all the users belonging to your account will be able to use this cluster, but they cannot delete it.\n\n\n\n\n\n\n\n\n\n\nBy default, Ambari Username and Ambari Password are set to \nadmin\n. You can override it in the \"\nConfigure Cluster\n\" tab.\n\n\n\n\n\n\n\n\nOn the \nSet up Network and Security\n page, provide the following parameters:\n\n\n\n\n\n\n\n\nParameter\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nNetwork\n\n\nSelect the virtual network in which you would like your cluster to be provisioned. You can define custom network configurations or use default network configurations.\n\n\n\n\n\n\nEnable Knox Gateway\n\n\n(Optional) Select this option to enable secure access to Ambari web UI and other cluster UIs via Knox gateway.\n\n\n\n\n\n\nEnable Kerberos Security\n\n\n(Optional) Select this option to enable Kerberos for your cluster. You will have an option to create a new kerberos or use an existing one. For more information refer to Kerberos \ndocumentation\n.\n\n\n\n\n\n\n\n\n\n\n\n\nOn the \nChoose Blueprint\n page, select the blueprint that you would like to use for your cluster. You can either choose one of the pre-configured blueprints, or add your own in the \nmanage blueprints\n tab.\n\n\nFor each host group you must provide the following:\n\n\n\n\n\n\n\n\nParameter\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nGroup Size\n\n\nEnter a number defining how many nodes to create per host group. Default is 1. The \"Group Size\" for that host group on which Ambari Server is installed must be set to \"1\".\n\n\n\n\n\n\nTemplate\n\n\nIf you have previously created a template for VMs and storage, you can select it here. If you don't make a selection, default will be used.\n\n\n\n\n\n\nSecurity Group\n\n\nIf you have previously created a template for a security group, you can select it here. If you don't make a selection, default will be used.\n\n\n\n\n\n\nAmbari Server\n\n\nYou must select one node for Ambari Server. The \"Group Size\" for that host group must be set to \"1\".\n\n\n\n\n\n\nRecipes\n\n\nYou can select a previously added recipe (custom script) to be executed on all nodes of the host group. Refer to \nRecipes\n.\n\n\n\n\n\n\n\n\n\n\n\n\nOn the \nAdd File System\n page, select to use one of the following filesystems:\n\n\n\n\nLocal HDFS\n: No external storage outside of HDFS will be used\n\n\n\n\nGCS file system\n: If you select to use Google Cloud Storage option, you must provide:\n\n\n\n\n\n\n\n\nParameter\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nProject Id\n\n\nThe project ID registered when creating a credential should be pre-populated.\n\n\n\n\n\n\nService Account Email Address\n\n\nThe email address registered when creating a credential should be pre-populated.\n\n\n\n\n\n\nDefault Bucket Name\n\n\n(Deprecated) The name of an existing Google Cloud Storage bucket. This is an optional and deprecated configuration parameter (mapped to \"fs.gs.system.bucket\" in core-site.xml) to set the GCS bucket as a default bucket for URIs without having to specify the \"gs:\" prefix.  For more information about the \nGCS file system\n and \nbucket naming\n, refer to  GCP documentation.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nClick on \nReview and Launch\n and then \n+create and start cluster\n.\n\n\n\n\n\n\nYou will be redirected to the Cloudbreak dashboard, and a new tile representing your cluster will appear at the top of the page.\n\n\n\n\n\n\nAdvanced Options\n\n\nClick on \nShow Advanced Options\n to enter additional configuration options.\n\n\nConfigure Cluster\n\n\nYou can optionally configure the following advanced parameters:\n\n\n\n\n\n\n\n\nParameter\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nAmbari Username\n\n\nYou can log in to the Ambari UI using this username. By default, this is set to \nadmin\n.\n\n\n\n\n\n\nAmbari Password\n\n\nYou can log in to the Ambari UI using this password. By default, this is set to \nadmin\n.\n\n\n\n\n\n\nProvision Cluster\n\n\nSALT\n is pre-selected to provision your cluster.\n\n\n\n\n\n\nEnable Lifetime Management\n\n\nCheck this option if you would like your cluster to be automatically terminated after a specific amount of time (defined as \"Time to Live\" in minuter) has passed.\n\n\n\n\n\n\nFlex Subscription\n\n\nThis option will appear if you have configured your deployment for a \nFlex Subscription\n.\n\n\n\n\n\n\n\n\nChoose Blueprint\n\n\nAfter selecting a blueprint, you can optionally configure the following advanced parameters:\n\n\n\n\n\n\n\n\nParameter\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nConfig Recommendation Strategy (Stack Advisor)\n\n\nSelect how configuration recommendations generated by stack advisor will be applied. Select one of \nALWAYS_APPLY: Configuration recommendations will be applied automatically.\nALWAYS_APPLY_DONT_OVERRIDE_CUSTOM_VALUES: Configuration recommendations will be applied automatically, but they will be ignored for custom configurations.\nNEVER_APPLY: Configuration recommendations will be ignored.\nONLY_STACK_DEFAULTS_APPLY: Configuration recommendations will be applied only on the default configurations for all included services.\n\n\n\n\n\n\nValidate Blueprint\n\n\nSelect to validate the blueprint.\n\n\n\n\n\n\n\n\nChoose Failure Action\n\n\nYou can optionally select what to do if cluster creation fails or if there aren't enough instances available to create all requested nodes:\n\n\n\n\n\n\n\n\nParameter\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nFailure Action\n\n\nSelect one of: \ndo NOT rollback resources\n (default) or \nrollback resources\n. \nBy default, if creating a cluster fails, the Azure resources that were created up to that point will not be rolled back. This means that they will remain accessible for troubleshooting and you will need to to delete them manually.\n\n\n\n\n\n\nMinimum Cluster Size\n\n\nThis defines the provisioning strategy in case the cloud provider cannot allocate all the requested nodes. Select \nbest effort\n or \nexact\n.\n\n\n\n\n\n\n\n\nConfigure Ambari Repos\n\n\nYou can optionally configure a different version of Ambari than the default by providing the following information:\n\n\n\n\n\n\n\n\nParameter\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nAmbari Version\n\n\nEnter Ambari version.\n\n\n\n\n\n\nAmbari Repo URL\n\n\nEnter Ambari repo URL.\n\n\n\n\n\n\nAmbari Repo Gpg Key URL\n\n\nEnter gpgkey URL.\n\n\n\n\n\n\n\n\nConfigure HDP Repos\n\n\nYou can optionally configure a different version of HDP than the default by providing the following information:\n\n\n\n\n\n\n\n\nParameter\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nStack\n\n\nEnter stack name.\n\n\n\n\n\n\nVersion\n\n\nEnter stack version.\n\n\n\n\n\n\nStack Repo ID\n\n\nEnter stack repo ID.\n\n\n\n\n\n\nBase URL\n\n\nEner stack repo base URL.\n\n\n\n\n\n\nUtils Repo ID\n\n\nEnter Utils repo ID.\n\n\n\n\n\n\nUtils Base URL\n\n\nEnter Utils repo base URL.\n\n\n\n\n\n\nVerify\n\n\nSelect to verify the repo information.\n\n\n\n\n\n\n\n\nConfigure Ambari Database\n\n\nBy default, Ambari stores data on an embedded database, which is sufficient for ephemeral or test clusters. However, as Ambari and Cloudbreak don't perform backups of this database, it is insufficient for long-running production clusters, and you may need to configure a remote database for Ambari and Cloudbreak.\n\n\n\n\n\n\n\n\nParameter\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nVendor\n\n\nSelect database vendor from the list.\n\n\n\n\n\n\nHost\n\n\nEnter database host IP.\n\n\n\n\n\n\nPort\n\n\nEnter port number.\n\n\n\n\n\n\nName\n\n\nEnter database name.\n\n\n\n\n\n\nUser Name\n\n\nEnter database user name.\n\n\n\n\n\n\nPassword\n\n\nEnter database password.\n\n\n\n\n\n\n\n\n\n\nNext: Access Cluster",
            "title": "Create a Cluster"
        },
        {
            "location": "/gcp-create/index.html#create-a-cluster-on-gcp",
            "text": "To create a cluster via CLoudbreak UI:    Log in to the Cloudbreak UI.    In the top right corner, select the credential that you want to use to create a cluster:        Click  +create cluster  and the  Create cluster  form is displayed.    On the  Configure Cluster  page, provide the following parameters:   To view advanced options, click  Show Advanced Options . To learn about advanced options, refer to  Advanced Options .      Parameter  Description      Cluster Name  Enter a name for your cluster. The name must be between 5 and 40 characters, must start with a letter, must only include lowercase letters, numbers, and hyphens.    Tags  (Optional) You can optionally add tags, which will help you find your cluster-related resources, such as VMs, in your cloud provider account.    Region  Select the region in which you would like to launch your cluster.    Availability Zone  Select the availability zone in which you would like to launch your cluster.    Send Email When Cluster is Ready  (Optional) Check this to receive an email each time the cluster status changes.    Public In Account  (Optional) If this option is checked, all the users belonging to your account will be able to use this cluster, but they cannot delete it.      By default, Ambari Username and Ambari Password are set to  admin . You can override it in the \" Configure Cluster \" tab.     On the  Set up Network and Security  page, provide the following parameters:     Parameter  Description      Network  Select the virtual network in which you would like your cluster to be provisioned. You can define custom network configurations or use default network configurations.    Enable Knox Gateway  (Optional) Select this option to enable secure access to Ambari web UI and other cluster UIs via Knox gateway.    Enable Kerberos Security  (Optional) Select this option to enable Kerberos for your cluster. You will have an option to create a new kerberos or use an existing one. For more information refer to Kerberos  documentation .       On the  Choose Blueprint  page, select the blueprint that you would like to use for your cluster. You can either choose one of the pre-configured blueprints, or add your own in the  manage blueprints  tab.  For each host group you must provide the following:     Parameter  Description      Group Size  Enter a number defining how many nodes to create per host group. Default is 1. The \"Group Size\" for that host group on which Ambari Server is installed must be set to \"1\".    Template  If you have previously created a template for VMs and storage, you can select it here. If you don't make a selection, default will be used.    Security Group  If you have previously created a template for a security group, you can select it here. If you don't make a selection, default will be used.    Ambari Server  You must select one node for Ambari Server. The \"Group Size\" for that host group must be set to \"1\".    Recipes  You can select a previously added recipe (custom script) to be executed on all nodes of the host group. Refer to  Recipes .       On the  Add File System  page, select to use one of the following filesystems:   Local HDFS : No external storage outside of HDFS will be used   GCS file system : If you select to use Google Cloud Storage option, you must provide:     Parameter  Description      Project Id  The project ID registered when creating a credential should be pre-populated.    Service Account Email Address  The email address registered when creating a credential should be pre-populated.    Default Bucket Name  (Deprecated) The name of an existing Google Cloud Storage bucket. This is an optional and deprecated configuration parameter (mapped to \"fs.gs.system.bucket\" in core-site.xml) to set the GCS bucket as a default bucket for URIs without having to specify the \"gs:\" prefix.  For more information about the  GCS file system  and  bucket naming , refer to  GCP documentation.         Click on  Review and Launch  and then  +create and start cluster .    You will be redirected to the Cloudbreak dashboard, and a new tile representing your cluster will appear at the top of the page.",
            "title": "Create a Cluster on GCP"
        },
        {
            "location": "/gcp-create/index.html#advanced-options",
            "text": "Click on  Show Advanced Options  to enter additional configuration options.",
            "title": "Advanced Options"
        },
        {
            "location": "/gcp-create/index.html#configure-cluster",
            "text": "You can optionally configure the following advanced parameters:     Parameter  Description      Ambari Username  You can log in to the Ambari UI using this username. By default, this is set to  admin .    Ambari Password  You can log in to the Ambari UI using this password. By default, this is set to  admin .    Provision Cluster  SALT  is pre-selected to provision your cluster.    Enable Lifetime Management  Check this option if you would like your cluster to be automatically terminated after a specific amount of time (defined as \"Time to Live\" in minuter) has passed.    Flex Subscription  This option will appear if you have configured your deployment for a  Flex Subscription .",
            "title": "Configure Cluster"
        },
        {
            "location": "/gcp-create/index.html#choose-blueprint",
            "text": "After selecting a blueprint, you can optionally configure the following advanced parameters:     Parameter  Description      Config Recommendation Strategy (Stack Advisor)  Select how configuration recommendations generated by stack advisor will be applied. Select one of  ALWAYS_APPLY: Configuration recommendations will be applied automatically. ALWAYS_APPLY_DONT_OVERRIDE_CUSTOM_VALUES: Configuration recommendations will be applied automatically, but they will be ignored for custom configurations. NEVER_APPLY: Configuration recommendations will be ignored. ONLY_STACK_DEFAULTS_APPLY: Configuration recommendations will be applied only on the default configurations for all included services.    Validate Blueprint  Select to validate the blueprint.",
            "title": "Choose Blueprint"
        },
        {
            "location": "/gcp-create/index.html#choose-failure-action",
            "text": "You can optionally select what to do if cluster creation fails or if there aren't enough instances available to create all requested nodes:     Parameter  Description      Failure Action  Select one of:  do NOT rollback resources  (default) or  rollback resources .  By default, if creating a cluster fails, the Azure resources that were created up to that point will not be rolled back. This means that they will remain accessible for troubleshooting and you will need to to delete them manually.    Minimum Cluster Size  This defines the provisioning strategy in case the cloud provider cannot allocate all the requested nodes. Select  best effort  or  exact .",
            "title": "Choose Failure Action"
        },
        {
            "location": "/gcp-create/index.html#configure-ambari-repos",
            "text": "You can optionally configure a different version of Ambari than the default by providing the following information:     Parameter  Description      Ambari Version  Enter Ambari version.    Ambari Repo URL  Enter Ambari repo URL.    Ambari Repo Gpg Key URL  Enter gpgkey URL.",
            "title": "Configure Ambari Repos"
        },
        {
            "location": "/gcp-create/index.html#configure-hdp-repos",
            "text": "You can optionally configure a different version of HDP than the default by providing the following information:     Parameter  Description      Stack  Enter stack name.    Version  Enter stack version.    Stack Repo ID  Enter stack repo ID.    Base URL  Ener stack repo base URL.    Utils Repo ID  Enter Utils repo ID.    Utils Base URL  Enter Utils repo base URL.    Verify  Select to verify the repo information.",
            "title": "Configure HDP Repos"
        },
        {
            "location": "/gcp-create/index.html#configure-ambari-database",
            "text": "By default, Ambari stores data on an embedded database, which is sufficient for ephemeral or test clusters. However, as Ambari and Cloudbreak don't perform backups of this database, it is insufficient for long-running production clusters, and you may need to configure a remote database for Ambari and Cloudbreak.     Parameter  Description      Vendor  Select database vendor from the list.    Host  Enter database host IP.    Port  Enter port number.    Name  Enter database name.    User Name  Enter database user name.    Password  Enter database password.      Next: Access Cluster",
            "title": "Configure Ambari Database"
        },
        {
            "location": "/os-launch/index.html",
            "text": "Launching Cloudbreak on OpenStack\n\n\nBefore launching Cloudbreak on OpenStack, review and meet the prerequisites. Next, import Cloudbreak image, launch a VM, SSH to the VM, and start Cloudbreak. Once Cloudbreak is running, log in to the Cloudbreak UI and create a Cloudbreak credential. \n\n\nMeet Minimum System Requirements\n\n\nBefore launching Cloudbreak on your OpenStack, make sure that your OpenStack deployment fulfills the following requirements.\n\n\nSupported Linux Distributions\n\n\nThe following versions of the \nRed Hat Distribution of OpenStack\n (RDO) are supported:\n\n\n\n\nJuno\n\n\nKilo\n\n\nLiberty\n\n\nMitaka\n\n\n\n\nStandard Modules\n\n\nCloudbreak requires that the following standard modules are installed and configured on OpenStack:\n\n\n\n\nKeystone V2 or Keystone V3  \n\n\nNeutron (Self-service and provider networking)  \n\n\nNova (KVM or Xen hypervisor)  \n\n\nGlance  \n\n\nCinder (Optional)  \n\n\nHeat (Optional but highly recommended, since provisioning through native API calls will be deprecated in the future)  \n\n\n\n\nMeet the Prerequisites\n\n\nBefore launching Cloudbreak on OpenStack, you must meet the following prerequisites.\n\n\nSSH Key Pair\n\n\nCreate a new SSH key pair or import an existing SSH key pair. \n\n\nSecurity Group\n\n\nIn order to launch Cloudbreak, you must have an existing security group with the following ports open: 22 (SSH) and 443 (HTTPS). \n\n\nFor information about OpenStack security groups, refer to the \nOpenStack Operations Guide\n.\n\n\nImport Images to OpenStack\n\n\nAn OpenStack administrator must perform these steps to add the Cloudbreak deployer and HDP images to your OpenStack deployment.\n\n\nImport Cloudbreak Deployer Image\n\n\n\n\n\n\nDownload the latest Cloudbreak deployer image to your local machine: \n\n\ncurl -O https://public-repo-1.hortonworks.com/HDP/cloudbreak/cloudbreak-deployer-1164-2017-08-25.img\n\n\n\n\n\n\nSet the following environment variables for the OpenStack image import: \n\n\nexport CBD_LATEST_IMAGE=cloudbreak-deployer-1164-2017-08-25.img\nexport OS_IMAGE_NAME=cloudbreak-deployer-1161-2017-06-15.img\nexport OS_USERNAME=your_os_user_name\nexport OS_AUTH_URL=your_authentication_url\nexport OS_TENANT_NAME=your_os_tenant_name\n\n\n\n\n\n\nImport the new image into your OpenStack:\n\n\nglance image-create --name \"$OS_IMAGE_NAME\" --file \"$CBD_LATEST_IMAGE\" --disk-format qcow2 --container-format bare --progress\n \n\n\n\n\n\n\nAfter performing the import, you should be able to see the Cloudbreak deployer image among your other OpenStack images. \n\n\n\n\n\n\n\n\n\n\nTO-DO: Some of this content was automatically generated. \n\n\n\n\n\n\n\n\n\n\nImport HDP Image\n\n\n\n\n\n\nDownload the latest HDP image to your local machine: \n\n\ncurl -O https://public-repo-1.hortonworks.com/HDP/cloudbreak/hdc-hdp--1706141444.img\n\n\n\n\n\n\nSet the following environment variables for the OpenStack image import: \n\n\nexport CB_LATEST_IMAGE=hdc-hdp--1706141444.img \nexport CB_LATEST_IMAGE_NAME=hdc-hdp--1705081316.img\nexport OS_USERNAME=your_os_user_name\nexport OS_AUTH_URL=your_authentication_url\nexport OS_TENANT_NAME=your_os_tenant_name\n\n\n\n\n\n\nImport the new image into your OpenStack:\n\n\nglance image-create --name \"$CB_LATEST_IMAGE_NAME\" --file \"$CB_LATEST_IMAGE\" --disk-format qcow2 --container-format bare --progress\n\n\n\n\n\n\nAfter performing the import, you should be able to see the Cloudbreak image among your OpenStack images. \n\n\n\n\n\n\n\n\n\n\nTO-DO: Some of this content was automatically generated.  \n\n\n\n\n\n\n\n\n\n\nLaunch the VM\n\n\nIn your OpenStack, launch and instance providing the following parameters:\n\n\n\n\nSelect a VM flavor which meets the following minimum requirements: 4GB RAM, 10GB disk, 2 cores.  \n\n\nSelect the Cloudbreak deployer image that you imported earlier and launch an instance using that image. \n\n\nSelect your SSH key pair.  \n\n\nSelect the security group which has the following ports open: 22 (SSH) and 443 (HTTPS). \n\n\nSelect your preconfigured network.  \n\n\n\n\nSSH to the VM\n\n\nNow that your VM is ready, access it via SSH: \n\n\n\n\nUse a private key matching the public key that you added to your OpenStack project.\n\n\nThe SSH user is called \"cloudbreak\".\n\n\nYou can obtain the VM's IP address from the details of your instance.\n\n\n\n\nInitialize Your Profile\n\n\nAfter accessing the VM via SSH: \n\n\n\n\n\n\nNavigate to the cloudbreak-deployment directory:\n\n\ncd /var/lib/cloudbreak-deployment/\n\n\nThis directory contains configuration files and the supporting binaries for Cloudbreak deployer.\n\n\n\n\n\n\nInitialize your profile by creating a new file called \nProfile\n and adding the following content:\n\n\nexport UAA_DEFAULT_SECRET=MY-SECRET\nexport UAA_DEFAULT_USER_PW=MY-PASSWORD\nexport PUBLIC_IP=VM-PUBLIC-IP\n  \n\n\nFor example: \n\n\nexport UAA_DEFAULT_SECRET=MySecret123\nexport UAA_DEFAULT_USER_PW=MySecurePassword123\nexport PUBLIC_IP=34.212.141.253\n \n\n\n\n\nYou will need to provide the password when logging in to the Cloudbreak web UI and when using the Cloudbreak Shell. The secret will be used by Cloudbreak for authentication.  \n\n\n\n\n\n\n\n\nPerform Optional Configurations\n\n\n\n\nThese configurations are optional. \n\n\n\n\nConfiguring a Self-Signed Certificate\n\n\nIf your OpenStack is secured with a self-signed certificate, you need to import that certificate into Cloudbreak, or else Cloudbreak won't be able to communicate with your OpenStack. \n\n\nTo import the certificate, place the certificate file in the \n/certs/trusted/\n directory:\n\n\n\n\nNavigate to the \ncerts\n directory (automatically generated).\n\n\nCreate the \ntrusted\n directory.\n\n\nCopy the certificate to the \ntrusted\n directory. \n\n\n\n\nCloudbreak will automatically pick up the certificate and import it into its truststore upon start.\n\n\nConfiguring Availability Zone and Region\n\n\nBy default, Cloudbreak uses \nRegionOne\n region with \nnova\n availability zone, but you can customize Cloudbreak deployment and enable multiple regions and availability zones by creating an \nopenstack-zone.json\n file in the \netc\n directory of Cloudbreak deployment (that is\n/var/lib/cloudbreak-deployment/etc/openstack-zone.json\n). If the etc directory does not exist in the Cloudbreak deployment directory, then create it. \n\n\nThe following is an example of \nopenstack-zone.json\n containing two regions and four availability zones:\n\n\n{\n  \"items\": [\n    {\n      \"name\": \"MyRegionOne\",\n      \"zones\": [ \"az1\", \"az2\", \"az3\"]\n    },\n    {\n      \"name\": \"MyRegionTwo\",\n      \"zones\": [ \"myaz\"]\n    }\n  ]\n}\n\n\n\n\n\n\nIf you are performing this after you have started cbd, perform \ncbd restart\n.  \n\n\n\n\nLaunch Cloudbreak Deployer\n\n\n\n\n\n\nStart the Cloudbreak application by using the following command:\n\n\ncbd start\n\n\nThis will start the Docker containers and initialize the application. The first time you start the Coudbreak app, the process will take longer than usual due to the download of all the necessary docker images.\n\n\nThe \ncbd start\n command includes the \ncbd generate\n command which applies the following steps:\n\n\n\n\nCreates the \ndocker-compose.yml\n file, which describes the configuration of all the Docker containers needed for the Cloudbreak deployment.\n\n\nCreates the \nuaa.yml\n file, which holds the configuration of the identity server used to authenticate users with Cloudbreak.\n\n\n\n\n\n\nOnce the \ncbd start\n has finished, it returns the \"Uluwatu (Cloudbreak UI) url\" which you can later paste in your browser and log in to Cloudbreak web UI.\n\n\n\n\n\n\n\n\nCheck Cloudbreak deployer version and health: \n\n\ncbd doctor\n\n\n\n\n\n\nNext, check Cloudbreak Application logs: \n\n\ncbd logs cloudbreak\n\n\nYou should see a message like this in the log: \nStarted CloudbreakApplication in 36.823 seconds.\n Cloudbreak normally takes less than a minute to start.\n\n\n\n\n\n\nAccess Cloudbreak UI\n\n\n\n\n\n\nYou can log into the Cloudbreak application at \nhttps://IP_Address\n where \"IP_Address\" if the public IP of your OpenStack VM. For example \nhttps://34.212.141.253\n.\n\n\n\n\n\n\nConfirm the security exception to proceed to the Cloudbreak web UI.\n\n\nThe first time you access Cloudbreak UI, Cloudbreak will automatically generate a self-signed certificate, due to which your browser will warn you about an untrusted connection and will ask you to confirm a security exception.\n\n\n\n\n\n\nLog in to the Cloudbreak web UI:\n\n\n\n\nThe default username is \nadmin@example.com\n but you should sign up with your own email address.\n\n\nThe password is the value of the \nUAA_DEFAULT_USER_PW\n variable that you configured in your \nProfile\n file when \nlaunching Cloudbreak deployer\n.\n\n\n\n\n  \n\n\n\n\n\n\nCreate Cloudbreak Credential\n\n\nCloudbreak works by connecting your OpenStack account through this credential, and then uses it to create resources on your behalf. Before you can start provisioning cluster using Cloudbreak, you must create a Cloudbreak credential.\n\n\n\n\n\n\nIn the Cloudbreak web UI, open the \nmanage credentials\n pane. \n\n\n\n\n\n\nClick \n+create credential\n. \n\n\n\n\n\n\nProvide the following information:\n\n\n\n\n\n\n\n\nParameter\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nKeystone Version\n\n\nSelect the keystone version.\n\n\n\n\n\n\nName\n\n\nEnter a name for your credential.\n\n\n\n\n\n\nDescription\n\n\n(Optional) Enter a description.\n\n\n\n\n\n\nUser\n\n\nEnter your OpenStack user name.\n\n\n\n\n\n\nPassword\n\n\nEnter your OpenStack password.\n\n\n\n\n\n\nTenant Name\n\n\nEnter the OpenStack tenant name.\n\n\n\n\n\n\nEndpoint\n\n\nEnter the OpenStack endpoint.\n\n\n\n\n\n\nAPI Facing\n\n\nSelect \npublic\n or \nprivate\n.\n\n\n\n\n\n\nSSH Public Key\n\n\nPaste your SSH public key.\n\n\n\n\n\n\nSelect Platform\n\n\n(Optional) Select a platform (if previously configured).\n\n\n\n\n\n\nPublic In Account\n\n\n(Optional) If you check this, other users added to your Cloudbreak instance will be able to use this credential to create clusters.\n\n\n\n\n\n\n\n\n\n\n\n\nClick \n+create credential\n.\n\n\n\n\n\n\nYour credential should now be displayed at the top of the page and in the \nmanage credentials\n tab.\n\n\nCongratulations! You've successfully launched and configured Cloudbreak. Now you can use Cloudbreak to \ncreate clusters\n. \n\n\n\n\n\n\n\n\nNext: Define Infrastructure Templates",
            "title": "Launch on Open Stack"
        },
        {
            "location": "/os-launch/index.html#launching-cloudbreak-on-openstack",
            "text": "Before launching Cloudbreak on OpenStack, review and meet the prerequisites. Next, import Cloudbreak image, launch a VM, SSH to the VM, and start Cloudbreak. Once Cloudbreak is running, log in to the Cloudbreak UI and create a Cloudbreak credential.",
            "title": "Launching Cloudbreak on OpenStack"
        },
        {
            "location": "/os-launch/index.html#meet-minimum-system-requirements",
            "text": "Before launching Cloudbreak on your OpenStack, make sure that your OpenStack deployment fulfills the following requirements.",
            "title": "Meet Minimum System Requirements"
        },
        {
            "location": "/os-launch/index.html#supported-linux-distributions",
            "text": "The following versions of the  Red Hat Distribution of OpenStack  (RDO) are supported:   Juno  Kilo  Liberty  Mitaka",
            "title": "Supported Linux Distributions"
        },
        {
            "location": "/os-launch/index.html#standard-modules",
            "text": "Cloudbreak requires that the following standard modules are installed and configured on OpenStack:   Keystone V2 or Keystone V3    Neutron (Self-service and provider networking)    Nova (KVM or Xen hypervisor)    Glance    Cinder (Optional)    Heat (Optional but highly recommended, since provisioning through native API calls will be deprecated in the future)",
            "title": "Standard Modules"
        },
        {
            "location": "/os-launch/index.html#meet-the-prerequisites",
            "text": "Before launching Cloudbreak on OpenStack, you must meet the following prerequisites.",
            "title": "Meet the Prerequisites"
        },
        {
            "location": "/os-launch/index.html#ssh-key-pair",
            "text": "Create a new SSH key pair or import an existing SSH key pair.",
            "title": "SSH Key Pair"
        },
        {
            "location": "/os-launch/index.html#security-group",
            "text": "In order to launch Cloudbreak, you must have an existing security group with the following ports open: 22 (SSH) and 443 (HTTPS).   For information about OpenStack security groups, refer to the  OpenStack Operations Guide .",
            "title": "Security Group"
        },
        {
            "location": "/os-launch/index.html#import-images-to-openstack",
            "text": "An OpenStack administrator must perform these steps to add the Cloudbreak deployer and HDP images to your OpenStack deployment.",
            "title": "Import Images to OpenStack"
        },
        {
            "location": "/os-launch/index.html#import-cloudbreak-deployer-image",
            "text": "Download the latest Cloudbreak deployer image to your local machine:   curl -O https://public-repo-1.hortonworks.com/HDP/cloudbreak/cloudbreak-deployer-1164-2017-08-25.img    Set the following environment variables for the OpenStack image import:   export CBD_LATEST_IMAGE=cloudbreak-deployer-1164-2017-08-25.img\nexport OS_IMAGE_NAME=cloudbreak-deployer-1161-2017-06-15.img\nexport OS_USERNAME=your_os_user_name\nexport OS_AUTH_URL=your_authentication_url\nexport OS_TENANT_NAME=your_os_tenant_name    Import the new image into your OpenStack:  glance image-create --name \"$OS_IMAGE_NAME\" --file \"$CBD_LATEST_IMAGE\" --disk-format qcow2 --container-format bare --progress      After performing the import, you should be able to see the Cloudbreak deployer image among your other OpenStack images.       TO-DO: Some of this content was automatically generated.",
            "title": "Import Cloudbreak Deployer Image"
        },
        {
            "location": "/os-launch/index.html#import-hdp-image",
            "text": "Download the latest HDP image to your local machine:   curl -O https://public-repo-1.hortonworks.com/HDP/cloudbreak/hdc-hdp--1706141444.img    Set the following environment variables for the OpenStack image import:   export CB_LATEST_IMAGE=hdc-hdp--1706141444.img \nexport CB_LATEST_IMAGE_NAME=hdc-hdp--1705081316.img\nexport OS_USERNAME=your_os_user_name\nexport OS_AUTH_URL=your_authentication_url\nexport OS_TENANT_NAME=your_os_tenant_name    Import the new image into your OpenStack:  glance image-create --name \"$CB_LATEST_IMAGE_NAME\" --file \"$CB_LATEST_IMAGE\" --disk-format qcow2 --container-format bare --progress    After performing the import, you should be able to see the Cloudbreak image among your OpenStack images.       TO-DO: Some of this content was automatically generated.",
            "title": "Import HDP Image"
        },
        {
            "location": "/os-launch/index.html#launch-the-vm",
            "text": "In your OpenStack, launch and instance providing the following parameters:   Select a VM flavor which meets the following minimum requirements: 4GB RAM, 10GB disk, 2 cores.    Select the Cloudbreak deployer image that you imported earlier and launch an instance using that image.   Select your SSH key pair.    Select the security group which has the following ports open: 22 (SSH) and 443 (HTTPS).   Select your preconfigured network.",
            "title": "Launch the VM"
        },
        {
            "location": "/os-launch/index.html#ssh-to-the-vm",
            "text": "Now that your VM is ready, access it via SSH:    Use a private key matching the public key that you added to your OpenStack project.  The SSH user is called \"cloudbreak\".  You can obtain the VM's IP address from the details of your instance.",
            "title": "SSH to the VM"
        },
        {
            "location": "/os-launch/index.html#initialize-your-profile",
            "text": "After accessing the VM via SSH:     Navigate to the cloudbreak-deployment directory:  cd /var/lib/cloudbreak-deployment/  This directory contains configuration files and the supporting binaries for Cloudbreak deployer.    Initialize your profile by creating a new file called  Profile  and adding the following content:  export UAA_DEFAULT_SECRET=MY-SECRET\nexport UAA_DEFAULT_USER_PW=MY-PASSWORD\nexport PUBLIC_IP=VM-PUBLIC-IP     For example:   export UAA_DEFAULT_SECRET=MySecret123\nexport UAA_DEFAULT_USER_PW=MySecurePassword123\nexport PUBLIC_IP=34.212.141.253     You will need to provide the password when logging in to the Cloudbreak web UI and when using the Cloudbreak Shell. The secret will be used by Cloudbreak for authentication.",
            "title": "Initialize Your Profile"
        },
        {
            "location": "/os-launch/index.html#perform-optional-configurations",
            "text": "These configurations are optional.",
            "title": "Perform Optional Configurations"
        },
        {
            "location": "/os-launch/index.html#configuring-a-self-signed-certificate",
            "text": "If your OpenStack is secured with a self-signed certificate, you need to import that certificate into Cloudbreak, or else Cloudbreak won't be able to communicate with your OpenStack.   To import the certificate, place the certificate file in the  /certs/trusted/  directory:   Navigate to the  certs  directory (automatically generated).  Create the  trusted  directory.  Copy the certificate to the  trusted  directory.    Cloudbreak will automatically pick up the certificate and import it into its truststore upon start.",
            "title": "Configuring a Self-Signed Certificate"
        },
        {
            "location": "/os-launch/index.html#configuring-availability-zone-and-region",
            "text": "By default, Cloudbreak uses  RegionOne  region with  nova  availability zone, but you can customize Cloudbreak deployment and enable multiple regions and availability zones by creating an  openstack-zone.json  file in the  etc  directory of Cloudbreak deployment (that is /var/lib/cloudbreak-deployment/etc/openstack-zone.json ). If the etc directory does not exist in the Cloudbreak deployment directory, then create it.   The following is an example of  openstack-zone.json  containing two regions and four availability zones:  {\n  \"items\": [\n    {\n      \"name\": \"MyRegionOne\",\n      \"zones\": [ \"az1\", \"az2\", \"az3\"]\n    },\n    {\n      \"name\": \"MyRegionTwo\",\n      \"zones\": [ \"myaz\"]\n    }\n  ]\n}   If you are performing this after you have started cbd, perform  cbd restart .",
            "title": "Configuring Availability Zone and Region"
        },
        {
            "location": "/os-launch/index.html#launch-cloudbreak-deployer",
            "text": "Start the Cloudbreak application by using the following command:  cbd start  This will start the Docker containers and initialize the application. The first time you start the Coudbreak app, the process will take longer than usual due to the download of all the necessary docker images.  The  cbd start  command includes the  cbd generate  command which applies the following steps:   Creates the  docker-compose.yml  file, which describes the configuration of all the Docker containers needed for the Cloudbreak deployment.  Creates the  uaa.yml  file, which holds the configuration of the identity server used to authenticate users with Cloudbreak.    Once the  cbd start  has finished, it returns the \"Uluwatu (Cloudbreak UI) url\" which you can later paste in your browser and log in to Cloudbreak web UI.     Check Cloudbreak deployer version and health:   cbd doctor    Next, check Cloudbreak Application logs:   cbd logs cloudbreak  You should see a message like this in the log:  Started CloudbreakApplication in 36.823 seconds.  Cloudbreak normally takes less than a minute to start.",
            "title": "Launch Cloudbreak Deployer"
        },
        {
            "location": "/os-launch/index.html#access-cloudbreak-ui",
            "text": "You can log into the Cloudbreak application at  https://IP_Address  where \"IP_Address\" if the public IP of your OpenStack VM. For example  https://34.212.141.253 .    Confirm the security exception to proceed to the Cloudbreak web UI.  The first time you access Cloudbreak UI, Cloudbreak will automatically generate a self-signed certificate, due to which your browser will warn you about an untrusted connection and will ask you to confirm a security exception.    Log in to the Cloudbreak web UI:   The default username is  admin@example.com  but you should sign up with your own email address.  The password is the value of the  UAA_DEFAULT_USER_PW  variable that you configured in your  Profile  file when  launching Cloudbreak deployer .",
            "title": "Access Cloudbreak UI"
        },
        {
            "location": "/os-launch/index.html#create-cloudbreak-credential",
            "text": "Cloudbreak works by connecting your OpenStack account through this credential, and then uses it to create resources on your behalf. Before you can start provisioning cluster using Cloudbreak, you must create a Cloudbreak credential.    In the Cloudbreak web UI, open the  manage credentials  pane.     Click  +create credential .     Provide the following information:     Parameter  Description      Keystone Version  Select the keystone version.    Name  Enter a name for your credential.    Description  (Optional) Enter a description.    User  Enter your OpenStack user name.    Password  Enter your OpenStack password.    Tenant Name  Enter the OpenStack tenant name.    Endpoint  Enter the OpenStack endpoint.    API Facing  Select  public  or  private .    SSH Public Key  Paste your SSH public key.    Select Platform  (Optional) Select a platform (if previously configured).    Public In Account  (Optional) If you check this, other users added to your Cloudbreak instance will be able to use this credential to create clusters.       Click  +create credential .    Your credential should now be displayed at the top of the page and in the  manage credentials  tab.  Congratulations! You've successfully launched and configured Cloudbreak. Now you can use Cloudbreak to  create clusters .      Next: Define Infrastructure Templates",
            "title": "Create Cloudbreak Credential"
        },
        {
            "location": "/os-create/index.html",
            "text": "Create a Cluster on OpenStack\n\n\nTo create a cluster via CLoudbreak UI:\n\n\n\n\n\n\nLog in to the Cloudbreak UI.\n\n\n\n\n\n\nIn the top right corner, select the credential that you want to use to create a cluster:\n\n\n  \n\n\n\n\n\n\nClick \n+create cluster\n and the \nCreate cluster\n form is displayed.\n\n\n\n\n\n\nOn the \nConfigure Cluster\n page, provide the following parameters:\n\n\n\n\nTo view advanced options, click \nShow Advanced Options\n. To learn about advanced options, refer to \nAdvanced Options\n.\n\n\n\n\n\n\n\n\n\n\nParameter\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nCluster Name\n\n\nEnter a name for your cluster. The name must be between 5 and 40 characters, must start with a letter, must only include lowercase letters, numbers, and hyphens.\n\n\n\n\n\n\nTags\n\n\n(Optional) You can optionally add tags, which will help you find your cluster-related resources, such as VMs, in your cloud provider account.\n\n\n\n\n\n\nRegion\n\n\nSelect the region in which you would like to launch your cluster.\n\n\n\n\n\n\nAvailability Zone\n\n\nSelect the availability zone in which you would like to launch your cluster.\n\n\n\n\n\n\nConnector Variant\n\n\nSelect \"HEAT\" or \"NATIVE\".\n\n\n\n\n\n\nSend Email When Cluster is Ready\n\n\n(Optional) Check this to receive an email each time the cluster status changes.\n\n\n\n\n\n\nPublic In Account\n\n\n(Optional) If this option is checked, all the users belonging to your account will be able to use this cluster, but they cannot delete it.\n\n\n\n\n\n\n\n\n\n\nBy default, Ambari Username and Ambari Password are set to \nadmin\n. You can override it in the \"\nConfigure Cluster\n\" tab.\n\n\n\n\n\n\n\n\nOn the \nSet up Network and Security\n page, provide the following parameters:\n\n\n\n\n\n\n\n\nParameter\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nNetwork\n\n\nSelect the virtual network in which you would like your cluster to be provisioned. You can define custom network configurations or use default network configurations.\n\n\n\n\n\n\nEnable Knox Gateway\n\n\n(Optional) Select this option to enable secure access to Ambari web UI and other cluster UIs via Knox gateway.\n\n\n\n\n\n\nEnable Kerberos Security\n\n\n(Optional) Select this option to enable Kerberos for your cluster. You will have an option to create a new kerberos or use an existing one. For more information refer to Kerberos \ndocumentation\n.\n\n\n\n\n\n\n\n\n\n\n\n\nOn the \nChoose Blueprint\n page, select the blueprint that you would like to use for your cluster. You can either choose one of the pre-configured blueprints, or add your own in the \nmanage blueprints\n tab.\n\n\nFor each host group you must provide the following:\n\n\n\n\n\n\n\n\nParameter\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nGroup Size\n\n\nEnter a number defining how many nodes to create per host group. Default is 1. The \"Group Size\" for that host group on which Ambari Server is installed must be set to \"1\".\n\n\n\n\n\n\nTemplate\n\n\nIf you have previously created a template for VMs and storage, you can select it here. If you don't make a selection, default will be used.\n\n\n\n\n\n\nSecurity Group\n\n\nIf you have previously created a template for a security group, you can select it here. If you don't make a selection, default will be used.\n\n\n\n\n\n\nAmbari Server\n\n\nYou must select one node for Ambari Server. The \"Group Size\" for that host group must be set to \"1\".\n\n\n\n\n\n\nRecipes\n\n\nYou can select a previously added recipe (custom script) to be executed on all nodes of the host group. Refer to \nRecipes\n.\n\n\n\n\n\n\n\n\n\n\n\n\nClick on \nReview and Launch\n and then \n+create and start cluster\n.\n\n\n\n\n\n\nYou will be redirected to the Cloudbreak dashboard, and a new tile representing your cluster will appear at the top of the page.\n\n\n\n\n\n\nAdvanced Options\n\n\nClick on \nShow Advanced Options\n to enter additional configuration options.\n\n\nConfigure Cluster\n\n\nYou can optionally configure the following advanced parameters:\n\n\n\n\n\n\n\n\nParameter\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nAmbari Username\n\n\nYou can log in to the Ambari UI using this username. By default, this is set to \nadmin\n.\n\n\n\n\n\n\nAmbari Password\n\n\nYou can log in to the Ambari UI using this password. By default, this is set to \nadmin\n.\n\n\n\n\n\n\nProvision Cluster\n\n\nSALT\n is pre-selected to provision your cluster.\n\n\n\n\n\n\nEnable Lifetime Management\n\n\nCheck this option if you would like your cluster to be automatically terminated after a specific amount of time (defined as \"Time to Live\" in minuter) has passed.\n\n\n\n\n\n\nFlex Subscription\n\n\nThis option will appear if you have configured your deployment for a \nFlex Subscription\n.\n\n\n\n\n\n\n\n\nChoose Blueprint\n\n\nAfter selecting a blueprint, you can optionally configure the following advanced parameters:\n\n\n\n\n\n\n\n\nParameter\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nConfig Recommendation Strategy (Stack Advisor)\n\n\nSelect how configuration recommendations generated by stack advisor will be applied. Select one of \nALWAYS_APPLY: Configuration recommendations will be applied automatically.\nALWAYS_APPLY_DONT_OVERRIDE_CUSTOM_VALUES: Configuration recommendations will be applied automatically, but they will be ignored for custom configurations.\nNEVER_APPLY: Configuration recommendations will be ignored.\nONLY_STACK_DEFAULTS_APPLY: Configuration recommendations will be applied only on the default configurations for all included services.\n\n\n\n\n\n\nValidate Blueprint\n\n\nSelect to validate the blueprint.\n\n\n\n\n\n\n\n\nChoose Failure Action\n\n\nYou can optionally select what to do if cluster creation fails or if there aren't enough instances available to create all requested nodes:\n\n\n\n\n\n\n\n\nParameter\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nFailure Action\n\n\nSelect one of: \ndo NOT rollback resources\n (default) or \nrollback resources\n. \nBy default, if creating a cluster fails, the Azure resources that were created up to that point will not be rolled back. This means that they will remain accessible for troubleshooting and you will need to to delete them manually.\n\n\n\n\n\n\nMinimum Cluster Size\n\n\nThis defines the provisioning strategy in case the cloud provider cannot allocate all the requested nodes. Select \nbest effort\n or \nexact\n.\n\n\n\n\n\n\n\n\nConfigure Ambari Repos\n\n\nYou can optionally configure a different version of Ambari than the default by providing the following information:\n\n\n\n\n\n\n\n\nParameter\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nAmbari Version\n\n\nEnter Ambari version.\n\n\n\n\n\n\nAmbari Repo URL\n\n\nEnter Ambari repo URL.\n\n\n\n\n\n\nAmbari Repo Gpg Key URL\n\n\nEnter gpgkey URL.\n\n\n\n\n\n\n\n\nConfigure HDP Repos\n\n\nYou can optionally configure a different version of HDP than the default by providing the following information:\n\n\n\n\n\n\n\n\nParameter\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nStack\n\n\nEnter stack name.\n\n\n\n\n\n\nVersion\n\n\nEnter stack version.\n\n\n\n\n\n\nStack Repo ID\n\n\nEnter stack repo ID.\n\n\n\n\n\n\nBase URL\n\n\nEner stack repo base URL.\n\n\n\n\n\n\nUtils Repo ID\n\n\nEnter Utils repo ID.\n\n\n\n\n\n\nUtils Base URL\n\n\nEnter Utils repo base URL.\n\n\n\n\n\n\nVerify\n\n\nSelect to verify the repo information.\n\n\n\n\n\n\n\n\nConfigure Ambari Database\n\n\nBy default, Ambari stores data on an embedded database, which is sufficient for ephemeral or test clusters. However, as Ambari and Cloudbreak don't perform backups of this database, it is insufficient for long-running production clusters, and you may need to configure a remote database for Ambari and Cloudbreak.\n\n\n\n\n\n\n\n\nParameter\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nVendor\n\n\nSelect database vendor from the list.\n\n\n\n\n\n\nHost\n\n\nEnter database host IP.\n\n\n\n\n\n\nPort\n\n\nEnter port number.\n\n\n\n\n\n\nName\n\n\nEnter database name.\n\n\n\n\n\n\nUser Name\n\n\nEnter database user name.\n\n\n\n\n\n\nPassword\n\n\nEnter database password.\n\n\n\n\n\n\n\n\n\n\nNext: Access Cluster",
            "title": "Create a Cluster"
        },
        {
            "location": "/os-create/index.html#create-a-cluster-on-openstack",
            "text": "To create a cluster via CLoudbreak UI:    Log in to the Cloudbreak UI.    In the top right corner, select the credential that you want to use to create a cluster:        Click  +create cluster  and the  Create cluster  form is displayed.    On the  Configure Cluster  page, provide the following parameters:   To view advanced options, click  Show Advanced Options . To learn about advanced options, refer to  Advanced Options .      Parameter  Description      Cluster Name  Enter a name for your cluster. The name must be between 5 and 40 characters, must start with a letter, must only include lowercase letters, numbers, and hyphens.    Tags  (Optional) You can optionally add tags, which will help you find your cluster-related resources, such as VMs, in your cloud provider account.    Region  Select the region in which you would like to launch your cluster.    Availability Zone  Select the availability zone in which you would like to launch your cluster.    Connector Variant  Select \"HEAT\" or \"NATIVE\".    Send Email When Cluster is Ready  (Optional) Check this to receive an email each time the cluster status changes.    Public In Account  (Optional) If this option is checked, all the users belonging to your account will be able to use this cluster, but they cannot delete it.      By default, Ambari Username and Ambari Password are set to  admin . You can override it in the \" Configure Cluster \" tab.     On the  Set up Network and Security  page, provide the following parameters:     Parameter  Description      Network  Select the virtual network in which you would like your cluster to be provisioned. You can define custom network configurations or use default network configurations.    Enable Knox Gateway  (Optional) Select this option to enable secure access to Ambari web UI and other cluster UIs via Knox gateway.    Enable Kerberos Security  (Optional) Select this option to enable Kerberos for your cluster. You will have an option to create a new kerberos or use an existing one. For more information refer to Kerberos  documentation .       On the  Choose Blueprint  page, select the blueprint that you would like to use for your cluster. You can either choose one of the pre-configured blueprints, or add your own in the  manage blueprints  tab.  For each host group you must provide the following:     Parameter  Description      Group Size  Enter a number defining how many nodes to create per host group. Default is 1. The \"Group Size\" for that host group on which Ambari Server is installed must be set to \"1\".    Template  If you have previously created a template for VMs and storage, you can select it here. If you don't make a selection, default will be used.    Security Group  If you have previously created a template for a security group, you can select it here. If you don't make a selection, default will be used.    Ambari Server  You must select one node for Ambari Server. The \"Group Size\" for that host group must be set to \"1\".    Recipes  You can select a previously added recipe (custom script) to be executed on all nodes of the host group. Refer to  Recipes .       Click on  Review and Launch  and then  +create and start cluster .    You will be redirected to the Cloudbreak dashboard, and a new tile representing your cluster will appear at the top of the page.",
            "title": "Create a Cluster on OpenStack"
        },
        {
            "location": "/os-create/index.html#advanced-options",
            "text": "Click on  Show Advanced Options  to enter additional configuration options.",
            "title": "Advanced Options"
        },
        {
            "location": "/os-create/index.html#configure-cluster",
            "text": "You can optionally configure the following advanced parameters:     Parameter  Description      Ambari Username  You can log in to the Ambari UI using this username. By default, this is set to  admin .    Ambari Password  You can log in to the Ambari UI using this password. By default, this is set to  admin .    Provision Cluster  SALT  is pre-selected to provision your cluster.    Enable Lifetime Management  Check this option if you would like your cluster to be automatically terminated after a specific amount of time (defined as \"Time to Live\" in minuter) has passed.    Flex Subscription  This option will appear if you have configured your deployment for a  Flex Subscription .",
            "title": "Configure Cluster"
        },
        {
            "location": "/os-create/index.html#choose-blueprint",
            "text": "After selecting a blueprint, you can optionally configure the following advanced parameters:     Parameter  Description      Config Recommendation Strategy (Stack Advisor)  Select how configuration recommendations generated by stack advisor will be applied. Select one of  ALWAYS_APPLY: Configuration recommendations will be applied automatically. ALWAYS_APPLY_DONT_OVERRIDE_CUSTOM_VALUES: Configuration recommendations will be applied automatically, but they will be ignored for custom configurations. NEVER_APPLY: Configuration recommendations will be ignored. ONLY_STACK_DEFAULTS_APPLY: Configuration recommendations will be applied only on the default configurations for all included services.    Validate Blueprint  Select to validate the blueprint.",
            "title": "Choose Blueprint"
        },
        {
            "location": "/os-create/index.html#choose-failure-action",
            "text": "You can optionally select what to do if cluster creation fails or if there aren't enough instances available to create all requested nodes:     Parameter  Description      Failure Action  Select one of:  do NOT rollback resources  (default) or  rollback resources .  By default, if creating a cluster fails, the Azure resources that were created up to that point will not be rolled back. This means that they will remain accessible for troubleshooting and you will need to to delete them manually.    Minimum Cluster Size  This defines the provisioning strategy in case the cloud provider cannot allocate all the requested nodes. Select  best effort  or  exact .",
            "title": "Choose Failure Action"
        },
        {
            "location": "/os-create/index.html#configure-ambari-repos",
            "text": "You can optionally configure a different version of Ambari than the default by providing the following information:     Parameter  Description      Ambari Version  Enter Ambari version.    Ambari Repo URL  Enter Ambari repo URL.    Ambari Repo Gpg Key URL  Enter gpgkey URL.",
            "title": "Configure Ambari Repos"
        },
        {
            "location": "/os-create/index.html#configure-hdp-repos",
            "text": "You can optionally configure a different version of HDP than the default by providing the following information:     Parameter  Description      Stack  Enter stack name.    Version  Enter stack version.    Stack Repo ID  Enter stack repo ID.    Base URL  Ener stack repo base URL.    Utils Repo ID  Enter Utils repo ID.    Utils Base URL  Enter Utils repo base URL.    Verify  Select to verify the repo information.",
            "title": "Configure HDP Repos"
        },
        {
            "location": "/os-create/index.html#configure-ambari-database",
            "text": "By default, Ambari stores data on an embedded database, which is sufficient for ephemeral or test clusters. However, as Ambari and Cloudbreak don't perform backups of this database, it is insufficient for long-running production clusters, and you may need to configure a remote database for Ambari and Cloudbreak.     Parameter  Description      Vendor  Select database vendor from the list.    Host  Enter database host IP.    Port  Enter port number.    Name  Enter database name.    User Name  Enter database user name.    Password  Enter database password.      Next: Access Cluster",
            "title": "Configure Ambari Database"
        },
        {
            "location": "/clusters-access/index.html",
            "text": "Accessing Your Cluster\n\n\nThe following section describes how to access the various services in the cluster.\n\n\nFinding Cluster Details\n\n\nOnce your cluster is up and running, you can find information about the cluster on the cluster details page in the Cloudbreak UI. To access cluster details page, click on the tile representing your cluster in the Cloudbreak UI. The information presented includes:\n\n\n\n\nCluster status\n\n\nEvent history \n\n\nCluster instance public IP addresses\n\n\nUI links \n\n\n\n\nAccess Cluster via SSH\n\n\nIf you plan to access the cluster via the command line clients, SSH into the master node instance in the cluster. \n\n\n\n\nIn order to use SSH, you must generate an SSH key pair or use an existing SSH keypair.  \n\n\nYou can find the cluster instance public IP addresses on the cluster details page.  \n\n\nWhen accessing instances via SSH use the \ncloudbreak\n user. \n\n\n\n\nOn Mac OS, you can use the following syntax to SSH to the VM:\n\nssh -i \"privatekey.pem\" cloudbreak@publicIP\n\nFor example:\n\nssh -i \"dominika-kp.pem\" cloudbreak@p52.25.169.132\n\n\nOn Windows, you can SSH using an SSH client such as PuTTY. \n\n\n\n\nNext: Manage and Monitor Clusters",
            "title": "Access Cluster"
        },
        {
            "location": "/clusters-access/index.html#accessing-your-cluster",
            "text": "The following section describes how to access the various services in the cluster.",
            "title": "Accessing Your Cluster"
        },
        {
            "location": "/clusters-access/index.html#finding-cluster-details",
            "text": "Once your cluster is up and running, you can find information about the cluster on the cluster details page in the Cloudbreak UI. To access cluster details page, click on the tile representing your cluster in the Cloudbreak UI. The information presented includes:   Cluster status  Event history   Cluster instance public IP addresses  UI links",
            "title": "Finding Cluster Details"
        },
        {
            "location": "/clusters-access/index.html#access-cluster-via-ssh",
            "text": "If you plan to access the cluster via the command line clients, SSH into the master node instance in the cluster.    In order to use SSH, you must generate an SSH key pair or use an existing SSH keypair.    You can find the cluster instance public IP addresses on the cluster details page.    When accessing instances via SSH use the  cloudbreak  user.    On Mac OS, you can use the following syntax to SSH to the VM: ssh -i \"privatekey.pem\" cloudbreak@publicIP \nFor example: ssh -i \"dominika-kp.pem\" cloudbreak@p52.25.169.132  On Windows, you can SSH using an SSH client such as PuTTY.    Next: Manage and Monitor Clusters",
            "title": "Access Cluster via SSH"
        },
        {
            "location": "/clusters-manage/index.html",
            "text": "Managing and Monitoring Your Clusters\n\n\nYou can manage monitor your clusters from the Cloudbreak UI. To do that, click on the tile representing the cluster that you want to access: \n\n\n \n\n\nRepairing Your Cluster\n\n\nTo trigger repair process for your cluster, click \nrepair\n. Faulty nodes will be deleted from the cluster and new ones will be added in their place.\n\n\nSynchronizing with Cloud Provider\n\n\nTBD\n\n\nCloning Your Cluster\n\n\nTBD\n\n\nResizing Your Cluster\n\n\nTBD\n\n\nAuto Scaling\n\n\nTBD\n\n\nStopping and Restarting\n\n\nTBD\n\n\nTerminating Your Cluster\n\n\nTo terminate your cluster, click \nterminate\n. All cluster-related resources will be deleted, unless the network is used by other VMs, in which case it will not be deleted.",
            "title": "Manage and Monitor Cluster"
        },
        {
            "location": "/clusters-manage/index.html#managing-and-monitoring-your-clusters",
            "text": "You can manage monitor your clusters from the Cloudbreak UI. To do that, click on the tile representing the cluster that you want to access:",
            "title": "Managing and Monitoring Your Clusters"
        },
        {
            "location": "/clusters-manage/index.html#repairing-your-cluster",
            "text": "To trigger repair process for your cluster, click  repair . Faulty nodes will be deleted from the cluster and new ones will be added in their place.",
            "title": "Repairing Your Cluster"
        },
        {
            "location": "/clusters-manage/index.html#synchronizing-with-cloud-provider",
            "text": "TBD",
            "title": "Synchronizing with Cloud Provider"
        },
        {
            "location": "/clusters-manage/index.html#cloning-your-cluster",
            "text": "TBD",
            "title": "Cloning Your Cluster"
        },
        {
            "location": "/clusters-manage/index.html#resizing-your-cluster",
            "text": "TBD",
            "title": "Resizing Your Cluster"
        },
        {
            "location": "/clusters-manage/index.html#auto-scaling",
            "text": "TBD",
            "title": "Auto Scaling"
        },
        {
            "location": "/clusters-manage/index.html#stopping-and-restarting",
            "text": "TBD",
            "title": "Stopping and Restarting"
        },
        {
            "location": "/clusters-manage/index.html#terminating-your-cluster",
            "text": "To terminate your cluster, click  terminate . All cluster-related resources will be deleted, unless the network is used by other VMs, in which case it will not be deleted.",
            "title": "Terminating Your Cluster"
        },
        {
            "location": "/blueprints/index.html",
            "text": "Blueprints\n\n\nAmbari blueprints\n are your declarative definition of a Hadoop cluster, defining the host groups and which components to install on which host group. Ambari uses them as a base for your clusters. \n\n\nYou have three options:\n\n\n\n\nUse one of the pre-defined blueprints.  \n\n\nCopy and edit one of the pre-defined blueprints.   \n\n\nAdd your custom blueprint by uploading a JSON file or pasting the JSON text. \n\n\n\n\nWe recommend that you review the default blueprints to check if they meet your requirements. You can do this by expanding  the \nmanage bluerints\n pane in the Cloudbreak web UI (shown in the screenshot) or by reading the documentation below.  \n\n\n \n\n\nHere\n is an example of a blueprint. \n\n\nThe host groups in the JSON will be mapped to a set of instances when starting the cluster, and the specified services and components will be installed on the corresponding nodes. It is not necessary to define a complete configuration in the blueprint. If a configuration is missing, Ambari will fill that with a default value. \n\n\nA blueprint can be modified later from the Ambari UI.\n\n\nA blueprint can be exported from a running Ambari cluster and can be reused in Cloudbreak after slight modifications. When a blueprint is exported, some configurations are hardcoded for example domain names, memory configurations, and so on, that won't be applicable to the Cloudbreak cluster. There is no automatic way to modify an exported blueprint and make it instantly usable in Cloudbreak, the modifications have to be done manually.\n\n\nDefault Blueprints\n\n\nCloudbreak includes three default HDP cluster blueprints:\n\n\nHDP Version: \nHDP 2.6\n\n\n\n\n\n\n\n\nCluster Type\n\n\nServices\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nData Science\n\n\n Spark 1.6,\nZeppelin 0.7.0\n\n\nThis cluster configuration includes Spark 1.6 with Zeppelin.\n\n\n\n\n\n\nData Science\n\n\n Spark 2.1,\nZeppelin 0.7.0\n\n\nThis cluster configuration includes Spark 2.1 with Zeppelin.\n\n\n\n\n\n\nEDW - Analytics\n\n\n Hive 2 LLAP\n,\nZeppelin 0.7.0\n\n\nThis cluster configuration includes Hive 2 LLAP.\n\n\n\n\n\n\nEDW - ETL\n\n\n Hive 1.2.1,\nSpark 1.6\n\n\nThis cluster configuration includes Hive and Spark 1.6.\n\n\n\n\n\n\nEDW - ETL\n\n\n Hive 1.2.1,\n Spark 2.1\n\n\nThis cluster configuration includes Hive and Spark 2.1.\n\n\n\n\n\n\nBI\n\n\n Druid 0.9.2\n\n\nThis cluster configuration includes a Technical Preview of Druid.\n\n\n\n\n\n\n\n\nHDP Version: \nHDP 2.5\n\n\n\n\n\n\n\n\nCluster Type\n\n\nServices\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nData Science\n\n\n Spark 1.6,\nZeppelin 0.6.0\n\n\nThis cluster configuration includes Spark 1.6 and Zeppelin.\n\n\n\n\n\n\nEDW - ETL\n\n\n Hive 1.2.1,\nSpark 1.6\n\n\nThis cluster configuration includes Hive and Spark 1.6.\n\n\n\n\n\n\nEDW - ETL\n\n\n Hive 1.2.1,\n Spark 2.0\n\n\nThis cluster configuration includes a Technical Preview of Spark 2.0.\n\n\n\n\n\n\nEDW - Analytics\n\n\n Hive 2 LLAP\n,\nZeppelin 0.6.0\n\n\nThis cluster configuration includes a Technical Preview of Hive 2 LLAP.\n\n\n\n\n\n\n\n\n\n    \nChoosing Your Configuration\n\n    \n\nWhen creating a cluster, you can choose a more stable cluster configuration for a predicable experience.\nAlternatively, you can try the latest capabilities by choosing a cluster configuration\nthat is much more experimental. The following configuration classification applies:\n\n\n\n Stable configurations are the best choice if you want to avoid issues and other problems with launching and using clusters.\n\n\n If you want to use a Technical Preview version of a component in a release of HDP, use these configurations.\n\n\n These are the most cutting edge of the configurations, including Technical Preview components in a Technical Preview HDP release.\n\n\n\n\n\n\n\n\n\nThe following services are included in the respective blueprints:\n\n\nHDP 2.6\n\n\n\n\n\n\n\n\nService\n\n\nData Science\n(Spark 1.6)\n\n\nData Science\n(Spark 2.1)\n\n\nEDW-ETL\n(Spark 1.6)\n\n\nEDW-ETL\n(Spark 2.1)\n\n\nEDW-Analytics\n\n\nBI-Druid\n\n\n\n\n\n\n\n\n\n\nHDFS\n\n\nx\n\n\nx\n\n\nx\n\n\nx\n\n\nx\n\n\nx\n\n\n\n\n\n\nYARN\n\n\nx\n\n\nx\n\n\nx\n\n\nx\n\n\nx\n\n\nx\n\n\n\n\n\n\nMapReduce2\n\n\nx\n\n\nx\n\n\nx\n\n\nx\n\n\nx\n\n\nx\n\n\n\n\n\n\nTez\n\n\nx\n\n\nx\n\n\nx\n\n\nx\n\n\nx\n\n\nx\n\n\n\n\n\n\nHive 1.2.1\n\n\nx\n\n\nx\n\n\nx\n\n\nx\n\n\n\n\n\n\n\n\n\n\nHive 2 LLAP\n\n\n\n\n\n\n\n\n\n\nx\n\n\n\n\n\n\n\n\nDruid\n\n\n\n\n\n\n\n\n\n\n\n\nx\n\n\n\n\n\n\nPig\n\n\nx\n\n\nx\n\n\nx\n\n\nx\n\n\nx\n\n\n\n\n\n\n\n\nSqoop\n\n\nx\n\n\nx\n\n\nx\n\n\n\n\n\n\nx\n\n\n\n\n\n\nZooKeeper\n\n\nx\n\n\nx\n\n\nx\n\n\nx\n\n\nx\n\n\nx\n\n\n\n\n\n\nAmbari Metrics\n\n\nx\n\n\nx\n\n\nx\n\n\nx\n\n\nx\n\n\nx\n\n\n\n\n\n\nSpark 1.6\n\n\nx\n\n\n\n\nx\n\n\n\n\nx\n\n\n\n\n\n\n\n\nSpark 2.1\n\n\n\n\nx\n\n\n\n\nx\n\n\n\n\n\n\n\n\n\n\nZeppelin 0.7.0\n\n\nx\n\n\nx\n\n\n\n\n\n\nx\n\n\n\n\n\n\n\n\nSlider\n\n\n\n\n\n\n\n\n\n\nx\n\n\n\n\n\n\n\n\n\n\nHDP 2.5\n\n\n\n\n\n\n\n\nService\n\n\nData Science\n\n\nEDW-ETL (Spark 1.6)\n\n\nEDW-ETL (Spark 2.0)\n\n\nEDW-Analytics\n\n\n\n\n\n\n\n\n\n\nHDFS\n\n\nx\n\n\nx\n\n\nx\n\n\nx\n\n\n\n\n\n\nYARN\n\n\nx\n\n\nx\n\n\nx\n\n\nx\n\n\n\n\n\n\nMapReduce2\n\n\nx\n\n\nx\n\n\nx\n\n\nx\n\n\n\n\n\n\nTez\n\n\nx\n\n\nx\n\n\nx\n\n\nx\n\n\n\n\n\n\nHive 1.2.1\n\n\nx\n\n\nx\n\n\nx\n\n\n\n\n\n\n\n\nHive 2 LLAP\n\n\n\n\n\n\n\n\nx\n\n\n\n\n\n\nPig\n\n\nx\n\n\nx\n\n\nx\n\n\nx\n\n\n\n\n\n\nSqoop\n\n\nx\n\n\nx\n\n\n\n\n\n\n\n\n\n\nZooKeeper\n\n\nx\n\n\nx\n\n\nx\n\n\nx\n\n\n\n\n\n\nAmbari Metrics\n\n\nx\n\n\nx\n\n\nx\n\n\nx\n\n\n\n\n\n\nSpark 1.6\n\n\nx\n\n\nx\n\n\n\n\nx\n\n\n\n\n\n\nSpark 2.0\n\n\n\n\n\n\nx\n\n\n\n\n\n\n\n\nZeppelin 0.6.0\n\n\nx\n\n\n\n\n\n\nx\n\n\n\n\n\n\nSlider\n\n\n\n\n\n\n\n\nx\n\n\n\n\n\n\n\n\nCopy and Edit Existing Blueprint\n\n\nYou can modify default or previously added blueprints in the \nmanage blueprints\n tab. To do that, expand the entry in the Cloudbreak UI and then click \ncopy & edit\n. \n\n\nAdd Custom Blueprint\n\n\nThis option allows you to save your custom blueprints. For correct blueprint layout and other useful information about Ambari blueprints, refer to the \nAmbari cwiki\n page.\n\n\nOnce you have your blueprints ready, you can save them in the \nmanage blueprints\n tab. To add your own blueprint, click \n+create blueprint\n and enter the following parameters:\n\n\n\n\n\n\n\n\nParameter\n\n\nValue\n\n\n\n\n\n\n\n\n\n\nName\n\n\nEnter a name for your blueprint.\n\n\n\n\n\n\nDescription\n\n\n(Optional) Enter a description for your blueprint.\n\n\n\n\n\n\nBlueprint Source\n\n\nSelect one of: \nText\n: Paste blueprint in JSON format.\n \nFile\n: Upload a file that contains the blueprint.\n \nURL\n: Specify the URL for your blueprint.\n\n\n\n\n\n\nPublic In Account\n\n\n(Optional) If this option is checked, all the users belonging to your account will be able to use this blueprint to create clusters, but they cannot delete it.",
            "title": "Blueprints"
        },
        {
            "location": "/blueprints/index.html#blueprints",
            "text": "Ambari blueprints  are your declarative definition of a Hadoop cluster, defining the host groups and which components to install on which host group. Ambari uses them as a base for your clusters.   You have three options:   Use one of the pre-defined blueprints.    Copy and edit one of the pre-defined blueprints.     Add your custom blueprint by uploading a JSON file or pasting the JSON text.    We recommend that you review the default blueprints to check if they meet your requirements. You can do this by expanding  the  manage bluerints  pane in the Cloudbreak web UI (shown in the screenshot) or by reading the documentation below.       Here  is an example of a blueprint.   The host groups in the JSON will be mapped to a set of instances when starting the cluster, and the specified services and components will be installed on the corresponding nodes. It is not necessary to define a complete configuration in the blueprint. If a configuration is missing, Ambari will fill that with a default value.   A blueprint can be modified later from the Ambari UI.  A blueprint can be exported from a running Ambari cluster and can be reused in Cloudbreak after slight modifications. When a blueprint is exported, some configurations are hardcoded for example domain names, memory configurations, and so on, that won't be applicable to the Cloudbreak cluster. There is no automatic way to modify an exported blueprint and make it instantly usable in Cloudbreak, the modifications have to be done manually.",
            "title": "Blueprints"
        },
        {
            "location": "/blueprints/index.html#default-blueprints",
            "text": "Cloudbreak includes three default HDP cluster blueprints:",
            "title": "Default Blueprints"
        },
        {
            "location": "/blueprints/index.html#hdp-version-hdp-26",
            "text": "Cluster Type  Services  Description      Data Science   Spark 1.6, Zeppelin 0.7.0  This cluster configuration includes Spark 1.6 with Zeppelin.    Data Science   Spark 2.1, Zeppelin 0.7.0  This cluster configuration includes Spark 2.1 with Zeppelin.    EDW - Analytics   Hive 2 LLAP , Zeppelin 0.7.0  This cluster configuration includes Hive 2 LLAP.    EDW - ETL   Hive 1.2.1, Spark 1.6  This cluster configuration includes Hive and Spark 1.6.    EDW - ETL   Hive 1.2.1,  Spark 2.1  This cluster configuration includes Hive and Spark 2.1.    BI   Druid 0.9.2  This cluster configuration includes a Technical Preview of Druid.",
            "title": "HDP Version: HDP 2.6"
        },
        {
            "location": "/blueprints/index.html#hdp-version-hdp-25",
            "text": "Cluster Type  Services  Description      Data Science   Spark 1.6, Zeppelin 0.6.0  This cluster configuration includes Spark 1.6 and Zeppelin.    EDW - ETL   Hive 1.2.1, Spark 1.6  This cluster configuration includes Hive and Spark 1.6.    EDW - ETL   Hive 1.2.1,  Spark 2.0  This cluster configuration includes a Technical Preview of Spark 2.0.    EDW - Analytics   Hive 2 LLAP , Zeppelin 0.6.0  This cluster configuration includes a Technical Preview of Hive 2 LLAP.     \n     Choosing Your Configuration \n     \nWhen creating a cluster, you can choose a more stable cluster configuration for a predicable experience.\nAlternatively, you can try the latest capabilities by choosing a cluster configuration\nthat is much more experimental. The following configuration classification applies:   Stable configurations are the best choice if you want to avoid issues and other problems with launching and using clusters.   If you want to use a Technical Preview version of a component in a release of HDP, use these configurations.   These are the most cutting edge of the configurations, including Technical Preview components in a Technical Preview HDP release.     The following services are included in the respective blueprints:  HDP 2.6     Service  Data Science (Spark 1.6)  Data Science (Spark 2.1)  EDW-ETL (Spark 1.6)  EDW-ETL (Spark 2.1)  EDW-Analytics  BI-Druid      HDFS  x  x  x  x  x  x    YARN  x  x  x  x  x  x    MapReduce2  x  x  x  x  x  x    Tez  x  x  x  x  x  x    Hive 1.2.1  x  x  x  x      Hive 2 LLAP      x     Druid       x    Pig  x  x  x  x  x     Sqoop  x  x  x    x    ZooKeeper  x  x  x  x  x  x    Ambari Metrics  x  x  x  x  x  x    Spark 1.6  x   x   x     Spark 2.1   x   x      Zeppelin 0.7.0  x  x    x     Slider      x      HDP 2.5     Service  Data Science  EDW-ETL (Spark 1.6)  EDW-ETL (Spark 2.0)  EDW-Analytics      HDFS  x  x  x  x    YARN  x  x  x  x    MapReduce2  x  x  x  x    Tez  x  x  x  x    Hive 1.2.1  x  x  x     Hive 2 LLAP     x    Pig  x  x  x  x    Sqoop  x  x      ZooKeeper  x  x  x  x    Ambari Metrics  x  x  x  x    Spark 1.6  x  x   x    Spark 2.0    x     Zeppelin 0.6.0  x    x    Slider     x",
            "title": "HDP Version: HDP 2.5"
        },
        {
            "location": "/blueprints/index.html#copy-and-edit-existing-blueprint",
            "text": "You can modify default or previously added blueprints in the  manage blueprints  tab. To do that, expand the entry in the Cloudbreak UI and then click  copy & edit .",
            "title": "Copy and Edit Existing Blueprint"
        },
        {
            "location": "/blueprints/index.html#add-custom-blueprint",
            "text": "This option allows you to save your custom blueprints. For correct blueprint layout and other useful information about Ambari blueprints, refer to the  Ambari cwiki  page.  Once you have your blueprints ready, you can save them in the  manage blueprints  tab. To add your own blueprint, click  +create blueprint  and enter the following parameters:     Parameter  Value      Name  Enter a name for your blueprint.    Description  (Optional) Enter a description for your blueprint.    Blueprint Source  Select one of:  Text : Paste blueprint in JSON format.   File : Upload a file that contains the blueprint.   URL : Specify the URL for your blueprint.    Public In Account  (Optional) If this option is checked, all the users belonging to your account will be able to use this blueprint to create clusters, but they cannot delete it.",
            "title": "Add Custom Blueprint"
        },
        {
            "location": "/recipes/index.html",
            "text": "Recipes (Custom Scripts)\n\n\nWhen creating a cluster, you can optionally upload one or more recipes (i.e. scripts) and they will be executed on specific host group before or after the cluster installation. You can use recipes for tasks such as installing additional software or performing advanced cluster configuration. \n\n\nWriting Recipes\n\n\nWhen using recipes, consider the following:\n\n\n\n\nThe scripts will be executed on the node types you specify (such as \"master\", \"worker\", \"compute\"). If you want to run a a script on all nodes, define the recipe one per node type.  \n\n\nThe script will execute on all of the nodes of that type as root.  \n\n\nIn order to be executed, your script must be in a network location which is accessible from the cloud controller and cluster instances VPC.  \n\n\nMake sure to follow Linux best practices when creating your scripts. For example, don't forget to script \"Yes\" auto-answers where needed.  \n\n\nDo not execute yum update \u2013y since it may update other components on the instances (such as salt), which can create unintended or unstable behavior.  \n\n\n\n\nAdding Node Recipes\n\n\nTo add node recipes:\n\n\n\n\n\n\nPlace your scripts in a network location accessible from Cloudbreak and cluster instances virtual network. \n\n\n\n\n\n\nDefine the recipe when creating a cluster using the Cloudbreak UI or Cloudbreak Shell. You must provide:\n\n\n\n\n\n\n\n\nParameter\n\n\nValue\n\n\n\n\n\n\n\n\n\n\nName\n\n\nEnter a name for your recipe.\n\n\n\n\n\n\nDescription\n\n\n(Optional) Enter a description for your recipe.\n\n\n\n\n\n\nExecution Type\n\n\nSelect \nPRE\n or \nPOST\n, depending on whether you want the script to be executed prior to or post Ambari cluster deployment.\n\n\n\n\n\n\nScript\n\n\nSelect one of: \nScript\n: Paste the script.\n \nFile\n: Point to a file on your machine that contains the recipe.\n \nURL\n: Specify the URL for your recipe.\n\n\n\n\n\n\nPublic In Account\n\n\n(Optional) If this option is checked, all the users belonging to your account will be able to use this recipe to create clusters, but they cannot delete it.\n\n\n\n\n\n\n\n\n\n\n\n\nWhen creating a cluster, select \nShow Advanced Options\n > \nChoose Blueprint\n and specify which recipe you want to execute on which host group. \n\n\n\n\n\n\nManaging Recipes\n\n\nYou can define reusable cluster recipes (i.e. custom scripts that will be run on selected cluster nodes before or after Ambari cluster deployment) in the \nmanage recipes\n tab by clicking on \n+create recipee\n and providing required parameters.\n\n\nYou can delete previously defined items using the \ndelete\n option.\n\n\nExecuting Recipes\n\n\nThe scripts will be executed as root. The recipe output is written to \n/var/log/recipes\n on each node on which it was executed.\n\n\nSample Recipe for Yum Proxy Setting\n\n\n#!/bin/bash\ncat >> /etc/yum.conf \n<\n\n\n\n\n\n\n\n\n\n\n\nTO-DO: Move Shell commands to the Cb Shell doc.",
            "title": "Recipes"
        },
        {
            "location": "/recipes/index.html#recipes-custom-scripts",
            "text": "When creating a cluster, you can optionally upload one or more recipes (i.e. scripts) and they will be executed on specific host group before or after the cluster installation. You can use recipes for tasks such as installing additional software or performing advanced cluster configuration.",
            "title": "Recipes (Custom Scripts)"
        },
        {
            "location": "/recipes/index.html#writing-recipes",
            "text": "When using recipes, consider the following:   The scripts will be executed on the node types you specify (such as \"master\", \"worker\", \"compute\"). If you want to run a a script on all nodes, define the recipe one per node type.    The script will execute on all of the nodes of that type as root.    In order to be executed, your script must be in a network location which is accessible from the cloud controller and cluster instances VPC.    Make sure to follow Linux best practices when creating your scripts. For example, don't forget to script \"Yes\" auto-answers where needed.    Do not execute yum update \u2013y since it may update other components on the instances (such as salt), which can create unintended or unstable behavior.",
            "title": "Writing Recipes"
        },
        {
            "location": "/recipes/index.html#adding-node-recipes",
            "text": "To add node recipes:    Place your scripts in a network location accessible from Cloudbreak and cluster instances virtual network.     Define the recipe when creating a cluster using the Cloudbreak UI or Cloudbreak Shell. You must provide:     Parameter  Value      Name  Enter a name for your recipe.    Description  (Optional) Enter a description for your recipe.    Execution Type  Select  PRE  or  POST , depending on whether you want the script to be executed prior to or post Ambari cluster deployment.    Script  Select one of:  Script : Paste the script.   File : Point to a file on your machine that contains the recipe.   URL : Specify the URL for your recipe.    Public In Account  (Optional) If this option is checked, all the users belonging to your account will be able to use this recipe to create clusters, but they cannot delete it.       When creating a cluster, select  Show Advanced Options  >  Choose Blueprint  and specify which recipe you want to execute on which host group.",
            "title": "Adding Node Recipes"
        },
        {
            "location": "/recipes/index.html#managing-recipes",
            "text": "You can define reusable cluster recipes (i.e. custom scripts that will be run on selected cluster nodes before or after Ambari cluster deployment) in the  manage recipes  tab by clicking on  +create recipee  and providing required parameters.  You can delete previously defined items using the  delete  option.",
            "title": "Managing Recipes"
        },
        {
            "location": "/recipes/index.html#executing-recipes",
            "text": "The scripts will be executed as root. The recipe output is written to  /var/log/recipes  on each node on which it was executed.",
            "title": "Executing Recipes"
        },
        {
            "location": "/recipes/index.html#sample-recipe-for-yum-proxy-setting",
            "text": "#!/bin/bash\ncat >> /etc/yum.conf  <      TO-DO: Move Shell commands to the Cb Shell doc.",
            "title": "Sample Recipe for Yum Proxy Setting"
        },
        {
            "location": "/tags/index.html",
            "text": "Resource Tagging\n\n\nWhen you manually create resources (such as VMs) in the cloud, you have an option to add custom tags that help you track these resources. Likewise, when creating clusters, you can instruct Cloudbreak to tag the cloud resources that it creates on your behalf.\n\n\nThe tags added during cluster creation will be displayed on your cloud account, allowing you to track your resources. \n\n\nYou can use tags to categorize your cloud resources by purpose, owner, and so on. Tags come in especially handy when you are using a corporate AWS account and you want to quickly identify which resources belong to your cluster(s). In fact, your corporate cloud account admin may require you to tag all the resources that you create, in particular resources, such as VMs, which incur charges.\n\n\nAdd Tags When Creating a Cluster\n\n\nYou can tag the cloud resources used for a cluster by providing custom tag names and values when creating a cluster via UI or CLI. In the UI, this option is available on the \nConfigure Cluster\n page > \nTags\n.\n\n\nNote that:\n\n\n\n\nIt is not possible to add tags after your cluster has been created.  \n\n\nWhen you clone your cluster, all tags associated with the source cluster will be added to the template of the clone.  \n\n\nWhen you save a cluster template, all tags will be saved as part of the template, and they will be listed on the cluster template page.  \n\n\n\n\nAdd Tags in Profile (AWS)\n\n\nIn order to differentiate launched instances, you can optionally define custom tags for your AWS resources deployed by Cloudbreak. \n\n\n\n\n\n\nIf you want just one custom tag for your Cloudformation resources, set this variable in the \nProfile\n:\n\n\nexport CB_AWS_DEFAULT_CF_TAG=mytagcontent\n\n\nIn this example, the name of the tag will be \nCloudbreakId\n and the value will be \nmytagcontent\n.\n\n\n\n\n\n\nIf you prefer to customize the tag name, set this variable:\n\n\nexport CB_AWS_CUSTOM_CF_TAGS=mytagname:mytagvalue\n\n\nIn this example the name of the tag will be \nmytagname\n and the value will be \nmytagvalue\n. \n\n\n\n\n\n\nYou can specify a list of tags with a comma separated list: \n\n\nexport CB_AWS_CUSTOM_CF_TAGS=tag1:value1,tag2:value2,tag3:value3\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTO-DO: What exactly gets tagged? \nTO-DO: This configuration is available on AWS only??\n\n\n\n\n\n\n\n\n\n\nCloud Provider Documentation\n\n\nTo learn more about tags and their restrictions, refer to the cloud provider documentation:\n\n\n\n\nTags on AWS\n    \n\n\nTags on Azure\n  \n\n\nLabels on GCP\n  \n\n\nTags on OpenStack",
            "title": "Resource Tagging"
        },
        {
            "location": "/tags/index.html#resource-tagging",
            "text": "When you manually create resources (such as VMs) in the cloud, you have an option to add custom tags that help you track these resources. Likewise, when creating clusters, you can instruct Cloudbreak to tag the cloud resources that it creates on your behalf.  The tags added during cluster creation will be displayed on your cloud account, allowing you to track your resources.   You can use tags to categorize your cloud resources by purpose, owner, and so on. Tags come in especially handy when you are using a corporate AWS account and you want to quickly identify which resources belong to your cluster(s). In fact, your corporate cloud account admin may require you to tag all the resources that you create, in particular resources, such as VMs, which incur charges.",
            "title": "Resource Tagging"
        },
        {
            "location": "/tags/index.html#add-tags-when-creating-a-cluster",
            "text": "You can tag the cloud resources used for a cluster by providing custom tag names and values when creating a cluster via UI or CLI. In the UI, this option is available on the  Configure Cluster  page >  Tags .  Note that:   It is not possible to add tags after your cluster has been created.    When you clone your cluster, all tags associated with the source cluster will be added to the template of the clone.    When you save a cluster template, all tags will be saved as part of the template, and they will be listed on the cluster template page.",
            "title": "Add Tags When Creating a Cluster"
        },
        {
            "location": "/tags/index.html#add-tags-in-profile-aws",
            "text": "In order to differentiate launched instances, you can optionally define custom tags for your AWS resources deployed by Cloudbreak.     If you want just one custom tag for your Cloudformation resources, set this variable in the  Profile :  export CB_AWS_DEFAULT_CF_TAG=mytagcontent  In this example, the name of the tag will be  CloudbreakId  and the value will be  mytagcontent .    If you prefer to customize the tag name, set this variable:  export CB_AWS_CUSTOM_CF_TAGS=mytagname:mytagvalue  In this example the name of the tag will be  mytagname  and the value will be  mytagvalue .     You can specify a list of tags with a comma separated list:   export CB_AWS_CUSTOM_CF_TAGS=tag1:value1,tag2:value2,tag3:value3        TO-DO: What exactly gets tagged? \nTO-DO: This configuration is available on AWS only??",
            "title": "Add Tags in Profile (AWS)"
        },
        {
            "location": "/tags/index.html#cloud-provider-documentation",
            "text": "To learn more about tags and their restrictions, refer to the cloud provider documentation:   Tags on AWS       Tags on Azure     Labels on GCP     Tags on OpenStack",
            "title": "Cloud Provider Documentation"
        },
        {
            "location": "/autoscaling/index.html",
            "text": "Auto Scaling",
            "title": "Auto Scaling"
        },
        {
            "location": "/autoscaling/index.html#auto-scaling",
            "text": "",
            "title": "Auto Scaling"
        },
        {
            "location": "/images/index.html",
            "text": "Images",
            "title": "Custom Cloud Images (TP)"
        },
        {
            "location": "/images/index.html#images",
            "text": "",
            "title": "Images"
        },
        {
            "location": "/ambari-db/index.html",
            "text": "Ambari Database",
            "title": "Ambari Database (TP)"
        },
        {
            "location": "/ambari-db/index.html#ambari-database",
            "text": "",
            "title": "Ambari Database"
        },
        {
            "location": "/cb-db/index.html",
            "text": "Ambari Database",
            "title": "Cloudbreak Database"
        },
        {
            "location": "/cb-db/index.html#ambari-database",
            "text": "",
            "title": "Ambari Database"
        },
        {
            "location": "/credentials/index.html",
            "text": "Credentials\n\n\nYou can manage Cloudbreak credentials in the \nmanage credentials\n tab by clicking on \n+create credential\n and providing required parameters. You must create at least one credential in order to be able to create a cluster. \n\n\nCreating Cloudbreak Credental\n\n\nFor steps, refer to:\n\n\n\n\nCreate Credential on AWS\n  \n\n\nCreate Credential on Azure\n  \n\n\nCreate Credential on GCP\n \n\n\nCreate Credential on OpenStack\n\n\n\n\nManaging Cloudbrek Credentials\n\n\nYou can manage (add and delete) your credentials from the \nmanage credentials\n tab. \n\n\nAll credentials that was cerated with \"Public In Account\" unchecked (which is the default behavior) are only visible to the user who created them. \n\n\nAll credentials that were cerated with \"Public In Account\" checked are visible to all users of the Cloudbreak instance, but only the user who created them can delete them.",
            "title": "Manage Cloudbreak Credentials"
        },
        {
            "location": "/credentials/index.html#credentials",
            "text": "You can manage Cloudbreak credentials in the  manage credentials  tab by clicking on  +create credential  and providing required parameters. You must create at least one credential in order to be able to create a cluster.",
            "title": "Credentials"
        },
        {
            "location": "/credentials/index.html#creating-cloudbreak-credental",
            "text": "For steps, refer to:   Create Credential on AWS     Create Credential on Azure     Create Credential on GCP    Create Credential on OpenStack",
            "title": "Creating Cloudbreak Credental"
        },
        {
            "location": "/credentials/index.html#managing-cloudbrek-credentials",
            "text": "You can manage (add and delete) your credentials from the  manage credentials  tab.   All credentials that was cerated with \"Public In Account\" unchecked (which is the default behavior) are only visible to the user who created them.   All credentials that were cerated with \"Public In Account\" checked are visible to all users of the Cloudbreak instance, but only the user who created them can delete them.",
            "title": "Managing Cloudbrek Credentials"
        },
        {
            "location": "/cb-account/index.html",
            "text": "Manage Your Account\n\n\nYou can manage your Cloudbreak account from the Cloudbreak UI by clicking \naccount\n in the top right corner.\n\n\nGet Azure Usage Report\n\n\nYou can generate a usage report for all cluster resources related to your Cloudbreak instance. To generate a report:\n\n\n\n\nFrom the Cloudbreak dashboard, click on \naccount\n in the top right corner.  \n\n\nNavigate to the \nusage report\n tab.  \n\n\nSelect the range of dates for which you want the report.  \n\n\nSelect a specific user or \nall\n.  \n\n\nSelect a region.  \n\n\nClick \ngenerate\n to generate the report.  \n\n\nThe report will be displayed, including instance types and running time for each node group.   \n\n\n\n\nCheck Account Details\n\n\nTo view your account details, navigate to the \naccount details\n tab. \n\n\nManage Users\n\n\nYou can manage existing users (activate and deactivate) and invite new users to use your Cloudbreak deployment from the \nmanage users\n tab.\n\n\nInvite a New User\n\n\n\n    \nNote\n\n    \nSMTP is not supported by Azure, so in order to use this feature you have to configure an external email service using the steps described in the \n Cloudbreak\n documentation.\n\n\n\n\n\nTo invite a new user:\n\n\n\n\nFrom the Cloudbreak dashboard, click on \naccount\n in the top right corner.  \n\n\nNavigate to the \nmanage users\n tab.   \n\n\nClick on \n+invite new user\n. \n\n\nSpecify the email address and the scope of access for the user. \n\n\nClick \n+invite new user\n and you will see the name of the new user added to the user list.\n\n\nClick on the name of the user.\n\n\nClick \nactivate\n. \n\n\n\n\nActivate an Existing User\n\n\nTo activate an existing user:\n\n\n\n\nFrom the Cloudbreak dashboard, click on \naccount\n in the top right corner.  \n\n\nNavigate to the \nmanage users\n tab. \n\n\nClick on the name of the user.\n\n\nClick \nactivate\n. \n\n\n\n\nDeactivate an Existing User\n\n\nTo deactivate an existing user:\n\n\n\n\nFrom the Cloudbreak dashboard, click on \naccount\n in the top right corner.  \n\n\nNavigate to the \nmanage users\n tab. \n\n\nClick on the name of the user.\n\n\nClick \ndeactivate\n.",
            "title": "Manage Your Account"
        },
        {
            "location": "/cb-account/index.html#manage-your-account",
            "text": "You can manage your Cloudbreak account from the Cloudbreak UI by clicking  account  in the top right corner.",
            "title": "Manage Your Account"
        },
        {
            "location": "/cb-account/index.html#get-azure-usage-report",
            "text": "You can generate a usage report for all cluster resources related to your Cloudbreak instance. To generate a report:   From the Cloudbreak dashboard, click on  account  in the top right corner.    Navigate to the  usage report  tab.    Select the range of dates for which you want the report.    Select a specific user or  all .    Select a region.    Click  generate  to generate the report.    The report will be displayed, including instance types and running time for each node group.",
            "title": "Get Azure Usage Report"
        },
        {
            "location": "/cb-account/index.html#check-account-details",
            "text": "To view your account details, navigate to the  account details  tab.",
            "title": "Check Account Details"
        },
        {
            "location": "/cb-account/index.html#manage-users",
            "text": "You can manage existing users (activate and deactivate) and invite new users to use your Cloudbreak deployment from the  manage users  tab.",
            "title": "Manage Users"
        },
        {
            "location": "/cb-account/index.html#invite-a-new-user",
            "text": "Note \n     SMTP is not supported by Azure, so in order to use this feature you have to configure an external email service using the steps described in the   Cloudbreak  documentation.   To invite a new user:   From the Cloudbreak dashboard, click on  account  in the top right corner.    Navigate to the  manage users  tab.     Click on  +invite new user .   Specify the email address and the scope of access for the user.   Click  +invite new user  and you will see the name of the new user added to the user list.  Click on the name of the user.  Click  activate .",
            "title": "Invite a New User"
        },
        {
            "location": "/cb-account/index.html#activate-an-existing-user",
            "text": "To activate an existing user:   From the Cloudbreak dashboard, click on  account  in the top right corner.    Navigate to the  manage users  tab.   Click on the name of the user.  Click  activate .",
            "title": "Activate an Existing User"
        },
        {
            "location": "/cb-account/index.html#deactivate-an-existing-user",
            "text": "To deactivate an existing user:   From the Cloudbreak dashboard, click on  account  in the top right corner.    Navigate to the  manage users  tab.   Click on the name of the user.  Click  deactivate .",
            "title": "Deactivate an Existing User"
        },
        {
            "location": "/upgrade/index.html",
            "text": "Upgrade Cloudbreak\n\n\nUpdate Cloudbreak Deployer\n\n\nTo upgrade Cloudbreak to the newest version, perform the following steps:\n\n\n\n\n\n\nOn the VM where Cloudbreak ir running, navigate to the directory where your Profile file is located:\n\n\ncd /var/lib/cloudbreak-deployment/\n\n\n\n\n\n\nStop all of the running Cloudbreak components:\n\n\ncbd kill\n\n\n\n\n\n\nUpdate Cloudbreak deployer:\n\n\ncbd update\n\n\n\n\n\n\nUpdate the \ndocker-compose.yml\n file with new Docker containers needed for the cbd:\n\n\ncbd regenerate\n\n\n\n\n\n\nIf there are no other Cloudbreak instances that still use old Cloudbreak versions, remove the obsolete containers:\n\n\ncbd util cleanup\n\n\n\n\n\n\nCheck the health and version of the updated cbd:\n\n\ncbd doctor\n\n\n\n\n\n\nStart the new version of the cbd:\n\n\ncbd start\n\n\n\n\n\n\n\n\nCloudbreak needs to download updated docker images for the new version, so this step may take a while.\n\n\n\n\nIn addition, if you have any clusters running, you must update them using the folloing steps. \n\n\nUpdate Existing Clusters\n\n\n\n\n\n\n\n\n\n\nTO-DO: Maybe the sentence below should say \"Upgrading from version 1.4.0 \nor newer\n to the newest version\"??\n\n\n\n\n\n\n\n\n\n\nUpgrading from version 1.4.0 to the newest version does not require any manual modification from the users.\n\n\nUpgrading from version 1.3.0 to the newest version requires that you update existing clusters. To update existing clusters, run the following commands on the \ncbgateway\n node of the cluster:\n\n\n\n\n\n\nUpdate the version of the Salt-Bootsrap tool on the nodes:\n    \nsalt '*' cmd.run 'curl -Ls https://github.com/sequenceiq/salt-bootstrap/releases/download/v0.1.2/salt-bootstrap_0.1.2_Linux_x86_64.tgz | tar -zx -C /usr/sbin/ salt-bootstrap'\n\n\n\n\n\n\nTrigger restart of the tool on the nodes:\n\n\nsalt '*' service.dead salt-bootstrap\n\n\n\n\nTo check the version of the Salt-Bootsrap on the nodes, use \nsalt '*' cmd.run 'salt-bootstrap --version'",
            "title": "Upgrade Clodbreak"
        },
        {
            "location": "/upgrade/index.html#upgrade-cloudbreak",
            "text": "",
            "title": "Upgrade Cloudbreak"
        },
        {
            "location": "/upgrade/index.html#update-cloudbreak-deployer",
            "text": "To upgrade Cloudbreak to the newest version, perform the following steps:    On the VM where Cloudbreak ir running, navigate to the directory where your Profile file is located:  cd /var/lib/cloudbreak-deployment/    Stop all of the running Cloudbreak components:  cbd kill    Update Cloudbreak deployer:  cbd update    Update the  docker-compose.yml  file with new Docker containers needed for the cbd:  cbd regenerate    If there are no other Cloudbreak instances that still use old Cloudbreak versions, remove the obsolete containers:  cbd util cleanup    Check the health and version of the updated cbd:  cbd doctor    Start the new version of the cbd:  cbd start     Cloudbreak needs to download updated docker images for the new version, so this step may take a while.   In addition, if you have any clusters running, you must update them using the folloing steps.",
            "title": "Update Cloudbreak Deployer"
        },
        {
            "location": "/upgrade/index.html#update-existing-clusters",
            "text": "TO-DO: Maybe the sentence below should say \"Upgrading from version 1.4.0  or newer  to the newest version\"??      Upgrading from version 1.4.0 to the newest version does not require any manual modification from the users.  Upgrading from version 1.3.0 to the newest version requires that you update existing clusters. To update existing clusters, run the following commands on the  cbgateway  node of the cluster:    Update the version of the Salt-Bootsrap tool on the nodes:\n     salt '*' cmd.run 'curl -Ls https://github.com/sequenceiq/salt-bootstrap/releases/download/v0.1.2/salt-bootstrap_0.1.2_Linux_x86_64.tgz | tar -zx -C /usr/sbin/ salt-bootstrap'    Trigger restart of the tool on the nodes:  salt '*' service.dead salt-bootstrap   To check the version of the Salt-Bootsrap on the nodes, use  salt '*' cmd.run 'salt-bootstrap --version'",
            "title": "Update Existing Clusters"
        },
        {
            "location": "/delete/index.html",
            "text": "Deleting Resources\n\n\nDelete Cloudbreak Controller\n\n\nTo delete Cloudbreak Controller, delete the whole related resource group:\n\n\n\n\nFrom the Microsoft Azure Portal dashboard, select \n.\n\n\n\n\nFind the resource group that you want to delete, click on \n...\n and select \nDelete\n:\n\n\n  \n\n\n\n\n\n\nType the name of the resource group to delete and click \nDelete\n.\n\n\n\n\n\n\nDeleting Clusters\n\n\nYou can delete clusters from the Cloudbreak UI. If needed, you can also delete the cluster manually by deleting the whole resource group created when the cluster was deployed. \n\n\nThe name of the resource group, under which the cluster-related resources are organized always includes the name of the cluster, so you should be able to find the group by searching for that name in the \nResource groups\n.",
            "title": "Delete Cloudbreak"
        },
        {
            "location": "/delete/index.html#deleting-resources",
            "text": "",
            "title": "Deleting Resources"
        },
        {
            "location": "/delete/index.html#delete-cloudbreak-controller",
            "text": "To delete Cloudbreak Controller, delete the whole related resource group:   From the Microsoft Azure Portal dashboard, select  .   Find the resource group that you want to delete, click on  ...  and select  Delete :        Type the name of the resource group to delete and click  Delete .",
            "title": "Delete Cloudbreak Controller"
        },
        {
            "location": "/delete/index.html#deleting-clusters",
            "text": "You can delete clusters from the Cloudbreak UI. If needed, you can also delete the cluster manually by deleting the whole resource group created when the cluster was deployed.   The name of the resource group, under which the cluster-related resources are organized always includes the name of the cluster, so you should be able to find the group by searching for that name in the  Resource groups .",
            "title": "Deleting Clusters"
        },
        {
            "location": "/vm-launch/index.html",
            "text": "Install Cloudbreak in Your Own VM\n\n\nThis is an advanced deployment option. Select this option if you have custom VM requirements. Otherwise, you should use one of the pre-built images and follow these instructions:\n\n\n\n\nLaunch on AWS\n  \n\n\nLaunch on Azure\n  \n\n\nLaunch on GCP\n  \n\n\nLaunch on OpenStack\n   \n\n\n\n\nSystem Requirements\n\n\nTo launch the Cloudbreak deployer and install the Cloudbreak application, your system must meet the following requirements:\n\n\n\n\nMinimum VM requirements: 8GB RAM, 10GB disk, 2 cores\n\n\nSupported operating systems: RHEL, CentOS, and Oracle Linux 7 (64-bit)\n\n\nDocker 1.9.1 must be installed \n\n\n\n\n\n\nYou can install Cloudbreak on Mac OS X for evaluation purposes only. Mac OS X is not supported for a production deployment of Cloudbreak.\n\n\n\n\nPrerequisites\n\n\nTo launch the Cloudbreak deployer and install the Cloudbreak application, you must first meet the following prerequisites:\n\n\nPorts\n\n\nPorts 22 (SSH) and 443 (HTTPS) must be open.\n\n\nRoot Access\n\n\nEvery command must be executed as root. In order to get root privileges execute: \n\n\nsudo -i\n\n\n\nSystem Updates\n\n\nEnsure that your system is up-to-date by executing:\n\n\nyum -y update\n\n\n\nReboot it if necessary.\n\n\nIptables\n\n\nInstall iptables-services:\n\n\nyum -y install iptables-services net-tools\n\n\n\nWithout iptables-services installed the \niptables save\n command will not be available.\n\n\nNext, configure permissive iptables on your machine:\n\n\n\niptables --flush INPUT && \\\niptables --flush FORWARD && \\\nservice iptables save\n\n\n\n\nInstall Cloudbreak on Your Own VM\n\n\nInstall Cloudbreak using the following steps\"\n\n\n\n\n\n\nInstall the Cloudbreak deployer and unzip the platform-specific single binary to your PATH. For example:\n\n\nyum -y install unzip tar\ncurl -Ls s3.amazonaws.com/public-repo-1.hortonworks.com/HDP/cloudbreak/cloudbreak-deployer_1.16.1_$(uname)_x86_64.tgz | sudo tar -xz -C /bin cbd\ncbd --version\n\n\nOnce the Cloudbreak Deployer is installed, you can set up the Cloudbreak application.\n\n\n\n\n\n\nCreate a Cloudbreak deployment directory and navigate to it:\n\n\nmkdir cloudbreak-deployment\ncd cloudbreak-deployment\n\n\n\n\n\n\nIn the directory, create a file called \nProfile\n with the following content:\n\n\nexport UAA_DEFAULT_SECRET=MY-SECRET\nexport UAA_DEFAULT_USER_PW=MY-PASSWORD\n\n\nFor example:\n\n\nexport UAA_DEFAULT_SECRET=MySecret123\nexport UAA_DEFAULT_USER_PW=MySecurePassword123\n\n\n\n\nYou will need to provide the password when logging in to the Cloudbreak web UI and when using the Cloudbreak Shell. The secret will be used by Cloudbreak for authentication.\n\n\n\n\n\n\n\n\nGenerate configurations by executing:\n\n\nrm *.yml\ncbd generate\n   \n\n\nThe cbd start command includes the cbd generate command which applies the following steps:\n\n\n\n\nCreates the \ndocker-compose.yml\n file, which describes the configuration of all the Docker containers required for the Cloudbreak deployment.  \n\n\nCreates the \nuaa.yml\n file, which holds the configuration of the identity server used to authenticate users with Cloudbreak.   \n\n\n\n\n\n\n\n\nStart the Cloudbreak application by using the following commands:\n\n\ncbd pull\ncbd start\n\n\nThis will start the Docker containers and initialize the application. The first time you start the Coudbreak app, the process will take longer than usual due to the download of all the necessary docker images.\n\n\n\n\n\n\nNext, check Cloudbreak Application logs: \n\n\ncbd logs cloudbreak\n\n\nYou should see a message like this in the log: \nStarted CloudbreakApplication in 36.823 seconds.\n Cloudbreak normally takes less than a minute to start.\n\n\n\n\n\n\nTroubleshooting\n\n\nCbd Cannot Get VM's Public IP\n\n\nBy default the \ncbd\n tool tries to get the VM's public IP to bind Cloudbreak UI to it. But if \ncbd\n cannot get the IP address during the initialization, you must set it manually. Check your \nProfile\n and if \nPUBLIC_IP\n is not set, add the \nPUBLIC_IP\n variable and set it to the public IP of the VM. For example: \n\n\nexport PUBLIC_IP=192.134.23.10\n\n\n\nPermission or Connection Problems\n\n\nIf you face permission or connection issues, disable SELinux:\n\n\n\n\nSet \nSELINUX=disabled\n in \n/etc/selinux/config\n.  \n\n\nReboot the machine.  \n\n\n\n\nEnsure the SELinux is not turned on afterwards:\n\n\n\n\n\n\n\n\nNext Steps\n\n\nFollow the platform-specific instructions. Make sure to review the prerequisites for creating a Cloudbreak credential and then log in to the Cloudbreak web UI and create a credential for Cloubdreak.\n\n\n\n\nLaunch on AWS\n\n\nLaunch on Azure\n\n\nLaunch on GCP\n\n\nLaunch on OpenStack",
            "title": "Install on Your Own VM"
        },
        {
            "location": "/vm-launch/index.html#install-cloudbreak-in-your-own-vm",
            "text": "This is an advanced deployment option. Select this option if you have custom VM requirements. Otherwise, you should use one of the pre-built images and follow these instructions:   Launch on AWS     Launch on Azure     Launch on GCP     Launch on OpenStack",
            "title": "Install Cloudbreak in Your Own VM"
        },
        {
            "location": "/vm-launch/index.html#system-requirements",
            "text": "To launch the Cloudbreak deployer and install the Cloudbreak application, your system must meet the following requirements:   Minimum VM requirements: 8GB RAM, 10GB disk, 2 cores  Supported operating systems: RHEL, CentOS, and Oracle Linux 7 (64-bit)  Docker 1.9.1 must be installed     You can install Cloudbreak on Mac OS X for evaluation purposes only. Mac OS X is not supported for a production deployment of Cloudbreak.",
            "title": "System Requirements"
        },
        {
            "location": "/vm-launch/index.html#prerequisites",
            "text": "To launch the Cloudbreak deployer and install the Cloudbreak application, you must first meet the following prerequisites:",
            "title": "Prerequisites"
        },
        {
            "location": "/vm-launch/index.html#ports",
            "text": "Ports 22 (SSH) and 443 (HTTPS) must be open.",
            "title": "Ports"
        },
        {
            "location": "/vm-launch/index.html#root-access",
            "text": "Every command must be executed as root. In order to get root privileges execute:   sudo -i",
            "title": "Root Access"
        },
        {
            "location": "/vm-launch/index.html#system-updates",
            "text": "Ensure that your system is up-to-date by executing:  yum -y update  Reboot it if necessary.",
            "title": "System Updates"
        },
        {
            "location": "/vm-launch/index.html#iptables",
            "text": "Install iptables-services:  yum -y install iptables-services net-tools  Without iptables-services installed the  iptables save  command will not be available.  Next, configure permissive iptables on your machine:  \niptables --flush INPUT && \\\niptables --flush FORWARD && \\\nservice iptables save",
            "title": "Iptables"
        },
        {
            "location": "/vm-launch/index.html#install-cloudbreak-on-your-own-vm",
            "text": "Install Cloudbreak using the following steps\"    Install the Cloudbreak deployer and unzip the platform-specific single binary to your PATH. For example:  yum -y install unzip tar\ncurl -Ls s3.amazonaws.com/public-repo-1.hortonworks.com/HDP/cloudbreak/cloudbreak-deployer_1.16.1_$(uname)_x86_64.tgz | sudo tar -xz -C /bin cbd\ncbd --version  Once the Cloudbreak Deployer is installed, you can set up the Cloudbreak application.    Create a Cloudbreak deployment directory and navigate to it:  mkdir cloudbreak-deployment\ncd cloudbreak-deployment    In the directory, create a file called  Profile  with the following content:  export UAA_DEFAULT_SECRET=MY-SECRET\nexport UAA_DEFAULT_USER_PW=MY-PASSWORD  For example:  export UAA_DEFAULT_SECRET=MySecret123\nexport UAA_DEFAULT_USER_PW=MySecurePassword123   You will need to provide the password when logging in to the Cloudbreak web UI and when using the Cloudbreak Shell. The secret will be used by Cloudbreak for authentication.     Generate configurations by executing:  rm *.yml\ncbd generate      The cbd start command includes the cbd generate command which applies the following steps:   Creates the  docker-compose.yml  file, which describes the configuration of all the Docker containers required for the Cloudbreak deployment.    Creates the  uaa.yml  file, which holds the configuration of the identity server used to authenticate users with Cloudbreak.        Start the Cloudbreak application by using the following commands:  cbd pull\ncbd start  This will start the Docker containers and initialize the application. The first time you start the Coudbreak app, the process will take longer than usual due to the download of all the necessary docker images.    Next, check Cloudbreak Application logs:   cbd logs cloudbreak  You should see a message like this in the log:  Started CloudbreakApplication in 36.823 seconds.  Cloudbreak normally takes less than a minute to start.",
            "title": "Install Cloudbreak on Your Own VM"
        },
        {
            "location": "/vm-launch/index.html#troubleshooting",
            "text": "",
            "title": "Troubleshooting"
        },
        {
            "location": "/vm-launch/index.html#cbd-cannot-get-vms-public-ip",
            "text": "By default the  cbd  tool tries to get the VM's public IP to bind Cloudbreak UI to it. But if  cbd  cannot get the IP address during the initialization, you must set it manually. Check your  Profile  and if  PUBLIC_IP  is not set, add the  PUBLIC_IP  variable and set it to the public IP of the VM. For example:   export PUBLIC_IP=192.134.23.10",
            "title": "Cbd Cannot Get VM's Public IP"
        },
        {
            "location": "/vm-launch/index.html#permission-or-connection-problems",
            "text": "If you face permission or connection issues, disable SELinux:   Set  SELINUX=disabled  in  /etc/selinux/config .    Reboot the machine.     Ensure the SELinux is not turned on afterwards:",
            "title": "Permission or Connection Problems"
        },
        {
            "location": "/vm-launch/index.html#next-steps",
            "text": "Follow the platform-specific instructions. Make sure to review the prerequisites for creating a Cloudbreak credential and then log in to the Cloudbreak web UI and create a credential for Cloubdreak.   Launch on AWS  Launch on Azure  Launch on GCP  Launch on OpenStack",
            "title": "Next Steps"
        },
        {
            "location": "/security/index.html",
            "text": "Network and Security\n\n\nVirtual Network\n\n\nAzure uses Virtual network (VNet) service to create virtual networks that resembles a traditional networks. Your Cloudbreak controller and clusters are launched into the virtual network infrastructure, with a new VNet created for each resource (Cloudbreak controller, cluster1, cluster2, and so on).\n\n\nNetwork Security Groups\n\n\nNetwork security groups are set up to control network traffic to the VMs in the system. By default, the system is configured to restrict inbound network traffic to the minimal set of ports. You can add or modify rules to each security group that allow traffic to or from its associated instances.\n\n\nThis section describes the default security group configuration for the various components in the system.\n\n\n\n\nThe inbound and outbound rules (protocols, port and IP ranges) for the security groups can be modified later using the Network Security Groups dashboard.\n\n\n\n\nThe naming convention for the security groups that are automatically created is:\n\n\n\n\nCloudbreak controller VM: cbdeployerNsg\n\n\nCluster node VMs: {host_group_name}{cluster_name}sg\n\n\n\n\nThe following security groups are created automatically:\n\n\nCloudbreak Security Group\n\n\nThe \ncbdeployerNsg\n security group is created when launching  Cloudbreak and is associated with the Cloudbreak VM. The following table lists the security group port configuration for the cloud controller instance.\nThe security group \nSource\n for these ports is set to the \nRemote Access\n CIDR IP specified when\nlaunching Cloudbreak.\n\n\n\n\n\n\n\n\nInbound Port\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\n22\n\n\nSSH access to the Cloudbreak VM.\n\n\n\n\n\n\n80\n\n\nHTTP access to the Cloudbreak UI. This is automatically redirected to the HTTPS (443) port.\n\n\n\n\n\n\n443\n\n\nHTTPS access to the Cloudbreak UI.\n\n\n\n\n\n\n\n\nCluster Security Groups\n\n\nMultiple security groups are created when you create a cluster, one for each host group. The\nsecurity group \nSource\n for these ports is set to the \nRemote Access\n CIDR IP specified when creating the cluster.\n\n\nThe following table lists the \nmaster node\n security group port configuration.\n\n\n\n\n\n\n\n\nInbound Port\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\n22\n\n\nSSH access to the node instance.\n\n\n\n\n\n\n9443\n\n\nInternal management port, used by the cloud controller to communicate with the cluster master node.\n\n\n\n\n\n\n8443\n1\n\n\nSecured HTTPS gateway access to the Ambari, Zeppelin, Hive JDBC, and other Cluster Components.\n\n\n\n\n\n\n\n\n\n\n1\n Port 8443 is only opened on the master node if when you create cluster, you check the checkbox under \nSetup Network and Security > Enable Knox Gateway\n (to Ambari and Zeppelin Web UIs, Hive JDBC and/or Cluster Components UIs).\n\n\n\n\nThe following table lists the security group port configuration for other host groups:\n\n\n\n\n\n\n\n\nInbound Port\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\n22\n\n\nSSH access to the node instance.\n\n\n\n\n\n\n\n\n\n    \nLearn More\n\n    \n\nRefer to the Azure network security groups \n documentation\n\nfor more information about viewing and modifying network security group rules for VMs.",
            "title": "Network and Security"
        },
        {
            "location": "/security/index.html#network-and-security",
            "text": "",
            "title": "Network and Security"
        },
        {
            "location": "/security/index.html#virtual-network",
            "text": "Azure uses Virtual network (VNet) service to create virtual networks that resembles a traditional networks. Your Cloudbreak controller and clusters are launched into the virtual network infrastructure, with a new VNet created for each resource (Cloudbreak controller, cluster1, cluster2, and so on).",
            "title": "Virtual Network"
        },
        {
            "location": "/security/index.html#network-security-groups",
            "text": "Network security groups are set up to control network traffic to the VMs in the system. By default, the system is configured to restrict inbound network traffic to the minimal set of ports. You can add or modify rules to each security group that allow traffic to or from its associated instances.  This section describes the default security group configuration for the various components in the system.   The inbound and outbound rules (protocols, port and IP ranges) for the security groups can be modified later using the Network Security Groups dashboard.   The naming convention for the security groups that are automatically created is:   Cloudbreak controller VM: cbdeployerNsg  Cluster node VMs: {host_group_name}{cluster_name}sg   The following security groups are created automatically:",
            "title": "Network Security Groups"
        },
        {
            "location": "/security/index.html#cloudbreak-security-group",
            "text": "The  cbdeployerNsg  security group is created when launching  Cloudbreak and is associated with the Cloudbreak VM. The following table lists the security group port configuration for the cloud controller instance.\nThe security group  Source  for these ports is set to the  Remote Access  CIDR IP specified when\nlaunching Cloudbreak.     Inbound Port  Description      22  SSH access to the Cloudbreak VM.    80  HTTP access to the Cloudbreak UI. This is automatically redirected to the HTTPS (443) port.    443  HTTPS access to the Cloudbreak UI.",
            "title": "Cloudbreak Security Group"
        },
        {
            "location": "/security/index.html#cluster-security-groups",
            "text": "Multiple security groups are created when you create a cluster, one for each host group. The\nsecurity group  Source  for these ports is set to the  Remote Access  CIDR IP specified when creating the cluster.  The following table lists the  master node  security group port configuration.     Inbound Port  Description      22  SSH access to the node instance.    9443  Internal management port, used by the cloud controller to communicate with the cluster master node.    8443 1  Secured HTTPS gateway access to the Ambari, Zeppelin, Hive JDBC, and other Cluster Components.      1  Port 8443 is only opened on the master node if when you create cluster, you check the checkbox under  Setup Network and Security > Enable Knox Gateway  (to Ambari and Zeppelin Web UIs, Hive JDBC and/or Cluster Components UIs).   The following table lists the security group port configuration for other host groups:     Inbound Port  Description      22  SSH access to the node instance.     \n     Learn More \n     \nRefer to the Azure network security groups   documentation \nfor more information about viewing and modifying network security group rules for VMs.",
            "title": "Cluster Security Groups"
        },
        {
            "location": "/security-cb/index.html",
            "text": "Securing Cloudbreak After Launch\n\n\nCloudbreak comes with default settings designed for easy first experience rather than strict security. To secure Cloudbreak, follow these recommendations. \n\n\nRestricting Inbound Access\n\n\nWe recommend that you block all communication ports except 443 on the firewall or security group (depending on the provider). \n\n\nIf you have to log in to the Cloudbreak host remotely, use the SSH port (usually 22).\n\n\n\n\n\n\n\n\n\n\nTO-DO: Is the step above up-to-date? I think ports 80 and 22 is also open. \n\n\n\n\n\n\n\n\n\n\nConfiguring the Profile\n\n\nBefore starting Cloudbreak for the first time, configure the Profile file as directed below. Changes are applied during startup so a restart (\ncbd restart\n) is required after each change.\n\n\n\n\n\n\nExecute the following command in the directory where you want to store Cloudbreak-related files:\n\n\n\necho export PUBLIC_IP=[the ip or hostname to bind] > Profile\n\n\n\n\n\n\n\n\n\n\n\nTO-DO: Do you mean that this needs to be executed in the deployment directory? Or?\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAfter you have a base Profile file, add the following custom properties to it:\n\n\n\nexport UAA_DEFAULT_SECRET='[custom secret]'\nexport UAA_DEFAULT_USER_EMAIL='[default admin email address]'\nexport UAA_DEFAULT_USER_PW='[default admin password]'\nexport UAA_DEFAULT_USER_FIRSTNAME='[default admin first name]'\nexport UAA_DEFAULT_USER_LASTNAME='[default admin last name]'\n\n\n\nCloudbreak has additional secrets which by default inherit their values from \nUAA_DEFAULT_SECRET\n. Instead of using the default, you can define different values in the Profile for each of these service clients:\n\n\n\nexport UAA_CLOUDBREAK_SECRET='[cloudbreak secret]'\nexport UAA_PERISCOPE_SECRET='[auto scaling secret]'\nexport UAA_ULUWATU_SECRET='[web ui secret]'\nexport UAA_SULTANS_SECRET='[authenticator secret]'\n\n\n\nYou can change these secrets at any time, except \nUAA_CLOUDBREAK_SECRET\n which is used to encrypt sensitive information at database level. \n\n\n\n\n\n\n\n\n\n\nTO-DO: The info below is explained in a way that is confusing. Can you rephrase? \n\n\n\n\n\n\n\n\n\n\nUAA_DEFAULT_USER_PW\n is stored in plain text format, but if \nUAA_DEFAULT_USER_PW\n is missing from the Profile, it gets a default value. Because default password is not an option, if you set an empty password explicitly in the Profile Cloudbreak deployer will ask for password all the time when it is needed for the operation.\n\n\n\nexport UAA_DEFAULT_USER_PW=''\n\n\n\nIn this case, Cloudbreak deployer wouldn't be able to add the default user, so you have to do it manually by executing the following command:\n\n\n\ncbd util add-default-user\n\n\n\n\n\n\n\nAdding SSL Certificate for Cloudbreak",
            "title": "Securing Cloudbreak After Launch"
        },
        {
            "location": "/security-cb/index.html#securing-cloudbreak-after-launch",
            "text": "Cloudbreak comes with default settings designed for easy first experience rather than strict security. To secure Cloudbreak, follow these recommendations.",
            "title": "Securing Cloudbreak After Launch"
        },
        {
            "location": "/security-cb/index.html#restricting-inbound-access",
            "text": "We recommend that you block all communication ports except 443 on the firewall or security group (depending on the provider).   If you have to log in to the Cloudbreak host remotely, use the SSH port (usually 22).      TO-DO: Is the step above up-to-date? I think ports 80 and 22 is also open.",
            "title": "Restricting Inbound Access"
        },
        {
            "location": "/security-cb/index.html#configuring-the-profile",
            "text": "Before starting Cloudbreak for the first time, configure the Profile file as directed below. Changes are applied during startup so a restart ( cbd restart ) is required after each change.    Execute the following command in the directory where you want to store Cloudbreak-related files:  \necho export PUBLIC_IP=[the ip or hostname to bind] > Profile      TO-DO: Do you mean that this needs to be executed in the deployment directory? Or?        After you have a base Profile file, add the following custom properties to it:  \nexport UAA_DEFAULT_SECRET='[custom secret]'\nexport UAA_DEFAULT_USER_EMAIL='[default admin email address]'\nexport UAA_DEFAULT_USER_PW='[default admin password]'\nexport UAA_DEFAULT_USER_FIRSTNAME='[default admin first name]'\nexport UAA_DEFAULT_USER_LASTNAME='[default admin last name]'  Cloudbreak has additional secrets which by default inherit their values from  UAA_DEFAULT_SECRET . Instead of using the default, you can define different values in the Profile for each of these service clients:  \nexport UAA_CLOUDBREAK_SECRET='[cloudbreak secret]'\nexport UAA_PERISCOPE_SECRET='[auto scaling secret]'\nexport UAA_ULUWATU_SECRET='[web ui secret]'\nexport UAA_SULTANS_SECRET='[authenticator secret]'  You can change these secrets at any time, except  UAA_CLOUDBREAK_SECRET  which is used to encrypt sensitive information at database level.       TO-DO: The info below is explained in a way that is confusing. Can you rephrase?       UAA_DEFAULT_USER_PW  is stored in plain text format, but if  UAA_DEFAULT_USER_PW  is missing from the Profile, it gets a default value. Because default password is not an option, if you set an empty password explicitly in the Profile Cloudbreak deployer will ask for password all the time when it is needed for the operation.  \nexport UAA_DEFAULT_USER_PW=''  In this case, Cloudbreak deployer wouldn't be able to add the default user, so you have to do it manually by executing the following command:  \ncbd util add-default-user",
            "title": "Configuring the Profile"
        },
        {
            "location": "/security-cb/index.html#adding-ssl-certificate-for-cloudbreak",
            "text": "",
            "title": "Adding SSL Certificate for Cloudbreak"
        },
        {
            "location": "/security-ldap/index.html",
            "text": "LDAP",
            "title": "Configuring LDAP/AD"
        },
        {
            "location": "/security-ldap/index.html#ldap",
            "text": "",
            "title": "LDAP"
        },
        {
            "location": "/security-kerberos/index.html",
            "text": "Kerberos",
            "title": "Configuring Kerberos"
        },
        {
            "location": "/security-kerberos/index.html#kerberos",
            "text": "",
            "title": "Kerberos"
        },
        {
            "location": "/releasenotes/index.html",
            "text": "Release Notes\n\n\nSystem Info\n\n\nCloudbreak 2.1\n\n\nNew Features\n\n\nChange Log\n\n\nKnown Issues",
            "title": "Release Notes"
        },
        {
            "location": "/releasenotes/index.html#release-notes",
            "text": "",
            "title": "Release Notes"
        },
        {
            "location": "/releasenotes/index.html#system-info",
            "text": "Cloudbreak 2.1",
            "title": "System Info"
        },
        {
            "location": "/releasenotes/index.html#new-features",
            "text": "",
            "title": "New Features"
        },
        {
            "location": "/releasenotes/index.html#change-log",
            "text": "",
            "title": "Change Log"
        },
        {
            "location": "/releasenotes/index.html#known-issues",
            "text": "",
            "title": "Known Issues"
        },
        {
            "location": "/cb-shell/index.html",
            "text": "Cloudbreak Shell\n\n\n\n\n\n\n\n\n\n\nTO-DO: If we have new CLI, then this info will need to be updated.\n\n\n\n\n\n\n\n\n\n\nCloudbreak Shell is an interactive command line tool which:\n\n\n\n\nSupports all functionality available through the REST API and Cloudbreak web UI\n\n\nMakes possible complete automation of management task via scripts\n\n\nIncludes context-aware commands\n\n\nAllows tab completion\n\n\nSupports required and optional parameters\n\n\nIncludes a hint command to guide you\n\n\n\n\nInstall and Start Cloudbreak Shell\n\n\nThere are three ways to install and run Cloudbreak Shell:\n\n\n\n\nFrom the Cloudbreak VM (Recommended) \n\n\nFrom the Docker Image  \n\n\nBuild From Source  \n\n\n\n\n\n\nThe latter two methods run the CLI on your local machine. The first method runs the CLI on the VM but provides an option to run the commands on your local machine. \n\n\n\n\nFrom the Cloudbreak VM\n\n\nThe easiest way to install and start Cloudbreak Shell is from the VM on which you have deployed Cloudbreak. On the VM, navigate to the \n/var/lib/cloudbreak-deployment/\n directory and run:\n\n\ncbd util cloudbreak-shell\n\n\n\nIf you would like to use Cloudbreak Shell from your local machine:\n\n\n\n\n\n\nExecute \ncbd util cloudbreak-shell-remote\n on the VM where Cloudbreak is running.\n\n\n\n\n\n\nCopy the output of the above command. \n\n\n\n\n\n\nPaste and execute the output on your local machine.\n\n\n\n\n\n\nFrom the Docker Image\n\n\nYou can find the docker image and its documentation \nhere\n.\n\n\n\n\n\n\n\n\n\n\nTO-DO: Add from the github doc? Is this still valid? \n\n\n\n\n\n\n\n\n\n\nBuild From Source\n\n\n\n\n\n\n\n\n\n\nTO-DO: Add from the old doc. Is this still valid? \n\n\nTO-DO: Include documentation from: (the repository url changed because the project is now under hwx)\n\n\n\n\n\n\n\n\n\n\n\n\nCloudbreak docs\n \n\n\nCloudbreak Shell\n \n\n\nProvisioning clusters via Cloudbreak Shell",
            "title": "Cloudbreak Shell"
        },
        {
            "location": "/cb-shell/index.html#cloudbreak-shell",
            "text": "TO-DO: If we have new CLI, then this info will need to be updated.      Cloudbreak Shell is an interactive command line tool which:   Supports all functionality available through the REST API and Cloudbreak web UI  Makes possible complete automation of management task via scripts  Includes context-aware commands  Allows tab completion  Supports required and optional parameters  Includes a hint command to guide you",
            "title": "Cloudbreak Shell"
        },
        {
            "location": "/cb-shell/index.html#install-and-start-cloudbreak-shell",
            "text": "There are three ways to install and run Cloudbreak Shell:   From the Cloudbreak VM (Recommended)   From the Docker Image    Build From Source      The latter two methods run the CLI on your local machine. The first method runs the CLI on the VM but provides an option to run the commands on your local machine.",
            "title": "Install and Start Cloudbreak Shell"
        },
        {
            "location": "/cb-shell/index.html#from-the-cloudbreak-vm",
            "text": "The easiest way to install and start Cloudbreak Shell is from the VM on which you have deployed Cloudbreak. On the VM, navigate to the  /var/lib/cloudbreak-deployment/  directory and run:  cbd util cloudbreak-shell  If you would like to use Cloudbreak Shell from your local machine:    Execute  cbd util cloudbreak-shell-remote  on the VM where Cloudbreak is running.    Copy the output of the above command.     Paste and execute the output on your local machine.",
            "title": "From the Cloudbreak VM"
        },
        {
            "location": "/cb-shell/index.html#from-the-docker-image",
            "text": "You can find the docker image and its documentation  here .      TO-DO: Add from the github doc? Is this still valid?",
            "title": "From the Docker Image"
        },
        {
            "location": "/cb-shell/index.html#build-from-source",
            "text": "TO-DO: Add from the old doc. Is this still valid?   TO-DO: Include documentation from: (the repository url changed because the project is now under hwx)       Cloudbreak docs    Cloudbreak Shell    Provisioning clusters via Cloudbreak Shell",
            "title": "Build From Source"
        },
        {
            "location": "/profile/index.html",
            "text": "Profile Variables\n\n\nDuring startup, Cloudbreak deployer tries to determine the underlying infrastructure and then sets required environment variables with appropriate default values. If these environment variables are not sufficient for your use case, you can set additional environment variables in your \nProfile\n file. Refer to the list below for available custom environment variables. The variables are listed with their default values. If default is unset, no value is listed. \n\n\nSetting Provfle Variables\n\n\n\n\n\n\n\n\n\n\nTO-DO: How to set them? \n\n\n\n\n\n\n\n\n\n\nCloudbreak Variables\n\n\n\n\n\n\n\n\nVariable Name\n\n\nDefault Value\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nADDRESS_RESOLVING_TIMEOUT\n\n\n120000\n\n\nDNS lookup timeout for internal service discovery\n\n\n\n\n\n\nCAPTURE_CRON_EXPRESSION\n\n\n\n\nSmartSense bundle generation time interval in cron format\n\n\n\n\n\n\nCBD_CERT_ROOT_PATH\n\n\n\"${PWD}/certs\"\n\n\nPath where deployer stores Cloudbreak certificates\n\n\n\n\n\n\nCBD_LOG_NAME\n\n\ncbreak\n\n\nName of the Cloudbreak log file\n\n\n\n\n\n\nCBD_TRAEFIK_TLS\n\n\n\"/certs/traefik/client.pem,/certs/traefik/client-key.pem\"\n\n\nPath inside of the Traefik container where TLS files are located\n\n\n\n\n\n\nCB_BLUEPRINT_DEFAULTS\n\n\n\"hdp-small-default;hdp-spark-cluster;hdp-streaming-cluster\"\n\n\nComma separated list of the default blueprints that Cloudbreak initializes in its database\n\n\n\n\n\n\nCB_BYOS_DFS_DATA_DIR\n\n\n\"/hadoop/hdfs/data\"\n\n\n(Deprecated) Default data directory for BYOP orchestrators\n\n\n\n\n\n\nCB_COMPONENT_CLUSTER_ID\n\n\n\n\nSmartSense component cluster ID\n\n\n\n\n\n\nCB_COMPONENT_ID\n\n\n\n\nSmartSense component ID\n\n\n\n\n\n\nCB_COMPOSE_PROJECT\n\n\ncbreak\n\n\nName of the Docker Compose project; it will appear in container names\n\n\n\n\n\n\nCB_DB_ENV_DB\n\n\n\"cbdb\"\n\n\nName of the Cloudbreak database\n\n\n\n\n\n\nCB_DB_ENV_PASS\n\n\n\"\"\n\n\nPassword for the Cloudbreak database authentication\n\n\n\n\n\n\nCB_DB_ENV_SCHEMA\n\n\n\"public\"\n\n\nSchema used in the Cloudbreak database\n\n\n\n\n\n\nCB_DB_ENV_USER\n\n\n\"postgres\"\n\n\nUser for the Cloudbreak database authentication\n\n\n\n\n\n\nCB_DB_ROOT_PATH\n\n\n\"/var/lib/cloudbreak\"\n\n\n(Deprecated) Location of the database volume on Cloudbreak host\n\n\n\n\n\n\nCB_DEFAULT_SUBSCRIPTION_ADDRESS\n\n\nhttp://uluwatu.service.consul:3000/notifications\n\n\nURL of the default subscription for Cloudbreak notifications\n\n\n\n\n\n\nCB_ENABLEDPLATFORMS\n\n\n\n\nDisables Cloudbreak resource called Platform\n\n\n\n\n\n\nCB_ENABLE_CUSTOM_IMAGE\n\n\n\"false\"\n\n\nSet to \"true\" to enable custom cloud images\n\n\n\n\n\n\nCBD_FORCE_START\n\n\n\n\nDisables docker-compose.yml and uaa.yml validation\n\n\n\n\n\n\nCB_HBM2DDL_STRATEGY\n\n\n\"validate\"\n\n\nConfigures hibernate.hbm2ddl.auto in Cloudbreak\n\n\n\n\n\n\nCB_HOST_DISCOVERY_CUSTOM_DOMAIN\n\n\n\"\"\n\n\nCustom domain of the provisioned cluster\n\n\n\n\n\n\nCB_HTTPS_PROXY\n\n\n\"\"\n\n\nHTTPS proxy URL\n\n\n\n\n\n\nCB_HTTP_PROXY\n\n\n\"\"\n\n\nHTTP proxy URL\n\n\n\n\n\n\nCB_IMAGE_CATALOG_URL\n\n\n\"https://s3-eu-west-1.amazonaws.com/cloudbreak-info/cb-image-catalog.json\"\n\n\nImage catalog URL\n\n\n\n\n\n\nCB_INSTANCE_NODE_ID\n\n\n\n\nUnique identifier of the Cloudbreak node\n\n\n\n\n\n\nCB_INSTANCE_PROVIDER\n\n\n\n\nCloud provider of the Cloudbreak instance\n\n\n\n\n\n\nCB_INSTANCE_REGION\n\n\n\n\nCloud region of the Cloudbreak instance\n\n\n\n\n\n\nCB_INSTANCE_UUID\n\n\n\n\nUnique identifier of Cloudbreak deployment\n\n\n\n\n\n\nCB_JAVA_OPTS\n\n\n\"\"\n\n\nExtra Java options for Autoscale and Cloudbreak\n\n\n\n\n\n\nCB_LOG_LEVEL\n\n\n\"INFO\"\n\n\nLog level of the Cloudbreak service\n\n\n\n\n\n\nCB_MAX_SALT_NEW_SERVICE_RETRY\n\n\n90\n\n\nSalt orchestrator max retry count\n\n\n\n\n\n\nCB_MAX_SALT_RECIPE_EXECUTION_RETRY\n\n\n90\n\n\nSalt orchestrator max retry count for recipes\n\n\n\n\n\n\nCB_PLATFORM_DEFAULT_REGIONS\n\n\n\n\nComma separated list of default regions by platform. For example: \nAWS:eu-west-1\n.\n\n\n\n\n\n\nCB_PRODUCT_ID\n\n\n\n\nSmartSense product ID\n\n\n\n\n\n\nCB_SCHEMA_MIGRATION_AUTO\n\n\ntrue\n\n\nWhen set to true, enables Cloudbreak automatic database schema update\n\n\n\n\n\n\nCB_SMARTSENSE_CONFIGURE\n\n\n\"false\"\n\n\nSet to \u201ctrue\u201d to install and configure SmartSense on cluster nodes\n\n\n\n\n\n\nCB_SMARTSENSE_CLUSTER_NAME_PREFIX\n\n\n\n\nSmartSense Cloudbreak cluster name prefix\n\n\n\n\n\n\nCB_SMARTSENSE_ID\n\n\n\"\"\n\n\nSmartSense subscription ID\n\n\n\n\n\n\nCB_TEMPLATE_DEFAULTS\n\n\n\"minviable-gcp,minviable-azure,minviable-aws\"\n\n\nComma separated list of the default templates that Cloudbreak initializes in its database\n\n\n\n\n\n\nCB_UI_MAX_WAIT\n\n\n400\n\n\nWait timeout for \ncbd start-wait\n command\n\n\n\n\n\n\nCERT_VALIDATION\n\n\n\"true\"\n\n\nWhen set to \"true\", enables cert validation in Cloudbreak and Autoscale\n\n\n\n\n\n\nCLOUDBREAK_SMTP_AUTH\n\n\n\"true\"\n\n\nWhen set to \"true\", configures mail.smtp.auth in Cloudbreak\n\n\n\n\n\n\nCLOUDBREAK_SMTP_SENDER_FROM\n\n\n\"noreply@hortonworks.com\"\n\n\nEmail address of the sender\n\n\n\n\n\n\nCLOUDBREAK_SMTP_SENDER_HOST\n\n\n\"smtp.service.consul\"\n\n\nSMTP server address of the hostname\n\n\n\n\n\n\nCLOUDBREAK_SMTP_SENDER_PASSWORD\n\n\n\"$LOCAL_SMTP_PASSWORD\"\n\n\nSMTP server password\n\n\n\n\n\n\nCLOUDBREAK_SMTP_SENDER_PORT\n\n\n25\n\n\nPort of the SMTP server\n\n\n\n\n\n\nCLOUDBREAK_SMTP_SENDER_USERNAME\n\n\n\"admin\"\n\n\nUsername for SMTP authentication\n\n\n\n\n\n\nCLOUDBREAK_SMTP_STARTTLS_ENABLE\n\n\n\"false\"\n\n\nSet to \"true\" to configure mail.smtp.starttls.enable in Cloudbreak\n\n\n\n\n\n\nCLOUDBREAK_SMTP_TYPE\n\n\n\"smtp\"\n\n\nDefines mail.transport.protocol in CLoudbreak\n\n\n\n\n\n\nCOMMON_DB\n\n\ncommondb\n\n\nName of the database container\n\n\n\n\n\n\nCOMMON_DB_VOL\n\n\ncommon\n\n\nName of the database volume\n\n\n\n\n\n\nCOMPOSE_HTTP_TIMEOUT\n\n\n120\n\n\nDocker Compose execution timeout\n\n\n\n\n\n\nDB_DUMP_VOLUME\n\n\ncbreak_dump\n\n\nName of the database dump volume\n\n\n\n\n\n\nDB_MIGRATION_LOG\n\n\n\"db_migration.log\"\n\n\nDatabase migration log file\n\n\n\n\n\n\nDEFAULT_INBOUND_ACCESS_IP\n\n\n\"\"\n\n\nOpens default ports on AWS instances for address\n\n\n\n\n\n\nDOCKER_CONSUL_OPTIONS\n\n\n\"\"\n\n\nExtra options for Consul\n\n\n\n\n\n\nDOCKER_IMAGE_CBD_SMARTSENSE\n\n\nhortonworks/cbd-smartsense\n\n\nSmartSense Docker image name\n\n\n\n\n\n\nDOCKER_IMAGE_CLOUDBREAK\n\n\nhortonworks/cloudbreak\n\n\nCloudbreak Docker image name\n\n\n\n\n\n\nDOCKER_IMAGE_CLOUDBREAK_AUTH\n\n\nhortonworks/cloudbreak-auth\n\n\nAuthentication service Docker image name\n\n\n\n\n\n\nDOCKER_IMAGE_CLOUDBREAK_PERISCOPE\n\n\nhortonworks/cloudbreak-autoscale\n\n\nAutoscale Docker image name\n\n\n\n\n\n\nDOCKER_IMAGE_CLOUDBREAK_SHELL\n\n\nhortonworks/cloudbreak-shell\n\n\nCloudbreak Shell Docker image name\n\n\n\n\n\n\nDOCKER_IMAGE_CLOUDBREAK_WEB\n\n\nhortonworks/cloudbreak-web\n\n\nWeb UI Docker image name\n\n\n\n\n\n\nDOCKER_TAG_ALPINE\n\n\n3.1\n\n\nAlpine container version\n\n\n\n\n\n\nDOCKER_TAG_CBD_SMARTSENSE\n\n\n0.10.0\n\n\nSmartSense container version\n\n\n\n\n\n\nDOCKER_TAG_CERT_TOOL\n\n\n0.2.0\n\n\nCert tool container version\n\n\n\n\n\n\nDOCKER_TAG_CLOUDBREAK\n\n\n2.1.0-dev.70\n\n\nCloudbreak container version\n\n\n\n\n\n\nDOCKER_TAG_CLOUDBREAK_SHELL\n\n\n2.1.0-dev.70\n\n\nCloudbreak Shell container version\n\n\n\n\n\n\nDOCKER_TAG_CONSUL\n\n\n0.5\n\n\nConsul container version\n\n\n\n\n\n\nDOCKER_TAG_HAVEGED\n\n\n1.1.0\n\n\nHaveged container version\n\n\n\n\n\n\nDOCKER_TAG_LOGROTATE\n\n\n1.0.0\n\n\nLogrotate container version\n\n\n\n\n\n\nDOCKER_TAG_MIGRATION\n\n\n1.0.0\n\n\nMigration container version\n\n\n\n\n\n\nDOCKER_TAG_PERISCOPE\n\n\n2.1.0-dev.70\n\n\nAutoscale container version\n\n\n\n\n\n\nDOCKER_TAG_POSTFIX\n\n\nlatest\n\n\nPostfix container version\n\n\n\n\n\n\nDOCKER_TAG_POSTGRES\n\n\n9.6.1-alpine\n\n\nPostgresql container version\n\n\n\n\n\n\nDOCKER_TAG_REGISTRATOR\n\n\nv5\n\n\nRegistrator container version\n\n\n\n\n\n\nDOCKER_TAG_SULTANS\n\n\n2.1.0-dev.70\n\n\nAuthentication service container version\n\n\n\n\n\n\nDOCKER_TAG_TRAEFIK\n\n\nv1.2.0\n\n\nTraefik container version\n\n\n\n\n\n\nDOCKER_TAG_UAA\n\n\n3.6.5\n\n\nIdentity container version\n\n\n\n\n\n\nDOCKER_TAG_ULUWATU\n\n\n2.1.0-dev.70\n\n\nWeb UI container version\n\n\n\n\n\n\nIDENTITY_DB_NAME\n\n\n\"uaadb\"\n\n\nName of the Identity database\n\n\n\n\n\n\nIDENTITY_DB_PASS\n\n\n\"\"\n\n\nPassword for the Identity database authentication\n\n\n\n\n\n\nIDENTITY_DB_URL\n\n\n\"${COMMON_DB}.service.consul:5432\"\n\n\nURL for the Identity database connection, including the port number\n\n\n\n\n\n\nIDENTITY_DB_USER\n\n\n\"postgres\"\n\n\nUser for the Identity database authentication\n\n\n\n\n\n\nLOCAL_SMTP_PASSWORD\n\n\n\"$UAA_DEFAULT_USER_PW\"\n\n\nDefault password for the internal mail server\n\n\n\n\n\n\nPERISCOPE_DB_HBM2DDL_STRATEGY\n\n\n\"validate\"\n\n\nConfigures hibernate.hbm2ddl.auto in Autoscale\n\n\n\n\n\n\nPERISCOPE_DB_NAME\n\n\n\"periscopedb\"\n\n\nName of the Autoscale database\n\n\n\n\n\n\nPERISCOPE_DB_PASS\n\n\n\"\"\n\n\nPassword for the Autoscale database authentication\n\n\n\n\n\n\nPERISCOPE_DB_SCHEMA_NAME\n\n\n\"public\"\n\n\nSchema used in the Autoscale database\n\n\n\n\n\n\nPERISCOPE_DB_USER\n\n\n\"postgres\"\n\n\nUser for the Autoscale database authentication\n\n\n\n\n\n\nPERISCOPE_DB_TCP_ADDR\n\n\n\n\nAddress of the Autoscale database\n\n\n\n\n\n\nPERISCOPE_DB_TCP_PORT\n\n\n\n\nPort number of the Autoscale database\n\n\n\n\n\n\nPERISCOPE_LOG_LEVEL\n\n\n\"INFO\"\n\n\nLog level of the Autoscale service\n\n\n\n\n\n\nPERISCOPE_SCHEMA_MIGRATION_AUTO\n\n\ntrue\n\n\nWhen set to \"true\", enables Autoscale automatic database schema update\n\n\n\n\n\n\nPUBLIC_IP\n\n\n\n\nIP address or hostname of the public interface\n\n\n\n\n\n\nREST_DEBUG\n\n\n\"false\"\n\n\nSet to \"true\" to enable REST call debug level in Cloudbreak and Autoscale\n\n\n\n\n\n\nSL_ADDRESS_RESOLVING_TIMEOUT\n\n\n\n\nDNS lookup timeout of Authentication service for internal service discovery\n\n\n\n\n\n\nSL_NODE_TLS_REJECT_UNAUTHORIZED\n\n\n\"0\"\n\n\nWhen set to \"0\", enables self-signed certifications in Authentication service\n\n\n\n\n\n\nSULTANS_CONTAINER_PATH\n\n\n/sultans\n\n\nDefault project location in Authentication service container\n\n\n\n\n\n\nTRAEFIK_MAX_IDLE_CONNECTION\n\n\n100\n\n\nSets --maxidleconnsperhost for Traefik to the value entered\n\n\n\n\n\n\nUAA_CLOUDBREAK_ID\n\n\ncloudbreak\n\n\nIdentity of the Cloudbreak scope in Identity\n\n\n\n\n\n\nUAA_CLOUDBREAK_SECRET\n\n\n$UAA_DEFAULT_SECRET\n\n\nSecret of the Cloudbreak scope in Identity\n\n\n\n\n\n\nUAA_CLOUDBREAK_SHELL_ID\n\n\ncloudbreak_shell\n\n\nIdentity of the Cloudbreak Shell scope in Identity\n\n\n\n\n\n\nUAA_DEFAULT_ACCOUNT\n\n\n\"seq1234567.SequenceIQ\"\n\n\nDefault account for users as an Identity group\n\n\n\n\n\n\nUAA_DEFAULT_SECRET\n\n\n\n\nDefault secret for all the scopes and encryptions\n\n\n\n\n\n\nUAA_DEFAULT_USER_EMAIL\n\n\nadmin@example.com\n\n\nEmail address of default admin user\n\n\n\n\n\n\nUAA_DEFAULT_USER_FIRSTNAME\n\n\nJoe\n\n\nFirst name of default admin user\n\n\n\n\n\n\nUAA_DEFAULT_USER_GROUPS\n\n\nSee \nhere\n\n\nDefault user groups of the users\n\n\n\n\n\n\nUAA_DEFAULT_USER_LASTNAME\n\n\nAdmin\n\n\nLast name of default admin user\n\n\n\n\n\n\nUAA_DEFAULT_USER_PW\n\n\n\n\nPassword of default admin user\n\n\n\n\n\n\nUAA_FLEX_USAGE_CLIENT_ID\n\n\nflex_usage_client\n\n\nIdentity of the Flex usage generator scope in Identity\n\n\n\n\n\n\nUAA_FLEX_USAGE_CLIENT_SECRET\n\n\n$UAA_DEFAULT_SECRET\n\n\nSecret of the Flex usage generator scope in Identity\n\n\n\n\n\n\nUAA_PERISCOPE_ID\n\n\nperiscope\n\n\nIdentity of the Autoscale scope in Identity\n\n\n\n\n\n\nUAA_PERISCOPE_SECRET\n\n\n$UAA_DEFAULT_SECRET\n\n\nSecret of the Autoscale scope in Identity\n\n\n\n\n\n\nUAA_PORT\n\n\n8089\n\n\nIdentity service public port\n\n\n\n\n\n\nUAA_SULTANS_ID\n\n\nsultans\n\n\nIdentity of the Authentication service scope in Identity\n\n\n\n\n\n\nUAA_SULTANS_SECRET\n\n\n$UAA_DEFAULT_SECRET\n\n\nSecret of the Authentication service scope in Identity\n\n\n\n\n\n\nUAA_ULUWATU_ID\n\n\nuluwatu\n\n\nIdentity of the Web UI scope in Identity\n\n\n\n\n\n\nUAA_ULUWATU_SECRET\n\n\n$UAA_DEFAULT_SECRET\n\n\nSecret of the Web UI scope in Identity\n\n\n\n\n\n\nUAA_ZONE_DOMAIN\n\n\nexample.com\n\n\nExternal domain name for zone in Identity\n\n\n\n\n\n\nULUWATU_CONTAINER_PATH\n\n\n/uluwatu\n\n\nDefault project location in the Web UI container\n\n\n\n\n\n\nULU_DEFAULT_SSH_KEY\n\n\n\"\"\n\n\nDefault SSH key for the credentials in Cloudbreak\n\n\n\n\n\n\nULU_HOST_ADDRESS\n\n\n\"https://$PUBLIC_IP\"\n\n\nURL for the Web UI host\n\n\n\n\n\n\nULU_NODE_TLS_REJECT_UNAUTHORIZED\n\n\n\"0\"\n\n\nWhen set to \"0\", enables self-signed certifications in Web UI\n\n\n\n\n\n\nULU_OAUTH_REDIRECT_URI\n\n\n\"$ULU_HOST_ADDRESS/authorize\"\n\n\nAuthorization page on Web UI\n\n\n\n\n\n\nULU_SUBSCRIBE_TO_NOTIFICATIONS\n\n\n\"false\"\n\n\nSet to \u201ctrue\u201d to enable email notifications for Cloudbreak events\n\n\n\n\n\n\nULU_SULTANS_ADDRESS\n\n\n\"https://$PUBLIC_IP/sl\"\n\n\nAuthentication service URL\n\n\n\n\n\n\nVERBOSE_MIGRATION\n\n\nfalse\n\n\nWhen set to true, enables verbose database migration\n\n\n\n\n\n\n\n\nAWS Variables\n\n\n\n\n\n\n\n\nVariable Name\n\n\nDefault Value\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nAWS_ACCESS_KEY_ID\n\n\n\"\"\n\n\nAccess key of the AWS account\n\n\n\n\n\n\nAWS_ROLE_NAME\n\n\ncbreak-deployer\n\n\nName of the AWS role for the \ncbd aws [generate-rol, show role]\n commands\n\n\n\n\n\n\nAWS_SECRET_ACCESS_KEY\n\n\n\"\"\n\n\nSecret access key of the AWS account\n\n\n\n\n\n\nCB_AWS_CUSTOM_CF_TAGS\n\n\n\"\"\n\n\nComma separated list of AWS CloudFormation stack tags\n\n\n\n\n\n\nCB_AWS_DEFAULT_CF_TAG\n\n\n\"\"\n\n\nDefault tag for AWS CloudFormation stack\n\n\n\n\n\n\nCB_AWS_DEFAULT_INBOUND_SECURITY_GROUP\n\n\n\"\"\n\n\nDefault inbound policy name for AWS CloudFormation stack\n\n\n\n\n\n\nCB_AWS_EXTERNAL_ID\n\n\nprovision-ambari\n\n\nExternal ID of the assume role policy\n\n\n\n\n\n\nCB_AWS_HOSTKEY_VERIFY\n\n\n\"false\"\n\n\nEnables host fingerprint verification on AWS\n\n\n\n\n\n\nCB_AWS_VPC\n\n\n\"\"\n\n\nConfigures the VPC ID on AWS. Set this variable if you are provisioning cluster to the same VPC where Cloudbreak is deployed on AWS.\n\n\n\n\n\n\nCERTS_BUCKET\n\n\n\"\"\n\n\nS3 bucket name for backup and restore certificates via \ncbd aws [certs-restore-s3  certs-upload-s3]\n commands\n\n\n\n\n\n\n\n\nAzure Variables\n\n\n\n\n\n\n\n\nVariable Name\n\n\nDefault Value\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nAZURE_SUBSCRIPTION_ID\n\n\n\n\nAzure subscription ID for interactive login in the web UI\n\n\n\n\n\n\nAZURE_TENANT_ID\n\n\n\n\nAzure tenant ID for interactive login in the web UI\n\n\n\n\n\n\n\n\nGCP Variables\n\n\n\n\n\n\n\n\nVariable Name\n\n\nDefault Value\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nCB_GCP_HOSTKEY_VERIFY\n\n\n\"false\"\n\n\nWhen set to \"true\", enables host fingerprint verification on GCP\n\n\n\n\n\n\n\n\nLocal Development Variables\n\n\n\n\n\n\n\n\nVariable Name\n\n\nDefault Value\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nCB_LOCAL_DEV_BIND_ADDR\n\n\n\"192.168.64.1\"\n\n\nAmbassador external address for local development of Cloudbreak and Autoscale\n\n\n\n\n\n\nCB_SCHEMA_SCRIPTS_LOCATION\n\n\n\"container\"\n\n\nLocation of Cloudbreak schema update files\n\n\n\n\n\n\nDOCKER_TAG_AMBASSADOR\n\n\n0.5.0\n\n\nAmbassador container version for local development\n\n\n\n\n\n\nPERISCOPE_SCHEMA_SCRIPTS_LOCATION\n\n\n\"container\"\n\n\nLocation of Cloudbreak schema update files\n\n\n\n\n\n\nPRIVATE_IP\n\n\n$BRIDGE_IP\n\n\nIP address or hostname of the private interface\n\n\n\n\n\n\nREMOVE_CONTAINER\n\n\n\"--rm\"\n\n\nKeeps side effect containers for debug purpose\n\n\n\n\n\n\nSULTANS_VOLUME_HOST\n\n\n/dev/null\n\n\nLocation of the locally developed Authentication service project\n\n\n\n\n\n\nUAA_SCHEMA_SCRIPTS_LOCATION\n\n\n\"container\"\n\n\nLocation of Identity schema update files\n\n\n\n\n\n\nULUWATU_VOLUME_HOST\n\n\n/dev/null\n\n\nLocation of the locally developed web UI project\n\n\n\n\n\n\n\n\nMacOS Variables\n\n\n\n\n\n\n\n\nVariable Name\n\n\nDefault Value\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nDOCKER_MACHINE\n\n\n\"\"\n\n\nName of the Docker Machine where Cloudbreak runs\n\n\n\n\n\n\nDOCKER_PROFILE\n\n\nProfile\n\n\nProfile file for environment variables related to Docker Machine\n\n\n\n\n\n\nMACHINE_CPU\n\n\n2\n\n\nNumber of the CPU cores on the Docker Machine instance\n\n\n\n\n\n\nMACHINE_MEM\n\n\n4096\n\n\nAmount of RAM on the Docker Machine instance\n\n\n\n\n\n\nMACHINE_NAME\n\n\ncbd\n\n\nName of the Docker Machine instance\n\n\n\n\n\n\nMACHINE_OPTS\n\n\n\"--xhyve-virtio-9p\"\n\n\nExtra options for Docker Machine instance\n\n\n\n\n\n\nMACHINE_STORAGE_PATH\n\n\n$HOME/.docker/machine\n\n\nDocker Machine storage path\n\n\n\n\n\n\n\n\nMore\n\n\nUAA_DEFAULT_USER_GROUPS\n\n\nDefault value fro \nUAA_DEFAULT_USER_GROUPS\n is:\n\n\n\"openid,cloudbreak.networks,cloudbreak.securitygroups,cloudbreak.templates,cloudbreak.blueprints,cloudbreak.credentials,cloudbreak.stacks,sequenceiq.cloudbreak.admin,sequenceiq.cloudbreak.user,sequenceiq.account.${UAA_DEFAULT_ACCOUNT},cloudbreak.events,cloudbreak.usages.global,cloudbreak.usages.account,cloudbreak.usages.user,periscope.cluster,cloudbreak.recipes,cloudbreak.blueprints.read,cloudbreak.templates.read,cloudbreak.credentials.read,cloudbreak.recipes.read,cloudbreak.networks.read,cloudbreak.securitygroups.read,cloudbreak.stacks.read,cloudbreak.sssdconfigs,cloudbreak.sssdconfigs.read,cloudbreak.platforms,cloudbreak.platforms.read\"",
            "title": "Profile File"
        },
        {
            "location": "/profile/index.html#profile-variables",
            "text": "During startup, Cloudbreak deployer tries to determine the underlying infrastructure and then sets required environment variables with appropriate default values. If these environment variables are not sufficient for your use case, you can set additional environment variables in your  Profile  file. Refer to the list below for available custom environment variables. The variables are listed with their default values. If default is unset, no value is listed.",
            "title": "Profile Variables"
        },
        {
            "location": "/profile/index.html#setting-provfle-variables",
            "text": "TO-DO: How to set them?",
            "title": "Setting Provfle Variables"
        },
        {
            "location": "/profile/index.html#cloudbreak-variables",
            "text": "Variable Name  Default Value  Description      ADDRESS_RESOLVING_TIMEOUT  120000  DNS lookup timeout for internal service discovery    CAPTURE_CRON_EXPRESSION   SmartSense bundle generation time interval in cron format    CBD_CERT_ROOT_PATH  \"${PWD}/certs\"  Path where deployer stores Cloudbreak certificates    CBD_LOG_NAME  cbreak  Name of the Cloudbreak log file    CBD_TRAEFIK_TLS  \"/certs/traefik/client.pem,/certs/traefik/client-key.pem\"  Path inside of the Traefik container where TLS files are located    CB_BLUEPRINT_DEFAULTS  \"hdp-small-default;hdp-spark-cluster;hdp-streaming-cluster\"  Comma separated list of the default blueprints that Cloudbreak initializes in its database    CB_BYOS_DFS_DATA_DIR  \"/hadoop/hdfs/data\"  (Deprecated) Default data directory for BYOP orchestrators    CB_COMPONENT_CLUSTER_ID   SmartSense component cluster ID    CB_COMPONENT_ID   SmartSense component ID    CB_COMPOSE_PROJECT  cbreak  Name of the Docker Compose project; it will appear in container names    CB_DB_ENV_DB  \"cbdb\"  Name of the Cloudbreak database    CB_DB_ENV_PASS  \"\"  Password for the Cloudbreak database authentication    CB_DB_ENV_SCHEMA  \"public\"  Schema used in the Cloudbreak database    CB_DB_ENV_USER  \"postgres\"  User for the Cloudbreak database authentication    CB_DB_ROOT_PATH  \"/var/lib/cloudbreak\"  (Deprecated) Location of the database volume on Cloudbreak host    CB_DEFAULT_SUBSCRIPTION_ADDRESS  http://uluwatu.service.consul:3000/notifications  URL of the default subscription for Cloudbreak notifications    CB_ENABLEDPLATFORMS   Disables Cloudbreak resource called Platform    CB_ENABLE_CUSTOM_IMAGE  \"false\"  Set to \"true\" to enable custom cloud images    CBD_FORCE_START   Disables docker-compose.yml and uaa.yml validation    CB_HBM2DDL_STRATEGY  \"validate\"  Configures hibernate.hbm2ddl.auto in Cloudbreak    CB_HOST_DISCOVERY_CUSTOM_DOMAIN  \"\"  Custom domain of the provisioned cluster    CB_HTTPS_PROXY  \"\"  HTTPS proxy URL    CB_HTTP_PROXY  \"\"  HTTP proxy URL    CB_IMAGE_CATALOG_URL  \"https://s3-eu-west-1.amazonaws.com/cloudbreak-info/cb-image-catalog.json\"  Image catalog URL    CB_INSTANCE_NODE_ID   Unique identifier of the Cloudbreak node    CB_INSTANCE_PROVIDER   Cloud provider of the Cloudbreak instance    CB_INSTANCE_REGION   Cloud region of the Cloudbreak instance    CB_INSTANCE_UUID   Unique identifier of Cloudbreak deployment    CB_JAVA_OPTS  \"\"  Extra Java options for Autoscale and Cloudbreak    CB_LOG_LEVEL  \"INFO\"  Log level of the Cloudbreak service    CB_MAX_SALT_NEW_SERVICE_RETRY  90  Salt orchestrator max retry count    CB_MAX_SALT_RECIPE_EXECUTION_RETRY  90  Salt orchestrator max retry count for recipes    CB_PLATFORM_DEFAULT_REGIONS   Comma separated list of default regions by platform. For example:  AWS:eu-west-1 .    CB_PRODUCT_ID   SmartSense product ID    CB_SCHEMA_MIGRATION_AUTO  true  When set to true, enables Cloudbreak automatic database schema update    CB_SMARTSENSE_CONFIGURE  \"false\"  Set to \u201ctrue\u201d to install and configure SmartSense on cluster nodes    CB_SMARTSENSE_CLUSTER_NAME_PREFIX   SmartSense Cloudbreak cluster name prefix    CB_SMARTSENSE_ID  \"\"  SmartSense subscription ID    CB_TEMPLATE_DEFAULTS  \"minviable-gcp,minviable-azure,minviable-aws\"  Comma separated list of the default templates that Cloudbreak initializes in its database    CB_UI_MAX_WAIT  400  Wait timeout for  cbd start-wait  command    CERT_VALIDATION  \"true\"  When set to \"true\", enables cert validation in Cloudbreak and Autoscale    CLOUDBREAK_SMTP_AUTH  \"true\"  When set to \"true\", configures mail.smtp.auth in Cloudbreak    CLOUDBREAK_SMTP_SENDER_FROM  \"noreply@hortonworks.com\"  Email address of the sender    CLOUDBREAK_SMTP_SENDER_HOST  \"smtp.service.consul\"  SMTP server address of the hostname    CLOUDBREAK_SMTP_SENDER_PASSWORD  \"$LOCAL_SMTP_PASSWORD\"  SMTP server password    CLOUDBREAK_SMTP_SENDER_PORT  25  Port of the SMTP server    CLOUDBREAK_SMTP_SENDER_USERNAME  \"admin\"  Username for SMTP authentication    CLOUDBREAK_SMTP_STARTTLS_ENABLE  \"false\"  Set to \"true\" to configure mail.smtp.starttls.enable in Cloudbreak    CLOUDBREAK_SMTP_TYPE  \"smtp\"  Defines mail.transport.protocol in CLoudbreak    COMMON_DB  commondb  Name of the database container    COMMON_DB_VOL  common  Name of the database volume    COMPOSE_HTTP_TIMEOUT  120  Docker Compose execution timeout    DB_DUMP_VOLUME  cbreak_dump  Name of the database dump volume    DB_MIGRATION_LOG  \"db_migration.log\"  Database migration log file    DEFAULT_INBOUND_ACCESS_IP  \"\"  Opens default ports on AWS instances for address    DOCKER_CONSUL_OPTIONS  \"\"  Extra options for Consul    DOCKER_IMAGE_CBD_SMARTSENSE  hortonworks/cbd-smartsense  SmartSense Docker image name    DOCKER_IMAGE_CLOUDBREAK  hortonworks/cloudbreak  Cloudbreak Docker image name    DOCKER_IMAGE_CLOUDBREAK_AUTH  hortonworks/cloudbreak-auth  Authentication service Docker image name    DOCKER_IMAGE_CLOUDBREAK_PERISCOPE  hortonworks/cloudbreak-autoscale  Autoscale Docker image name    DOCKER_IMAGE_CLOUDBREAK_SHELL  hortonworks/cloudbreak-shell  Cloudbreak Shell Docker image name    DOCKER_IMAGE_CLOUDBREAK_WEB  hortonworks/cloudbreak-web  Web UI Docker image name    DOCKER_TAG_ALPINE  3.1  Alpine container version    DOCKER_TAG_CBD_SMARTSENSE  0.10.0  SmartSense container version    DOCKER_TAG_CERT_TOOL  0.2.0  Cert tool container version    DOCKER_TAG_CLOUDBREAK  2.1.0-dev.70  Cloudbreak container version    DOCKER_TAG_CLOUDBREAK_SHELL  2.1.0-dev.70  Cloudbreak Shell container version    DOCKER_TAG_CONSUL  0.5  Consul container version    DOCKER_TAG_HAVEGED  1.1.0  Haveged container version    DOCKER_TAG_LOGROTATE  1.0.0  Logrotate container version    DOCKER_TAG_MIGRATION  1.0.0  Migration container version    DOCKER_TAG_PERISCOPE  2.1.0-dev.70  Autoscale container version    DOCKER_TAG_POSTFIX  latest  Postfix container version    DOCKER_TAG_POSTGRES  9.6.1-alpine  Postgresql container version    DOCKER_TAG_REGISTRATOR  v5  Registrator container version    DOCKER_TAG_SULTANS  2.1.0-dev.70  Authentication service container version    DOCKER_TAG_TRAEFIK  v1.2.0  Traefik container version    DOCKER_TAG_UAA  3.6.5  Identity container version    DOCKER_TAG_ULUWATU  2.1.0-dev.70  Web UI container version    IDENTITY_DB_NAME  \"uaadb\"  Name of the Identity database    IDENTITY_DB_PASS  \"\"  Password for the Identity database authentication    IDENTITY_DB_URL  \"${COMMON_DB}.service.consul:5432\"  URL for the Identity database connection, including the port number    IDENTITY_DB_USER  \"postgres\"  User for the Identity database authentication    LOCAL_SMTP_PASSWORD  \"$UAA_DEFAULT_USER_PW\"  Default password for the internal mail server    PERISCOPE_DB_HBM2DDL_STRATEGY  \"validate\"  Configures hibernate.hbm2ddl.auto in Autoscale    PERISCOPE_DB_NAME  \"periscopedb\"  Name of the Autoscale database    PERISCOPE_DB_PASS  \"\"  Password for the Autoscale database authentication    PERISCOPE_DB_SCHEMA_NAME  \"public\"  Schema used in the Autoscale database    PERISCOPE_DB_USER  \"postgres\"  User for the Autoscale database authentication    PERISCOPE_DB_TCP_ADDR   Address of the Autoscale database    PERISCOPE_DB_TCP_PORT   Port number of the Autoscale database    PERISCOPE_LOG_LEVEL  \"INFO\"  Log level of the Autoscale service    PERISCOPE_SCHEMA_MIGRATION_AUTO  true  When set to \"true\", enables Autoscale automatic database schema update    PUBLIC_IP   IP address or hostname of the public interface    REST_DEBUG  \"false\"  Set to \"true\" to enable REST call debug level in Cloudbreak and Autoscale    SL_ADDRESS_RESOLVING_TIMEOUT   DNS lookup timeout of Authentication service for internal service discovery    SL_NODE_TLS_REJECT_UNAUTHORIZED  \"0\"  When set to \"0\", enables self-signed certifications in Authentication service    SULTANS_CONTAINER_PATH  /sultans  Default project location in Authentication service container    TRAEFIK_MAX_IDLE_CONNECTION  100  Sets --maxidleconnsperhost for Traefik to the value entered    UAA_CLOUDBREAK_ID  cloudbreak  Identity of the Cloudbreak scope in Identity    UAA_CLOUDBREAK_SECRET  $UAA_DEFAULT_SECRET  Secret of the Cloudbreak scope in Identity    UAA_CLOUDBREAK_SHELL_ID  cloudbreak_shell  Identity of the Cloudbreak Shell scope in Identity    UAA_DEFAULT_ACCOUNT  \"seq1234567.SequenceIQ\"  Default account for users as an Identity group    UAA_DEFAULT_SECRET   Default secret for all the scopes and encryptions    UAA_DEFAULT_USER_EMAIL  admin@example.com  Email address of default admin user    UAA_DEFAULT_USER_FIRSTNAME  Joe  First name of default admin user    UAA_DEFAULT_USER_GROUPS  See  here  Default user groups of the users    UAA_DEFAULT_USER_LASTNAME  Admin  Last name of default admin user    UAA_DEFAULT_USER_PW   Password of default admin user    UAA_FLEX_USAGE_CLIENT_ID  flex_usage_client  Identity of the Flex usage generator scope in Identity    UAA_FLEX_USAGE_CLIENT_SECRET  $UAA_DEFAULT_SECRET  Secret of the Flex usage generator scope in Identity    UAA_PERISCOPE_ID  periscope  Identity of the Autoscale scope in Identity    UAA_PERISCOPE_SECRET  $UAA_DEFAULT_SECRET  Secret of the Autoscale scope in Identity    UAA_PORT  8089  Identity service public port    UAA_SULTANS_ID  sultans  Identity of the Authentication service scope in Identity    UAA_SULTANS_SECRET  $UAA_DEFAULT_SECRET  Secret of the Authentication service scope in Identity    UAA_ULUWATU_ID  uluwatu  Identity of the Web UI scope in Identity    UAA_ULUWATU_SECRET  $UAA_DEFAULT_SECRET  Secret of the Web UI scope in Identity    UAA_ZONE_DOMAIN  example.com  External domain name for zone in Identity    ULUWATU_CONTAINER_PATH  /uluwatu  Default project location in the Web UI container    ULU_DEFAULT_SSH_KEY  \"\"  Default SSH key for the credentials in Cloudbreak    ULU_HOST_ADDRESS  \"https://$PUBLIC_IP\"  URL for the Web UI host    ULU_NODE_TLS_REJECT_UNAUTHORIZED  \"0\"  When set to \"0\", enables self-signed certifications in Web UI    ULU_OAUTH_REDIRECT_URI  \"$ULU_HOST_ADDRESS/authorize\"  Authorization page on Web UI    ULU_SUBSCRIBE_TO_NOTIFICATIONS  \"false\"  Set to \u201ctrue\u201d to enable email notifications for Cloudbreak events    ULU_SULTANS_ADDRESS  \"https://$PUBLIC_IP/sl\"  Authentication service URL    VERBOSE_MIGRATION  false  When set to true, enables verbose database migration",
            "title": "Cloudbreak Variables"
        },
        {
            "location": "/profile/index.html#aws-variables",
            "text": "Variable Name  Default Value  Description      AWS_ACCESS_KEY_ID  \"\"  Access key of the AWS account    AWS_ROLE_NAME  cbreak-deployer  Name of the AWS role for the  cbd aws [generate-rol, show role]  commands    AWS_SECRET_ACCESS_KEY  \"\"  Secret access key of the AWS account    CB_AWS_CUSTOM_CF_TAGS  \"\"  Comma separated list of AWS CloudFormation stack tags    CB_AWS_DEFAULT_CF_TAG  \"\"  Default tag for AWS CloudFormation stack    CB_AWS_DEFAULT_INBOUND_SECURITY_GROUP  \"\"  Default inbound policy name for AWS CloudFormation stack    CB_AWS_EXTERNAL_ID  provision-ambari  External ID of the assume role policy    CB_AWS_HOSTKEY_VERIFY  \"false\"  Enables host fingerprint verification on AWS    CB_AWS_VPC  \"\"  Configures the VPC ID on AWS. Set this variable if you are provisioning cluster to the same VPC where Cloudbreak is deployed on AWS.    CERTS_BUCKET  \"\"  S3 bucket name for backup and restore certificates via  cbd aws [certs-restore-s3  certs-upload-s3]  commands",
            "title": "AWS Variables"
        },
        {
            "location": "/profile/index.html#azure-variables",
            "text": "Variable Name  Default Value  Description      AZURE_SUBSCRIPTION_ID   Azure subscription ID for interactive login in the web UI    AZURE_TENANT_ID   Azure tenant ID for interactive login in the web UI",
            "title": "Azure Variables"
        },
        {
            "location": "/profile/index.html#gcp-variables",
            "text": "Variable Name  Default Value  Description      CB_GCP_HOSTKEY_VERIFY  \"false\"  When set to \"true\", enables host fingerprint verification on GCP",
            "title": "GCP Variables"
        },
        {
            "location": "/profile/index.html#local-development-variables",
            "text": "Variable Name  Default Value  Description      CB_LOCAL_DEV_BIND_ADDR  \"192.168.64.1\"  Ambassador external address for local development of Cloudbreak and Autoscale    CB_SCHEMA_SCRIPTS_LOCATION  \"container\"  Location of Cloudbreak schema update files    DOCKER_TAG_AMBASSADOR  0.5.0  Ambassador container version for local development    PERISCOPE_SCHEMA_SCRIPTS_LOCATION  \"container\"  Location of Cloudbreak schema update files    PRIVATE_IP  $BRIDGE_IP  IP address or hostname of the private interface    REMOVE_CONTAINER  \"--rm\"  Keeps side effect containers for debug purpose    SULTANS_VOLUME_HOST  /dev/null  Location of the locally developed Authentication service project    UAA_SCHEMA_SCRIPTS_LOCATION  \"container\"  Location of Identity schema update files    ULUWATU_VOLUME_HOST  /dev/null  Location of the locally developed web UI project",
            "title": "Local Development Variables"
        },
        {
            "location": "/profile/index.html#macos-variables",
            "text": "Variable Name  Default Value  Description      DOCKER_MACHINE  \"\"  Name of the Docker Machine where Cloudbreak runs    DOCKER_PROFILE  Profile  Profile file for environment variables related to Docker Machine    MACHINE_CPU  2  Number of the CPU cores on the Docker Machine instance    MACHINE_MEM  4096  Amount of RAM on the Docker Machine instance    MACHINE_NAME  cbd  Name of the Docker Machine instance    MACHINE_OPTS  \"--xhyve-virtio-9p\"  Extra options for Docker Machine instance    MACHINE_STORAGE_PATH  $HOME/.docker/machine  Docker Machine storage path",
            "title": "MacOS Variables"
        },
        {
            "location": "/profile/index.html#more",
            "text": "",
            "title": "More"
        },
        {
            "location": "/profile/index.html#uaa_default_user_groups",
            "text": "Default value fro  UAA_DEFAULT_USER_GROUPS  is:  \"openid,cloudbreak.networks,cloudbreak.securitygroups,cloudbreak.templates,cloudbreak.blueprints,cloudbreak.credentials,cloudbreak.stacks,sequenceiq.cloudbreak.admin,sequenceiq.cloudbreak.user,sequenceiq.account.${UAA_DEFAULT_ACCOUNT},cloudbreak.events,cloudbreak.usages.global,cloudbreak.usages.account,cloudbreak.usages.user,periscope.cluster,cloudbreak.recipes,cloudbreak.blueprints.read,cloudbreak.templates.read,cloudbreak.credentials.read,cloudbreak.recipes.read,cloudbreak.networks.read,cloudbreak.securitygroups.read,cloudbreak.stacks.read,cloudbreak.sssdconfigs,cloudbreak.sssdconfigs.read,cloudbreak.platforms,cloudbreak.platforms.read\"",
            "title": "UAA_DEFAULT_USER_GROUPS"
        },
        {
            "location": "/faq/index.html",
            "text": "FAQs\n\n\nHow to...\n\n\nGenerate a New SSH Key Pair\n\n\nAll the instances created by Cloudbreak are configured to allow key-based SSH, so you'll need to provide an SSH public key that can be used later to SSH onto the instances in the clusters you'll create with Cloudbreak. You can use one of your existing keys or you can generate a new one.\n\n\nTo generate a new SSH key pair, execute:\n\n\nssh-keygen -t rsa -b 4096 -C \"your_email@example.com\"\n# Creates a new ssh key, using the provided email as a label\n# Generating public/private rsa key pair.\n# Enter file in which to save the key (/Users/you/.ssh/id_rsa): [Press enter]\n\n\n\nYou'll be asked to enter a passphrase, but you can leave it empty:\n\n\n# Enter passphrase (empty for no passphrase): [Type a passphrase]\n# Enter same passphrase again: [Type passphrase again]\n\n\n\nAfter you enter (or not) a passphrase, the key pair is generated. The output should look similar to:\n\n\n# Your identification has been saved in /Users/you/.ssh/id_rsa.\n# Your public key has been saved in /Users/you/.ssh/id_rsa.pub.\n# The key fingerprint is:\n# 01:0f:f4:3b:ca:85:sd:17:sd:7d:sd:68:9d:sd:a2:sd your_email@example.com\n\n\n\nLater you'll need to pass the content of the \n.pub\n file to Cloudbreak and use the private key file to SSH to the instances. \n\n\nRecover My Public SSH Key\n\n\nThe \n-y\n option of \nssh-keygen\n outputs the public key. For example:\n\n\nssh-keygen -y -f ~/.ssh/id_rsa > ~/.ssh/id_rsa.pub\n\n\n\nSSH to the Hosts\n\n\nAccess Cloudbreak Logs\n\n\nCheck Cloudbreak Version",
            "title": "FAQs"
        },
        {
            "location": "/faq/index.html#faqs",
            "text": "How to...",
            "title": "FAQs"
        },
        {
            "location": "/faq/index.html#generate-a-new-ssh-key-pair",
            "text": "All the instances created by Cloudbreak are configured to allow key-based SSH, so you'll need to provide an SSH public key that can be used later to SSH onto the instances in the clusters you'll create with Cloudbreak. You can use one of your existing keys or you can generate a new one.  To generate a new SSH key pair, execute:  ssh-keygen -t rsa -b 4096 -C \"your_email@example.com\"\n# Creates a new ssh key, using the provided email as a label\n# Generating public/private rsa key pair.\n# Enter file in which to save the key (/Users/you/.ssh/id_rsa): [Press enter]  You'll be asked to enter a passphrase, but you can leave it empty:  # Enter passphrase (empty for no passphrase): [Type a passphrase]\n# Enter same passphrase again: [Type passphrase again]  After you enter (or not) a passphrase, the key pair is generated. The output should look similar to:  # Your identification has been saved in /Users/you/.ssh/id_rsa.\n# Your public key has been saved in /Users/you/.ssh/id_rsa.pub.\n# The key fingerprint is:\n# 01:0f:f4:3b:ca:85:sd:17:sd:7d:sd:68:9d:sd:a2:sd your_email@example.com  Later you'll need to pass the content of the  .pub  file to Cloudbreak and use the private key file to SSH to the instances.",
            "title": "Generate a New SSH Key Pair"
        },
        {
            "location": "/faq/index.html#recover-my-public-ssh-key",
            "text": "The  -y  option of  ssh-keygen  outputs the public key. For example:  ssh-keygen -y -f ~/.ssh/id_rsa > ~/.ssh/id_rsa.pub",
            "title": "Recover My Public SSH Key"
        },
        {
            "location": "/faq/index.html#ssh-to-the-hosts",
            "text": "",
            "title": "SSH to the Hosts"
        },
        {
            "location": "/faq/index.html#access-cloudbreak-logs",
            "text": "",
            "title": "Access Cloudbreak Logs"
        },
        {
            "location": "/faq/index.html#check-cloudbreak-version",
            "text": "",
            "title": "Check Cloudbreak Version"
        },
        {
            "location": "/trouble-cb/index.html",
            "text": "Troubleshooting Cloudbreak\n\n\nChecking the Logs\n\n\nWhen troubleshooting, you can access the following Cloudbreak logs.\n\n\nCloudbreak Logs\n\n\nWhen installing Cloudbreak using a pre-built cloud image, the  Cloudbreak Deployer location and the cbd root folder is \n/var/lib/cloudbreak-deployment\n. You must execute all cbd actions from the cbd root folder as a cloudbreak user. \n\n\n\n\nYour cbd root directory may be different if you installed Cloudbreak on your own VM. \n\n\n\n\nCloudbreak consists of multiple microservices deployed into Docker containers. \n\n\nAggregated Logs\n\n\nTo check aggregated service logs, use the following commands:\n\n\ncbd logs\n shows all service logs.\n\n\ncbd logs | tee cloudbreak.log\n allows you to redirect the input into a file for sharing these logs.\n\n\nIndividual Service Logs\n\n\nTo check individual service logs, use the following commands:\n\n\ncbd logs cloudbreak\n shows Cloudbreak logs. This service is the backend service that handles all deployments.\n\n\ncbd logs uluwatu\n shows Cloudbreak UI logs. Uluwatu is the UI component of Cloudbreak.\n\n\ncbd logs identity\n shows Identity logs. Identity is responsible for authentication and authorization.\n\n\ncbd logs periscope\n shows Periscope logs. Periscope is responsible for triggering autoscaling rules.\n\n\nDocker Logs\n\n\nThe same logs can be accessed via Docker commands:\n\n\ndocker logs cbreak_cloudbreak_1\n shows the same logs as \ncbd logs cloudbreak\n.\n\n\nCloudbreak logs are rotated and can be accessed later from the Cloudbreak deployment folder. Each time you restart the application via cbd restart a new log file is created with a timestamp in the name (for example, cbreak-20170821-105900.log). \n\n\n\n\nThere is a symlink called \ncbreak.log\n which points to the latest log file. Sharing this symlink does not share the log itself.\n\n\n\n\nSaltstack Logs\n\n\nCloudbreak uses Saltstack to install Ambari and the necessary packages for the HDP provisioning. Salt Master always runs alongside the Ambari Server node. Each instance in the cluster runs a Salt Minion, which connects to the Salt Master. There can be multiple Salt Masters if the cluster is configured to run in HA (High Availability) mode and in this case each Salt Minion connects to each Salt Master.\n\n\nCloudbreak also uses SaltStack to execute user-provided customization scripts called \"recipes\". \n\n\nSalt Master and Salt Minion logs can be found at the following location: \n/var/log/salt\n\n\nAmbari Logs\n\n\nCloudbreak uses Ambari to orchestrate the installation of the different HDP components. Each instance in the cluster runs an Ambari agent which connects to the Ambari server. Ambari server is declared by the user during the cluster installation wizard. \n\n\nAmbari Server Logs\n\n\nAmbari server logs can be found on the nodes where Ambari server is installed in the following locations:\n\n\n/var/log/ambari-server/ambari-server.log\n\n\n/var/log/ambari-server/ambari-server.out\n\n\nBoth files contain important information about the root cause of a certain issue so it is advised to check both.\n\n\nAmbari Agent Logs\n\n\nAmbari agent logs can be found on the nodes where Ambari agent is installed in the following locations:\n\n\n/var/log/ambari-agent/ambari-agent.log\n\n\nRecipe Logs\n\n\nCloudbreak supports \"recipes\" - user-provided customization scripts that can be run prior to or after cluster installtion. It is the user\u2019s responsibility to provide an idempotent well tested script. If the execution fails, the recipe logs can be found at \n/var/log/recipes\n on the nodes on which the recipes were executed.\n\n\nIt is advised, but not required to have an advanced logging mechanism in the script, as Cloudbreak always logs every script that are run. Recipes are often the sources of installation failures as users might try to remove necessary packages or reconfigure services.\n\n\nCommon Errors\n\n\nQuota Limitations\n\n\nEach cloud provider has quota limitations on various cloud resources, and these quotas can usually be increased on request. If there is an error message in Cloudbreak saying that there are no more available EIPs (Elastic IP Address) or VPCs, you need to request more of these resources. \n\n\nTo see the limitations visit the cloud provider\u2019s site:\n\n\n\n\nAWS Service Limits\n \n\n\nAzure subscription and service limits, quotas, and constraints\n\n\nGCP Resource Quotas\n \n\n\n\n\nConnection Timeout: Ports Not Open\n\n\nIn the cluster installation wizard, you must specify on which node you want to run the Ambari server. Cloudbreak communicates with this node to orchestrate the installation.\n\n\nA common reason for connection timeout is security group misconfiguration. Cloudbreak allows configuring different security groups for the different instance groups; however, there are certain requirements for the Ambari server node. Specifically, the following ports must be open in order to communicate with that node:\n\n\n\n\n22 (SSH)  \n\n\n9443 (two-way-ssl through nginx) \n\n\n\n\nBlueprints: Invalid Services and Configurations\n\n\nAmbari blueprints are a declarative definition of a cluster. With a blueprint, you specify a stack, the component layout, and the configurations to materialize a Hadoop cluster instance via a REST API without having to use the Ambari cluster install wizard. \n\n\nCloudbreak supports any type of blueprints, which is a common source of errors. These errors are only visible once the core infrastructure is up and running and Cloudbreak tries to initiate the cluster installation through Ambari. Ambari validates the blueprint and  rejects it if it's invalid. \n\n\nFor example, if there are configurations for a certain service like Hive but Hive as a service is not mapped to any host group, the blueprint is invalid.\n\n\nTo fix these type of issues, edit your blueprint and then reinstall your cluster. Cloudbreak UI has support for this so the infrastructure does not have to be terminated.\n\n\nThere are some cases when Ambari cannot validate your blueprint beforehand. In these cases, the issues are only visible in the Ambari server logs. To trubleshoot, check Ambari server logs.\n\n\nBlueprints: High Availability\n\n\nCloudbreak always tries to validate that a blueprint not to include multiple master services into different host groups. However, this exact setup is required for HA clusters. To overcome this, you can disable blueprint validation in the UI (using an advanced option in the Create Cluster wizard > Choose Blueprint), but you must include the necessary configurations.\n\n\nBlueprints: Wrong HDP Version\n\n\nIn the blueprint, only the major and minor HDP version should be defined (for example, \"2.6\"). If wrong version number is provided, the following error can be found in the logs:\n\n\n5/15/2017 12:23:19 PM testcluster26 - create failed: Cannot use the specified Ambari stack: HDPRepo\n{stack='null'; utils='null'}\n. Error: org.apache.ambari.server.controller.spi.NoSuchResourceException: The specified resource doesn't exist: Stack data, Stack HDP 2.6.0.3 is not found in Ambari metainfo\n\n\n\n\nFor correct blueprint layout, refer to the \nAmbari cwiki\n page.\n\n\nRecipes: Recipe Execution Times Out\n\n\nIf the scripts are taking too much time to execute, the processes will time out, as the threshold for all recipes is set to 15 minutes. To change this threshold, you must override the default value by adding the following to the cbd Profile file:\n\n\nexport CB_JAVA_OPTS=\u201d -Dcb.max.salt.recipe.execution.retry=90\u201d\n\n\n\n\nThis property indicates the number of tries for checking if the scripts have finished with a sleep time (i.e. the wait time between two polling attempts) of 10 seconds. The default value is 90. To increase the threshold, provide a number greater than 90. You must restart Cloudbreak after changing properties in the Profile file.\n\n\nRecipes: Recipe Execution Fails\n\n\nIt often happens that a script cannot be executed successfully because there are typos or errors in the script. To verify this you can check the recipe logs at\n\n/var/log/recipes\n. For each script, there will be a separate log file with the name of the script that you provided on the Cloudbreak UI.\n\n\nInvalid PUBLIC_IP in CBD Profile\n\n\nThe \nPUBLIC_IP\n property must be set in the cbd Profile file or else you won\u2019t be able to log in on the Cloudbreak UI. \n\n\nIf you are migrating your instance, make sure that after the start the IP remains valid. If you need to edit the \nPUBLIC_IP\n property in Profile, make sure to restart Cloudbreak using \ncbd restart\n.\n\n\nChanging Properties in the Cloudbreak Profile File\n\n\nThere are many properties that can be changed in the Cloudbreak application. These values must be changed in the Cloudbreak \nProfil\ne file. To see all possible options, use the following command:\n\ncbd env show\n.\n\n\nAfter changing a property, you must regenerate the config file and restart the application. There are two ways to do this:\n\n\nIn version 1.4.0 and newer of the cbd command line, you can regenerate the config file and restart the application with a single command:\n\n\ncbd restart\n - same as cbd regenerate/kill/start.\n\n\nIn versions earlier than 1.4.0, you must run the following three commands:\n\n\ncbd regenerate\n regenerates the Docker compose file\n\ncbd kill\n removes all Docker containers (there is no stop command for this).\n\ncbd start\n starts the application with the new compose file.\n\n\nChanging Amari Credentials\n\n\nIn Cloudbreak 1.14 and later, Cloudbreak creates a new admin user in Ambari, so you can change the credentials of the admin user in the Ambari web UI. You can also set the admin user credentials in the cluster installation wizard in the Cloudbreak UI.\n\n\nIn Cloudbreak versions earlier than 1.14 it is not possible to change the password in the Ambari UI.  If you change the admin credentials in the Ambari UI, Cloudbreak will no longer be able to orchestrate Ambari. To change the password, you must use the option available on the cluster details page in the Cloudbreak UI.",
            "title": "General Troubleshooting"
        },
        {
            "location": "/trouble-cb/index.html#troubleshooting-cloudbreak",
            "text": "",
            "title": "Troubleshooting Cloudbreak"
        },
        {
            "location": "/trouble-cb/index.html#checking-the-logs",
            "text": "When troubleshooting, you can access the following Cloudbreak logs.",
            "title": "Checking the Logs"
        },
        {
            "location": "/trouble-cb/index.html#cloudbreak-logs",
            "text": "When installing Cloudbreak using a pre-built cloud image, the  Cloudbreak Deployer location and the cbd root folder is  /var/lib/cloudbreak-deployment . You must execute all cbd actions from the cbd root folder as a cloudbreak user.    Your cbd root directory may be different if you installed Cloudbreak on your own VM.    Cloudbreak consists of multiple microservices deployed into Docker containers.   Aggregated Logs  To check aggregated service logs, use the following commands:  cbd logs  shows all service logs.  cbd logs | tee cloudbreak.log  allows you to redirect the input into a file for sharing these logs.  Individual Service Logs  To check individual service logs, use the following commands:  cbd logs cloudbreak  shows Cloudbreak logs. This service is the backend service that handles all deployments.  cbd logs uluwatu  shows Cloudbreak UI logs. Uluwatu is the UI component of Cloudbreak.  cbd logs identity  shows Identity logs. Identity is responsible for authentication and authorization.  cbd logs periscope  shows Periscope logs. Periscope is responsible for triggering autoscaling rules.  Docker Logs  The same logs can be accessed via Docker commands:  docker logs cbreak_cloudbreak_1  shows the same logs as  cbd logs cloudbreak .  Cloudbreak logs are rotated and can be accessed later from the Cloudbreak deployment folder. Each time you restart the application via cbd restart a new log file is created with a timestamp in the name (for example, cbreak-20170821-105900.log).    There is a symlink called  cbreak.log  which points to the latest log file. Sharing this symlink does not share the log itself.",
            "title": "Cloudbreak Logs"
        },
        {
            "location": "/trouble-cb/index.html#saltstack-logs",
            "text": "Cloudbreak uses Saltstack to install Ambari and the necessary packages for the HDP provisioning. Salt Master always runs alongside the Ambari Server node. Each instance in the cluster runs a Salt Minion, which connects to the Salt Master. There can be multiple Salt Masters if the cluster is configured to run in HA (High Availability) mode and in this case each Salt Minion connects to each Salt Master.  Cloudbreak also uses SaltStack to execute user-provided customization scripts called \"recipes\".   Salt Master and Salt Minion logs can be found at the following location:  /var/log/salt",
            "title": "Saltstack Logs"
        },
        {
            "location": "/trouble-cb/index.html#ambari-logs",
            "text": "Cloudbreak uses Ambari to orchestrate the installation of the different HDP components. Each instance in the cluster runs an Ambari agent which connects to the Ambari server. Ambari server is declared by the user during the cluster installation wizard.   Ambari Server Logs  Ambari server logs can be found on the nodes where Ambari server is installed in the following locations:  /var/log/ambari-server/ambari-server.log  /var/log/ambari-server/ambari-server.out  Both files contain important information about the root cause of a certain issue so it is advised to check both.  Ambari Agent Logs  Ambari agent logs can be found on the nodes where Ambari agent is installed in the following locations:  /var/log/ambari-agent/ambari-agent.log",
            "title": "Ambari Logs"
        },
        {
            "location": "/trouble-cb/index.html#recipe-logs",
            "text": "Cloudbreak supports \"recipes\" - user-provided customization scripts that can be run prior to or after cluster installtion. It is the user\u2019s responsibility to provide an idempotent well tested script. If the execution fails, the recipe logs can be found at  /var/log/recipes  on the nodes on which the recipes were executed.  It is advised, but not required to have an advanced logging mechanism in the script, as Cloudbreak always logs every script that are run. Recipes are often the sources of installation failures as users might try to remove necessary packages or reconfigure services.",
            "title": "Recipe Logs"
        },
        {
            "location": "/trouble-cb/index.html#common-errors",
            "text": "",
            "title": "Common Errors"
        },
        {
            "location": "/trouble-cb/index.html#quota-limitations",
            "text": "Each cloud provider has quota limitations on various cloud resources, and these quotas can usually be increased on request. If there is an error message in Cloudbreak saying that there are no more available EIPs (Elastic IP Address) or VPCs, you need to request more of these resources.   To see the limitations visit the cloud provider\u2019s site:   AWS Service Limits    Azure subscription and service limits, quotas, and constraints  GCP Resource Quotas",
            "title": "Quota Limitations"
        },
        {
            "location": "/trouble-cb/index.html#connection-timeout-ports-not-open",
            "text": "In the cluster installation wizard, you must specify on which node you want to run the Ambari server. Cloudbreak communicates with this node to orchestrate the installation.  A common reason for connection timeout is security group misconfiguration. Cloudbreak allows configuring different security groups for the different instance groups; however, there are certain requirements for the Ambari server node. Specifically, the following ports must be open in order to communicate with that node:   22 (SSH)    9443 (two-way-ssl through nginx)",
            "title": "Connection Timeout: Ports Not Open"
        },
        {
            "location": "/trouble-cb/index.html#blueprints-invalid-services-and-configurations",
            "text": "Ambari blueprints are a declarative definition of a cluster. With a blueprint, you specify a stack, the component layout, and the configurations to materialize a Hadoop cluster instance via a REST API without having to use the Ambari cluster install wizard.   Cloudbreak supports any type of blueprints, which is a common source of errors. These errors are only visible once the core infrastructure is up and running and Cloudbreak tries to initiate the cluster installation through Ambari. Ambari validates the blueprint and  rejects it if it's invalid.   For example, if there are configurations for a certain service like Hive but Hive as a service is not mapped to any host group, the blueprint is invalid.  To fix these type of issues, edit your blueprint and then reinstall your cluster. Cloudbreak UI has support for this so the infrastructure does not have to be terminated.  There are some cases when Ambari cannot validate your blueprint beforehand. In these cases, the issues are only visible in the Ambari server logs. To trubleshoot, check Ambari server logs.",
            "title": "Blueprints: Invalid Services and Configurations"
        },
        {
            "location": "/trouble-cb/index.html#blueprints-high-availability",
            "text": "Cloudbreak always tries to validate that a blueprint not to include multiple master services into different host groups. However, this exact setup is required for HA clusters. To overcome this, you can disable blueprint validation in the UI (using an advanced option in the Create Cluster wizard > Choose Blueprint), but you must include the necessary configurations.",
            "title": "Blueprints: High Availability"
        },
        {
            "location": "/trouble-cb/index.html#blueprints-wrong-hdp-version",
            "text": "In the blueprint, only the major and minor HDP version should be defined (for example, \"2.6\"). If wrong version number is provided, the following error can be found in the logs:  5/15/2017 12:23:19 PM testcluster26 - create failed: Cannot use the specified Ambari stack: HDPRepo\n{stack='null'; utils='null'}\n. Error: org.apache.ambari.server.controller.spi.NoSuchResourceException: The specified resource doesn't exist: Stack data, Stack HDP 2.6.0.3 is not found in Ambari metainfo  For correct blueprint layout, refer to the  Ambari cwiki  page.",
            "title": "Blueprints: Wrong HDP Version"
        },
        {
            "location": "/trouble-cb/index.html#recipes-recipe-execution-times-out",
            "text": "If the scripts are taking too much time to execute, the processes will time out, as the threshold for all recipes is set to 15 minutes. To change this threshold, you must override the default value by adding the following to the cbd Profile file:  export CB_JAVA_OPTS=\u201d -Dcb.max.salt.recipe.execution.retry=90\u201d  This property indicates the number of tries for checking if the scripts have finished with a sleep time (i.e. the wait time between two polling attempts) of 10 seconds. The default value is 90. To increase the threshold, provide a number greater than 90. You must restart Cloudbreak after changing properties in the Profile file.",
            "title": "Recipes: Recipe Execution Times Out"
        },
        {
            "location": "/trouble-cb/index.html#recipes-recipe-execution-fails",
            "text": "It often happens that a script cannot be executed successfully because there are typos or errors in the script. To verify this you can check the recipe logs at /var/log/recipes . For each script, there will be a separate log file with the name of the script that you provided on the Cloudbreak UI.",
            "title": "Recipes: Recipe Execution Fails"
        },
        {
            "location": "/trouble-cb/index.html#invalid-public_ip-in-cbd-profile",
            "text": "The  PUBLIC_IP  property must be set in the cbd Profile file or else you won\u2019t be able to log in on the Cloudbreak UI.   If you are migrating your instance, make sure that after the start the IP remains valid. If you need to edit the  PUBLIC_IP  property in Profile, make sure to restart Cloudbreak using  cbd restart .",
            "title": "Invalid PUBLIC_IP in CBD Profile"
        },
        {
            "location": "/trouble-cb/index.html#changing-properties-in-the-cloudbreak-profile-file",
            "text": "There are many properties that can be changed in the Cloudbreak application. These values must be changed in the Cloudbreak  Profil e file. To see all possible options, use the following command: cbd env show .  After changing a property, you must regenerate the config file and restart the application. There are two ways to do this:  In version 1.4.0 and newer of the cbd command line, you can regenerate the config file and restart the application with a single command:  cbd restart  - same as cbd regenerate/kill/start.  In versions earlier than 1.4.0, you must run the following three commands:  cbd regenerate  regenerates the Docker compose file cbd kill  removes all Docker containers (there is no stop command for this). cbd start  starts the application with the new compose file.",
            "title": "Changing Properties in the Cloudbreak Profile File"
        },
        {
            "location": "/trouble-cb/index.html#changing-amari-credentials",
            "text": "In Cloudbreak 1.14 and later, Cloudbreak creates a new admin user in Ambari, so you can change the credentials of the admin user in the Ambari web UI. You can also set the admin user credentials in the cluster installation wizard in the Cloudbreak UI.  In Cloudbreak versions earlier than 1.14 it is not possible to change the password in the Ambari UI.  If you change the admin credentials in the Ambari UI, Cloudbreak will no longer be able to orchestrate Ambari. To change the password, you must use the option available on the cluster details page in the Cloudbreak UI.",
            "title": "Changing Amari Credentials"
        },
        {
            "location": "/trouble-aws/index.html",
            "text": "Troubleshooting Cloudbreak on AWS",
            "title": "Troubleshooting AWS"
        },
        {
            "location": "/trouble-aws/index.html#troubleshooting-cloudbreak-on-aws",
            "text": "",
            "title": "Troubleshooting Cloudbreak on AWS"
        },
        {
            "location": "/trouble-azure/index.html",
            "text": "Troubleshooting Cloudbreak on Azure\n\n\nCredential Creation Errors\n\n\nRole already exists\n\n\nSymptom\n: You specified that you want to create a new role for Cloudbreak credential, but an existing role with the same name already exists in Azure.\n\n\n\n\nSolution\n: You should either rename the role during credential creation or select the \nReuse existing custom role\n option. \n\n\nRole does not exist\n\n\nSymptom\n: You specified that you want to reuse an existing role for your Cloudbreak credential, but that particular role does not exist in Azure.\n\n\n\n\nSolution\n: You should either rename the new role during the credential creation to match the existing role's name or select the \nLet Cloudbreak create a custom role\n option. \n\n\nRole does not have enough privileges\n\n\nSymptom\n: You specified that you want to reuse an  existing role for your Cloudbreak credential, but that particular role does not have the necessary privileges for Cloudbreak cluster management.\n\n\n\n\nSolution\n: You should either select an existing role with enough privileges or select the \nLet Cloudbreak create a custom role\n option.\n\n\nThe necessary action set for Cloudbreak to be able to manage the clusters includes:\n        \n\"Microsoft.Compute/*\",\n        \"Microsoft.Network/*\",\n        \"Microsoft.Storage/*\",\n        \"Microsoft.Resources/*\"\n\n\nCloud not validate publickey certificate\n\n\nSymptom\n: The syntax of your SSH public key is incorrect.\n\n\n\n\nSolution\n: You must correct the syntax of your SSH key. For information about the correct syntax, refer to \nthis\n page.",
            "title": "Troubleshooting Azure"
        },
        {
            "location": "/trouble-azure/index.html#troubleshooting-cloudbreak-on-azure",
            "text": "",
            "title": "Troubleshooting Cloudbreak on Azure"
        },
        {
            "location": "/trouble-azure/index.html#credential-creation-errors",
            "text": "",
            "title": "Credential Creation Errors"
        },
        {
            "location": "/trouble-azure/index.html#role-already-exists",
            "text": "Symptom : You specified that you want to create a new role for Cloudbreak credential, but an existing role with the same name already exists in Azure.   Solution : You should either rename the role during credential creation or select the  Reuse existing custom role  option.",
            "title": "Role already exists"
        },
        {
            "location": "/trouble-azure/index.html#role-does-not-exist",
            "text": "Symptom : You specified that you want to reuse an existing role for your Cloudbreak credential, but that particular role does not exist in Azure.   Solution : You should either rename the new role during the credential creation to match the existing role's name or select the  Let Cloudbreak create a custom role  option.",
            "title": "Role does not exist"
        },
        {
            "location": "/trouble-azure/index.html#role-does-not-have-enough-privileges",
            "text": "Symptom : You specified that you want to reuse an  existing role for your Cloudbreak credential, but that particular role does not have the necessary privileges for Cloudbreak cluster management.   Solution : You should either select an existing role with enough privileges or select the  Let Cloudbreak create a custom role  option.  The necessary action set for Cloudbreak to be able to manage the clusters includes:\n         \"Microsoft.Compute/*\",\n        \"Microsoft.Network/*\",\n        \"Microsoft.Storage/*\",\n        \"Microsoft.Resources/*\"",
            "title": "Role does not have enough privileges"
        },
        {
            "location": "/trouble-azure/index.html#cloud-not-validate-publickey-certificate",
            "text": "Symptom : The syntax of your SSH public key is incorrect.   Solution : You must correct the syntax of your SSH key. For information about the correct syntax, refer to  this  page.",
            "title": "Cloud not validate publickey certificate"
        },
        {
            "location": "/trouble-gcp/index.html",
            "text": "Troubleshooting Cloudbreak on GCP",
            "title": "Troubleshooting GCP"
        },
        {
            "location": "/trouble-gcp/index.html#troubleshooting-cloudbreak-on-gcp",
            "text": "",
            "title": "Troubleshooting Cloudbreak on GCP"
        },
        {
            "location": "/trouble-os/index.html",
            "text": "Troubleshooting Cloudbreak on OpenStack",
            "title": "Troubleshooting OpenStack"
        },
        {
            "location": "/trouble-os/index.html#troubleshooting-cloudbreak-on-openstack",
            "text": "",
            "title": "Troubleshooting Cloudbreak on OpenStack"
        },
        {
            "location": "/get-help/index.html",
            "text": "Get Help\n\n\nIf you need help with Cloudbreak, you have two options:\n\n\n\n\n\n\n\n\nOption\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nHortonworks Community Connection\n\n\nThis is free optional support via Hortonworks Community Connection (HCC).\n\n\n\n\n\n\nHortonworks Flex Support Subscription\n\n\nThis is paid Hortonworks enterprise support.\n\n\n\n\n\n\n\n\nHCC\n\n\nYou can optionally register for optional free community support at \nHortonworks Community Connection\n where you can browse articles and previously answered questions, and ask questions of your own. When posting questions related to Cloudbreak, make sure to use the \"Cloudbreak\" tag.\n\n\nFlex Subscription\n\n\nYou can optionally use your existing Hortonworks \nFlex subscription(s)\n to cover the Cloudbreak node and all clusters created. \n\n\nPrerequisites\n: You must have an existing SmartSense ID and a Flex subscription. For general information about the Hortonworks Flex Support Subscription, visit the Hortonworks Support page at \nhttps://hortonworks.com/services/support/enterprise/\n.\n\n\nThe general steps are:\n\n\n\n\nConfigure Smart Sense in your \nProfile\n file.   \n\n\nRegister your Flex subscription in the Cloudbreak web UI in the the \nmanage flex subscriptions\n pane. You can register and manage multiple Flex subscriptions.   \n\n\n\n\n\n\nAlternatively, you can perform these steps using the Cloudbreak Shell. \n\n\n\n\nConfiguring SmartSense\n\n\nTo configure SmartSense in Cloudbreak, enable SmartSense and add your SmartSense ID to the \nProfile\n by adding the following variables:\n\n\nexport CB_SMARTSENSE_CONFIGURE=true\nexport CB_SMARTSENSE_ID=YOUR-SMARTSENSE-ID\n\n\n\nFor example:\n\n\nexport CB_SMARTSENSE_CONFIGURE=true\nexport CB_SMARTSENSE_ID=A-00000000-C-00000000\n\n\n\nYou can do this in one of the two ways:\n\n\n\n\nWhen initiating Cloudbreak Deployer  \n\n\nAfter you've already initiated Cloudbreak Deployer. If you choose this option, you must restart Cloudbreak using \ncbd restart\n.\n\n\n\n\n\n\nSmartSense ID defined in the \nProfile\n file always overrides the ID registered via Cloudbreak Shell.\n\n\n\n\nManaging Flex Subscriptions\n\n\nOnce you log in to the Cloudbreak web UI, you can manage your Flex subscriptions from the \nmanage flex subscriptions\n pane. You can:\n\n\n\n\nRegister a new Flex subscription.  \n\n\nSet a default Flex subscription.  \n\n\nSelect a Flex subscription to be used for cloud controller.  \n\n\nDelete a Flex subscription.  \n\n\nCheck which clusters are connected to a specific subscription.  \n\n\n\n\nWhen creating a cluster using the advanced options, in the \nCONFIGURE CLUSTER\n > \nFlex Subscriptions\n, you can select the Flex subscription that you want to use.\n\n\nMore Resources\n\n\nCheck out the following documentation to learn more:\n\n\n\n\n Resource \nDescription\n\n\nHortonworks documentation \n\n\nDuring cluster create process, Hortonworks Data Cloud automatically installs Ambari and sets up a cluster for you. After this deployment is complete, refer to the \nAmbari documentation\n and \nHDP documentation\n for help.\n\n\n\n\n\n\nHortonworks tutorials\n\n\n\n\nUse Hortonworks tutorials to get started with Apache Spark, Apache Hive, Apache Zeppelin, and more.\n\n\nApache documentation\n\n\n\n\n In addition to Hortonworks documentation, refer to the Apache Software Foundation documentation to get information on specific Hadoop services. \n\n\n\n\n\nAmbari Blueprints\nLearn about Ambari Bleuprints. Ambari Blueprints are a declarative definition of a Hadoop cluster that Ambari can use to create Hadoop clusters.\n\n\nCloudbreak Project\nVisit the Hortonworks website to see Cloudbreak-related news and updates.\n\n\nApache Ambari Project\nLearn about the Apache Ambari Project. Apache Ambari is an operational platform for provisioning, managing, and monitoring Apache Hadoop clusters. Ambari exposes a robust set of REST APIs and a rich web interface for cluster management.",
            "title": "Getting Help"
        },
        {
            "location": "/get-help/index.html#get-help",
            "text": "If you need help with Cloudbreak, you have two options:     Option  Description      Hortonworks Community Connection  This is free optional support via Hortonworks Community Connection (HCC).    Hortonworks Flex Support Subscription  This is paid Hortonworks enterprise support.",
            "title": "Get Help"
        },
        {
            "location": "/get-help/index.html#hcc",
            "text": "You can optionally register for optional free community support at  Hortonworks Community Connection  where you can browse articles and previously answered questions, and ask questions of your own. When posting questions related to Cloudbreak, make sure to use the \"Cloudbreak\" tag.",
            "title": "HCC"
        },
        {
            "location": "/get-help/index.html#flex-subscription",
            "text": "You can optionally use your existing Hortonworks  Flex subscription(s)  to cover the Cloudbreak node and all clusters created.   Prerequisites : You must have an existing SmartSense ID and a Flex subscription. For general information about the Hortonworks Flex Support Subscription, visit the Hortonworks Support page at  https://hortonworks.com/services/support/enterprise/ .  The general steps are:   Configure Smart Sense in your  Profile  file.     Register your Flex subscription in the Cloudbreak web UI in the the  manage flex subscriptions  pane. You can register and manage multiple Flex subscriptions.       Alternatively, you can perform these steps using the Cloudbreak Shell.",
            "title": "Flex Subscription"
        },
        {
            "location": "/get-help/index.html#configuring-smartsense",
            "text": "To configure SmartSense in Cloudbreak, enable SmartSense and add your SmartSense ID to the  Profile  by adding the following variables:  export CB_SMARTSENSE_CONFIGURE=true\nexport CB_SMARTSENSE_ID=YOUR-SMARTSENSE-ID  For example:  export CB_SMARTSENSE_CONFIGURE=true\nexport CB_SMARTSENSE_ID=A-00000000-C-00000000  You can do this in one of the two ways:   When initiating Cloudbreak Deployer    After you've already initiated Cloudbreak Deployer. If you choose this option, you must restart Cloudbreak using  cbd restart .    SmartSense ID defined in the  Profile  file always overrides the ID registered via Cloudbreak Shell.",
            "title": "Configuring SmartSense"
        },
        {
            "location": "/get-help/index.html#managing-flex-subscriptions",
            "text": "Once you log in to the Cloudbreak web UI, you can manage your Flex subscriptions from the  manage flex subscriptions  pane. You can:   Register a new Flex subscription.    Set a default Flex subscription.    Select a Flex subscription to be used for cloud controller.    Delete a Flex subscription.    Check which clusters are connected to a specific subscription.     When creating a cluster using the advanced options, in the  CONFIGURE CLUSTER  >  Flex Subscriptions , you can select the Flex subscription that you want to use.",
            "title": "Managing Flex Subscriptions"
        },
        {
            "location": "/get-help/index.html#more-resources",
            "text": "Check out the following documentation to learn more:    Resource  Description  Hortonworks documentation   During cluster create process, Hortonworks Data Cloud automatically installs Ambari and sets up a cluster for you. After this deployment is complete, refer to the  Ambari documentation  and  HDP documentation  for help.    Hortonworks tutorials   Use Hortonworks tutorials to get started with Apache Spark, Apache Hive, Apache Zeppelin, and more.  Apache documentation    In addition to Hortonworks documentation, refer to the Apache Software Foundation documentation to get information on specific Hadoop services.    Ambari Blueprints Learn about Ambari Bleuprints. Ambari Blueprints are a declarative definition of a Hadoop cluster that Ambari can use to create Hadoop clusters.  Cloudbreak Project Visit the Hortonworks website to see Cloudbreak-related news and updates.  Apache Ambari Project Learn about the Apache Ambari Project. Apache Ambari is an operational platform for provisioning, managing, and monitoring Apache Hadoop clusters. Ambari exposes a robust set of REST APIs and a rich web interface for cluster management.",
            "title": "More Resources"
        },
        {
            "location": "/smartsense/index.html",
            "text": "SmartSense Telemetry\n\n\nHelp us make a better product by opt'ing in to automatically send information to Hortonworks. This includes enabling\nHortonworks SmartSense and sending performance and usage info. As you use the product,\nSmartSense measures and collects information and then sends these information bundles to Hortonworks.\n\n\nHow to Disable\n\n\nDisable Bundle Upload for the Cloud Controller and New Clusters\n\n\n\n    \nImportant\n\n    \n\n    Do not perform these steps when you have clusters currently in the process of being deployed.\n    Wait for all clusters to be deployed.\n\n\n\n\n\n\n\n\n\nSSH into the cloud controller host.\n\n\n\n\n\n\nEdit \n/var/lib/cloudbreak-deployment/Profile\n.\n\n\n\n\n\n\nChange \nCB_SMARTSENSE_CONFIGURE\n to \nfalse\n:\n\n    \nexport CB_SMARTSENSE_CONFIGURE=false\n\n\n\n\n\n\nRestart the cloud controller:\n\n    \ncd /var/lib/cloudbreak-deployment\ncbd restart\n\n\n\n\n\n\nDisable Bundle Upload for an Existing Cluster\n\n\n\n\n\n\nSSH into the Master node for the cluster.\n\n\n\n\n\n\nEdit \n/etc/hst/conf/hst-server.ini\n.\n\n\n\n\n\n\nChange \n[gateway]\n configuration to \nfalse\n:\n\n    \n[gateway]\nenabled=false\n\n\n\n\n\n\nRestart the SmartSense Server:\n    \nhst restart\n\n\n\n\n\n\n(Optional) Disable SmartSense daily bundle capture:\n\n\n\n\nSmartSense is scheduled to capture a telemetry bundle daily. With the bundle upload disabled, the bundle will still\nbe captured but just saved locally (i.e. not uploaded).\n\n\nTo disable the bundle capture, execute the following:\n\nhst capture-schedule -a pause\n\n\n\n\n\n\n\n\nRepeat on all existing clusters.",
            "title": "SmartSense"
        },
        {
            "location": "/smartsense/index.html#smartsense-telemetry",
            "text": "Help us make a better product by opt'ing in to automatically send information to Hortonworks. This includes enabling\nHortonworks SmartSense and sending performance and usage info. As you use the product,\nSmartSense measures and collects information and then sends these information bundles to Hortonworks.",
            "title": "SmartSense Telemetry"
        },
        {
            "location": "/smartsense/index.html#how-to-disable",
            "text": "",
            "title": "How to Disable"
        },
        {
            "location": "/smartsense/index.html#disable-bundle-upload-for-the-cloud-controller-and-new-clusters",
            "text": "Important \n     \n    Do not perform these steps when you have clusters currently in the process of being deployed.\n    Wait for all clusters to be deployed.     SSH into the cloud controller host.    Edit  /var/lib/cloudbreak-deployment/Profile .    Change  CB_SMARTSENSE_CONFIGURE  to  false : \n     export CB_SMARTSENSE_CONFIGURE=false    Restart the cloud controller: \n     cd /var/lib/cloudbreak-deployment\ncbd restart",
            "title": "Disable Bundle Upload for the Cloud Controller and New Clusters"
        },
        {
            "location": "/smartsense/index.html#disable-bundle-upload-for-an-existing-cluster",
            "text": "SSH into the Master node for the cluster.    Edit  /etc/hst/conf/hst-server.ini .    Change  [gateway]  configuration to  false : \n     [gateway]\nenabled=false    Restart the SmartSense Server:\n     hst restart    (Optional) Disable SmartSense daily bundle capture:   SmartSense is scheduled to capture a telemetry bundle daily. With the bundle upload disabled, the bundle will still\nbe captured but just saved locally (i.e. not uploaded).  To disable the bundle capture, execute the following: hst capture-schedule -a pause     Repeat on all existing clusters.",
            "title": "Disable Bundle Upload for an Existing Cluster"
        },
        {
            "location": "/dev-local-dev/index.html",
            "text": "Local Development\n\n\nThe following documentation will help you set up your local development environment for \nCloudbreak application\n and \nCloudbreak Deployer\n.\n\n\nCloudbreak Application\n\n\nThe following steps will help you set up your local development environment for Cloudbreak application.\n\n\nSet up Local Development\n\n\nPrerequisites\n \n\nTo use this development environment on Mac OS X, you need to have Docker and Boot2docker installed.\n\n\nSteps\n  \n\n\n\n\n\n\nThe simplest way to prepare the working environment is to start the Cloudbreak application on your local machine using \nCloudbreak deployer\n.\n\n\n\n\n\n\n\n\n\n\nTO-DO: What exactly do I need to do in the step above? Am I supposed to clone this repo? How do I \"start\" the Cloudbreak applicaton?\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCreate a directory which will store the necessary configuration files and dependencies of Cloudbreak deployer. This directory must be created outside of the cloned Cloudbreak git repository:\n\n\n\nmkdir cbd-local\n\n\n\n\n\n\nTo start the complete Cloudbreak ecosystem on your machine, execute the following sequence of commands:\n\n\n\ncd cbd-local\ncurl https://raw.githubusercontent.com/hortonworks/cloudbreak-deployer/master/install | sh && cbd --version\ncbd update master\ncbd init\ncbd start\n\n\n\n\n\n\nAfter running these commands, Cloudbreak is available at http://192.168.59.103:3000. For more details and config parameters please check the documentation of \nCloudbreak Deployer\n.\n\n\nFurthermore, Cloudbreak deployer generates a \ncerts\n directory inside the \ncbd-local\n directory. You will need it later when setting up the IDEA.\n\n\n\n\n\n\nOpen the \ncbd-local/Profile\n file with a text editor and add the CB_SCHEMA_SCRIPTS_LOCATION environment variable to configure the location of SQL scripts, which is the 'core/src/main/resources/schema' directory located in the cloned Cloudbreak git repository. Note that the full path needs to be configured and environment variables like $USER cannot be used. For example:\n\n\n\nexport CB_SCHEMA_SCRIPTS_LOCATION=/Users/myusername/prj/cloudbreak/core/src/main/resources/schema\n\n\n\n\n\n\nTip\n: If you need to kill a Cloudbreak container running inside the boot2docker and redirect the Cloudbreak related traffic to the Cloudbreak running in IDEA, use the following command:\n\n\n\ncbd util local-dev\n\n\n\nSet up IDEA\n\n\nThe following steps will help you set up IDEA for the local development environment of Cloudbreak application.\n\n\nSteps\n\n\n\n\n\n\nImport Cloudbreak into IDEA as gradle project. Once done, import the proper code formatter by using the \nFile > Import Settings...\n menu and selecting the \nidea_settings.jar\n located in the \nconfig\n directory in Cloudbreak git repository.\n\n\n\n\n\n\nTo launch the Cloudbreak application execute the \ncom.sequenceiq.cloudbreak.CloudbreakApplication\n class with VM options. \n\n\nThe \nFULL_PATH_OF_THE_CERTS_DIR_GENERATED_BY_CBD\n below needs to be replaced with the full path of \ncerts\n directory generated by Cloudbreak deployer; for example: \n-Dcb.cert.dir=/Users/myusername/prj/cbd-local/certs\n.\n\n\n\n-XX:MaxPermSize=1024m\n-Dcb.cert.dir=FULL_PATH_OF_THE_CERTS_DIR_GENERATED_BY_CBD\n-Dcb.client.id=cloudbreak\n-Dcb.client.secret=cbsecret2015\n-Dcb.db.port.5432.tcp.addr=192.168.59.103\n-Dcb.db.port.5432.tcp.port=5432\n-Dspring.cloud.consul.host=192.168.59.103\n-Dcb.identity.server.url=http://192.168.59.103:8089\n-Dserver.port=9091\n\n\n\n\n\n\nSet up Command line\n\n\nThe following steps will help you set up the command line for the local development environment of Cloudbreak application..\n\n\nSteps\n\n\n\n\n\n\nTo run Cloudbreak from command line, create a property file, for example \"application.properties\", with the content below\n    \n\ncb.cert.dir=FULL_PATH_OF_THE_CERTS_DIR_GENERATED_BY_CBD\ncb.client.id=cloudbreak\ncb.client.secret=cbsecret2015\ncb.db.port.5432.tcp.addr=192.168.59.103\ncb.db.port.5432.tcp.port=5432\nspring.cloud.consul.host=192.168.59.103\ncb.identity.server.url=http://192.168.59.103:8089\nserver.port=9091\n\n\n\n\n\n\nExecute the \n./gradlew bootRun -Dspring.config.location=file:/path/of/property/application.properties\n command at project root.\n\n\n\n\n\n\n\n\n\n\nTO-DO: Do you mean \"at the project's root directory\" or \"as the root user\"? \n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSet up Database Development\n\n\nIf schema change is required in Cloudbreak database (cbdb), then the developer needs to write SQL scripts to migrate the database accordingly. In Cloudbreak, the schema migration is managed by the \nMYBATIS Migrations\n, for which the cbd tool provides an easy-to-use wrapper. The syntax for using the migration commands is \n\n\ncbd migrate <database name> <command> [parameters]\n\n\nFor example:\n\n\ncbd migrate migrate status\n\n\nSteps\n\n\n\n\n\n\nCreate a SQL template for schema changes:\n\n\n\ncbd migrate cbdb new \"CLOUD-123 schema change for new feature\"\n\n\n\nAs as result of the above command, an SQL file template is generated under the path specified in \nCB_SCHEMA_SCRIPTS_LOCATION\n environment variable, which is defined in Profile. The structure of the generated SQL template looks like the following:\n\n\n\n-- // CLOUD-123 schema change for new feature\n-- Migration SQL that makes the change goes here.\n\n\n-- //@UNDO\n-- SQL to undo the change goes here.\n\n\n\n\n\n\n\nOnce you have implemented your SQLs, you can execute them by using:\n\n\n\ncbd migrate cbdb up\n\n\n\n\n\n\n\nOptional Steps\n\n\nIf you would like to rollback the last SQL file, use the \ndown\n command:\n\n\n\ncbd migrate cbdb down\n\n\n\nIn order to check the status of database, use: \n\n\ncbd migrate cbdb status\n\n#Every script that has not been executed will be marked as ...pending... in the output of status command:\n\n------------------------------------------------------------------------\n-- MyBatis Migrations - status\n------------------------------------------------------------------------\nID             Applied At          Description\n================================================================================\n20150421140021 2015-07-08 10:04:28 create changelog\n20150421150000 2015-07-08 10:04:28 CLOUD-607 create baseline schema\n20150507121756 2015-07-08 10:04:28 CLOUD-576 change instancegrouptype hostgroup to core\n20151008090632    ...pending...    CLOUD-123 schema change for new feature\n\n------------------------------------------------------------------------\n\n\n\n\nBuilding\n\n\nCloudbreak uses Gradle for build and dependency management. Gradle wrapper is added to Cloudbreak git repository, therefore you build by using the following command:\n\n\n\n./gradlew clean build\n\n\n\n\nCloudbreak Deployer\n\n\nThe following steps will help you set up your local development environment for Cloudbreak deployer.\n\n\nContributing\n\n\nWhen developing, work on separate branches and then open a pull request.\n\n\n\n\n\n\n\n\n\n\nTO-DO: Branching from master or from the latest released branch?\n\n\n\n\n\n\n\n\n\n\nTo build the project for the first time, use:\n\n\n\nmake deps\n\nmake install\n\n\n\n\n\n\nYou only need to run \nmake deps\n once.\n\n\n\n\nTo build the project, use:\n\n\n\nmake install\n\n\n\n\nTo run the unit tests, use:\n\n\n\nmake tests\n\n\n\n\nTo test the binary CircleCI build from your branch named \nfix-something\n, to validate the PR binary \ncbd\n tool will be tested. It is built by CircleCI for each branch.\n\n\n\n\n\n\n\n\n\n\nTO-DO: Can you rephrase the paragraph above? I don't understand it at all. \n\n\n\n\n\n\n\n\n\n\n\ncbd update fix-something\n\n\n\n\nTaking Snapshots\n\n\nWe recommend to always use the latest release.\n\n\n\n\n\n\n\n\n\n\nTO-DO: How is this recommendation related to taking snapshots?\n\n\n\n\n\n\n\n\n\n\nAll successful builds from the \nmaster\n branch are uploaded to the public S3 bucket. You can download a build using:\n\n\n\n\n\n\n\n\n\n\nTO-DO: How is this related to taking snapshots?\n\n\n\n\n\n\n\n\n\n\n\ncurl -L s3.amazonaws.com/public-repo-1.hortonworks.com/HDP/cloudbreak/cloudbreak-deployer_snapshot_$(uname)_x86_64.tgz | tar -xz\n\n\n\n\nInstead of overwriting the released version, download it to a local directory and use it by referring to it as \n./cbd\n\n\n\n\n\n\n\n\n\n\nTO-DO: How is this related to taking snapshots?\n\n\n\n\n\n\n\n\n\n\n\n./cbd --version\n\n\n\n\nTesting\n\n\nShell scripts shouldn\u2019t raise exceptions when it comes to unit testing. \n\n\nbasht\n is\n used for testing. See \nwhy not bats or shunit2\n.\n\n\n\n\n\n\n\n\n\n\nTO-DO: Do you mean \"Shell scripts shouldn\u2019t raise exceptions \nduring\n unit testing.\"?\n\n\n\n\n\n\n\n\n\n\nCover your bash functions with unit tests and run tests using:\n\n\n\nmake tests\n\n\n\n\nClodbreak Deployer Release Process\n\n\nThe master branch is always built on \nCircleCI\n.\n\n\nTo create a new release, all you have to do is run:\n\n\n\nmake release-next-ver\n\n\n\n\nThis performs the following steps:\n\n\n\n\nOn the \nmaster\n branch:\n\n\nUpdates the \nVERSION\n file by increasing the \npatch\n version number (for example, from 0.5.2 to 0.5.3).\n\n\nUpdates \nCHANGELOG.md\n with the release date.\n\n\nCreates a new \nUnreleased\n section at the top of \nCHANGELOG.md\n.\n\n\n\n\n\n\nCreates a pull request for the release branch:\n\n\nCreates a new branch with a name like \nrelease-0.5.x\n. This branch should be the same as \norigin/master\n.\n\n\nCreates a pull request to the \nrelease\n branch.\n\n\n\n\n\n\n\n\nAcceptance\n\n\nYou can get the release by using \ncbd update release-x.y.z\n. \nTest it and when done, comment using \"LGTM\" (Looking Good To Me).\n\n\nOnce the PR is merged, CircleCI will:\n\n\n\n\nCreate a new release on \nGitHub releases tab\n, with the\n help of \ngh-release\n\n\nCreate the git tag with \nv\n prefix such as \nv0.0.3",
            "title": "Local Development"
        },
        {
            "location": "/dev-local-dev/index.html#local-development",
            "text": "The following documentation will help you set up your local development environment for  Cloudbreak application  and  Cloudbreak Deployer .",
            "title": "Local Development"
        },
        {
            "location": "/dev-local-dev/index.html#cloudbreak-application",
            "text": "The following steps will help you set up your local development environment for Cloudbreak application.",
            "title": "Cloudbreak Application"
        },
        {
            "location": "/dev-local-dev/index.html#set-up-local-development",
            "text": "Prerequisites   \nTo use this development environment on Mac OS X, you need to have Docker and Boot2docker installed.  Steps       The simplest way to prepare the working environment is to start the Cloudbreak application on your local machine using  Cloudbreak deployer .      TO-DO: What exactly do I need to do in the step above? Am I supposed to clone this repo? How do I \"start\" the Cloudbreak applicaton?        Create a directory which will store the necessary configuration files and dependencies of Cloudbreak deployer. This directory must be created outside of the cloned Cloudbreak git repository:  \nmkdir cbd-local    To start the complete Cloudbreak ecosystem on your machine, execute the following sequence of commands:  \ncd cbd-local\ncurl https://raw.githubusercontent.com/hortonworks/cloudbreak-deployer/master/install | sh && cbd --version\ncbd update master\ncbd init\ncbd start    After running these commands, Cloudbreak is available at http://192.168.59.103:3000. For more details and config parameters please check the documentation of  Cloudbreak Deployer .  Furthermore, Cloudbreak deployer generates a  certs  directory inside the  cbd-local  directory. You will need it later when setting up the IDEA.    Open the  cbd-local/Profile  file with a text editor and add the CB_SCHEMA_SCRIPTS_LOCATION environment variable to configure the location of SQL scripts, which is the 'core/src/main/resources/schema' directory located in the cloned Cloudbreak git repository. Note that the full path needs to be configured and environment variables like $USER cannot be used. For example:  \nexport CB_SCHEMA_SCRIPTS_LOCATION=/Users/myusername/prj/cloudbreak/core/src/main/resources/schema    Tip : If you need to kill a Cloudbreak container running inside the boot2docker and redirect the Cloudbreak related traffic to the Cloudbreak running in IDEA, use the following command:  \ncbd util local-dev",
            "title": "Set up Local Development"
        },
        {
            "location": "/dev-local-dev/index.html#set-up-idea",
            "text": "The following steps will help you set up IDEA for the local development environment of Cloudbreak application.  Steps    Import Cloudbreak into IDEA as gradle project. Once done, import the proper code formatter by using the  File > Import Settings...  menu and selecting the  idea_settings.jar  located in the  config  directory in Cloudbreak git repository.    To launch the Cloudbreak application execute the  com.sequenceiq.cloudbreak.CloudbreakApplication  class with VM options.   The  FULL_PATH_OF_THE_CERTS_DIR_GENERATED_BY_CBD  below needs to be replaced with the full path of  certs  directory generated by Cloudbreak deployer; for example:  -Dcb.cert.dir=/Users/myusername/prj/cbd-local/certs .  \n-XX:MaxPermSize=1024m\n-Dcb.cert.dir=FULL_PATH_OF_THE_CERTS_DIR_GENERATED_BY_CBD\n-Dcb.client.id=cloudbreak\n-Dcb.client.secret=cbsecret2015\n-Dcb.db.port.5432.tcp.addr=192.168.59.103\n-Dcb.db.port.5432.tcp.port=5432\n-Dspring.cloud.consul.host=192.168.59.103\n-Dcb.identity.server.url=http://192.168.59.103:8089\n-Dserver.port=9091",
            "title": "Set up IDEA"
        },
        {
            "location": "/dev-local-dev/index.html#set-up-command-line",
            "text": "The following steps will help you set up the command line for the local development environment of Cloudbreak application..  Steps    To run Cloudbreak from command line, create a property file, for example \"application.properties\", with the content below\n     \ncb.cert.dir=FULL_PATH_OF_THE_CERTS_DIR_GENERATED_BY_CBD\ncb.client.id=cloudbreak\ncb.client.secret=cbsecret2015\ncb.db.port.5432.tcp.addr=192.168.59.103\ncb.db.port.5432.tcp.port=5432\nspring.cloud.consul.host=192.168.59.103\ncb.identity.server.url=http://192.168.59.103:8089\nserver.port=9091    Execute the  ./gradlew bootRun -Dspring.config.location=file:/path/of/property/application.properties  command at project root.      TO-DO: Do you mean \"at the project's root directory\" or \"as the root user\"?",
            "title": "Set up Command line"
        },
        {
            "location": "/dev-local-dev/index.html#set-up-database-development",
            "text": "If schema change is required in Cloudbreak database (cbdb), then the developer needs to write SQL scripts to migrate the database accordingly. In Cloudbreak, the schema migration is managed by the  MYBATIS Migrations , for which the cbd tool provides an easy-to-use wrapper. The syntax for using the migration commands is   cbd migrate <database name> <command> [parameters]  For example:  cbd migrate migrate status  Steps    Create a SQL template for schema changes:  \ncbd migrate cbdb new \"CLOUD-123 schema change for new feature\"  As as result of the above command, an SQL file template is generated under the path specified in  CB_SCHEMA_SCRIPTS_LOCATION  environment variable, which is defined in Profile. The structure of the generated SQL template looks like the following:  \n-- // CLOUD-123 schema change for new feature\n-- Migration SQL that makes the change goes here.  -- //@UNDO\n-- SQL to undo the change goes here.    Once you have implemented your SQLs, you can execute them by using:  \ncbd migrate cbdb up    Optional Steps  If you would like to rollback the last SQL file, use the  down  command:  \ncbd migrate cbdb down  In order to check the status of database, use:   cbd migrate cbdb status\n\n#Every script that has not been executed will be marked as ...pending... in the output of status command:\n\n------------------------------------------------------------------------\n-- MyBatis Migrations - status\n------------------------------------------------------------------------\nID             Applied At          Description\n================================================================================\n20150421140021 2015-07-08 10:04:28 create changelog\n20150421150000 2015-07-08 10:04:28 CLOUD-607 create baseline schema\n20150507121756 2015-07-08 10:04:28 CLOUD-576 change instancegrouptype hostgroup to core\n20151008090632    ...pending...    CLOUD-123 schema change for new feature\n\n------------------------------------------------------------------------",
            "title": "Set up Database Development"
        },
        {
            "location": "/dev-local-dev/index.html#building",
            "text": "Cloudbreak uses Gradle for build and dependency management. Gradle wrapper is added to Cloudbreak git repository, therefore you build by using the following command:  \n./gradlew clean build",
            "title": "Building"
        },
        {
            "location": "/dev-local-dev/index.html#cloudbreak-deployer",
            "text": "The following steps will help you set up your local development environment for Cloudbreak deployer.",
            "title": "Cloudbreak Deployer"
        },
        {
            "location": "/dev-local-dev/index.html#contributing",
            "text": "When developing, work on separate branches and then open a pull request.      TO-DO: Branching from master or from the latest released branch?      To build the project for the first time, use:  \nmake deps\n\nmake install   You only need to run  make deps  once.   To build the project, use:  \nmake install  To run the unit tests, use:  \nmake tests  To test the binary CircleCI build from your branch named  fix-something , to validate the PR binary  cbd  tool will be tested. It is built by CircleCI for each branch.      TO-DO: Can you rephrase the paragraph above? I don't understand it at all.       \ncbd update fix-something",
            "title": "Contributing"
        },
        {
            "location": "/dev-local-dev/index.html#taking-snapshots",
            "text": "We recommend to always use the latest release.      TO-DO: How is this recommendation related to taking snapshots?      All successful builds from the  master  branch are uploaded to the public S3 bucket. You can download a build using:      TO-DO: How is this related to taking snapshots?      \ncurl -L s3.amazonaws.com/public-repo-1.hortonworks.com/HDP/cloudbreak/cloudbreak-deployer_snapshot_$(uname)_x86_64.tgz | tar -xz  Instead of overwriting the released version, download it to a local directory and use it by referring to it as  ./cbd      TO-DO: How is this related to taking snapshots?      \n./cbd --version",
            "title": "Taking Snapshots"
        },
        {
            "location": "/dev-local-dev/index.html#testing",
            "text": "Shell scripts shouldn\u2019t raise exceptions when it comes to unit testing.   basht  is\n used for testing. See  why not bats or shunit2 .      TO-DO: Do you mean \"Shell scripts shouldn\u2019t raise exceptions  during  unit testing.\"?      Cover your bash functions with unit tests and run tests using:  \nmake tests",
            "title": "Testing"
        },
        {
            "location": "/dev-local-dev/index.html#clodbreak-deployer-release-process",
            "text": "The master branch is always built on  CircleCI .  To create a new release, all you have to do is run:  \nmake release-next-ver  This performs the following steps:   On the  master  branch:  Updates the  VERSION  file by increasing the  patch  version number (for example, from 0.5.2 to 0.5.3).  Updates  CHANGELOG.md  with the release date.  Creates a new  Unreleased  section at the top of  CHANGELOG.md .    Creates a pull request for the release branch:  Creates a new branch with a name like  release-0.5.x . This branch should be the same as  origin/master .  Creates a pull request to the  release  branch.",
            "title": "Clodbreak Deployer Release Process"
        },
        {
            "location": "/dev-local-dev/index.html#acceptance",
            "text": "You can get the release by using  cbd update release-x.y.z . \nTest it and when done, comment using \"LGTM\" (Looking Good To Me).  Once the PR is merged, CircleCI will:   Create a new release on  GitHub releases tab , with the\n help of  gh-release  Create the git tag with  v  prefix such as  v0.0.3",
            "title": "Acceptance"
        },
        {
            "location": "/dev-api/index.html",
            "text": "Cloudbreak APIs\n\n\nCloudbreak is a RESTful application development platform whose goal is to help developers deploy HDP clusters in various cloud environments. Once Cloudbreak is deployed in your favorite servlet container, it exposes REST APIs, allowing you to spin up Hadoop clusters of any size with your chosen cloud provider.\n\n\nAPI Documentation\n\n\nThe Cloudbreak API documentation is available \nhere\n. \n\n\n\n\nThis documentation was generated from the code using \nSwagger\n.\n\n\n\n\n\n\n\n\nTO-DO: The link above points to 1.16.1 (rather than 1.16.4).\n\n\nTO-DO: The API doc also needs an edit.",
            "title": "API"
        },
        {
            "location": "/dev-api/index.html#cloudbreak-apis",
            "text": "Cloudbreak is a RESTful application development platform whose goal is to help developers deploy HDP clusters in various cloud environments. Once Cloudbreak is deployed in your favorite servlet container, it exposes REST APIs, allowing you to spin up Hadoop clusters of any size with your chosen cloud provider.",
            "title": "Cloudbreak APIs"
        },
        {
            "location": "/dev-api/index.html#api-documentation",
            "text": "The Cloudbreak API documentation is available  here .    This documentation was generated from the code using  Swagger .     TO-DO: The link above points to 1.16.1 (rather than 1.16.4).  TO-DO: The API doc also needs an edit.",
            "title": "API Documentation"
        },
        {
            "location": "/dev-spi/index.html",
            "text": "SPI",
            "title": "SPI"
        },
        {
            "location": "/dev-spi/index.html#spi",
            "text": "",
            "title": "SPI"
        },
        {
            "location": "/dev-flows/index.html",
            "text": "Flows",
            "title": "Flows"
        },
        {
            "location": "/dev-flows/index.html#flows",
            "text": "",
            "title": "Flows"
        }
    ]
}