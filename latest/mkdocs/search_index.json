{
    "docs": [
        {
            "location": "/index.html", 
            "text": "Introduction\n\n\nWelcome to the \nCloudbreak 2.1 Technical Preview\n documentation!\n\n\nCloudbreak simplifies the provisioning, management, and monitoring of on-demand HDP clusters in virtual and cloud environments. It leverages cloud infrastructure to create host instances, and uses Apache Ambari via Ambari blueprints to provision and manage HDP clusters. \n\n\nCloudbreak allows you to create clusters using the Cloudbreak web UI, Cloudbreak Shell, and Cloudbreak REST API. Clusters can be launched on public cloud infrastructure platforms \nMicrosoft Azure\n, \nAmazon Web Services (AWS)\n, and \nGoogle Cloud Platform (GCP)\n, and on the private cloud infrastructure platform \nOpenStack\n.\n\n\n  \n\n\nUse Cases\n\n\nCloudbreak allows you to create, manage, and monitor your clusters on your chosen cloud platform:\n\n\n\n\nQuickly create a cluster using one of the default cluster blueprints and infrastructure settings.  \n\n\nCreate a cluster based on the requirements of your workloads and provision infrastructure based on your IT requirements.\n\n\nSecure your cluster by enabling Kerberos.\n\n\nAutomate cluster creation using the Cloudbreak CLI. \n\n\nDevelop your application using Cloudbreak API.\n\n\n\n\nArchitecture\n\n\nRefer to \nArchitecture\n.\n\n\nGet Started\n\n\nTo get started with Cloudbreak:\n\n\n\n\nSelect the \ncloud platform\n on which you would like to launch Cloudbreak.   \n\n\nSelect the \ndeployment option\n that you would like to use. \n\n\nLaunch Cloudbreak\n. \n\n\n\n\nSelect Cloud Platform\n\n\nYou can deploy and use Cloudbreak on the following cloud platforms:\n\n\n\n\nAmazon Web Services (AWS)\n\n\nMicrosoft Azure\n\n\nGoogle Cloud Platform (GCP)\n\n\nOpenStack\n\n\n\n\nSelect Deployment Option\n\n\nThere are two basic deployment options:\n\n\n\n\n\n\n\n\nDeployment option\n\n\nWhen to use\n\n\n\n\n\n\n\n\n\n\nInstantiate one of the pre-built cloud images\n\n\nThis is the recommended basic deployment option.\n The cloud images include Cloudbreak deployer pre-installed on a CentOS VM.\n\n\n\n\n\n\nInstall the Cloudbreak deployer on your own VM\n\n\nThis is an advanced deployment option.\n \nSelect this option if you have custom VM requirements. The supported operating systems are RHEL, CentOS, and Oracle Linux 7 (64-bit).\n\n\n\n\n\n\n\n\nLaunch Cloudbreak\n\n\n(Option 1) You can launch Cloudbreak from one of the pre-built images:  \n\n\n\n\nLaunch on AWS\n  \n\n\nLaunch on Azure\n  \n\n\nLaunch on GCP\n   \n\n\nLaunch on OpenStack\n    \n\n\n\n\n(Option 2) Or you can launch Cloudbreak \non your own VM\n on one of these cloud platforms. This is an advanced deployment option that you should only use if you have custom VM requirements. \n\n\nIn general, the steps include meeting the prerequisites, launching Cloudbreak on a VM, and creating the Cloudbreak credential. After performing these steps, you can create a cluster based on one of the default blueprints or upload your own blueprint and then create a cluster. \n\n\n\n    \nNote\n\n    \nThe Cloudbreak software runs in your cloud environment. You are responsible for cloud infrastructure related charges while running Cloudbreak and the clusters being managed by Cloudbreak.", 
            "title": "Introduction"
        }, 
        {
            "location": "/index.html#introduction", 
            "text": "Welcome to the  Cloudbreak 2.1 Technical Preview  documentation!  Cloudbreak simplifies the provisioning, management, and monitoring of on-demand HDP clusters in virtual and cloud environments. It leverages cloud infrastructure to create host instances, and uses Apache Ambari via Ambari blueprints to provision and manage HDP clusters.   Cloudbreak allows you to create clusters using the Cloudbreak web UI, Cloudbreak Shell, and Cloudbreak REST API. Clusters can be launched on public cloud infrastructure platforms  Microsoft Azure ,  Amazon Web Services (AWS) , and  Google Cloud Platform (GCP) , and on the private cloud infrastructure platform  OpenStack .", 
            "title": "Introduction"
        }, 
        {
            "location": "/index.html#use-cases", 
            "text": "Cloudbreak allows you to create, manage, and monitor your clusters on your chosen cloud platform:   Quickly create a cluster using one of the default cluster blueprints and infrastructure settings.    Create a cluster based on the requirements of your workloads and provision infrastructure based on your IT requirements.  Secure your cluster by enabling Kerberos.  Automate cluster creation using the Cloudbreak CLI.   Develop your application using Cloudbreak API.", 
            "title": "Use Cases"
        }, 
        {
            "location": "/index.html#architecture", 
            "text": "Refer to  Architecture .", 
            "title": "Architecture"
        }, 
        {
            "location": "/index.html#get-started", 
            "text": "To get started with Cloudbreak:   Select the  cloud platform  on which you would like to launch Cloudbreak.     Select the  deployment option  that you would like to use.   Launch Cloudbreak .", 
            "title": "Get Started"
        }, 
        {
            "location": "/index.html#select-cloud-platform", 
            "text": "You can deploy and use Cloudbreak on the following cloud platforms:   Amazon Web Services (AWS)  Microsoft Azure  Google Cloud Platform (GCP)  OpenStack", 
            "title": "Select Cloud Platform"
        }, 
        {
            "location": "/index.html#select-deployment-option", 
            "text": "There are two basic deployment options:     Deployment option  When to use      Instantiate one of the pre-built cloud images  This is the recommended basic deployment option.  The cloud images include Cloudbreak deployer pre-installed on a CentOS VM.    Install the Cloudbreak deployer on your own VM  This is an advanced deployment option.   Select this option if you have custom VM requirements. The supported operating systems are RHEL, CentOS, and Oracle Linux 7 (64-bit).", 
            "title": "Select Deployment Option"
        }, 
        {
            "location": "/index.html#launch-cloudbreak", 
            "text": "(Option 1) You can launch Cloudbreak from one of the pre-built images:     Launch on AWS     Launch on Azure     Launch on GCP      Launch on OpenStack        (Option 2) Or you can launch Cloudbreak  on your own VM  on one of these cloud platforms. This is an advanced deployment option that you should only use if you have custom VM requirements.   In general, the steps include meeting the prerequisites, launching Cloudbreak on a VM, and creating the Cloudbreak credential. After performing these steps, you can create a cluster based on one of the default blueprints or upload your own blueprint and then create a cluster.   \n     Note \n     The Cloudbreak software runs in your cloud environment. You are responsible for cloud infrastructure related charges while running Cloudbreak and the clusters being managed by Cloudbreak.", 
            "title": "Launch Cloudbreak"
        }, 
        {
            "location": "/architecture/index.html", 
            "text": "Architecture\n\n\nCloudbreak deployer\n installs Cloudbreak components on your AWS VM. Once these components are deployed, you can use \nCloudbreak application\n to create, manage, and monitor clusters. \n\n\nCloudbreak Deployer Architecture\n\n\nCloudbreak deployer includes the following components:\n\n\n\n\n\n\n\n\nComponent\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nCloudbreak Application\n\n\nCloudbreak application is built on the foundation of cloud provider APIs and Apache Ambari.\n\n\n\n\n\n\nUluwatu\n\n\nThis is Cloudbreak web UI, which can be used to create, manage, and monitor clusters.\n\n\n\n\n\n\nCloudbreak Shell\n\n\nThis is Cloudbreak's command line tool, which can be used to create, manage, and monitor clusters.\n\n\n\n\n\n\nIdentity\n\n\nThis is Cloudbreak's OAuth identity server implementation, which utilizes UAA.\n\n\n\n\n\n\nSultans\n\n\nThis is Cloudbreak's user management system.\n\n\n\n\n\n\nPeriscope\n\n\nThis is Cloudbreak's autoscaling application, which is responsible for automatically increasing or decreasing the capacity of the cluster when your pre-defined conditions are met.\n\n\n\n\n\n\n\n\n\n\nThese component names are used in Cloudbreak logs, so for troubleshooting purposes it is useful to know what they refer to.\n\n\n\n\nCloudbreak Application Architecture\n\n\nThe Cloudbreak application is a web application that communicates with the cloud provider account to create cloud resources on your behalf. Once the cloud resources are in place, Cloudbreak uses Apache Ambari to deploy and configure the cluster on cloud VMs. Once your cluster is deployed, you can use Cloudbreak to scale the cluster.\n\n\nCloudbreak application is built on the foundation of cloud provider APIs and Apache Ambari:\n\n\n\n\n\n\nCloudbreak uses \nApache Ambari\n to provision, manage, and monitor HDP clusters. \n\n\nAmbari \nblueprints\n are a declarative definition of a cluster. With a blueprint, you can specify stack, component layout, and configurations to materialize an HDP cluster instance via Ambari REST API, without having to use the Ambari cluster install wizard. \n\n\n\n\n\n\nCloudbreak uses \ncloud provider APIs\n to create cloud resources required for the HDP clusters. \n\n\nYou can define these resources (networks, security groups, VMs and storage, and so on) in the create a cluster wizard in the Cloudbreak web UI. Resources are only provisioned once you create the cluster.  \n\n\n\n\n\n\nThe use of blueprints is illustrated in the following image:\n\n\n \n\n\nAmbari Blueprints\n\n\nAmbari blueprints\n are a declarative definition of a cluster. With a blueprint, you can specify stack, component layout, and configurations to materialize an HDP cluster instance via Ambari REST API, without having to use the Ambari cluster install wizard. \n\n\nAmbari blueprints are specified in the JSON format. After you provide the blueprint to Cloudbreak, the host groups in the JSON will be mapped to a set of instances when starting the cluster, and the specified services and components will be installed on the corresponding nodes.\n\n\nCloudbreak includes a few default blueprints and allows you to upload your own blueprints.\n\n\nTo learn more about Ambari blueprints, refer to \nApache documentation\n.\n\n\nCloudbreak Credential\n\n\nCloudbreak credential\n allows Cloudbreak to authenticate with the cloud provider and create resources on your behalf. This is typically done via assigning a specific IAM role to Cloudbreak which allows Cloudbreak to perform certain actions within your cloud provider account.\n\n\nAfter launching Cloudbreak, you must create a Cloudbreak credntial and only after you complete that step you can start creating clusters.", 
            "title": "Architecture"
        }, 
        {
            "location": "/architecture/index.html#architecture", 
            "text": "Cloudbreak deployer  installs Cloudbreak components on your AWS VM. Once these components are deployed, you can use  Cloudbreak application  to create, manage, and monitor clusters.", 
            "title": "Architecture"
        }, 
        {
            "location": "/architecture/index.html#cloudbreak-deployer-architecture", 
            "text": "Cloudbreak deployer includes the following components:     Component  Description      Cloudbreak Application  Cloudbreak application is built on the foundation of cloud provider APIs and Apache Ambari.    Uluwatu  This is Cloudbreak web UI, which can be used to create, manage, and monitor clusters.    Cloudbreak Shell  This is Cloudbreak's command line tool, which can be used to create, manage, and monitor clusters.    Identity  This is Cloudbreak's OAuth identity server implementation, which utilizes UAA.    Sultans  This is Cloudbreak's user management system.    Periscope  This is Cloudbreak's autoscaling application, which is responsible for automatically increasing or decreasing the capacity of the cluster when your pre-defined conditions are met.      These component names are used in Cloudbreak logs, so for troubleshooting purposes it is useful to know what they refer to.", 
            "title": "Cloudbreak Deployer Architecture"
        }, 
        {
            "location": "/architecture/index.html#cloudbreak-application-architecture", 
            "text": "The Cloudbreak application is a web application that communicates with the cloud provider account to create cloud resources on your behalf. Once the cloud resources are in place, Cloudbreak uses Apache Ambari to deploy and configure the cluster on cloud VMs. Once your cluster is deployed, you can use Cloudbreak to scale the cluster.  Cloudbreak application is built on the foundation of cloud provider APIs and Apache Ambari:    Cloudbreak uses  Apache Ambari  to provision, manage, and monitor HDP clusters.   Ambari  blueprints  are a declarative definition of a cluster. With a blueprint, you can specify stack, component layout, and configurations to materialize an HDP cluster instance via Ambari REST API, without having to use the Ambari cluster install wizard.     Cloudbreak uses  cloud provider APIs  to create cloud resources required for the HDP clusters.   You can define these resources (networks, security groups, VMs and storage, and so on) in the create a cluster wizard in the Cloudbreak web UI. Resources are only provisioned once you create the cluster.      The use of blueprints is illustrated in the following image:", 
            "title": "Cloudbreak Application Architecture"
        }, 
        {
            "location": "/architecture/index.html#ambari-blueprints", 
            "text": "Ambari blueprints  are a declarative definition of a cluster. With a blueprint, you can specify stack, component layout, and configurations to materialize an HDP cluster instance via Ambari REST API, without having to use the Ambari cluster install wizard.   Ambari blueprints are specified in the JSON format. After you provide the blueprint to Cloudbreak, the host groups in the JSON will be mapped to a set of instances when starting the cluster, and the specified services and components will be installed on the corresponding nodes.  Cloudbreak includes a few default blueprints and allows you to upload your own blueprints.  To learn more about Ambari blueprints, refer to  Apache documentation .", 
            "title": "Ambari Blueprints"
        }, 
        {
            "location": "/architecture/index.html#cloudbreak-credential", 
            "text": "Cloudbreak credential  allows Cloudbreak to authenticate with the cloud provider and create resources on your behalf. This is typically done via assigning a specific IAM role to Cloudbreak which allows Cloudbreak to perform certain actions within your cloud provider account.  After launching Cloudbreak, you must create a Cloudbreak credntial and only after you complete that step you can start creating clusters.", 
            "title": "Cloudbreak Credential"
        }, 
        {
            "location": "/releasenotes/index.html", 
            "text": "Release Notes\n\n\nSystem Info\n\n\nCloudbreak 2.1.0\n\n\nNew Features\n\n\nFirst release\n\n\nChange Log\n\n\nFirst release\n\n\nKnown Issues\n\n\nComing soon!", 
            "title": "Release Notes"
        }, 
        {
            "location": "/releasenotes/index.html#release-notes", 
            "text": "", 
            "title": "Release Notes"
        }, 
        {
            "location": "/releasenotes/index.html#system-info", 
            "text": "Cloudbreak 2.1.0", 
            "title": "System Info"
        }, 
        {
            "location": "/releasenotes/index.html#new-features", 
            "text": "First release", 
            "title": "New Features"
        }, 
        {
            "location": "/releasenotes/index.html#change-log", 
            "text": "First release", 
            "title": "Change Log"
        }, 
        {
            "location": "/releasenotes/index.html#known-issues", 
            "text": "Coming soon!", 
            "title": "Known Issues"
        }, 
        {
            "location": "/aws-launch/index.html", 
            "text": "Launching Cloudbreak on AWS\n\n\nBefore launching Cloudbreak on AWS, review and meet the prerequisites. Next, launch a VM using a Cloudbreak Amazon Machine Image, access the VM, and then start Cloudbreak. Once Cloudbreak is running, log in to the Cloudbreak UI and create a Cloudbreak credential. \n\n\nMeet the Prerequisites\n\n\nBefore launching Cloudbreak on AWS, you must meet the following prerequisites.\n\n\nAWS Account\n\n\nIn order to launch Cloudbreak on Azure, you must log in to your AWS account. If you don't have an account, you can create one at \nhttps://aws.amazon.com/\n.\n\n\nAWS Region\n\n\nDecide in which AWS region you would like to launch Cloudbreak. The following AWS regions are supported: \n\n\n\n\n\n\n\n\nRegion Name\n\n\nRegion\n\n\n\n\n\n\n\n\n\n\nEU (Ireland)\n\n\neu-west-1\n\n\n\n\n\n\nEU (Frankfurt)\n\n\neu-central-1\n\n\n\n\n\n\nUS East (N. Virginia)\n\n\nus-east-1\n\n\n\n\n\n\nUS West (N. California)\n\n\nus-west-1\n\n\n\n\n\n\nUS West (Oregon)\n\n\nus-west-2\n\n\n\n\n\n\nSouth America (S\u00e3o Paulo)\n\n\nsa-east-1\n\n\n\n\n\n\nAsia Pacific (Tokyo)\n\n\nap-northeast-1\n\n\n\n\n\n\nAsia Pacific (Singapore)\n\n\nap-southeast-1\n\n\n\n\n\n\nAsia Pacific (Sydney)\n\n\nap-southeast-2\n\n\n\n\n\n\n\n\nClusters created via Cloudbreak can be in the same or different region as Cloudbreak; when you launch a cluster, you select the region in which to launch it.\n\n\nFor detailed information about AWS regions, refer to \nAWS documentation\n. \n\n\nSSH Key Pair\n\n\nImport an existing key pair or generate a new key pair in the AWS region which you are planning to use for launching Cloudbreak and clusters. You can do this using the following steps.\n\n\nSteps\n \n\n\n\n\nNavigate to the Amazon EC2 console at https://console.aws.amazon.com/ec2/.  \n\n\nCheck the region listed in the top right corner to make sure that you are in the correct region.  \n\n\nIn the left pane, find \nNETWORK AND SECURITY\n and click \nKey Pairs\n.   \n\n\nDo one of the following:\n\n\nClick \nCreate Key Pair\n to create a new key pair. Your private key file will be automatically downloaded onto your computer. Make sure to save it in a secure location. You will need it to SSH to the cluster nodes. You may want to change access settings for the file using \nchmod 400 my-key-pair.pem\n.  \n\n\nClick \nImport Key Pair\n to upload an existing public key and then select it and click \nImport\n. Make sure that you have access to its corresponding private key.    \n\n\n\n\n\n\n\n\nFor more information, refer to \nAWS documentation\n.\n\n\nYou will need this SSH key pair to SSH to the Cloudbreak instance and start Cloudbreak. \n\n\nAuthentication\n\n\nBefore you can start using Cloudbreak for provisioning clusters, you must select a way for Cloudbreak to authenticate with your AWS account and create resources on your behalf. There are two ways to do this: \n\n\n\n\n\n\nKey-based\n: This is a simpler option which does not require additional configuration at this point. It requires that you provide your AWS access key and secret key pair in the Cloudbreak web UI later. All you need to do now is check your AWS account and ensure that you can access this key pair.\n\n\n\n\n\n\nRole-based\n: This requires that you or your AWS admin create an IAM role to allow Cloudbreak to assume AWS roles (the \"AssumeRole\" policy).\n\n\n\n\n\n\nOption 1: Key-based Authentication\n\n\nThis option requires your AWS access key and secret key pair. Cloudbreak will use these keys to launch the resources. You must provide the access and secret keys later in the Cloudbreak web UI later when creating a credential. \n\n\nIf you choose this option, all you need to do at this point is check your AWS account and make sure that you can access this key pair. You can generate new access and secret keys from the \nIAM Console\n \n \nUsers\n. Next, select a user and click on the \nSecurity credentials\n tab:\n\n\n \n\n\nIf you choose this option, you can proceed to \nLaunch the VM\n.\n\n\nOption 2: Role-based Authentication\n\n\nThis requires that you create two IAM roles: one to grant Cloudbreak access to allow Cloudbreak to assume AWS roles (using the \"AssumeRole\" policy) and the second one to provide Cloudbreak with the capabilities required for cluster creation (using the \"cb-policy\" policy).\n\n\nThe following table provides contextual information about the two roles required: \n\n\n\n\n\n\n\n\nRole\n\n\nPurpose\n\n\nOverview of Steps\n\n\nConfiguration\n\n\n\n\n\n\n\n\n\n\nCloudbreakRole\n\n\nAllows Cloudbreak to assume other IAM roles - specifically the CredentialRole.\n\n\nCreate a role called \"CloudbreakRole\" and attach the \"AssumeRole\" policy. The \"AssumeRole\" policy definition is provided below.\nAlternatively, you can generate the \"CredentialRole\" role later once your VM is running and once you have \nconfigured AWS CLI\n on your VM by running the \ncbd aws generate-role\n command. This command creates a role with the name \"cbreak-deployer\" (equivalent to the \"CredentialRole\"). To customize the name of the role, add \nexport AWS_ROLE_NAME=my-cloudbreak-role-name\n (where \"my-cloudbreak-role-name\" is your custom role name) as a new line to your Profile.\n\n\nWhen launching your Cloudbreak VM, during \nStep 3: Configure Instance Details\n \n \nIAM\n, you will attach the \"CloudbreakRole\" IAM role to the VM.\n\n\n\n\n\n\nCredentialRole\n\n\nAllows Cloudbreak to create AWS resources required for clusters.\n\n\nCreate a new IAM role called \"CredentialRole\" and attach the \"cb-policy\" policy to it. The \"cb-policy\" policy definition is provided below.\n When creating this role using the AWS Console, make sure that that it is a role for cross-account access and that the trust-relation is set up as follows: 'Account ID' is your own 12-digit AWS account ID and 'External ID' is \u201cprovision-ambari\u201d.\n\n\nOnce you log in to the Cloudbreak UI and are ready to create clusters, you will use this role to create the Cloudbreak Credential.\n\n\n\n\n\n\n\n\nYou can create these roles in the \nIAM console\n, on the \nRoles\n page via the \nCreate Role\n option. Detailed steps are provided below. \n\n\nFor more information about IAM, refer to \nUsing Instance Profiles\n and \nUsing an IAM Role to Grant Permissions to Applications Running on Amazon EC2 Instances\n. \n\n\nCreate CloudbreakRole\n\n\nUse these steps to create CloudbreakRole.\n\n\nSteps\n\n\n\n\n\n\nNavigate to the \nIAM console\n \n \nRoles\n and click \nCreate Role\n.\n\n\n \n\n\n\n\n\n\nIn the \"Create Role\" wizard, select \nAWS service\n role type and then select any service. \n\n\n \n\n\n\n\n\n\nWhen done, click \nNext: Permissions\n to navigate to the next page in the wizard.\n\n\n\n\n\n\nClick \nCreate policy\n.\n\n\n\n\n\n\n\n\nClick \nSelect\n next to \"Create Your Own Policy\".\n\n\n  \n\n\n\n\n\n\nIn the \nPolicy Name\n field, enter \"AssumeRole\" and in the \nPolicy Document\n paste the policy declaration as provided in the previous step.\n\n\n  \n\n\n\n\n\n\nWhen done, click \nCreate Policy\n.\n\n\n\n\n\n\nClick \nRefresh\n. Next, find the \"AsumeRole\" policy that you just created and select it by checking the box.\n\n\n \n\n\n\n\n\n\nWhen done, click \nNext: Review\n.\n\n\n\n\n\n\nIn the \nRoles name\n field, enter \"CloudbreakRole\". \n\n\n \n\n\n\n\n\n\nWhen done, click \nCreate role\n to finish the role creation process.\n\n\n\n\n\n\nCreate CredentialRole\n\n\nUse these steps to create CredentialRole.\n\n\nSteps\n\n\n\n\n\n\nNavigate to the \nIAM console\n \n \nRoles\n and click \nCreate Role\n.\n\n\n \n\n\n\n\n\n\nIn the \"Create Role\" wizard, select \nAnother AWS account\n role type. Next, provide the following:\n\n\n\n\nIn the \nAccount ID\n field, enter your AWS account ID.\n\n\nUnder \nOptions\n, check \nRequire external ID\n.\n\n\nIn the \nExternal ID\n, enter \"provision-ambari\".\n\n\n\n\n \n\n\n\n\n\n\nWhen done, click \nNext: Permissions\n to navigate to the next page in the wizard.\n\n\n\n\n\n\nClick \nCreate policy\n.\n\n\n\n\n\n\n\n\nClick \nSelect\n next to \"Create Your Own Policy\".\n\n\n \n\n\n\n\n\n\nCopy the \"cb-policy\" policy definition (see below).   \n\n\n\n\n\n\nIn the \nPolicy Name\n field, enter \"cb-policy\" and in the \nPolicy Document\n paste the policy declaration as provided below.\n\n\n  \n\n\n\n\n\n\nWhen done, click \nCreate Policy\n.\n\n\n\n\n\n\nClick \nRefresh\n. Next, find the \"cb-policy\" that you just created and select it by checking the box.\n\n\n \n\n\n\n\n\n\nWhen done, click \nNext: Review\n.\n\n\n\n\n\n\nIn the \nRoles name\n field, enter \"CredentialRole\". \n\n\n \n\n\n\n\n\n\nWhen done, click \nCreate role\n to finish the role creation process.\n\n\n\n\n\n\nPolicy Definitions\n\n\nThe \"AssumeRole\" policy definition: \n\n\n{\n\u2002\u2002\"Version\": \"2012-10-17\",\n\u2002\u2002\"Statement\": [\n\u2002\u2002\u2002\u2002{\n\u2002\u2002\u2002\u2002\u2002\u2002\"Sid\": \"Stmt1400068149000\",\n\u2002\u2002\u2002\u2002\u2002\u2002\"Effect\": \"Allow\",\n\u2002\u2002\u2002\u2002\u2002\u2002\"Action\": [\n\u2002\u2002\u2002\u2002\u2002\u2002\u2002\u2002\"sts:AssumeRole\"\n\u2002\u2002\u2002\u2002\u2002\u2002],\n\u2002\u2002\u2002\u2002\u2002\u2002\"Resource\": [\n\u2002\u2002\u2002\u2002\u2002\u2002\u2002\u2002\"*\"\n\u2002\u2002\u2002\u2002\u2002\u2002]\n\u2002\u2002\u2002\u2002}\n\u2002\u2002]\n}\n\n\n\nThe \"cb-policy\" policy definition: \n\n\n{\n  \"Version\": \"2012-10-17\",\n  \"Statement\": [\n    {\n      \"Effect\": \"Allow\",\n      \"Action\": [ \"cloudformation:*\" ],\n      \"Resource\": [ \"*\" ]\n    },\n    {\n      \"Effect\": \"Allow\",\n      \"Action\": [ \"ec2:*\" ],\n      \"Resource\": [ \"*\" ]\n    },\n    {\n      \"Effect\": \"Allow\",\n      \"Action\": [ \"iam:PassRole\" ],\n      \"Resource\": [ \"*\" ]\n    },\n    {\n      \"Effect\": \"Allow\",\n      \"Action\": [ \"autoscaling:*\" ],\n      \"Resource\": [ \"*\" ]\n    }\n  ]\n}\n\n\n\nOnce you are done, you can proceed to \nLaunch the VM\n.   \n\n\nLaunch the VM\n\n\nNow that you've met the prerequisites, you can launch the Cloudbreak deployer VM available as a Community AMI.\n\n\nSteps\n\n\n\n\n\n\nIn the AWS Management Console, navigate to the EC2 Console.  \n\n\n\n\n\n\nIn the top right corner, select the region in which you want to launch Cloudbreak.  \n\n\n \n\n\n\n\n\n\nFrom the left pane, select \nINSTANCES\n \n \nInstances\n.  \n\n\n\n\n\n\nClick on \nLaunch Instance\n.\n\n\n\n\n\n\nIn \nStep 1: Choose an Amazon Machine Image (AMI)\n page, from the left pane, select \nCommunity AMIs\n. \n\n\n \n\n\n\n\n\n\nIn the search box, enter the image name. The following Cloudbreak deployer images are available:\n\n\n\n\n\n\n\n\nRegion\n\n\nImage Name\n\n\n\n\n\n\n\n\n\n\neu-west-1\n\n\nami-79c63f00\n\n\n\n\n\n\nsa-east-1\n\n\nami-1af18176\n\n\n\n\n\n\nus-east-1\n\n\nami-e9bdbc92\n\n\n\n\n\n\nus-west-1\n\n\nami-2dd1e44d\n\n\n\n\n\n\nus-west-2\n\n\nami-ae6c85d6\n\n\n\n\n\n\neu-central-1\n\n\nami-bc18b3d3\n\n\n\n\n\n\nap-northeast-1\n\n\nami-f2fb0494\n\n\n\n\n\n\nap-southeast-1\n\n\nami-47036324\n\n\n\n\n\n\nap-southeast-2\n\n\nami-370b1154\n\n\n\n\n\n\n\n\n\n\n\n\nClick \nSelect\n.  \n\n\n\n\nThe steps listed below only mention required parameters. You may optionally review and adjust additional parameters. \n\n\n\n\n\n\n\n\nIn \nStep2: Choose Instance Type\n, choose an instance type. The minimum instance type which is suitable for Cloudbreak is \nm3.large\n. Minimum requirements are 8GB RAM, 10GB disk, 2 cores. Click \nNext\n.\n\n\n   \n\n\n\n\n\n\n(Perform this step only if you are using role-based authorization) In \nStep 3: Configure Instance Details\n \n \nIAM\n, select the \"CloudbreakRole\" IAM role which you \ncreated earlier\n.\n\n\n\n\n\n\nIn \nStep 6: Configure Security Group\n, open the following ports: 22 (for access via SSH) and 443 (for access via HTTPS). Click \nReview and Launch\n.\n\n\n \n\n\n\n\n\n\nIn \nStep 7: Review Instance Launch\n, review the information carefully and then click \nLaunch\n. \n\n\n\n\n\n\nWhen prompted select an existing key pair or create a new one. Next, acknowledge that you have access to the private key file and click \nLaunch Instance\n. \n\n\n  \n\n\n\n\n\n\nClick on the instance ID to navigate to the \nInstances\n view in your EC2 console. \n\n\n  \n\n\n\n\n\n\nSSH to the VM\n\n\nNow that your VM is ready, access it via SSH: \n\n\n\n\nUse the private key from the key pair that you selected when launching the instance. \n\n\nThe SSH user is called \"cloudbreak\".\n\n\nYou can obtain the host IP from the EC2 console \n \nInstances\n view by selecting the instance, selecting the \nDescription\n tab, and copying the value of the \nPublic DNS (IPv4)\n\n or \nIPv4 Public IP\n parameter.\n\n\n\n\nOn Mac OS, you can SSH to the VM by running the following from the Terminal app: \nssh -i \"your-private-key.pem\" cloudnreak@instance_IP\n where \"your-private-key.pem\" points to the location of your private key and \"instance_IP\" is the public IP address of the VM.\n\n\nOn Windows, you can use \nPuTTy\n.\n\n\nLaunch Cloudbreak Deployer\n\n\nAfter accessing the VM via SSH, launch Cloudbreak deployer using the following steps.\n\n\nSteps\n \n\n\n\n\n\n\nNavigate to the cloudbreak-deployment directory:\n\n\ncd /var/lib/cloudbreak-deployment/\n\n\nThis directory contains configuration files and the supporting binaries for Cloudbreak deployer.\n\n\n\n\n\n\nInitialize your profile by creating a new file called \nProfile\n and adding the following content:\n\n\nexport UAA_DEFAULT_SECRET=MY-SECRET\nexport UAA_DEFAULT_USER_PW=MY-PASSWORD\n  \n\n\nFor example: \n\n\nexport UAA_DEFAULT_SECRET=MySecret123\nexport UAA_DEFAULT_USER_PW=MySecurePassword123\n \n\n\n\n\nYou will need to provide the password when logging in to the Cloudbreak web UI and when using the Cloudbreak Shell. The secret will be used by Cloudbreak for authentication.  \n\n\n\n\n\n\n\n\nStart the Cloudbreak application by using the following command:\n\n\ncbd start\n\n\nThis will start the Docker containers and initialize the application. The first time you start the Coudbreak app, the process will take longer than usual due to the download of all the necessary docker images.\n\n\nThe \ncbd start\n command includes the \ncbd generate\n command which applies the following steps:\n\n\n\n\nCreates the \ndocker-compose.yml\n file, which describes the configuration of all the Docker containers needed for the Cloudbreak deployment.\n\n\nCreates the \nuaa.yml\n file, which holds the configuration of the identity server used to authenticate users with Cloudbreak.\n\n\n\n\n\n\nOnce the \ncbd start\n has finished, it returns the \"Uluwatu (Cloudbreak UI) url\" which you can later paste in your browser and log in to Cloudbreak web UI. \n\n\n\n\n\n\n\n\nCheck Cloudbreak deployer version and health: \n\n\ncbd doctor\n\n\n\n\n\n\nNext, check Cloudbreak Application logs: \n\n\ncbd logs cloudbreak\n\n\nYou should see a message like this in the log: \nStarted CloudbreakApplication in 36.823 seconds.\n Cloudbreak normally takes less than a minute to start.\n\n\n\n\n\n\nAccess Cloudbreak UI\n\n\nLog in to the Cloudbreak UI using the following steps.\n\n\nSteps\n\n\n\n\n\n\nYou can log into the Cloudbreak application at \nhttps://IPv4_Public_IP\n/\n or \nhttps://Public_DNS\n. For example \nhttps://34.212.141.253\n or \nhttps://ec2-34-212-141-253.us-west-2.compute.amazonaws.com\n. \n\n\n\n\n\n\nConfirm the security exception to proceed to the Cloudbreak web UI.\n\n\nThe first time you access Cloudbreak UI, Cloudbreak will automatically generate a self-signed certificate, due to which your browser will warn you about an untrusted connection and will ask you to confirm a security exception.\n\n\n\n\n\n\nThe login page is displayed:\n\n\n  \n\n\n\n\n\n\nLog in to the Cloudbreak web UI: \n\n\n\n\nThe default username is \nadmin@example.com\n but you should sign up with your own email address.    \n\n\nThe password is the value of the \nUAA_DEFAULT_USER_PW\n variable that you configured in your \nProfile\n file when \nlaunching Cloudbreak deployer\n.\n\n\n\n\n\n\n\n\nUpon a successful login, you are redirected to the Cluster dashboard:\n\n\n  \n\n\n\n\n\n\nCreate Cloudbreak Credential\n\n\nBefore you can start creating clusters, you must first create a \nCloudbreak credential\n. Without this credential, you will not be able to create clusters via Cloudbreak. \n\n\nAs part of the \nprerequisites\n, you had two options to allow Cloudbreak to authenticate with AWS and create resources on your behalf: key-based or role-based authentication. \n\n\nDepending on your choice, you must configure a key-based or role-based credential: \n\n\nCreate Key-Based Credential\n\n\nCreate Role-Based Credential\n\n\nCreate Key-Based Credential\n\n\nTo perform these steps, you must know your access and secret key. If needed, you or your AWS administrator can generate new access and secret keys from the \nIAM Console\n \n \nUsers\n \n select a user \n \nSecurity credentials\n. \n\n\nSteps\n\n\n\n\n\n\nIn the Cloudbreak web UI, select \nCredentials\n from the left pane. \n\n\n\n\n\n\nClick \nCreate Credential\n. \n\n\n\n\n\n\nUnder \nCloud provider\n, select \"Amazon Web Services\":\n\n\n  \n\n\n\n\n\n\nProvide the following information:\n\n\n\n\n\n\n\n\nParameter\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nSelect Credential Type\n\n\nSelect \nKey Based\n.\n\n\n\n\n\n\nName\n\n\nEnter a name for your credential.\n\n\n\n\n\n\nDescription\n\n\n(Optional) Enter a description.\n\n\n\n\n\n\nAccess Key\n\n\nPaste your access key.\n\n\n\n\n\n\nSecret Access Key\n\n\nPaste your secret key.\n\n\n\n\n\n\n\n\n\n\n\n\nClick \nCreate\n.\n\n\n\n\n\n\nYour credential should now be displayed in the \nCredentials\n pane.\n\n\nCongratulations! You've successfully launched Cloudbreak and create a Cloudbreak credential. Now it's time to \ncreate a cluster\n. \n\n\n\n\n\n\nCreate Role-Based Credential\n\n\nTo perform these steps, you must know the \nIAM Role ARN\n corresponding to the \"CredentialRole\" (configured as a \nprerequisite\n).  \n\n\nSteps\n\n\n\n\n\n\nIn the Cloudbreak web UI, select \nCredentials\n from the left pane. \n\n\n\n\n\n\nClick \nCreate Credential\n. \n\n\n\n\n\n\nUnder \nCloud provider\n, select \"Amazon Web Services\":\n\n\n  \n\n\n\n\n\n\nProvide the following information:\n\n\n\n\n\n\n\n\nParameter\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nSelect Credential Type\n\n\nSelect \nRole Based\n (default value).\n\n\n\n\n\n\nName\n\n\nEnter a name for your credential.\n\n\n\n\n\n\nDescription\n\n\n(Optional) Enter a description.\n\n\n\n\n\n\nIAM Role ARN\n\n\nPaste the IAM Role ARN corresponding to the \"CredentialRole\" that you created earlier. For example \narn:aws:iam::315627065446:role/CredentialRole\n is a valid IAM Role ARN.\n\n\n\n\n\n\n\n\n\n\n\n\nClick \nCreate\n.\n\n\n\n\n\n\nYour credential should now be displayed in the \nCredentials\n pane.\n\n\nCongratulations! You've successfully launched Cloudbreak and create a Cloudbreak credential. Now it's time to \ncreate a cluster\n. \n\n\n\n\n\n\n\n\nNext: Create a Cluster", 
            "title": "Launch on AWS"
        }, 
        {
            "location": "/aws-launch/index.html#launching-cloudbreak-on-aws", 
            "text": "Before launching Cloudbreak on AWS, review and meet the prerequisites. Next, launch a VM using a Cloudbreak Amazon Machine Image, access the VM, and then start Cloudbreak. Once Cloudbreak is running, log in to the Cloudbreak UI and create a Cloudbreak credential.", 
            "title": "Launching Cloudbreak on AWS"
        }, 
        {
            "location": "/aws-launch/index.html#meet-the-prerequisites", 
            "text": "Before launching Cloudbreak on AWS, you must meet the following prerequisites.", 
            "title": "Meet the Prerequisites"
        }, 
        {
            "location": "/aws-launch/index.html#aws-account", 
            "text": "In order to launch Cloudbreak on Azure, you must log in to your AWS account. If you don't have an account, you can create one at  https://aws.amazon.com/ .", 
            "title": "AWS Account"
        }, 
        {
            "location": "/aws-launch/index.html#aws-region", 
            "text": "Decide in which AWS region you would like to launch Cloudbreak. The following AWS regions are supported:      Region Name  Region      EU (Ireland)  eu-west-1    EU (Frankfurt)  eu-central-1    US East (N. Virginia)  us-east-1    US West (N. California)  us-west-1    US West (Oregon)  us-west-2    South America (S\u00e3o Paulo)  sa-east-1    Asia Pacific (Tokyo)  ap-northeast-1    Asia Pacific (Singapore)  ap-southeast-1    Asia Pacific (Sydney)  ap-southeast-2     Clusters created via Cloudbreak can be in the same or different region as Cloudbreak; when you launch a cluster, you select the region in which to launch it.  For detailed information about AWS regions, refer to  AWS documentation .", 
            "title": "AWS Region"
        }, 
        {
            "location": "/aws-launch/index.html#ssh-key-pair", 
            "text": "Import an existing key pair or generate a new key pair in the AWS region which you are planning to use for launching Cloudbreak and clusters. You can do this using the following steps.  Steps     Navigate to the Amazon EC2 console at https://console.aws.amazon.com/ec2/.    Check the region listed in the top right corner to make sure that you are in the correct region.    In the left pane, find  NETWORK AND SECURITY  and click  Key Pairs .     Do one of the following:  Click  Create Key Pair  to create a new key pair. Your private key file will be automatically downloaded onto your computer. Make sure to save it in a secure location. You will need it to SSH to the cluster nodes. You may want to change access settings for the file using  chmod 400 my-key-pair.pem .    Click  Import Key Pair  to upload an existing public key and then select it and click  Import . Make sure that you have access to its corresponding private key.         For more information, refer to  AWS documentation .  You will need this SSH key pair to SSH to the Cloudbreak instance and start Cloudbreak.", 
            "title": "SSH Key Pair"
        }, 
        {
            "location": "/aws-launch/index.html#authentication", 
            "text": "Before you can start using Cloudbreak for provisioning clusters, you must select a way for Cloudbreak to authenticate with your AWS account and create resources on your behalf. There are two ways to do this:     Key-based : This is a simpler option which does not require additional configuration at this point. It requires that you provide your AWS access key and secret key pair in the Cloudbreak web UI later. All you need to do now is check your AWS account and ensure that you can access this key pair.    Role-based : This requires that you or your AWS admin create an IAM role to allow Cloudbreak to assume AWS roles (the \"AssumeRole\" policy).", 
            "title": "Authentication"
        }, 
        {
            "location": "/aws-launch/index.html#option-1-key-based-authentication", 
            "text": "This option requires your AWS access key and secret key pair. Cloudbreak will use these keys to launch the resources. You must provide the access and secret keys later in the Cloudbreak web UI later when creating a credential.   If you choose this option, all you need to do at this point is check your AWS account and make sure that you can access this key pair. You can generate new access and secret keys from the  IAM Console     Users . Next, select a user and click on the  Security credentials  tab:     If you choose this option, you can proceed to  Launch the VM .", 
            "title": "Option 1: Key-based Authentication"
        }, 
        {
            "location": "/aws-launch/index.html#option-2-role-based-authentication", 
            "text": "This requires that you create two IAM roles: one to grant Cloudbreak access to allow Cloudbreak to assume AWS roles (using the \"AssumeRole\" policy) and the second one to provide Cloudbreak with the capabilities required for cluster creation (using the \"cb-policy\" policy).  The following table provides contextual information about the two roles required:      Role  Purpose  Overview of Steps  Configuration      CloudbreakRole  Allows Cloudbreak to assume other IAM roles - specifically the CredentialRole.  Create a role called \"CloudbreakRole\" and attach the \"AssumeRole\" policy. The \"AssumeRole\" policy definition is provided below. Alternatively, you can generate the \"CredentialRole\" role later once your VM is running and once you have  configured AWS CLI  on your VM by running the  cbd aws generate-role  command. This command creates a role with the name \"cbreak-deployer\" (equivalent to the \"CredentialRole\"). To customize the name of the role, add  export AWS_ROLE_NAME=my-cloudbreak-role-name  (where \"my-cloudbreak-role-name\" is your custom role name) as a new line to your Profile.  When launching your Cloudbreak VM, during  Step 3: Configure Instance Details     IAM , you will attach the \"CloudbreakRole\" IAM role to the VM.    CredentialRole  Allows Cloudbreak to create AWS resources required for clusters.  Create a new IAM role called \"CredentialRole\" and attach the \"cb-policy\" policy to it. The \"cb-policy\" policy definition is provided below.  When creating this role using the AWS Console, make sure that that it is a role for cross-account access and that the trust-relation is set up as follows: 'Account ID' is your own 12-digit AWS account ID and 'External ID' is \u201cprovision-ambari\u201d.  Once you log in to the Cloudbreak UI and are ready to create clusters, you will use this role to create the Cloudbreak Credential.     You can create these roles in the  IAM console , on the  Roles  page via the  Create Role  option. Detailed steps are provided below.   For more information about IAM, refer to  Using Instance Profiles  and  Using an IAM Role to Grant Permissions to Applications Running on Amazon EC2 Instances .   Create CloudbreakRole  Use these steps to create CloudbreakRole.  Steps    Navigate to the  IAM console     Roles  and click  Create Role .       In the \"Create Role\" wizard, select  AWS service  role type and then select any service.        When done, click  Next: Permissions  to navigate to the next page in the wizard.    Click  Create policy .     Click  Select  next to \"Create Your Own Policy\".        In the  Policy Name  field, enter \"AssumeRole\" and in the  Policy Document  paste the policy declaration as provided in the previous step.        When done, click  Create Policy .    Click  Refresh . Next, find the \"AsumeRole\" policy that you just created and select it by checking the box.       When done, click  Next: Review .    In the  Roles name  field, enter \"CloudbreakRole\".        When done, click  Create role  to finish the role creation process.    Create CredentialRole  Use these steps to create CredentialRole.  Steps    Navigate to the  IAM console     Roles  and click  Create Role .       In the \"Create Role\" wizard, select  Another AWS account  role type. Next, provide the following:   In the  Account ID  field, enter your AWS account ID.  Under  Options , check  Require external ID .  In the  External ID , enter \"provision-ambari\".        When done, click  Next: Permissions  to navigate to the next page in the wizard.    Click  Create policy .     Click  Select  next to \"Create Your Own Policy\".       Copy the \"cb-policy\" policy definition (see below).       In the  Policy Name  field, enter \"cb-policy\" and in the  Policy Document  paste the policy declaration as provided below.        When done, click  Create Policy .    Click  Refresh . Next, find the \"cb-policy\" that you just created and select it by checking the box.       When done, click  Next: Review .    In the  Roles name  field, enter \"CredentialRole\".        When done, click  Create role  to finish the role creation process.    Policy Definitions  The \"AssumeRole\" policy definition:   {\n\u2002\u2002\"Version\": \"2012-10-17\",\n\u2002\u2002\"Statement\": [\n\u2002\u2002\u2002\u2002{\n\u2002\u2002\u2002\u2002\u2002\u2002\"Sid\": \"Stmt1400068149000\",\n\u2002\u2002\u2002\u2002\u2002\u2002\"Effect\": \"Allow\",\n\u2002\u2002\u2002\u2002\u2002\u2002\"Action\": [\n\u2002\u2002\u2002\u2002\u2002\u2002\u2002\u2002\"sts:AssumeRole\"\n\u2002\u2002\u2002\u2002\u2002\u2002],\n\u2002\u2002\u2002\u2002\u2002\u2002\"Resource\": [\n\u2002\u2002\u2002\u2002\u2002\u2002\u2002\u2002\"*\"\n\u2002\u2002\u2002\u2002\u2002\u2002]\n\u2002\u2002\u2002\u2002}\n\u2002\u2002]\n}  The \"cb-policy\" policy definition:   {\n  \"Version\": \"2012-10-17\",\n  \"Statement\": [\n    {\n      \"Effect\": \"Allow\",\n      \"Action\": [ \"cloudformation:*\" ],\n      \"Resource\": [ \"*\" ]\n    },\n    {\n      \"Effect\": \"Allow\",\n      \"Action\": [ \"ec2:*\" ],\n      \"Resource\": [ \"*\" ]\n    },\n    {\n      \"Effect\": \"Allow\",\n      \"Action\": [ \"iam:PassRole\" ],\n      \"Resource\": [ \"*\" ]\n    },\n    {\n      \"Effect\": \"Allow\",\n      \"Action\": [ \"autoscaling:*\" ],\n      \"Resource\": [ \"*\" ]\n    }\n  ]\n}  Once you are done, you can proceed to  Launch the VM .", 
            "title": "Option 2: Role-based Authentication"
        }, 
        {
            "location": "/aws-launch/index.html#launch-the-vm", 
            "text": "Now that you've met the prerequisites, you can launch the Cloudbreak deployer VM available as a Community AMI.  Steps    In the AWS Management Console, navigate to the EC2 Console.      In the top right corner, select the region in which you want to launch Cloudbreak.         From the left pane, select  INSTANCES     Instances .      Click on  Launch Instance .    In  Step 1: Choose an Amazon Machine Image (AMI)  page, from the left pane, select  Community AMIs .        In the search box, enter the image name. The following Cloudbreak deployer images are available:     Region  Image Name      eu-west-1  ami-79c63f00    sa-east-1  ami-1af18176    us-east-1  ami-e9bdbc92    us-west-1  ami-2dd1e44d    us-west-2  ami-ae6c85d6    eu-central-1  ami-bc18b3d3    ap-northeast-1  ami-f2fb0494    ap-southeast-1  ami-47036324    ap-southeast-2  ami-370b1154       Click  Select .     The steps listed below only mention required parameters. You may optionally review and adjust additional parameters.      In  Step2: Choose Instance Type , choose an instance type. The minimum instance type which is suitable for Cloudbreak is  m3.large . Minimum requirements are 8GB RAM, 10GB disk, 2 cores. Click  Next .         (Perform this step only if you are using role-based authorization) In  Step 3: Configure Instance Details     IAM , select the \"CloudbreakRole\" IAM role which you  created earlier .    In  Step 6: Configure Security Group , open the following ports: 22 (for access via SSH) and 443 (for access via HTTPS). Click  Review and Launch .       In  Step 7: Review Instance Launch , review the information carefully and then click  Launch .     When prompted select an existing key pair or create a new one. Next, acknowledge that you have access to the private key file and click  Launch Instance .         Click on the instance ID to navigate to the  Instances  view in your EC2 console.", 
            "title": "Launch the VM"
        }, 
        {
            "location": "/aws-launch/index.html#ssh-to-the-vm", 
            "text": "Now that your VM is ready, access it via SSH:    Use the private key from the key pair that you selected when launching the instance.   The SSH user is called \"cloudbreak\".  You can obtain the host IP from the EC2 console    Instances  view by selecting the instance, selecting the  Description  tab, and copying the value of the  Public DNS (IPv4)  or  IPv4 Public IP  parameter.   On Mac OS, you can SSH to the VM by running the following from the Terminal app:  ssh -i \"your-private-key.pem\" cloudnreak@instance_IP  where \"your-private-key.pem\" points to the location of your private key and \"instance_IP\" is the public IP address of the VM.  On Windows, you can use  PuTTy .", 
            "title": "SSH to the VM"
        }, 
        {
            "location": "/aws-launch/index.html#launch-cloudbreak-deployer", 
            "text": "After accessing the VM via SSH, launch Cloudbreak deployer using the following steps.  Steps      Navigate to the cloudbreak-deployment directory:  cd /var/lib/cloudbreak-deployment/  This directory contains configuration files and the supporting binaries for Cloudbreak deployer.    Initialize your profile by creating a new file called  Profile  and adding the following content:  export UAA_DEFAULT_SECRET=MY-SECRET\nexport UAA_DEFAULT_USER_PW=MY-PASSWORD     For example:   export UAA_DEFAULT_SECRET=MySecret123\nexport UAA_DEFAULT_USER_PW=MySecurePassword123     You will need to provide the password when logging in to the Cloudbreak web UI and when using the Cloudbreak Shell. The secret will be used by Cloudbreak for authentication.       Start the Cloudbreak application by using the following command:  cbd start  This will start the Docker containers and initialize the application. The first time you start the Coudbreak app, the process will take longer than usual due to the download of all the necessary docker images.  The  cbd start  command includes the  cbd generate  command which applies the following steps:   Creates the  docker-compose.yml  file, which describes the configuration of all the Docker containers needed for the Cloudbreak deployment.  Creates the  uaa.yml  file, which holds the configuration of the identity server used to authenticate users with Cloudbreak.    Once the  cbd start  has finished, it returns the \"Uluwatu (Cloudbreak UI) url\" which you can later paste in your browser and log in to Cloudbreak web UI.      Check Cloudbreak deployer version and health:   cbd doctor    Next, check Cloudbreak Application logs:   cbd logs cloudbreak  You should see a message like this in the log:  Started CloudbreakApplication in 36.823 seconds.  Cloudbreak normally takes less than a minute to start.", 
            "title": "Launch Cloudbreak Deployer"
        }, 
        {
            "location": "/aws-launch/index.html#access-cloudbreak-ui", 
            "text": "Log in to the Cloudbreak UI using the following steps.  Steps    You can log into the Cloudbreak application at  https://IPv4_Public_IP /  or  https://Public_DNS . For example  https://34.212.141.253  or  https://ec2-34-212-141-253.us-west-2.compute.amazonaws.com .     Confirm the security exception to proceed to the Cloudbreak web UI.  The first time you access Cloudbreak UI, Cloudbreak will automatically generate a self-signed certificate, due to which your browser will warn you about an untrusted connection and will ask you to confirm a security exception.    The login page is displayed:        Log in to the Cloudbreak web UI:    The default username is  admin@example.com  but you should sign up with your own email address.      The password is the value of the  UAA_DEFAULT_USER_PW  variable that you configured in your  Profile  file when  launching Cloudbreak deployer .     Upon a successful login, you are redirected to the Cluster dashboard:", 
            "title": "Access Cloudbreak UI"
        }, 
        {
            "location": "/aws-launch/index.html#create-cloudbreak-credential", 
            "text": "Before you can start creating clusters, you must first create a  Cloudbreak credential . Without this credential, you will not be able to create clusters via Cloudbreak.   As part of the  prerequisites , you had two options to allow Cloudbreak to authenticate with AWS and create resources on your behalf: key-based or role-based authentication.   Depending on your choice, you must configure a key-based or role-based credential:   Create Key-Based Credential  Create Role-Based Credential", 
            "title": "Create Cloudbreak Credential"
        }, 
        {
            "location": "/aws-launch/index.html#create-key-based-credential", 
            "text": "To perform these steps, you must know your access and secret key. If needed, you or your AWS administrator can generate new access and secret keys from the  IAM Console     Users    select a user    Security credentials .   Steps    In the Cloudbreak web UI, select  Credentials  from the left pane.     Click  Create Credential .     Under  Cloud provider , select \"Amazon Web Services\":        Provide the following information:     Parameter  Description      Select Credential Type  Select  Key Based .    Name  Enter a name for your credential.    Description  (Optional) Enter a description.    Access Key  Paste your access key.    Secret Access Key  Paste your secret key.       Click  Create .    Your credential should now be displayed in the  Credentials  pane.  Congratulations! You've successfully launched Cloudbreak and create a Cloudbreak credential. Now it's time to  create a cluster .", 
            "title": "Create Key-Based Credential"
        }, 
        {
            "location": "/aws-launch/index.html#create-role-based-credential", 
            "text": "To perform these steps, you must know the  IAM Role ARN  corresponding to the \"CredentialRole\" (configured as a  prerequisite ).    Steps    In the Cloudbreak web UI, select  Credentials  from the left pane.     Click  Create Credential .     Under  Cloud provider , select \"Amazon Web Services\":        Provide the following information:     Parameter  Description      Select Credential Type  Select  Role Based  (default value).    Name  Enter a name for your credential.    Description  (Optional) Enter a description.    IAM Role ARN  Paste the IAM Role ARN corresponding to the \"CredentialRole\" that you created earlier. For example  arn:aws:iam::315627065446:role/CredentialRole  is a valid IAM Role ARN.       Click  Create .    Your credential should now be displayed in the  Credentials  pane.  Congratulations! You've successfully launched Cloudbreak and create a Cloudbreak credential. Now it's time to  create a cluster .      Next: Create a Cluster", 
            "title": "Create Role-Based Credential"
        }, 
        {
            "location": "/aws-create/index.html", 
            "text": "Create a Cluster on AWS\n\n\nOptional Prerequisites\n\n\nIf you are just getting started with Cloudbreak, follow the steps below to create a cluster using the default blueprints. \n\n\nIf you are already familiar with Cloudbreak and you want to use custom blueprints, refer to the \nBlueprints\n documentation to customize your blueprints.\n\n\nBasic Options\n\n\nUse these steps to create a cluster.\n\n\nSteps\n\n\n\n\n\n\nLog in to the Cloudbreak UI.\n\n\n\n\n\n\nClick \nCreate Cluster\n and the \nCreate Cluster\n form is displayed.\n\n\n\n\n\n\nOn the \nGeneral Configuration\n page, provide the following parameters:\n\n\n\n\nTo view advanced options, click \nAdvanced\n. To learn about advanced options, refer to \nAdvanced Options\n.\n\n\n\n\n\n\n\n\n\n\nParameter\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nSelect Credential\n\n\nSelect a previously created credential.\n\n\n\n\n\n\nCluster Name\n\n\nEnter a name for your cluster. The name must be between 5 and 40 characters, must start with a letter, must only include lowercase letters, numbers, and hyphens.\n\n\n\n\n\n\nTags\n\n\n(Optional) You can optionally add tags, which will help you find your cluster-related resources, such as VMs, in your cloud provider account.\n\n\n\n\n\n\nRegion\n\n\nSelect the region in which you would like to launch your cluster.\n\n\n\n\n\n\nSend Email When Cluster is Ready\n\n\n(Optional) Check this to receive an email each time the cluster status changes.\n\n\n\n\n\n\n\n\n\n\nBy default, Ambari Username and Ambari Password are set to \nadmin\n. You can override it in the \"\nConfigure Cluster\n\" tab.\n\n\n\n\n\n\n\n\nOn the \nHardware and Storage\n page, select the blueprint that you would like to use for your cluster. You can either choose one of the pre-configured blueprints, or add your own in the \nBlueprints\n tab.\n\n\nFor each host group you must provide the following:\n\n\n\n\n\n\n\n\nParameter\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nGroup Size\n\n\nEnter a number defining how many nodes to create per host group. Default is 1. The \"Group Size\" for that host group on which Ambari Server is installed must be set to \"1\".\n\n\n\n\n\n\nTemplate\n\n\nIf you have previously created a template for VMs and storage, you can select it here. If you don't make a selection, default will be used.\n\n\n\n\n\n\nSecurity Group\n\n\nIf you have previously created a template for a security group, you can select it here. If you don't make a selection, default will be used.\n\n\n\n\n\n\nAmbari Server\n\n\nYou must select one node for Ambari Server. The \"Group Size\" for that host group must be set to \"1\".\n\n\n\n\n\n\nRecipes\n\n\nYou can select a previously added recipe (custom script) to be executed on all nodes of the host group. Refer to \nRecipes\n.\n\n\n\n\n\n\n\n\n\n\n\n\nOn the \nNetwork\n page, provide the following parameters:\n\n\n\n\n\n\n\n\nParameter\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nNetwork\n\n\nSelect the virtual network in which you would like your cluster to be provisioned. You can define custom network configurations or use default network configurations.\n\n\n\n\n\n\nEnable Knox Gateway\n\n\n(Optional) Select this option to enable secure access to Ambari web UI and other cluster UIs via Knox gateway.\n\n\n\n\n\n\nEnable Kerberos Security\n\n\n(Optional) Select this option to enable Kerberos for your cluster. You will have an option to create a new kerberos or use an existing one. For more information refer to Kerberos \ndocumentation\n.\n\n\n\n\n\n\n\n\n\n\n\n\nOn the \nSecurity\n page, provide the following parameters:\n\n\n\n\n\n\nClick on \nCreate Cluster\n to create a cluster.\n\n\n\n\n\n\nYou will be redirected to the Cloudbreak dashboard, and a new tile representing your cluster will appear at the top of the page.\n\n\n\n\n\n\nAdvanced Options\n\n\nClick on \nShow Advanced Options\n to enter additional configuration options.\n\n\nGeneral Configuration\n\n\nYou can optionally configure the following advanced parameters:\n\n\n\n\n\n\n\n\nParameter\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nAmbari Username\n\n\nYou can log in to the Ambari UI using this username. By default, this is set to \nadmin\n.\n\n\n\n\n\n\nAmbari Password\n\n\nYou can log in to the Ambari UI using this password. By default, this is set to \nadmin\n.\n\n\n\n\n\n\nProvision Cluster\n\n\nSALT\n is pre-selected to provision your cluster.\n\n\n\n\n\n\nUse dedicated instances\n\n\nSelect this to use dedicated instances (i.e. EC2 instances that run in a virtual private cloud (VPC) on hardware that's dedicated to a single customer). For more information about dedicated instances, refer to \nAWS documentation\n.\n\n\n\n\n\n\nEnable Lifetime Management\n\n\nCheck this option if you would like your cluster to be automatically terminated after a specific amount of time (defined as \"Time to Live\" in minuter) has passed.\n\n\n\n\n\n\nFlex Subscription\n\n\nThis option will appear if you have configured your deployment for a \nFlex Subscription\n.\n\n\n\n\n\n\n\n\nHardware and Storage\n\n\nAfter selecting a blueprint, you can optionally configure the following advanced parameters:\n\n\n\n\n\n\n\n\nParameter\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nConfig Recommendation Strategy (Stack Advisor)\n\n\nSelect how configuration recommendations generated by stack advisor will be applied. Select one of \nALWAYS_APPLY: Configuration recommendations will be applied automatically.\nALWAYS_APPLY_DONT_OVERRIDE_CUSTOM_VALUES: Configuration recommendations will be applied automatically, but they will be ignored for custom configurations.\nNEVER_APPLY: Configuration recommendations will be ignored.\nONLY_STACK_DEFAULTS_APPLY: Configuration recommendations will be applied only on the default configurations for all included services.\n\n\n\n\n\n\nValidate Blueprint\n\n\nSelect to validate the blueprint.\n\n\n\n\n\n\nInstance Profile\n\n\nThis option allows you to optionally create or reuse an existing instance profile for your cluster VMs. An instance profile is a container for an IAM role that you can use to pass role information to an EC2 instance when the instance starts. You can use this option to allow access to AWS resources such as Amazon S3 object storage. The following choices are available:\nDisable Instance Profile attaching by default\nCreate Instance Profile and attach to the instance\nDefine existing Instance Profile and attach to the instances\n For more information about instance profile, refer to \nAWS documentation\n.\n\n\n\n\n\n\n\n\nChoose Failure Action\n\n\nYou can optionally select what to do if cluster creation fails or if there aren't enough instances available to create all requested nodes:\n\n\n\n\n\n\n\n\nParameter\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nFailure Action\n\n\nSelect one of: \ndo NOT rollback resources\n (default) or \nrollback resources\n. \nBy default, if creating a cluster fails, the Azure resources that were created up to that point will not be rolled back. This means that they will remain accessible for troubleshooting and you will need to to delete them manually.\n\n\n\n\n\n\nMinimum Cluster Size\n\n\nThis defines the provisioning strategy in case the cloud provider cannot allocate all the requested nodes. Select \nbest effort\n or \nexact\n.\n\n\n\n\n\n\n\n\nConfigure Ambari Repos\n\n\nYou can optionally configure a different version of Ambari than the default by providing the following information:\n\n\n\n\n\n\n\n\nParameter\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nAmbari Version\n\n\nEnter Ambari version.\n\n\n\n\n\n\nAmbari Repo URL\n\n\nEnter Ambari repo URL.\n\n\n\n\n\n\nAmbari Repo Gpg Key URL\n\n\nEnter gpgkey URL.\n\n\n\n\n\n\n\n\nConfigure HDP Repos\n\n\nYou can optionally configure a different version of HDP than the default by providing the following information:\n\n\n\n\n\n\n\n\nParameter\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nStack\n\n\nEnter stack name.\n\n\n\n\n\n\nVersion\n\n\nEnter stack version.\n\n\n\n\n\n\nStack Repo ID\n\n\nEnter stack repo ID.\n\n\n\n\n\n\nBase URL\n\n\nEner stack repo base URL.\n\n\n\n\n\n\nUtils Repo ID\n\n\nEnter Utils repo ID.\n\n\n\n\n\n\nUtils Base URL\n\n\nEnter Utils repo base URL.\n\n\n\n\n\n\nVerify\n\n\nSelect to verify the repo information.\n\n\n\n\n\n\n\n\nConfigure Ambari Database\n\n\nBy default, Ambari stores data on an embedded database, which is sufficient for ephemeral or test clusters. However, as Ambari and Cloudbreak don't perform backups of this database, it is insufficient for long-running production clusters, and you may need to configure a remote database for Ambari and Cloudbreak.\n\n\n\n\n\n\n\n\nParameter\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nVendor\n\n\nSelect database vendor from the list.\n\n\n\n\n\n\nHost\n\n\nEnter database host IP.\n\n\n\n\n\n\nPort\n\n\nEnter port number.\n\n\n\n\n\n\nName\n\n\nEnter database name.\n\n\n\n\n\n\nUser Name\n\n\nEnter database user name.\n\n\n\n\n\n\nPassword\n\n\nEnter database password.\n\n\n\n\n\n\n\n\n\n\nNext: Access Cluster", 
            "title": "Create a Cluster"
        }, 
        {
            "location": "/aws-create/index.html#create-a-cluster-on-aws", 
            "text": "", 
            "title": "Create a Cluster on AWS"
        }, 
        {
            "location": "/aws-create/index.html#optional-prerequisites", 
            "text": "If you are just getting started with Cloudbreak, follow the steps below to create a cluster using the default blueprints.   If you are already familiar with Cloudbreak and you want to use custom blueprints, refer to the  Blueprints  documentation to customize your blueprints.", 
            "title": "Optional Prerequisites"
        }, 
        {
            "location": "/aws-create/index.html#basic-options", 
            "text": "Use these steps to create a cluster.  Steps    Log in to the Cloudbreak UI.    Click  Create Cluster  and the  Create Cluster  form is displayed.    On the  General Configuration  page, provide the following parameters:   To view advanced options, click  Advanced . To learn about advanced options, refer to  Advanced Options .      Parameter  Description      Select Credential  Select a previously created credential.    Cluster Name  Enter a name for your cluster. The name must be between 5 and 40 characters, must start with a letter, must only include lowercase letters, numbers, and hyphens.    Tags  (Optional) You can optionally add tags, which will help you find your cluster-related resources, such as VMs, in your cloud provider account.    Region  Select the region in which you would like to launch your cluster.    Send Email When Cluster is Ready  (Optional) Check this to receive an email each time the cluster status changes.      By default, Ambari Username and Ambari Password are set to  admin . You can override it in the \" Configure Cluster \" tab.     On the  Hardware and Storage  page, select the blueprint that you would like to use for your cluster. You can either choose one of the pre-configured blueprints, or add your own in the  Blueprints  tab.  For each host group you must provide the following:     Parameter  Description      Group Size  Enter a number defining how many nodes to create per host group. Default is 1. The \"Group Size\" for that host group on which Ambari Server is installed must be set to \"1\".    Template  If you have previously created a template for VMs and storage, you can select it here. If you don't make a selection, default will be used.    Security Group  If you have previously created a template for a security group, you can select it here. If you don't make a selection, default will be used.    Ambari Server  You must select one node for Ambari Server. The \"Group Size\" for that host group must be set to \"1\".    Recipes  You can select a previously added recipe (custom script) to be executed on all nodes of the host group. Refer to  Recipes .       On the  Network  page, provide the following parameters:     Parameter  Description      Network  Select the virtual network in which you would like your cluster to be provisioned. You can define custom network configurations or use default network configurations.    Enable Knox Gateway  (Optional) Select this option to enable secure access to Ambari web UI and other cluster UIs via Knox gateway.    Enable Kerberos Security  (Optional) Select this option to enable Kerberos for your cluster. You will have an option to create a new kerberos or use an existing one. For more information refer to Kerberos  documentation .       On the  Security  page, provide the following parameters:    Click on  Create Cluster  to create a cluster.    You will be redirected to the Cloudbreak dashboard, and a new tile representing your cluster will appear at the top of the page.", 
            "title": "Basic Options"
        }, 
        {
            "location": "/aws-create/index.html#advanced-options", 
            "text": "Click on  Show Advanced Options  to enter additional configuration options.", 
            "title": "Advanced Options"
        }, 
        {
            "location": "/aws-create/index.html#general-configuration", 
            "text": "You can optionally configure the following advanced parameters:     Parameter  Description      Ambari Username  You can log in to the Ambari UI using this username. By default, this is set to  admin .    Ambari Password  You can log in to the Ambari UI using this password. By default, this is set to  admin .    Provision Cluster  SALT  is pre-selected to provision your cluster.    Use dedicated instances  Select this to use dedicated instances (i.e. EC2 instances that run in a virtual private cloud (VPC) on hardware that's dedicated to a single customer). For more information about dedicated instances, refer to  AWS documentation .    Enable Lifetime Management  Check this option if you would like your cluster to be automatically terminated after a specific amount of time (defined as \"Time to Live\" in minuter) has passed.    Flex Subscription  This option will appear if you have configured your deployment for a  Flex Subscription .", 
            "title": "General Configuration"
        }, 
        {
            "location": "/aws-create/index.html#hardware-and-storage", 
            "text": "After selecting a blueprint, you can optionally configure the following advanced parameters:     Parameter  Description      Config Recommendation Strategy (Stack Advisor)  Select how configuration recommendations generated by stack advisor will be applied. Select one of  ALWAYS_APPLY: Configuration recommendations will be applied automatically. ALWAYS_APPLY_DONT_OVERRIDE_CUSTOM_VALUES: Configuration recommendations will be applied automatically, but they will be ignored for custom configurations. NEVER_APPLY: Configuration recommendations will be ignored. ONLY_STACK_DEFAULTS_APPLY: Configuration recommendations will be applied only on the default configurations for all included services.    Validate Blueprint  Select to validate the blueprint.    Instance Profile  This option allows you to optionally create or reuse an existing instance profile for your cluster VMs. An instance profile is a container for an IAM role that you can use to pass role information to an EC2 instance when the instance starts. You can use this option to allow access to AWS resources such as Amazon S3 object storage. The following choices are available: Disable Instance Profile attaching by default Create Instance Profile and attach to the instance Define existing Instance Profile and attach to the instances  For more information about instance profile, refer to  AWS documentation .", 
            "title": "Hardware and Storage"
        }, 
        {
            "location": "/aws-create/index.html#choose-failure-action", 
            "text": "You can optionally select what to do if cluster creation fails or if there aren't enough instances available to create all requested nodes:     Parameter  Description      Failure Action  Select one of:  do NOT rollback resources  (default) or  rollback resources .  By default, if creating a cluster fails, the Azure resources that were created up to that point will not be rolled back. This means that they will remain accessible for troubleshooting and you will need to to delete them manually.    Minimum Cluster Size  This defines the provisioning strategy in case the cloud provider cannot allocate all the requested nodes. Select  best effort  or  exact .", 
            "title": "Choose Failure Action"
        }, 
        {
            "location": "/aws-create/index.html#configure-ambari-repos", 
            "text": "You can optionally configure a different version of Ambari than the default by providing the following information:     Parameter  Description      Ambari Version  Enter Ambari version.    Ambari Repo URL  Enter Ambari repo URL.    Ambari Repo Gpg Key URL  Enter gpgkey URL.", 
            "title": "Configure Ambari Repos"
        }, 
        {
            "location": "/aws-create/index.html#configure-hdp-repos", 
            "text": "You can optionally configure a different version of HDP than the default by providing the following information:     Parameter  Description      Stack  Enter stack name.    Version  Enter stack version.    Stack Repo ID  Enter stack repo ID.    Base URL  Ener stack repo base URL.    Utils Repo ID  Enter Utils repo ID.    Utils Base URL  Enter Utils repo base URL.    Verify  Select to verify the repo information.", 
            "title": "Configure HDP Repos"
        }, 
        {
            "location": "/aws-create/index.html#configure-ambari-database", 
            "text": "By default, Ambari stores data on an embedded database, which is sufficient for ephemeral or test clusters. However, as Ambari and Cloudbreak don't perform backups of this database, it is insufficient for long-running production clusters, and you may need to configure a remote database for Ambari and Cloudbreak.     Parameter  Description      Vendor  Select database vendor from the list.    Host  Enter database host IP.    Port  Enter port number.    Name  Enter database name.    User Name  Enter database user name.    Password  Enter database password.      Next: Access Cluster", 
            "title": "Configure Ambari Database"
        }, 
        {
            "location": "/aws-clusters-access/index.html", 
            "text": "Accessing Your Cluster\n\n\nThe following section describes how to access the various services in the cluster.\n\n\nFinding Cluster Details\n\n\nOnce your cluster is up and running, you can find information about the cluster on the cluster details page in the Cloudbreak UI. To access cluster details page, click on the tile representing your cluster in the Cloudbreak UI. The information presented includes:\n\n\n\n\nCluster status\n\n\nEvent history \n\n\nCluster instance public IP addresses\n\n\nUI links \n\n\n\n\nAccess Cluster via SSH\n\n\nIf you plan to access the cluster via the command line clients, SSH into the master node instance in the cluster. \n\n\n\n\nIn order to use SSH, you must generate an SSH key pair or use an existing SSH keypair.  \n\n\nYou can find the cluster instance public IP addresses on the cluster details page.  \n\n\nWhen accessing instances via SSH use the \ncloudbreak\n user. \n\n\n\n\nOn Mac OS, you can use the following syntax to SSH to the VM:\n\nssh -i \"privatekey.pem\" cloudbreak@publicIP\n\nFor example:\n\nssh -i \"dominika-kp.pem\" cloudbreak@p52.25.169.132\n\n\nOn Windows, you can SSH using an SSH client such as PuTTY.", 
            "title": "Access Cluster"
        }, 
        {
            "location": "/aws-clusters-access/index.html#accessing-your-cluster", 
            "text": "The following section describes how to access the various services in the cluster.", 
            "title": "Accessing Your Cluster"
        }, 
        {
            "location": "/aws-clusters-access/index.html#finding-cluster-details", 
            "text": "Once your cluster is up and running, you can find information about the cluster on the cluster details page in the Cloudbreak UI. To access cluster details page, click on the tile representing your cluster in the Cloudbreak UI. The information presented includes:   Cluster status  Event history   Cluster instance public IP addresses  UI links", 
            "title": "Finding Cluster Details"
        }, 
        {
            "location": "/aws-clusters-access/index.html#access-cluster-via-ssh", 
            "text": "If you plan to access the cluster via the command line clients, SSH into the master node instance in the cluster.    In order to use SSH, you must generate an SSH key pair or use an existing SSH keypair.    You can find the cluster instance public IP addresses on the cluster details page.    When accessing instances via SSH use the  cloudbreak  user.    On Mac OS, you can use the following syntax to SSH to the VM: ssh -i \"privatekey.pem\" cloudbreak@publicIP \nFor example: ssh -i \"dominika-kp.pem\" cloudbreak@p52.25.169.132  On Windows, you can SSH using an SSH client such as PuTTY.", 
            "title": "Access Cluster via SSH"
        }, 
        {
            "location": "/aws-data/index.html", 
            "text": "Accessing Data on S3\n\n\nPrerequisites\n\n\nTo use S3 storage, you must have one or more S3 buckets on your AWS account. For instructions on how to create a bucket on S3, refer to \nAWS documentation\n.\n\n\nConfiguring Access to S3\n\n\nAmazon S3 is not supported as a default file system, but access to data in S3 from your cluster VMs can be automatically configured through attaching an instance profile allowing access to S3. You can optionally create or attach an existing instance profile during \ncluster creation\n, on the \nChoose Blueprint\n page. \n\n\nAccess Path\n\n\nAmazon S3 access path syntax is:\n\n\ns3a://bucket/dir/file\n\n\n\nFor example, to access a file called \"mytestfile\" in a directory called \"mytestdir\", which is stored in a bucket called \"mytestbucket\", the URL is:\n\n\ns3a://mytestbucket/mytestdir/mytestfile\n\n\n\nThe following FileSystem shell commands demonstrate access to a bucket named \"mytestbucket\": \n\n\nhadoop fs -ls s3a://mytestbucket/\n\nhadoop fs -mkdir s3a://mytestbucket/testDir\n\nhadoop fs -put testFile s3a://mytestbucket/testFile\n\nhadoop fs -cat s3a://mytestbucket/testFile\ntest file content\n\n\n\nLearn More\n\n\nFor more information about configuring the S3 connector and working with data stored on S3, refer to \nCloud Data Access\n documentation.\n\n\n\n\nNext: Access Cluster", 
            "title": "Access Data on S3"
        }, 
        {
            "location": "/aws-data/index.html#accessing-data-on-s3", 
            "text": "", 
            "title": "Accessing Data on S3"
        }, 
        {
            "location": "/aws-data/index.html#prerequisites", 
            "text": "To use S3 storage, you must have one or more S3 buckets on your AWS account. For instructions on how to create a bucket on S3, refer to  AWS documentation .", 
            "title": "Prerequisites"
        }, 
        {
            "location": "/aws-data/index.html#configuring-access-to-s3", 
            "text": "Amazon S3 is not supported as a default file system, but access to data in S3 from your cluster VMs can be automatically configured through attaching an instance profile allowing access to S3. You can optionally create or attach an existing instance profile during  cluster creation , on the  Choose Blueprint  page.", 
            "title": "Configuring Access to S3"
        }, 
        {
            "location": "/aws-data/index.html#access-path", 
            "text": "Amazon S3 access path syntax is:  s3a://bucket/dir/file  For example, to access a file called \"mytestfile\" in a directory called \"mytestdir\", which is stored in a bucket called \"mytestbucket\", the URL is:  s3a://mytestbucket/mytestdir/mytestfile  The following FileSystem shell commands demonstrate access to a bucket named \"mytestbucket\":   hadoop fs -ls s3a://mytestbucket/\n\nhadoop fs -mkdir s3a://mytestbucket/testDir\n\nhadoop fs -put testFile s3a://mytestbucket/testFile\n\nhadoop fs -cat s3a://mytestbucket/testFile\ntest file content", 
            "title": "Access Path"
        }, 
        {
            "location": "/aws-data/index.html#learn-more", 
            "text": "For more information about configuring the S3 connector and working with data stored on S3, refer to  Cloud Data Access  documentation.   Next: Access Cluster", 
            "title": "Learn More"
        }, 
        {
            "location": "/azure-launch/index.html", 
            "text": "Launching Cloudbreak on Azure\n\n\nBefore launching Cloudbreak on Azure, review and meet the prerequisites. Next, launch Cloudbreak using one of the two available methods. Once Cloudbreak is running, log in to the Cloudbreak UI and create a Cloudbreak credential. \n\n\nMeet the Prerequisites\n\n\nBefore launching Cloudbreak on Azure, you must meet the following prerequisites.\n\n\nAzure Account\n\n\nIn order to launch Cloudbreak on the Azure Marketplace, log in to your existing Microsoft Azure account. If you don't have an account, you can set it up at \nhttps://azure.microsoft.com\n.\n\n\nAzure Roles\n\n\nIn order to provision clusters on Azure, Cloudbreak must be able to assume a sufficient Azure role (\"Owner\" or \"Contributor\") via Cloudbreak credential: \n\n\n\n\n\n\nYour account must have the \"\nOwner\n\" role in the subscription in order to \ncreate a Cloudbreak credential\n using the interactive credential method. \n\n\n\n\n\n\nYour account must have the \"\nContributor\n\" role (or higher) in the subscription in order to \ncreate a Cloudbreak credential\n using the app-based credential method. \n\n\n\n\n\n\nTo check the roles that your subscription has, in your Azure account navigate to \nSubscriptions\n. \n\n\nAzure Region\n\n\nDecide in which Azure region you would like to launch Cloudbreak. You can launch Cloudbreak and provision your clusters in all regions \nsupported by Microsoft Azure\n. \n\n\nClusters created via Cloudbreak can be in the same or different region as Cloudbreak; when you launch a cluster, you select the region in which to launch it.\n\n\nSSH Key Pair\n\n\nWhen launching Cloudbreak, you will be required to provide your public SSH key. If needed, you can generate a new SSH keypair:\n\n\n\n\nOn MacOS and Linux using \nssh-keygen -t rsa -b 4096 -C \"your_email@example.com\"\n  \n\n\nOn Windows using \nPuTTygen\n\n\n\n\nAzure Data Lake Store (Optional)\n\n\nIf you want to use \nAzure Data Lake Store\n with your cluster, you must create an\nAzure Data Lake Store account in the same subscription as you use for launching Cloudbreak and clusters. In the cluster creation phase, you will specify the created account name only, and access to the Azure Data Lake Store will be\nconfigured automatically.\n\n\nYou may also review the \nMicrosoft Azure Documentation\n for instructions on how these configuration steps\nare manually done from the Azure portal. After the cluster is deployed, you must define which parts of the ADLS store this cluster\nwill have access and test access to ADLS. Refer to \nAccessing Data\n for more information.\n\n\nLaunch Cloudbreak\n\n\nYou have two options to launch Cloudbreak on Azure:\n\n\n\n\nLaunch via Azure Resource Manager Template\n\n\nLaunch from Azure Marketplace (Technical Preview)\n\n\n\n\n(Option 1) Launch via Azure Resource Manager Template\n\n\nLaunch Cloudbreak deployer using the following steps.\n\n\nSteps\n\n\n\n\n\n\nLog in to your \nAzure Portal\n.\n\n\n\n\n\n\nClick here to get started with Cloudbreak installation using the Azure Resource Manager template:\n\n\n \n\n\n\n\n\n\nThe template for installing Cloudbreak will appear. On the \nBasics\n page, provide the following basic parameters:   \n\n\nBASICS\n\n\n\n\n\n\n\n\nParameter\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nSubscription\n\n\n(Required) Select which existing subscription you want to use.\n\n\n\n\n\n\nResource group\n\n\n(Required) Select an existing resource group or create a new one by selecting \nCreate new\n and entering a name for your new resource group. Cloudbreak resources will later be accessible in that chosen resource group.\n\n\n\n\n\n\nLocation\n\n\n(Required) Select an Azure region in which you want to deploy Cloudbreak.\n\n\n\n\n\n\n\n\nSETTINGS\n\n\n\n\n\n\n\n\nParameter\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nBase Url\n\n\nThis is the URL to the page where the template is stored.\n\n\n\n\n\n\nLocation\n\n\nThis is an internal parameter. Do not change it.\n\n\n\n\n\n\nVM Size\n\n\nSelect virtual machine instance type to use for the Cloudbreak controller. The minimum instance type suitable for Cloudbreak is \nD2\n.\n\n\n\n\n\n\nAdmin Username\n\n\nCreate an admin login that you will use to log in to the Cloudbreak UI. Must be a valid email address.\n\n\n\n\n\n\nAdmin User Password\n\n\n(Required) Password for the admin login. Must be at least 8 characters containing letters, numbers, and symbols.\n\n\n\n\n\n\nUsername\n\n\nEnter an admin username for the virtual machine. You will use it to SSH to the VM.\n\n\n\n\n\n\nSmartSense\n\n\nSelect whether you want to use SmartSense telemetry. Default is \"false\" (not using SmartSense telemetry).\n\n\n\n\n\n\nRemote Location\n\n\nEnter a valid \nCIDR IP\n or use one of the default tags. Default value is \nInternet\n which allows access from all IP addresses. Examples: \n10.0.0.0/24 will allow access from 10.0.0.0 through 10.0.0.255\n'Internet' will allow access from all. This is not a secure option but you can use it it you are just getting started and are not planning to have the instance on for a longer period. \n(Advanced) 'VirtualNetwork' will allow access from the address space of the Virtual Network.\n (Advanced) 'AzureLoadBalancer' will allow access from the address space of the load balancer.\nFor more information, refer to the \nAzure documentation\n.\n\n\n\n\n\n\nSsh Key\n\n\n(Required) Paste your SSH public key.\nYou can use \npbcopy\n to quickly copy it. For example: \npbcopy \n /Users/homedir/.ssh/id_rsa.pub\n\n\n\n\n\n\nVnet New Or Existing\n\n\nBy default, Cloudbreak is launched in a new VNet called \ncbdeployerVnet\n and a new subnet called \ncbdeployerSubnet\n; if needed, you can customize the settings for the new VNet using available VNet and Subnet parameters.\n\n\n\n\n\n\nVnet Name\n\n\nProvide the name for a new Vnet. Default is \n`cbdeployerVnet\n.\n\n\n\n\n\n\nVnet Subnet Name\n\n\nProvide a name for a new subnet. Default is \ncbdeployerSubnet\n.\n\n\n\n\n\n\nVnet Address Prefix\n\n\nProvide a CIDR for the virtual network. Default is \n10.0.0.0/16\n.\n\n\n\n\n\n\nVnet Subnet Address Prefix\n\n\nProvide a CIDR for the subnet. Default is \n10.0.0.0/24\n.\n\n\n\n\n\n\nVnet RG Name\n\n\nThe name of the resource group in which the Vnet is located. If creating a new Vnet, enter the same resource group name as provided in the \nResource group\n field in the \nBASICS\n section.\n\n\n\n\n\n\n\n\n\n\n\n\nReview terms of use and check \"I agree to the terms and conditions stated above\". \n\n\n\n\n\n\nClick \nPurchase\n.\n\n\n\n\n\n\nProceed to the next step: \nExplore Newly Created Resources\n.\n\n\n\n\n\n\n(Option 2) Launch from Azure Marketplace\n\n\nLaunch Cloudbreak deployer using the following steps.\n\n\n\n\nThis feature is Technical Preview.  \n\n\n\n\nSteps\n\n\n\n\n\n\nLog in to your \nAzure Portal\n.\n\n\n\n\n\n\nFrom the services menu, select \n.\n\n\n\n\n\n\nIn the search box, enter \"Cloudbreak\":  \n\n\n  \n\n\n\n\n\n\nSelect \n.\n\n    The information about the Cloudbreak for Hortonworks Data Platform will be displayed.\n    In the bottom of the page, you will see a dropdown for selecting a deployment model, with the \nResource Manager\n deployment model pre-selected. This is the only deployment model available for this offering.\n\n\n\n\n\n\nClick \nCreate\n.   \n\n\n\n\n\n\nThe template for installing Cloudbreak will appear. On the \nBasics\n page, provide the following basic parameters:\n\n\n\n\n\n\n\n\nParameter\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nAdministrator email address\n\n\nCreate an admin login that you will use to log in to the Cloudbreak UI. Must be a valid email address.\n\n\n\n\n\n\nAdministrator user password\n\n\nPassword for the admin login. Must be at least 8 characters containing letters, numbers, and symbols.\n\n\n\n\n\n\nConfirm password\n\n\nConfirm the password for the admin login.\n\n\n\n\n\n\nVM Username\n\n\nEnter an admin username for the virtual machine. You will use it to SSH to the VM.\n\n\n\n\n\n\nVM SSH public key\n\n\nPaste your SSH public key.\nYou can use \npbcopy\n to quickly copy it. For example: \npbcopy \n /Users/homedir/.ssh/id_rsa.pub\n\n\n\n\n\n\nSubscription\n\n\nSelect which existing subscription you want to use.\n\n\n\n\n\n\nResource group\n\n\nOnly \nCreate new\n is supported. Select \nCreate new\n to create a new resource group and enter a name for your new resource group. Cloudbreak resources will later be accessible in that chosen resource group.\n\n\n\n\n\n\nLocation\n\n\nSelect an Azure region in which you want to deploy Cloudbreak.\n\n\n\n\n\n\n\n\n\n\n\n\nOnce done, click \nOK\n.\n\n\n\n\n\n\nOn the \nAdvanced Settings\n page, provide the following advanced parameters:\n\n\n\n\n\n\n\n\nParameter\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nController Instance Type\n\n\nSelect virtual machine instance type to use for the Cloudbreak. The minimum instance type suitable for Cloudbreak is \nD2\n.\n\n\n\n\n\n\nAllow connections to the cloud controller from this address range or \ndefault tag\n\n\nEnter a valid \nCIDR IP\n or use one of the default tags such as  \nInternet\n. For example: \n10.0.0.0/24 will allow access from 10.0.0.0 through 10.0.0.255\n'Internet' will allow access from all. This is not a secure option but you can use it it you are just getting started and are not planning to have the instance on for a longer period. \n(Advanced) 'VirtualNetwork' will allow access from the address space of the Virtual Network.\n (Advanced) 'AzureLoadBalancer' will allow access from the address space of the load balancer.\nFor more information, refer to the \nAzure documentation\n.\n\n\n\n\n\n\nEnable SmartSense\n\n\n(Optional) Select whether to enable SmartSense telemetry. Default is \nI have read and opt-in to SmartSense telemetry\n. SmartSense provides product telemetry and usage information to Hortonworks. For more information, refer to the \nSmartSense\n terms.\n\n\n\n\n\n\nVirtual network\n\n\nCreate a new Vnet (default) or select an existing Vnet.\n\n\n\n\n\n\nSubnets\n\n\nIf you created a new Vnet, create subnets within it. If selected an existing Vnet, select exiting subnets.\n\n\n\n\n\n\n\n\n\n\n\n\nOnce done, click \nOK\n.\n\n\n\n\n\n\nOn the \nSummary\n page, validate the information that you provided.\n    Before proceeding to the next page, you have an option to \nDownload template and parameters\n. You can use them to launch Cloudbreak via CLI. Once done, click \nOK\n.\n\n\n\n\n\n\nReview terms of use and click \nPurchase\n.\n\n\n\n\n\n\nProceed to the next step: \nExplore Newly Created Resources\n.\n\n\n\n\n\n\nExplore Newly Created Resources\n\n\n\n\nThis step is optional.  \n\n\n\n\nWhile the deployment is in progress, you can optionally navigate to the newly created resource group and see what Azure resources are being created.\n\n\nSteps\n\n\n\n\n\n\nFrom the left pane, select \n.\n\n\n\n\n\n\nFind the the resource group that you just created and select it to view details.\n\n\n\n\n\n\nThe following resources should have been created in your resource group:\n\n\n\n\n\n\nIf you chose to use an existing virtual network, the virtual network will not be added to the resource group. \n\n\n\n\n\n\nVirtual network\n (VNet) securely connects Azure resources to each other.    \n\n\nNetwork security group\n (NSG) defines inbound and outbound security rules, which control network traffic flow.  \n\n\nVirtual machine\n runs Cloudbreak.   \n\n\nPublic IP address\n is assigned to your VM so that it can communicate with other Azure resources.  \n\n\nNetwork interface\n (NIC) attached to the VM provides the interconnection between the VM and the underlying software network.    \n\n\nBlob storage container\n is created to store Cloudbreak Deployer OS disk's data.  \n\n\n\n\n\n\n\n\nYou can click on each entry to view details of the resource. For example, click on \n to view details, including Cloudbreak IP address.\n\n\n\n\n\n\nOnce your deployment is ready, the status will change from \"Deploying\" to \"Success\".\n\n\n\n\n\n\nAccess Cloudbreak UI\n\n\nLog in to the Cloudbreak UI using the following steps.\n\n\nSteps\n\n\n\n\n\n\nWhen your deployment succeeds, you will receive a notification in the top-right corner. You can click on the link provided to navigate to the resource group created earlier.\n\n\n\n\nThis only works right after deployment. At other times, you can find your resource group by selecting \nResource Groups\n from the service menu and then finding your resource group by name.\n\n\n\n\n\n\n\n\nOnce you've navigated to your resource group, click on \nDeployments\n and then click on \nhortonworks.cloudbreal-for-hortonworks-data-platf-...\n:\n\n\n \n\n\n\n\n\n\nFrom \nOutputs\n, you can copy the link by clicking on the \n icon:\n\n\n   \n\n\n\n\n\n\nPaste the link in your browser's address bar.\n\n\nThe first time you access Cloudbreak UI, Cloudbreak will automatically generate a self-signed certificate, due to which your browser will warn you about an untrusted connection and will ask you to confirm a security exception. You can safely proceed to the website. \n\n\n\n\n\n\n\n\nBrowser\n\n\nSteps\n\n\n\n\n\n\n\n\n\n\nFirefox\n\n\nClick \nAdvanced\n \n Click \nAdd Exception...\n \n Click \nConfirm Security Exception\n\n\n\n\n\n\nSafari\n\n\nClick \nContinue\n\n\n\n\n\n\nChrome\n\n\nClick \nAdvanced\n \n Click \nProceed...\n\n\n\n\n\n\n\n\n\n\n\n\nNow you should be able to access Cloudbreak UI and log in with the \nAdmin email address\n and \nAdmin password\n that you created when launching Cloudbreak:\n\n\n\n\nThe last task that you need to perform before you can use Cloudbreak is to \ncreate a cloudbreak credential\n.         \n\n\n\n\n\n\nCreate Cloudbreak Credential\n\n\nBefore you can start creating clusters, you must first create a \nCloudbreak credential\n. Without this credential, you will not be able to create clusters via Cloudbreak. Cloudbreak works by connecting your Azure account through this credential, and then uses it to create resources on your behalf.\n\n\nThere are two ways to create a Cloudbreak credential:\n\n\n\n\n\n\nInteractive\n: This is the recommended simpler method. The advantage of using this method is that the app and service principal creation and role assignment is fully automated, so the only input that you need to provide is the name and the SSH key. To configure an interactive credential, refer to \nCreate an Interactive Credential\n.  \n\n\n\n\n\n\nApp-based\n: This is the more complex method. The advantage of the app-based credential creation is that it allows you to create a credential without logging in to the Azure account, as long as you have been given all the information. In addition to providing your \ntenant_id\n and \nsubscription_id\n, you must provide information for your previously created Azure AD application, including its password. To configure an app based credential, refer to \nCreate an App Based Credential\n. Alternatively, when using this method, you use a utility called \nazure-cli-tools\n. The utility supports app creation and role assignment. It is available at \nhttps://github.com/sequenceiq/azure-cli-tools/blob/master/cli_tools\n.\n\n\n\n\n\n\nCreate an Interactive Credential\n\n\nSteps\n\n\n\n\n\n\nIn the Cloudbreak web UI, select \nCredentials\n from the left pane. \n\n\n\n\n\n\nClick \nCreate Credential\n. \n\n\n\n\n\n\nUnder \nCloud provider\n, select \"Microsoft Azure\". \n\n\n\n\n\n\nSelect \nInteractive Login\n:\n\n\n     \n\n\n\n\n\n\nProvide the following information:\n\n\n\n\n\n\n\n\nParameter\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nName\n\n\nEnter a name for your credential.\n\n\n\n\n\n\nDescription\n\n\n(Optional) Enter a description.\n\n\n\n\n\n\nSubscription Id\n\n\nCopy and paste the Subscription ID from your \nSubscriptions\n.\n\n\n\n\n\n\nTenant Id\n\n\nCopy and paste your Directory ID from your \nActive Directory\n \n \nProperties\n.\n\n\n\n\n\n\nAzure role type\n\n\nYou have the following options:\n\"Use existing Contributor role\" (default): If you select this option, Cloudbreak will use the \"\nContributor\n\" role to create resources. This requires no further input.\n\"Reuse existing custom role\": If you select this option and enter the name of an existing role, Cloudbreak will use this role to create resources.\n\"Let Cloudbreak create a custom role\": If you select this option and enter a name for the new role, the role will be created. When choosing role name, make sure that there is no existing role with the name chosen. For information on creating custom roles, refer to \nAzure\n documentation. \nIf using a custom role, make sure that it includes the necessary Action set for Cloudbreak to be able to manage clusters: \nMicrosoft.Compute/*\n, \nMicrosoft.Network/*\n, \nMicrosoft.Storage/*\n, \nMicrosoft.Resources/*\n.\n\n\n\n\n\n\n\n\nTo obtain the \nSubscription Id\n: \n\n\n   \n\n\nTo obtain the \nDirectory Id\n: \n\n\n    \n\n\n\n\n\n\nAfter providing the parameters, click \nInteractive Login\n.\n\n\n\n\n\n\nCopy the code provided in the UI:\n\n\n     \n\n\n\n\n\n\nClick \nAzure login\n and a new \nDevice login\n page will open in a new browser tab:\n\n\n  \n\n\n\n\n\n\nNext, paste the code in field on the  \nDevice login\n page and click \nContinue\n.\n\n\n\n\n\n\nConfirm your account by selecting it:\n\n\n\n\n\n\n\n\nA confirmation page will appear, confirming that you have signed in to the Microsoft Azure Cross-platform Command Line Interface application on your device. You may now close this window.\n\n\nCongratulations! You've successfully launched and configured Cloudbreak. Now you can use Cloudbreak to \ncreate clusters\n.\n\n\n\n\n\n\nCreate an App Based Credential\n\n\nSteps\n\n\n\n\n\n\nOn Azure Portal, navigate to the \nActive Directory\n \n \nApp Registrations\n and register a new application. For more information, refer to \nCreate an Azure AD Application\n.\n\n\n  \n\n\n\n\n\n\nNavigate to the \nSubscriptions\n, choose \nAccess control (IAM)\n. Click \nAdd\n and then assign the \"Contributor\" role to your newly created application by selecting \"Contributor\" under \nRole\n and your app name under \nSelect\n:\n\n\n   \n\n\n\n\n\n\nIn the Cloudbreak web UI, select \nCredentials\n from the left pane. \n\n\n \n\n\n\n\n\n\nClick \nCreate Credential\n. \n\n\n\n\n\n\nUnder \nCloud provider\n, select \"Microsoft Azure\". \n\n\n\n\n\n\nSelect \nApp based Login\n:\n\n\n\n\n\n\nOn the \nConfigure credential\n page, provide the following parameters:\n\n\n\n\n\n\n\n\nParameter\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nSelect Credential Type\n\n\nSelect \nApp based\n.\n\n\n\n\n\n\nName\n\n\nEnter a name for your credential.\n\n\n\n\n\n\nDescription\n\n\n(Optional) Enter a description.\n\n\n\n\n\n\nSubscription Id\n\n\nCopy and paste the Subscription ID from your \nSubscriptions\n.\n\n\n\n\n\n\nApp Id\n\n\nCopy and paste the Application ID from your \nAzure Active Directory\n \n \nApp Registrations\n \n your app registration's \nSettings\n \n \nProperties\n.\n\n\n\n\n\n\nPassword\n\n\nThis is your application key. You can generate it from your from your \nAzure Active Directory\n app registration's \nSettings\n \n \nKeys\n.\n\n\n\n\n\n\nApp Owner Tenant Id\n\n\nCopy and paste your Directory ID from your \nActive Directory\n \n \nProperties\n.\n\n\n\n\n\n\nSSH Public Key\n\n\nThe SSH key that you provided when launching Cloudbreak should be pre-entered.\n\n\n\n\n\n\nSelect Azure role type\n\n\nYou have the following options:\n\"Use existing Contributor role\" (default): If you select this option, Cloudbreak will use the \"\nContributor\n\" role to create resources. This requires no further input.\n\"Reuse existing custom role\": If you select this option and enter the name of an existing role, Cloudbreak will use this role to create resources.\n\"Let Cloudbreak create a custom role\": If you select this option and enter a name for the new role, the role will be created. When choosing role name, make sure that there is no existing role with the name chosen. For information on creating custom roles, refer to \nAzure\n documentation. \nIf using a custom role, make sure that it includes the necessary Action set for Cloudbreak to be able to manage clusters: \nMicrosoft.Compute/*\n, \nMicrosoft.Network/*\n, \nMicrosoft.Storage/*\n, \nMicrosoft.Resources/*\n.\n\n\n\n\n\n\n\n\nTo obtain the \nSubscription Id\n from Subscriptions: \n\n\n   \n\n\nTo obtain the \nApplication ID\n and an application key from Azure Active Directory: \n\n\n  \n\n\n      \n\n\nTo obtain the \nDirectory Id\n from Azure Active Directory: \n\n\n  \n\n\n\n\n\n\nClick \nCreate\n.\n\n\nCongratulations! You've successfully launched and configured Cloudbreak. Now you can use Cloudbreak to \ncreate clusters\n.\n\n\n\n\n\n\n\n\nNext: Create a Cluster", 
            "title": "Launch on Azure"
        }, 
        {
            "location": "/azure-launch/index.html#launching-cloudbreak-on-azure", 
            "text": "Before launching Cloudbreak on Azure, review and meet the prerequisites. Next, launch Cloudbreak using one of the two available methods. Once Cloudbreak is running, log in to the Cloudbreak UI and create a Cloudbreak credential.", 
            "title": "Launching Cloudbreak on Azure"
        }, 
        {
            "location": "/azure-launch/index.html#meet-the-prerequisites", 
            "text": "Before launching Cloudbreak on Azure, you must meet the following prerequisites.", 
            "title": "Meet the Prerequisites"
        }, 
        {
            "location": "/azure-launch/index.html#azure-account", 
            "text": "In order to launch Cloudbreak on the Azure Marketplace, log in to your existing Microsoft Azure account. If you don't have an account, you can set it up at  https://azure.microsoft.com .", 
            "title": "Azure Account"
        }, 
        {
            "location": "/azure-launch/index.html#azure-roles", 
            "text": "In order to provision clusters on Azure, Cloudbreak must be able to assume a sufficient Azure role (\"Owner\" or \"Contributor\") via Cloudbreak credential:     Your account must have the \" Owner \" role in the subscription in order to  create a Cloudbreak credential  using the interactive credential method.     Your account must have the \" Contributor \" role (or higher) in the subscription in order to  create a Cloudbreak credential  using the app-based credential method.     To check the roles that your subscription has, in your Azure account navigate to  Subscriptions .", 
            "title": "Azure Roles"
        }, 
        {
            "location": "/azure-launch/index.html#azure-region", 
            "text": "Decide in which Azure region you would like to launch Cloudbreak. You can launch Cloudbreak and provision your clusters in all regions  supported by Microsoft Azure .   Clusters created via Cloudbreak can be in the same or different region as Cloudbreak; when you launch a cluster, you select the region in which to launch it.", 
            "title": "Azure Region"
        }, 
        {
            "location": "/azure-launch/index.html#ssh-key-pair", 
            "text": "When launching Cloudbreak, you will be required to provide your public SSH key. If needed, you can generate a new SSH keypair:   On MacOS and Linux using  ssh-keygen -t rsa -b 4096 -C \"your_email@example.com\"     On Windows using  PuTTygen", 
            "title": "SSH Key Pair"
        }, 
        {
            "location": "/azure-launch/index.html#azure-data-lake-store-optional", 
            "text": "If you want to use  Azure Data Lake Store  with your cluster, you must create an\nAzure Data Lake Store account in the same subscription as you use for launching Cloudbreak and clusters. In the cluster creation phase, you will specify the created account name only, and access to the Azure Data Lake Store will be\nconfigured automatically.  You may also review the  Microsoft Azure Documentation  for instructions on how these configuration steps\nare manually done from the Azure portal. After the cluster is deployed, you must define which parts of the ADLS store this cluster\nwill have access and test access to ADLS. Refer to  Accessing Data  for more information.", 
            "title": "Azure Data Lake Store (Optional)"
        }, 
        {
            "location": "/azure-launch/index.html#launch-cloudbreak", 
            "text": "You have two options to launch Cloudbreak on Azure:   Launch via Azure Resource Manager Template  Launch from Azure Marketplace (Technical Preview)", 
            "title": "Launch Cloudbreak"
        }, 
        {
            "location": "/azure-launch/index.html#option-1-launch-via-azure-resource-manager-template", 
            "text": "Launch Cloudbreak deployer using the following steps.  Steps    Log in to your  Azure Portal .    Click here to get started with Cloudbreak installation using the Azure Resource Manager template:       The template for installing Cloudbreak will appear. On the  Basics  page, provide the following basic parameters:     BASICS     Parameter  Description      Subscription  (Required) Select which existing subscription you want to use.    Resource group  (Required) Select an existing resource group or create a new one by selecting  Create new  and entering a name for your new resource group. Cloudbreak resources will later be accessible in that chosen resource group.    Location  (Required) Select an Azure region in which you want to deploy Cloudbreak.     SETTINGS     Parameter  Description      Base Url  This is the URL to the page where the template is stored.    Location  This is an internal parameter. Do not change it.    VM Size  Select virtual machine instance type to use for the Cloudbreak controller. The minimum instance type suitable for Cloudbreak is  D2 .    Admin Username  Create an admin login that you will use to log in to the Cloudbreak UI. Must be a valid email address.    Admin User Password  (Required) Password for the admin login. Must be at least 8 characters containing letters, numbers, and symbols.    Username  Enter an admin username for the virtual machine. You will use it to SSH to the VM.    SmartSense  Select whether you want to use SmartSense telemetry. Default is \"false\" (not using SmartSense telemetry).    Remote Location  Enter a valid  CIDR IP  or use one of the default tags. Default value is  Internet  which allows access from all IP addresses. Examples:  10.0.0.0/24 will allow access from 10.0.0.0 through 10.0.0.255 'Internet' will allow access from all. This is not a secure option but you can use it it you are just getting started and are not planning to have the instance on for a longer period.  (Advanced) 'VirtualNetwork' will allow access from the address space of the Virtual Network.  (Advanced) 'AzureLoadBalancer' will allow access from the address space of the load balancer. For more information, refer to the  Azure documentation .    Ssh Key  (Required) Paste your SSH public key. You can use  pbcopy  to quickly copy it. For example:  pbcopy   /Users/homedir/.ssh/id_rsa.pub    Vnet New Or Existing  By default, Cloudbreak is launched in a new VNet called  cbdeployerVnet  and a new subnet called  cbdeployerSubnet ; if needed, you can customize the settings for the new VNet using available VNet and Subnet parameters.    Vnet Name  Provide the name for a new Vnet. Default is  `cbdeployerVnet .    Vnet Subnet Name  Provide a name for a new subnet. Default is  cbdeployerSubnet .    Vnet Address Prefix  Provide a CIDR for the virtual network. Default is  10.0.0.0/16 .    Vnet Subnet Address Prefix  Provide a CIDR for the subnet. Default is  10.0.0.0/24 .    Vnet RG Name  The name of the resource group in which the Vnet is located. If creating a new Vnet, enter the same resource group name as provided in the  Resource group  field in the  BASICS  section.       Review terms of use and check \"I agree to the terms and conditions stated above\".     Click  Purchase .    Proceed to the next step:  Explore Newly Created Resources .", 
            "title": "(Option 1) Launch via Azure Resource Manager Template"
        }, 
        {
            "location": "/azure-launch/index.html#option-2-launch-from-azure-marketplace", 
            "text": "Launch Cloudbreak deployer using the following steps.   This feature is Technical Preview.     Steps    Log in to your  Azure Portal .    From the services menu, select  .    In the search box, enter \"Cloudbreak\":          Select  . \n    The information about the Cloudbreak for Hortonworks Data Platform will be displayed.\n    In the bottom of the page, you will see a dropdown for selecting a deployment model, with the  Resource Manager  deployment model pre-selected. This is the only deployment model available for this offering.    Click  Create .       The template for installing Cloudbreak will appear. On the  Basics  page, provide the following basic parameters:     Parameter  Description      Administrator email address  Create an admin login that you will use to log in to the Cloudbreak UI. Must be a valid email address.    Administrator user password  Password for the admin login. Must be at least 8 characters containing letters, numbers, and symbols.    Confirm password  Confirm the password for the admin login.    VM Username  Enter an admin username for the virtual machine. You will use it to SSH to the VM.    VM SSH public key  Paste your SSH public key. You can use  pbcopy  to quickly copy it. For example:  pbcopy   /Users/homedir/.ssh/id_rsa.pub    Subscription  Select which existing subscription you want to use.    Resource group  Only  Create new  is supported. Select  Create new  to create a new resource group and enter a name for your new resource group. Cloudbreak resources will later be accessible in that chosen resource group.    Location  Select an Azure region in which you want to deploy Cloudbreak.       Once done, click  OK .    On the  Advanced Settings  page, provide the following advanced parameters:     Parameter  Description      Controller Instance Type  Select virtual machine instance type to use for the Cloudbreak. The minimum instance type suitable for Cloudbreak is  D2 .    Allow connections to the cloud controller from this address range or  default tag  Enter a valid  CIDR IP  or use one of the default tags such as   Internet . For example:  10.0.0.0/24 will allow access from 10.0.0.0 through 10.0.0.255 'Internet' will allow access from all. This is not a secure option but you can use it it you are just getting started and are not planning to have the instance on for a longer period.  (Advanced) 'VirtualNetwork' will allow access from the address space of the Virtual Network.  (Advanced) 'AzureLoadBalancer' will allow access from the address space of the load balancer. For more information, refer to the  Azure documentation .    Enable SmartSense  (Optional) Select whether to enable SmartSense telemetry. Default is  I have read and opt-in to SmartSense telemetry . SmartSense provides product telemetry and usage information to Hortonworks. For more information, refer to the  SmartSense  terms.    Virtual network  Create a new Vnet (default) or select an existing Vnet.    Subnets  If you created a new Vnet, create subnets within it. If selected an existing Vnet, select exiting subnets.       Once done, click  OK .    On the  Summary  page, validate the information that you provided.\n    Before proceeding to the next page, you have an option to  Download template and parameters . You can use them to launch Cloudbreak via CLI. Once done, click  OK .    Review terms of use and click  Purchase .    Proceed to the next step:  Explore Newly Created Resources .", 
            "title": "(Option 2) Launch from Azure Marketplace"
        }, 
        {
            "location": "/azure-launch/index.html#explore-newly-created-resources", 
            "text": "This step is optional.     While the deployment is in progress, you can optionally navigate to the newly created resource group and see what Azure resources are being created.  Steps    From the left pane, select  .    Find the the resource group that you just created and select it to view details.    The following resources should have been created in your resource group:    If you chose to use an existing virtual network, the virtual network will not be added to the resource group.     Virtual network  (VNet) securely connects Azure resources to each other.      Network security group  (NSG) defines inbound and outbound security rules, which control network traffic flow.    Virtual machine  runs Cloudbreak.     Public IP address  is assigned to your VM so that it can communicate with other Azure resources.    Network interface  (NIC) attached to the VM provides the interconnection between the VM and the underlying software network.      Blob storage container  is created to store Cloudbreak Deployer OS disk's data.       You can click on each entry to view details of the resource. For example, click on   to view details, including Cloudbreak IP address.    Once your deployment is ready, the status will change from \"Deploying\" to \"Success\".", 
            "title": "Explore Newly Created Resources"
        }, 
        {
            "location": "/azure-launch/index.html#access-cloudbreak-ui", 
            "text": "Log in to the Cloudbreak UI using the following steps.  Steps    When your deployment succeeds, you will receive a notification in the top-right corner. You can click on the link provided to navigate to the resource group created earlier.   This only works right after deployment. At other times, you can find your resource group by selecting  Resource Groups  from the service menu and then finding your resource group by name.     Once you've navigated to your resource group, click on  Deployments  and then click on  hortonworks.cloudbreal-for-hortonworks-data-platf-... :       From  Outputs , you can copy the link by clicking on the   icon:         Paste the link in your browser's address bar.  The first time you access Cloudbreak UI, Cloudbreak will automatically generate a self-signed certificate, due to which your browser will warn you about an untrusted connection and will ask you to confirm a security exception. You can safely proceed to the website.      Browser  Steps      Firefox  Click  Advanced    Click  Add Exception...    Click  Confirm Security Exception    Safari  Click  Continue    Chrome  Click  Advanced    Click  Proceed...       Now you should be able to access Cloudbreak UI and log in with the  Admin email address  and  Admin password  that you created when launching Cloudbreak:   The last task that you need to perform before you can use Cloudbreak is to  create a cloudbreak credential .", 
            "title": "Access Cloudbreak UI"
        }, 
        {
            "location": "/azure-launch/index.html#create-cloudbreak-credential", 
            "text": "Before you can start creating clusters, you must first create a  Cloudbreak credential . Without this credential, you will not be able to create clusters via Cloudbreak. Cloudbreak works by connecting your Azure account through this credential, and then uses it to create resources on your behalf.  There are two ways to create a Cloudbreak credential:    Interactive : This is the recommended simpler method. The advantage of using this method is that the app and service principal creation and role assignment is fully automated, so the only input that you need to provide is the name and the SSH key. To configure an interactive credential, refer to  Create an Interactive Credential .      App-based : This is the more complex method. The advantage of the app-based credential creation is that it allows you to create a credential without logging in to the Azure account, as long as you have been given all the information. In addition to providing your  tenant_id  and  subscription_id , you must provide information for your previously created Azure AD application, including its password. To configure an app based credential, refer to  Create an App Based Credential . Alternatively, when using this method, you use a utility called  azure-cli-tools . The utility supports app creation and role assignment. It is available at  https://github.com/sequenceiq/azure-cli-tools/blob/master/cli_tools .", 
            "title": "Create Cloudbreak Credential"
        }, 
        {
            "location": "/azure-launch/index.html#create-an-interactive-credential", 
            "text": "Steps    In the Cloudbreak web UI, select  Credentials  from the left pane.     Click  Create Credential .     Under  Cloud provider , select \"Microsoft Azure\".     Select  Interactive Login :           Provide the following information:     Parameter  Description      Name  Enter a name for your credential.    Description  (Optional) Enter a description.    Subscription Id  Copy and paste the Subscription ID from your  Subscriptions .    Tenant Id  Copy and paste your Directory ID from your  Active Directory     Properties .    Azure role type  You have the following options: \"Use existing Contributor role\" (default): If you select this option, Cloudbreak will use the \" Contributor \" role to create resources. This requires no further input. \"Reuse existing custom role\": If you select this option and enter the name of an existing role, Cloudbreak will use this role to create resources. \"Let Cloudbreak create a custom role\": If you select this option and enter a name for the new role, the role will be created. When choosing role name, make sure that there is no existing role with the name chosen. For information on creating custom roles, refer to  Azure  documentation.  If using a custom role, make sure that it includes the necessary Action set for Cloudbreak to be able to manage clusters:  Microsoft.Compute/* ,  Microsoft.Network/* ,  Microsoft.Storage/* ,  Microsoft.Resources/* .     To obtain the  Subscription Id :        To obtain the  Directory Id :           After providing the parameters, click  Interactive Login .    Copy the code provided in the UI:           Click  Azure login  and a new  Device login  page will open in a new browser tab:        Next, paste the code in field on the   Device login  page and click  Continue .    Confirm your account by selecting it:     A confirmation page will appear, confirming that you have signed in to the Microsoft Azure Cross-platform Command Line Interface application on your device. You may now close this window.  Congratulations! You've successfully launched and configured Cloudbreak. Now you can use Cloudbreak to  create clusters .", 
            "title": "Create an Interactive Credential"
        }, 
        {
            "location": "/azure-launch/index.html#create-an-app-based-credential", 
            "text": "Steps    On Azure Portal, navigate to the  Active Directory     App Registrations  and register a new application. For more information, refer to  Create an Azure AD Application .        Navigate to the  Subscriptions , choose  Access control (IAM) . Click  Add  and then assign the \"Contributor\" role to your newly created application by selecting \"Contributor\" under  Role  and your app name under  Select :         In the Cloudbreak web UI, select  Credentials  from the left pane.        Click  Create Credential .     Under  Cloud provider , select \"Microsoft Azure\".     Select  App based Login :    On the  Configure credential  page, provide the following parameters:     Parameter  Description      Select Credential Type  Select  App based .    Name  Enter a name for your credential.    Description  (Optional) Enter a description.    Subscription Id  Copy and paste the Subscription ID from your  Subscriptions .    App Id  Copy and paste the Application ID from your  Azure Active Directory     App Registrations    your app registration's  Settings     Properties .    Password  This is your application key. You can generate it from your from your  Azure Active Directory  app registration's  Settings     Keys .    App Owner Tenant Id  Copy and paste your Directory ID from your  Active Directory     Properties .    SSH Public Key  The SSH key that you provided when launching Cloudbreak should be pre-entered.    Select Azure role type  You have the following options: \"Use existing Contributor role\" (default): If you select this option, Cloudbreak will use the \" Contributor \" role to create resources. This requires no further input. \"Reuse existing custom role\": If you select this option and enter the name of an existing role, Cloudbreak will use this role to create resources. \"Let Cloudbreak create a custom role\": If you select this option and enter a name for the new role, the role will be created. When choosing role name, make sure that there is no existing role with the name chosen. For information on creating custom roles, refer to  Azure  documentation.  If using a custom role, make sure that it includes the necessary Action set for Cloudbreak to be able to manage clusters:  Microsoft.Compute/* ,  Microsoft.Network/* ,  Microsoft.Storage/* ,  Microsoft.Resources/* .     To obtain the  Subscription Id  from Subscriptions:        To obtain the  Application ID  and an application key from Azure Active Directory:               To obtain the  Directory Id  from Azure Active Directory:         Click  Create .  Congratulations! You've successfully launched and configured Cloudbreak. Now you can use Cloudbreak to  create clusters .     Next: Create a Cluster", 
            "title": "Create an App Based Credential"
        }, 
        {
            "location": "/azure-create/index.html", 
            "text": "Create a Cluster on Azure\n\n\nOptional Prerequisites\n\n\nIf you are just getting started with Cloudbreak, follow the steps below to create a cluster using the default blueprints. \n\n\nIf you are already familiar with Cloudbreak and you want to use custom blueprints, refer to the \nBlueprints\n documentation to customize your blueprints.\n\n\nBasic Options\n\n\nUse these steps to create a cluster.\n\n\nSteps\n\n\n\n\n\n\nLog in to the Cloudbreak UI.\n\n\n\n\n\n\nClick \nCreate Cluster\n and the \nCreate Cluster\n form is displayed.\n\n\n\n\n\n\nOn the \nGeneral Configuration\n page, provide the following parameters:\n\n\n\n\nTo view advanced options, click \nAdvanced\n. To learn about advanced options, refer to \nAdvanced Options\n.\n\n\n\n\n\n\n\n\n\n\nParameter\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nSelect Credential\n\n\nSelect a previously created credential.\n\n\n\n\n\n\nCluster Name\n\n\nEnter a name for your cluster. The name must be between 5 and 40 characters, must start with a letter, must only include lowercase letters, numbers, and hyphens.\n\n\n\n\n\n\nTags\n\n\n(Optional) You can optionally add tags, which will help you find your cluster-related resources, such as VMs, in your cloud provider account.\n\n\n\n\n\n\nRegion\n\n\nSelect the region in which you would like to launch your cluster.\n\n\n\n\n\n\nSend Email When Cluster is Ready\n\n\n(Optional) Check this to receive an email each time the cluster status changes.\n\n\n\n\n\n\n\n\n\n\nBy default, Ambari Username and Ambari Password are set to \nadmin\n. You can override it in the \"\nConfigure Cluster\n\" tab.\n\n\n\n\n\n\n\n\nOn the \nHardware and Storage\n page, select the blueprint that you would like to use for your cluster. You can either choose one of the pre-configured blueprints, or add your own in the \nBlueprints\n tab.\n\n\nFor each host group you must provide the following:\n\n\n\n\n\n\n\n\nParameter\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nGroup Size\n\n\nEnter a number defining how many nodes to create per host group. Default is 1. The \"Group Size\" for that host group on which Ambari Server is installed must be set to \"1\".\n\n\n\n\n\n\nTemplate\n\n\nIf you have previously created a template for VMs and storage, you can select it here. If you don't make a selection, default will be used.\n\n\n\n\n\n\nSecurity Group\n\n\nIf you have previously created a template for a security group, you can select it here. If you don't make a selection, default will be used.\n\n\n\n\n\n\nAmbari Server\n\n\nYou must select one node for Ambari Server. The \"Group Size\" for that host group must be set to \"1\".\n\n\n\n\n\n\nRecipes\n\n\nYou can select a previously added recipe (custom script) to be executed on all nodes of the host group. Refer to \nRecipes\n.\n\n\n\n\n\n\n\n\n\n\n\n\nOn the \nFile System\n page, select to use one of the following filesystems:\n\n\n\n\nLocal HDFS\n: No external storage outside of HDFS will be used.\n\n\n\n\nWindows Azure Data Lake Storage\n: If you select this option, HDFS will be used as your default file system and access to ADLS will be through the adl connector. You must provide:\n\n\n\n\n\n\n\n\nParameter\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nData Lake Store account name\n\n\nEnter your account name.\n\n\n\n\n\n\n\n\nPostrequisites\n: After cluster installation, you must perform additional steps described in \nConfiguring Access to ADLS\n. \n\n\n\n\n\n\nWindows Azure Blob Storage\n: If you select this option, HDFS will be used as your default file system and access to WASB will be through the wasb connector, unless you select \nUse File System As Default\n. You must provide:\n\n\n\n\n\n\n\n\nParameter\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nStorage Account Name\n\n\nEnter your account name.\n\n\n\n\n\n\nStorage Account Access Key\n\n\nEnter your access key.\n\n\n\n\n\n\nUse File System As Default\n\n\nSelect this option if you want to make WASB your default file system, instead of HDFS.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nOn the \nNetwork\n page, provide the following parameters:\n\n\n\n\n\n\n\n\nParameter\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nNetwork\n\n\nSelect the virtual network in which you would like your cluster to be provisioned. You can define custom network configurations or use default network configurations.\n\n\n\n\n\n\nEnable Knox Gateway\n\n\n(Optional) Select this option to enable secure access to Ambari web UI and other cluster UIs via Knox gateway.\n\n\n\n\n\n\nEnable Kerberos Security\n\n\n(Optional) Select this option to enable Kerberos for your cluster. You will have an option to create a new kerberos or use an existing one. For more information refer to Kerberos \ndocumentation\n.\n\n\n\n\n\n\n\n\n\n\n\n\nOn the \nSecurity\n page, provide the following parameters:\n\n\n\n\n\n\nClick on \nCreate Cluster\n to create a cluster.\n\n\n\n\n\n\nYou will be redirected to the Cloudbreak dashboard, and a new tile representing your cluster will appear at the top of the page.\n\n\n\n\n\n\nAdvanced Options\n\n\nClick on \nShow Advanced Options\n to enter additional configuration options.\n\n\nGeneral Configuration\n\n\nYou can optionally configure the following advanced parameters:\n\n\n\n\n\n\n\n\nParameter\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nAmbari Username\n\n\nYou can log in to the Ambari UI using this username. By default, this is set to \nadmin\n.\n\n\n\n\n\n\nAmbari Password\n\n\nYou can log in to the Ambari UI using this password. By default, this is set to \nadmin\n.\n\n\n\n\n\n\nProvision Cluster\n\n\nSALT\n is pre-selected to provision your cluster.\n\n\n\n\n\n\nEnable availability sets\n\n\nAzure implements the concept of \navailability sets\n to support fault tolerance for VM's. You can enable availability sets by using the checkbox, and then add the desired availability sets by providing a name and the desired fault domain count (2 or 3). Next, these defined availability sets can be assigned to specific host groups in the \nChoose Blueprint\n tab. For more information about availability sets, refer to \nAvailability Sets\n.\n\n\n\n\n\n\nEnable Lifetime Management\n\n\nCheck this option if you would like your cluster to be automatically terminated after a specific amount of time (defined as \"Time to Live\" in minuter) has passed.\n\n\n\n\n\n\nFlex Subscription\n\n\nThis option will appear if you have configured your deployment for a \nFlex Subscription\n.\n\n\n\n\n\n\n\n\nHardware and Storage\n\n\nAfter selecting a blueprint, you can optionally configure the following advanced parameters:\n\n\n\n\n\n\n\n\nParameter\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nConfig Recommendation Strategy (Stack Advisor)\n\n\nSelect how configuration recommendations generated by stack advisor will be applied. Select one of \nALWAYS_APPLY: Configuration recommendations will be applied automatically.\nALWAYS_APPLY_DONT_OVERRIDE_CUSTOM_VALUES: Configuration recommendations will be applied automatically, but they will be ignored for custom configurations.\nNEVER_APPLY: Configuration recommendations will be ignored.\nONLY_STACK_DEFAULTS_APPLY: Configuration recommendations will be applied only on the default configurations for all included services.\n\n\n\n\n\n\nValidate Blueprint\n\n\nSelect to validate the blueprint.\n\n\n\n\n\n\n\n\nAdd File System\n\n\nAfter selecting the filesystem, you can optionally configure the following advanced parameters:\n\n\n\n\n\n\n\n\nParameter\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nAttached Storage Type\n\n\nSelect \nsingle storage for all VMs\n or \nseparate storage for every VM\n. Selecting single storage means that your whole cluster's OS disks will be placed in one storage account. Using separate storage for every VM will deploy as many storage accounts as the number of nodes in your cluster, avoiding the IOPS limit of a particular storage account.\n\n\n\n\n\n\nPersistent Storage Name\n\n\nEnter a name for the persistent storage directory. Default is \ncbstore\n.\n\n\n\n\n\n\n\n\nChoose Failure Action\n\n\nYou can optionally select what to do if cluster creation fails or if there aren't enough instances available to create all requested nodes:\n\n\n\n\n\n\n\n\nParameter\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nFailure Action\n\n\nSelect one of: \ndo NOT rollback resources\n (default) or \nrollback resources\n. \nBy default, if creating a cluster fails, the Azure resources that were created up to that point will not be rolled back. This means that they will remain accessible for troubleshooting and you will need to to delete them manually.\n\n\n\n\n\n\nMinimum Cluster Size\n\n\nThis defines the provisioning strategy in case the cloud provider cannot allocate all the requested nodes. Select \nbest effort\n or \nexact\n.\n\n\n\n\n\n\n\n\nConfigure Ambari Repos\n\n\nYou can optionally configure a different version of Ambari than the default by providing the following information:\n\n\n\n\n\n\n\n\nParameter\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nAmbari Version\n\n\nEnter Ambari version.\n\n\n\n\n\n\nAmbari Repo URL\n\n\nEnter Ambari repo URL.\n\n\n\n\n\n\nAmbari Repo Gpg Key URL\n\n\nEnter gpgkey URL.\n\n\n\n\n\n\n\n\nConfigure HDP Repos\n\n\nYou can optionally configure a different version of HDP than the default by providing the following information:\n\n\n\n\n\n\n\n\nParameter\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nStack\n\n\nEnter stack name.\n\n\n\n\n\n\nVersion\n\n\nEnter stack version.\n\n\n\n\n\n\nStack Repo ID\n\n\nEnter stack repo ID.\n\n\n\n\n\n\nBase URL\n\n\nEner stack repo base URL.\n\n\n\n\n\n\nUtils Repo ID\n\n\nEnter Utils repo ID.\n\n\n\n\n\n\nUtils Base URL\n\n\nEnter Utils repo base URL.\n\n\n\n\n\n\nVerify\n\n\nSelect to verify the repo information.\n\n\n\n\n\n\n\n\nConfigure Ambari Database\n\n\nBy default, Ambari stores data on an embedded database, which is sufficient for ephemeral or test clusters. However, as Ambari and Cloudbreak don't perform backups of this database, it is insufficient for long-running production clusters, and you may need to configure a remote database for Ambari and Cloudbreak.\n\n\n\n\n\n\n\n\nParameter\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nVendor\n\n\nSelect database vendor from the list.\n\n\n\n\n\n\nHost\n\n\nEnter database host IP.\n\n\n\n\n\n\nPort\n\n\nEnter port number.\n\n\n\n\n\n\nName\n\n\nEnter database name.\n\n\n\n\n\n\nUser Name\n\n\nEnter database user name.\n\n\n\n\n\n\nPassword\n\n\nEnter database password.\n\n\n\n\n\n\n\n\nAvailability Sets\n\n\nTo support fault tolerance for VMs, Azure introduced the concept of \navailability sets\n. This allows two or more VMs to be mapped to multiple fault domains, each of which defines a group of virtual machines that share a common power source and a network switch. When adding VMs to an availability set, Azure automatically assigns each VM a fault domain. This SLA includes guarantees that during OS Patching in Azure or during maintenance operations, at least one VM belonging to a given fault domain will be available.\n\n\nIn Cloudbreak UI, availability sets can be configured during cluster creation:\n\n\n\n\n\n\nEnable availability sets using the checkbox on the \nConfigure Cluster\n page. \n\n\n\n\n\n\nAdd the desired availability sets by providing a name and the desired fault domain count (2 or 3).\n\n\n\n\n\n\nThe sets defined here can be assigned to the host groups on the \nChoose Blueprint\n page. One availability set can be assigned to only one host group, so you should define in advance as many availability sets as needed for your host groups. The assignment of fault domains is automated by Azure, so there is no option for this in Cloudbreak UI.\n\n\n\n\nIMPORTANT\n: The availability sets should only be used when there is a group of two or more application-tier VMs. Single instances placed in an availability set are not subject to Azure\u2019s SLA, and you will not receive warnings of planned maintenance events. \n\n\n\n\n\n\n\n\nAfter the deployment is finished, you can check the layout of the VMs inside an availability set on Azure Portal. You will find the \"Availability set\" resources corresponding to the host groups inside the deployment's resource group. \n\n\n\n\n\n\n\n\nNext: Access Cluster", 
            "title": "Create a Cluster"
        }, 
        {
            "location": "/azure-create/index.html#create-a-cluster-on-azure", 
            "text": "", 
            "title": "Create a Cluster on Azure"
        }, 
        {
            "location": "/azure-create/index.html#optional-prerequisites", 
            "text": "If you are just getting started with Cloudbreak, follow the steps below to create a cluster using the default blueprints.   If you are already familiar with Cloudbreak and you want to use custom blueprints, refer to the  Blueprints  documentation to customize your blueprints.", 
            "title": "Optional Prerequisites"
        }, 
        {
            "location": "/azure-create/index.html#basic-options", 
            "text": "Use these steps to create a cluster.  Steps    Log in to the Cloudbreak UI.    Click  Create Cluster  and the  Create Cluster  form is displayed.    On the  General Configuration  page, provide the following parameters:   To view advanced options, click  Advanced . To learn about advanced options, refer to  Advanced Options .      Parameter  Description      Select Credential  Select a previously created credential.    Cluster Name  Enter a name for your cluster. The name must be between 5 and 40 characters, must start with a letter, must only include lowercase letters, numbers, and hyphens.    Tags  (Optional) You can optionally add tags, which will help you find your cluster-related resources, such as VMs, in your cloud provider account.    Region  Select the region in which you would like to launch your cluster.    Send Email When Cluster is Ready  (Optional) Check this to receive an email each time the cluster status changes.      By default, Ambari Username and Ambari Password are set to  admin . You can override it in the \" Configure Cluster \" tab.     On the  Hardware and Storage  page, select the blueprint that you would like to use for your cluster. You can either choose one of the pre-configured blueprints, or add your own in the  Blueprints  tab.  For each host group you must provide the following:     Parameter  Description      Group Size  Enter a number defining how many nodes to create per host group. Default is 1. The \"Group Size\" for that host group on which Ambari Server is installed must be set to \"1\".    Template  If you have previously created a template for VMs and storage, you can select it here. If you don't make a selection, default will be used.    Security Group  If you have previously created a template for a security group, you can select it here. If you don't make a selection, default will be used.    Ambari Server  You must select one node for Ambari Server. The \"Group Size\" for that host group must be set to \"1\".    Recipes  You can select a previously added recipe (custom script) to be executed on all nodes of the host group. Refer to  Recipes .       On the  File System  page, select to use one of the following filesystems:   Local HDFS : No external storage outside of HDFS will be used.   Windows Azure Data Lake Storage : If you select this option, HDFS will be used as your default file system and access to ADLS will be through the adl connector. You must provide:     Parameter  Description      Data Lake Store account name  Enter your account name.     Postrequisites : After cluster installation, you must perform additional steps described in  Configuring Access to ADLS .     Windows Azure Blob Storage : If you select this option, HDFS will be used as your default file system and access to WASB will be through the wasb connector, unless you select  Use File System As Default . You must provide:     Parameter  Description      Storage Account Name  Enter your account name.    Storage Account Access Key  Enter your access key.    Use File System As Default  Select this option if you want to make WASB your default file system, instead of HDFS.         On the  Network  page, provide the following parameters:     Parameter  Description      Network  Select the virtual network in which you would like your cluster to be provisioned. You can define custom network configurations or use default network configurations.    Enable Knox Gateway  (Optional) Select this option to enable secure access to Ambari web UI and other cluster UIs via Knox gateway.    Enable Kerberos Security  (Optional) Select this option to enable Kerberos for your cluster. You will have an option to create a new kerberos or use an existing one. For more information refer to Kerberos  documentation .       On the  Security  page, provide the following parameters:    Click on  Create Cluster  to create a cluster.    You will be redirected to the Cloudbreak dashboard, and a new tile representing your cluster will appear at the top of the page.", 
            "title": "Basic Options"
        }, 
        {
            "location": "/azure-create/index.html#advanced-options", 
            "text": "Click on  Show Advanced Options  to enter additional configuration options.", 
            "title": "Advanced Options"
        }, 
        {
            "location": "/azure-create/index.html#general-configuration", 
            "text": "You can optionally configure the following advanced parameters:     Parameter  Description      Ambari Username  You can log in to the Ambari UI using this username. By default, this is set to  admin .    Ambari Password  You can log in to the Ambari UI using this password. By default, this is set to  admin .    Provision Cluster  SALT  is pre-selected to provision your cluster.    Enable availability sets  Azure implements the concept of  availability sets  to support fault tolerance for VM's. You can enable availability sets by using the checkbox, and then add the desired availability sets by providing a name and the desired fault domain count (2 or 3). Next, these defined availability sets can be assigned to specific host groups in the  Choose Blueprint  tab. For more information about availability sets, refer to  Availability Sets .    Enable Lifetime Management  Check this option if you would like your cluster to be automatically terminated after a specific amount of time (defined as \"Time to Live\" in minuter) has passed.    Flex Subscription  This option will appear if you have configured your deployment for a  Flex Subscription .", 
            "title": "General Configuration"
        }, 
        {
            "location": "/azure-create/index.html#hardware-and-storage", 
            "text": "After selecting a blueprint, you can optionally configure the following advanced parameters:     Parameter  Description      Config Recommendation Strategy (Stack Advisor)  Select how configuration recommendations generated by stack advisor will be applied. Select one of  ALWAYS_APPLY: Configuration recommendations will be applied automatically. ALWAYS_APPLY_DONT_OVERRIDE_CUSTOM_VALUES: Configuration recommendations will be applied automatically, but they will be ignored for custom configurations. NEVER_APPLY: Configuration recommendations will be ignored. ONLY_STACK_DEFAULTS_APPLY: Configuration recommendations will be applied only on the default configurations for all included services.    Validate Blueprint  Select to validate the blueprint.", 
            "title": "Hardware and Storage"
        }, 
        {
            "location": "/azure-create/index.html#add-file-system", 
            "text": "After selecting the filesystem, you can optionally configure the following advanced parameters:     Parameter  Description      Attached Storage Type  Select  single storage for all VMs  or  separate storage for every VM . Selecting single storage means that your whole cluster's OS disks will be placed in one storage account. Using separate storage for every VM will deploy as many storage accounts as the number of nodes in your cluster, avoiding the IOPS limit of a particular storage account.    Persistent Storage Name  Enter a name for the persistent storage directory. Default is  cbstore .", 
            "title": "Add File System"
        }, 
        {
            "location": "/azure-create/index.html#choose-failure-action", 
            "text": "You can optionally select what to do if cluster creation fails or if there aren't enough instances available to create all requested nodes:     Parameter  Description      Failure Action  Select one of:  do NOT rollback resources  (default) or  rollback resources .  By default, if creating a cluster fails, the Azure resources that were created up to that point will not be rolled back. This means that they will remain accessible for troubleshooting and you will need to to delete them manually.    Minimum Cluster Size  This defines the provisioning strategy in case the cloud provider cannot allocate all the requested nodes. Select  best effort  or  exact .", 
            "title": "Choose Failure Action"
        }, 
        {
            "location": "/azure-create/index.html#configure-ambari-repos", 
            "text": "You can optionally configure a different version of Ambari than the default by providing the following information:     Parameter  Description      Ambari Version  Enter Ambari version.    Ambari Repo URL  Enter Ambari repo URL.    Ambari Repo Gpg Key URL  Enter gpgkey URL.", 
            "title": "Configure Ambari Repos"
        }, 
        {
            "location": "/azure-create/index.html#configure-hdp-repos", 
            "text": "You can optionally configure a different version of HDP than the default by providing the following information:     Parameter  Description      Stack  Enter stack name.    Version  Enter stack version.    Stack Repo ID  Enter stack repo ID.    Base URL  Ener stack repo base URL.    Utils Repo ID  Enter Utils repo ID.    Utils Base URL  Enter Utils repo base URL.    Verify  Select to verify the repo information.", 
            "title": "Configure HDP Repos"
        }, 
        {
            "location": "/azure-create/index.html#configure-ambari-database", 
            "text": "By default, Ambari stores data on an embedded database, which is sufficient for ephemeral or test clusters. However, as Ambari and Cloudbreak don't perform backups of this database, it is insufficient for long-running production clusters, and you may need to configure a remote database for Ambari and Cloudbreak.     Parameter  Description      Vendor  Select database vendor from the list.    Host  Enter database host IP.    Port  Enter port number.    Name  Enter database name.    User Name  Enter database user name.    Password  Enter database password.", 
            "title": "Configure Ambari Database"
        }, 
        {
            "location": "/azure-create/index.html#availability-sets", 
            "text": "To support fault tolerance for VMs, Azure introduced the concept of  availability sets . This allows two or more VMs to be mapped to multiple fault domains, each of which defines a group of virtual machines that share a common power source and a network switch. When adding VMs to an availability set, Azure automatically assigns each VM a fault domain. This SLA includes guarantees that during OS Patching in Azure or during maintenance operations, at least one VM belonging to a given fault domain will be available.  In Cloudbreak UI, availability sets can be configured during cluster creation:    Enable availability sets using the checkbox on the  Configure Cluster  page.     Add the desired availability sets by providing a name and the desired fault domain count (2 or 3).    The sets defined here can be assigned to the host groups on the  Choose Blueprint  page. One availability set can be assigned to only one host group, so you should define in advance as many availability sets as needed for your host groups. The assignment of fault domains is automated by Azure, so there is no option for this in Cloudbreak UI.   IMPORTANT : The availability sets should only be used when there is a group of two or more application-tier VMs. Single instances placed in an availability set are not subject to Azure\u2019s SLA, and you will not receive warnings of planned maintenance events.      After the deployment is finished, you can check the layout of the VMs inside an availability set on Azure Portal. You will find the \"Availability set\" resources corresponding to the host groups inside the deployment's resource group.      Next: Access Cluster", 
            "title": "Availability Sets"
        }, 
        {
            "location": "/azure-clusters-access/index.html", 
            "text": "Accessing Your Cluster\n\n\nThe following section describes how to access the various services in the cluster.\n\n\nFinding Cluster Details\n\n\nOnce your cluster is up and running, you can find information about the cluster on the cluster details page in the Cloudbreak UI. To access cluster details page, click on the tile representing your cluster in the Cloudbreak UI. The information presented includes:\n\n\n\n\nCluster status\n\n\nEvent history \n\n\nCluster instance public IP addresses\n\n\nUI links \n\n\n\n\nAccess Cluster via SSH\n\n\nIf you plan to access the cluster via the command line clients, SSH into the master node instance in the cluster. \n\n\n\n\nIn order to use SSH, you must generate an SSH key pair or use an existing SSH keypair.  \n\n\nYou can find the cluster instance public IP addresses on the cluster details page.  \n\n\nWhen accessing instances via SSH use the \ncloudbreak\n user. \n\n\n\n\nOn Mac OS, you can use the following syntax to SSH to the VM:\n\nssh -i \"privatekey.pem\" cloudbreak@publicIP\n\nFor example:\n\nssh -i \"dominika-kp.pem\" cloudbreak@p52.25.169.132\n\n\nOn Windows, you can SSH using an SSH client such as PuTTY.", 
            "title": "Access Cluster"
        }, 
        {
            "location": "/azure-clusters-access/index.html#accessing-your-cluster", 
            "text": "The following section describes how to access the various services in the cluster.", 
            "title": "Accessing Your Cluster"
        }, 
        {
            "location": "/azure-clusters-access/index.html#finding-cluster-details", 
            "text": "Once your cluster is up and running, you can find information about the cluster on the cluster details page in the Cloudbreak UI. To access cluster details page, click on the tile representing your cluster in the Cloudbreak UI. The information presented includes:   Cluster status  Event history   Cluster instance public IP addresses  UI links", 
            "title": "Finding Cluster Details"
        }, 
        {
            "location": "/azure-clusters-access/index.html#access-cluster-via-ssh", 
            "text": "If you plan to access the cluster via the command line clients, SSH into the master node instance in the cluster.    In order to use SSH, you must generate an SSH key pair or use an existing SSH keypair.    You can find the cluster instance public IP addresses on the cluster details page.    When accessing instances via SSH use the  cloudbreak  user.    On Mac OS, you can use the following syntax to SSH to the VM: ssh -i \"privatekey.pem\" cloudbreak@publicIP \nFor example: ssh -i \"dominika-kp.pem\" cloudbreak@p52.25.169.132  On Windows, you can SSH using an SSH client such as PuTTY.", 
            "title": "Access Cluster via SSH"
        }, 
        {
            "location": "/azure-data/index.html", 
            "text": "Accessing Data on Azure\n\n\nHortonworks Data Platform (HDP) supports reading and writing both block blobs and page blobs\nfrom/to \nWindows Azure Storage Blob (WASB)\n object store, as well as reading and writing files stored in an\n\nAzure Data Lake Storage (ADLS)\n account. This allows you to:\n\n\n\n\nPersist data using cloud storage services beyond the lifetime of your HDP clusters.  \n\n\nLoad data in Hadoop ecosystem applications directly from Azure storage services, without first importing or uploading data from external resources to HDFS.  \n\n\nUse other applications (not necessarily in your Hadoop ecosystem) to manipulate the data stored in Azure storage services beyond the lifetime of your HDP clusters.  \n\n\nShare data between multiple HDP clusters fast and easily by pointing to the same Azure data sets. \n\n\nMove or copy data between different Azure storage services or between Azure storage services and HDFS to facilitate different scenarios for big data analytics workloads.  \n\n\nBack up unlimited archive data at any scale from HDP cluster to fully managed, durable, and highly available Azure storage services.   \n\n\n\n\nAccessing Data in ADLS\n\n\nAzure Data Lake Store (ADLS)\n is an enterprise-wide hyper-scale repository for big data analytic workloads.\n\n\nPrerequisites\n\n\nIf you want to use \nAzure Data Lake Store\n to store your data, you must enable Azure subscription for Data Lake Store, and then create an Azure Data Lake Store \nstorage account\n.\n\n\nConfiguring Access to ADLS\n\n\nADLS is not supported as a default file system, but access to data in ADLS via the adl connector is configured if you select ADLS during \ncluster creation\n, on the \nAdd File System\n page. This option automates the configuration of the cluster with ADLS with the exception of the last step. \n\n\nAfter the \ncluster is deployed\n, you must perform the following steps manually to define which parts of the ADLS store this cluster will have access by adding the client credentials for the cluster to the data access control for the ADLS account. \n\n\nThis last configuration option should not be automated, since you need to select which files in ADLS the cluster should access. For more information, review the \ndocumentation\n on the Microsoft Azure Portal.  \n\n\nSteps\n\n\n\n\nIn the Cloudbreak UI, navigate to the \nManage Credentials\n section, select the credential used to deploy your cluster, and copy the \"App Id\u201d associated with it.   \n\n\nNavigate to the Azure Portal \n \nData Lake Store\n and select your account.\n\n\nSelect the \nData Explorer\n tab at the top and select the folder that you want the cluster to access. Select the root folder for full access. \n\n\nSelect the \nAccess\n tab at the top and then click \nAdd\n.\n\n\nClick \n+Add\n and paste the App Id (copied in step 1) in the search box to find client certificate name for the cluster.  (Alternatively, you can look up the App Name in the \nAzure Active Directory\n \n \nApp Registrations\n).\n\n\nChoose the appropriate permissions. Note that if you do select the root folder, you need to provide \u201cexecute\u201d access to all parent directories. For more information, refer to the \nAzure documentation\n. \n\n\nTest access to ADLS. Review the next sections for information on how to access data in ADLS from the cluster once it is deployed, for example from the command line of the cluster name node.\n\n\n\n\nAccess Path\n\n\nADLS access path syntax is:\n\n\nadl://\naccount_name\n.azuredatalakestore.net/\ndir/file\n\n\n\nFor example, the following Hadoop FileSystem shell commands demonstrate access to a storage account named \"myaccount\":\n\n\nhadoop fs -mkdir adl://myaccount.azuredatalakestore.net/testdir\n\n\n\nhadoop fs -put testfile adl://myaccount.azuredatalakestore.net/testdir/testfile\n\n\n\nTo use DistCp against ADLS, use the following syntax:\n\nhadoop distcp\n    [-D hadoop.security.credential.provider.path=localjceks://file/home/user/adls.jceks]\n    hdfs://\nnamenode_hostname\n:9001/user/foo/007020615\n    adl://\nmyaccount\n.azuredatalakestore.net/testDir/\n\n\nAccessing Data in WASB\n\n\nWindows Azure Storage Blob (WASB) is an object store service available on Azure.\n\n\nPrerequisites\n\n\nIf you want to use Windows Azure Storage Blob to store your data, you must enable Azure subscription for Blob Storage, and then create a \nstorage account\n.  \n\n\nConfiguring Access to WASB\n\n\nIn order to access data stored in your Azure blob storage account, you must configure your storage account access key in \ncore-site.xml\n. The configuration property that you must use is \nfs.azure.account.key.\naccount name\n.blob.core.windows.net\n and the value is the access key. \n\n\nFor example the following property should be used for a storage account called \"testaccount\": \n\n\nproperty\n\n  \nname\nfs.azure.account.key.testaccount.blob.core.windows.net\n/name\n\n  \nvalue\nTESTACCOUNT-ACCESS-KEY\n/value\n\n\n/property\n\n\n\n\n\nYou can obtain your access key from the Access keys in your storage account settings.\n\n\nAlternatively, it is possible, although not recommended or supported, to configure \nfs.defaultFS\n to use a wasb or wasbs URL. This causes all bare paths, such as /testDir/testFile to resolve automatically to that file system.\n\n\nAccess Path\n\n\nWASB access path syntax is:\n\n\nwasb://\ncontainer_name\n@\nstorage_account_name\n.blob.core.windows.net/\ndir/file\n\n\n\nFor example, to access a file called \"testfile\" located in a directory called \"testdir\", stored in the container called \"testcontainer\" on the account called \"hortonworks\", the URL is:\n\n\nwasb://testcontainer@hortonworks.blob.core.windows.net/testdir/testfile\n\n\n\nYou can also use \"wasbs\" prefix to utilize SSL-encrypted HTTPS access:\n\n\nwasbs://\n@\n.blob.core.windows.net/dir/file\n\n\n\nThe following Hadoop FileSystem shell commands demonstrate access to a storage account named \"myaccount\" and a container named \"mycontainer\":\n\n\nhadoop fs -ls wasb://mycontainer@myaccount.blob.core.windows.net/\n\nhadoop fs -mkdir wasb://mycontainer@myaccount.blob.core.windows.net/testDir\n\nhadoop fs -put testFile wasb://mycontainer@myaccount.blob.core.windows.net/testDir/testFile\n\nhadoop fs -cat wasb://mycontainer@myaccount.blob.core.windows.net/testDir/testFile\ntest file content\n\n\n\nLearn More\n\n\nFor more information about configuring the ADLS and WASB connectors and working with data stored in ADLS and WASB, refer to \nCloud Data Access\n documentation.\n\n\n\n\nNext: Access Cluster", 
            "title": "Access Data on Azure"
        }, 
        {
            "location": "/azure-data/index.html#accessing-data-on-azure", 
            "text": "Hortonworks Data Platform (HDP) supports reading and writing both block blobs and page blobs\nfrom/to  Windows Azure Storage Blob (WASB)  object store, as well as reading and writing files stored in an Azure Data Lake Storage (ADLS)  account. This allows you to:   Persist data using cloud storage services beyond the lifetime of your HDP clusters.    Load data in Hadoop ecosystem applications directly from Azure storage services, without first importing or uploading data from external resources to HDFS.    Use other applications (not necessarily in your Hadoop ecosystem) to manipulate the data stored in Azure storage services beyond the lifetime of your HDP clusters.    Share data between multiple HDP clusters fast and easily by pointing to the same Azure data sets.   Move or copy data between different Azure storage services or between Azure storage services and HDFS to facilitate different scenarios for big data analytics workloads.    Back up unlimited archive data at any scale from HDP cluster to fully managed, durable, and highly available Azure storage services.", 
            "title": "Accessing Data on Azure"
        }, 
        {
            "location": "/azure-data/index.html#accessing-data-in-adls", 
            "text": "Azure Data Lake Store (ADLS)  is an enterprise-wide hyper-scale repository for big data analytic workloads.", 
            "title": "Accessing Data in ADLS"
        }, 
        {
            "location": "/azure-data/index.html#prerequisites", 
            "text": "If you want to use  Azure Data Lake Store  to store your data, you must enable Azure subscription for Data Lake Store, and then create an Azure Data Lake Store  storage account .", 
            "title": "Prerequisites"
        }, 
        {
            "location": "/azure-data/index.html#configuring-access-to-adls", 
            "text": "ADLS is not supported as a default file system, but access to data in ADLS via the adl connector is configured if you select ADLS during  cluster creation , on the  Add File System  page. This option automates the configuration of the cluster with ADLS with the exception of the last step.   After the  cluster is deployed , you must perform the following steps manually to define which parts of the ADLS store this cluster will have access by adding the client credentials for the cluster to the data access control for the ADLS account.   This last configuration option should not be automated, since you need to select which files in ADLS the cluster should access. For more information, review the  documentation  on the Microsoft Azure Portal.    Steps   In the Cloudbreak UI, navigate to the  Manage Credentials  section, select the credential used to deploy your cluster, and copy the \"App Id\u201d associated with it.     Navigate to the Azure Portal    Data Lake Store  and select your account.  Select the  Data Explorer  tab at the top and select the folder that you want the cluster to access. Select the root folder for full access.   Select the  Access  tab at the top and then click  Add .  Click  +Add  and paste the App Id (copied in step 1) in the search box to find client certificate name for the cluster.  (Alternatively, you can look up the App Name in the  Azure Active Directory     App Registrations ).  Choose the appropriate permissions. Note that if you do select the root folder, you need to provide \u201cexecute\u201d access to all parent directories. For more information, refer to the  Azure documentation .   Test access to ADLS. Review the next sections for information on how to access data in ADLS from the cluster once it is deployed, for example from the command line of the cluster name node.", 
            "title": "Configuring Access to ADLS"
        }, 
        {
            "location": "/azure-data/index.html#access-path", 
            "text": "ADLS access path syntax is:  adl:// account_name .azuredatalakestore.net/ dir/file  For example, the following Hadoop FileSystem shell commands demonstrate access to a storage account named \"myaccount\":  hadoop fs -mkdir adl://myaccount.azuredatalakestore.net/testdir  hadoop fs -put testfile adl://myaccount.azuredatalakestore.net/testdir/testfile  To use DistCp against ADLS, use the following syntax: hadoop distcp\n    [-D hadoop.security.credential.provider.path=localjceks://file/home/user/adls.jceks]\n    hdfs:// namenode_hostname :9001/user/foo/007020615\n    adl:// myaccount .azuredatalakestore.net/testDir/", 
            "title": "Access Path"
        }, 
        {
            "location": "/azure-data/index.html#accessing-data-in-wasb", 
            "text": "Windows Azure Storage Blob (WASB) is an object store service available on Azure.", 
            "title": "Accessing Data in WASB"
        }, 
        {
            "location": "/azure-data/index.html#prerequisites_1", 
            "text": "If you want to use Windows Azure Storage Blob to store your data, you must enable Azure subscription for Blob Storage, and then create a  storage account .", 
            "title": "Prerequisites"
        }, 
        {
            "location": "/azure-data/index.html#configuring-access-to-wasb", 
            "text": "In order to access data stored in your Azure blob storage account, you must configure your storage account access key in  core-site.xml . The configuration property that you must use is  fs.azure.account.key. account name .blob.core.windows.net  and the value is the access key.   For example the following property should be used for a storage account called \"testaccount\":   property \n   name fs.azure.account.key.testaccount.blob.core.windows.net /name \n   value TESTACCOUNT-ACCESS-KEY /value  /property   You can obtain your access key from the Access keys in your storage account settings.  Alternatively, it is possible, although not recommended or supported, to configure  fs.defaultFS  to use a wasb or wasbs URL. This causes all bare paths, such as /testDir/testFile to resolve automatically to that file system.", 
            "title": "Configuring Access to WASB"
        }, 
        {
            "location": "/azure-data/index.html#access-path_1", 
            "text": "WASB access path syntax is:  wasb:// container_name @ storage_account_name .blob.core.windows.net/ dir/file  For example, to access a file called \"testfile\" located in a directory called \"testdir\", stored in the container called \"testcontainer\" on the account called \"hortonworks\", the URL is:  wasb://testcontainer@hortonworks.blob.core.windows.net/testdir/testfile  You can also use \"wasbs\" prefix to utilize SSL-encrypted HTTPS access:  wasbs:// @ .blob.core.windows.net/dir/file  The following Hadoop FileSystem shell commands demonstrate access to a storage account named \"myaccount\" and a container named \"mycontainer\":  hadoop fs -ls wasb://mycontainer@myaccount.blob.core.windows.net/\n\nhadoop fs -mkdir wasb://mycontainer@myaccount.blob.core.windows.net/testDir\n\nhadoop fs -put testFile wasb://mycontainer@myaccount.blob.core.windows.net/testDir/testFile\n\nhadoop fs -cat wasb://mycontainer@myaccount.blob.core.windows.net/testDir/testFile\ntest file content", 
            "title": "Access Path"
        }, 
        {
            "location": "/azure-data/index.html#learn-more", 
            "text": "For more information about configuring the ADLS and WASB connectors and working with data stored in ADLS and WASB, refer to  Cloud Data Access  documentation.   Next: Access Cluster", 
            "title": "Learn More"
        }, 
        {
            "location": "/gcp-launch/index.html", 
            "text": "Launching Cloudbreak on GCP\n\n\nBefore launching Cloudbreak on Google Cloud, review and meet the prerequisites. Next, import Cloudbreak image, launch a VM, SSH to the VM, and start Cloudbreak. Once Cloudbreak is running, log in to the Cloudbreak UI and create a Cloudbreak credential. \n\n\nMeet the Prerequisites\n\n\nBefore launching Cloudbreak on GCP, you must meet the following prerequisites.\n\n\nGCP Account\n\n\nIn order to launch Cloudbreak on GCP, you must log in to your GCP account. If you don't have an account, you can create one at \nhttps://console.cloud.google.com\n.\n\n\nOnce you log in to your GCP account, you must either create a project or use an existing project. \n\n\nService Account\n\n\nIn order to launch clusters on GCP via Cloudbreak, you must have a Service Account that Cloudbreak can use to create resources. In addition, you must also have a P12 key associated with the account. If you need to create these, refer to \nGCP documentation\n on how to create a service account and generate a P12 key. \n\n\nOnce you have the service account that you want to use for Cloudbreak, make sure that your service account fulfills one of the following APIs are enabled for your service account:\n\n\n\n\nCompute Image User   \n\n\nCompute Instance Admin (v1)  \n\n\nCompute Network Admin  \n\n\nCompute Security Admin  \n\n\n\n\nA user with an \"Owner\" role can assign roles or access rules to service accounts from \nIAM \n Admin\n \n \nIAM\n. For example:\n\n\n \n\n\nVPC Network\n\n\nWhen launching Cloudbreak, you will be required to select an existing network in which Cloudbreak can be placed. The following ports must be open on the security group: 22 (SSH) and 443 (HTTPS). You may use the \ndefault\n network as long as the aforementioned ports are open. \n\n\nYou can manage networks under \nNetworking\n \n \nVPC Networks\n. To edit ports, click on the network name and then click on \nAdd firewall rules\n.\n\n\nRegion and Zone\n\n\nDecide in which region and zone you would like to launch Cloudbreak. You can launch Cloudbreak and provision your clusters in all regions \nsupported by GCP\n.  \n\n\nClusters created via Cloudbreak can be in the same or different region as Cloudbreak; when you launch a cluster, you select the region in which to launch it. \n\n\nLaunch the VM\n\n\nSteps\n\n\n\n\n\n\nLog in to Google Cloud Platform.\n\n\n\n\n\n\nOpen the \nGoogle Cloud Shell\n by clicking on the  \n icon in the top-right corner:\n\n\n \n\n\n\n\n\n\nImport the Cloudbreak deployer image by executing the following command: \n\n\ngcloud compute images create cloudbreak-deployer-1164-2017-08-29 --source-uri gs://sequenceiqimage/cloudbreak-deployer-1164-2017-08-29.tar.gz\n\n\n[comment]: \n (TO-DO: This should be generated automatically.) \n\n\n\n\n\n\nIn the GCP UI, from the \nProducts and services\n menu, select \nCompute Engine\n \n \nImages\n.\n\n\n\n\n\n\nIn the search bar, type the name of the Cloudbreak deployer image that you imported earlier.\n\n\n\n\n\n\nSelect the image and then select \nCreate Instance\n:  \n\n\n  \n\n\n\n\n\n\nYou will be redirected to \nVM instances\n \n \nCreate an instance\n form. Provide the following parameters for your VM:\n\n\n  \n\n\n\n\n\n\n\n\nParameter\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nName\n\n\nEnter a name for the VM.\n\n\n\n\n\n\nZone\n\n\nSelect the zone in which to launch the VM.\n\n\n\n\n\n\nMachine type\n\n\nThe minimum instance type suitable for Cloudbreak is \nn1-standard-2\n. The minimum requirements are 4GB RAM, 10GB disk, 2 cores.\n\n\n\n\n\n\nBoot disk\n\n\nVerify that the Cloudbreak deployer disk which you imported earlier is pre-selected.\n\n\n\n\n\n\n\n\n\n\n\n\nClick on \nManagement, disks, networking, SSH keys\n to view the options.\n\n\n\n\n\n\nUnder \nNetworking\n \n \nNetwork interfaces\n, select the network in which you want to launch Cloudbreak. \n\n\n\n\n\n\nUnder \nSSH Keys\n, check \nBlock project-wise SSH keys\n and paste your public SSH key.\n\n\n\n\n\n\nClick \nCreate\n. \n\n\n\n\n\n\nSSH to the VM\n\n\nNow that your VM is ready, access it via SSH: \n\n\n\n\nUse a private key matching the public key that you added to your  project.\n\n\nThe SSH user is called \"cloudbreak\".\n\n\nYou can obtain the VM's IP address from \nCompute Engine\n \n \nVM Instances\n, the \nExternal IP\n column.\n\n\n\n\nOn Mac OS, you can SSH to the VM by running the following from the Terminal app: \nssh -i \"your-private-key.pem\" cloudnreak@instance_IP\n where \"your-private-key.pem\" points to the location of your private key and \"instance_IP\" is the public IP address of the VM.\n\n\nOn Windows, you can use \nPuTTy\n.\n\n\nLaunch Cloudbreak Deployer\n\n\nAfter accessing the VM via SSH, launch Cloudbreak deployer using the following steps.\n\n\nSteps\n \n\n\n\n\n\n\nNavigate to the cloudbreak-deployment directory:\n\n\ncd /var/lib/cloudbreak-deployment/\n\n\nThis directory contains configuration files and the supporting binaries for Cloudbreak deployer.\n\n\n\n\n\n\nInitialize your profile by creating a new file called \nProfile\n and adding the following content:\n\n\nexport UAA_DEFAULT_SECRET=MY-SECRET\nexport UAA_DEFAULT_USER_PW=MY-PASSWORDP\n  \n\n\nFor example: \n\n\nexport UAA_DEFAULT_SECRET=MySecret123\nexport UAA_DEFAULT_USER_PW=MySecurePassword123\n \n\n\n\n\nYou will need to provide the password when logging in to the Cloudbreak web UI and when using the Cloudbreak Shell. The secret will be used by Cloudbreak for authentication.  \n\n\n\n\n\n\n\n\nStart the Cloudbreak application by using the following command:\n\n\ncbd start\n\n\nThis will start the Docker containers and initialize the application. The first time you start the Coudbreak app, the process will take longer than usual due to the download of all the necessary docker images.\n\n\nThe \ncbd start\n command includes the \ncbd generate\n command which applies the following steps:\n\n\n\n\nCreates the \ndocker-compose.yml\n file, which describes the configuration of all the Docker containers needed for the Cloudbreak deployment.\n\n\nCreates the \nuaa.yml\n file, which holds the configuration of the identity server used to authenticate users with Cloudbreak.\n\n\n\n\n\n\nOnce the \ncbd start\n has finished, it returns the \"Uluwatu (Cloudbreak UI) url\" which you can later paste in your browser and log in to Cloudbreak web UI.\n\n\n\n\n\n\n\n\nCheck Cloudbreak deployer version and health: \n\n\ncbd doctor\n\n\n\n\n\n\nNext, check Cloudbreak Application logs: \n\n\ncbd logs cloudbreak\n\n\nYou should see a message like this in the log: \nStarted CloudbreakApplication in 36.823 seconds.\n Cloudbreak normally takes less than a minute to start.\n\n\n\n\n\n\nAccess Cloudbreak UI\n\n\nLog in to the Cloudbreak UI using the following steps.\n\n\nSteps\n\n\n\n\n\n\nYou can log into the Cloudbreak application at  \nhttps://IP_Address\n. For example \nhttps://34.212.141.253\n.  You can obtain the VM's IP address from \nCompute Engine\n \n \nVM Instances\n, the \nExternal IP\n column.\n\n\n\n\n\n\nConfirm the security exception to proceed to the Cloudbreak web UI.\n\n\nThe first time you access Cloudbreak UI, Cloudbreak will automatically generate a self-signed certificate, due to which your browser will warn you about an untrusted connection and will ask you to confirm a security exception.\n\n\n\n\n\n\nThe login page is displayed:\n\n\n  \n\n\n\n\n\n\nLog in to the Cloudbreak web UI: \n\n\n\n\nThe default username is \nadmin@example.com\n but you should sign up with your own email address.    \n\n\nThe password is the value of the \nUAA_DEFAULT_USER_PW\n variable that you configured in your \nProfile\n file when \nlaunching Cloudbreak deployer\n.\n\n\n\n\n\n\n\n\nCreate Cloudbreak Credential\n\n\nCloudbreak works by connecting your GCP account through this credential, and then uses it to create resources on your behalf. Before you can start provisioning cluster using Cloudbreak, you must create a Cloudbreak credential.\n\n\nSteps\n\n\n\n\n\n\nIn the Cloudbreak web UI, select \nCredentials\n from the left pane. \n\n\n\n\n\n\nClick \nCreate Credential\n. \n\n\n\n\n\n\nUnder \nCloud provider\n, select \"Google Cloud Platform\":\n\n\n  \n\n\n\n\n\n\nProvide the following information:\n\n\n\n\n\n\n\n\nParameter\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nName\n\n\nEnter a name for your credential.\n\n\n\n\n\n\nDescription\n\n\n(Optional) Enter a description.\n\n\n\n\n\n\nProject Id\n\n\nEnter the project ID. You can obtain it from your GCP account by clicking on the name of your project at the top of the page and copying the \nID\n.\n\n\n\n\n\n\nService Account Email Address\n\n\n\"Service account ID\" value for your service account created in prerequisites. You can find it on GCP at \nIAM \n Admin\n \n \nService accounts\n.\n\n\n\n\n\n\nService Account Private (p12) Key\n\n\nUpload the P12 key that you created in the prerequisites when creating a service account.\n\n\n\n\n\n\n\n\n\n\n\n\nClick \nCreate\n.\n\n\n\n\n\n\nYour credential should now be displayed in the \nCredentials\n pane.\n\n\nCongratulations! You've successfully launched and configured Cloudbreak. Now you can use Cloudbreak to \ncreate clusters\n. \n\n\n\n\n\n\n\n\nNext: Create a Cluster", 
            "title": "Launch on GCP"
        }, 
        {
            "location": "/gcp-launch/index.html#launching-cloudbreak-on-gcp", 
            "text": "Before launching Cloudbreak on Google Cloud, review and meet the prerequisites. Next, import Cloudbreak image, launch a VM, SSH to the VM, and start Cloudbreak. Once Cloudbreak is running, log in to the Cloudbreak UI and create a Cloudbreak credential.", 
            "title": "Launching Cloudbreak on GCP"
        }, 
        {
            "location": "/gcp-launch/index.html#meet-the-prerequisites", 
            "text": "Before launching Cloudbreak on GCP, you must meet the following prerequisites.", 
            "title": "Meet the Prerequisites"
        }, 
        {
            "location": "/gcp-launch/index.html#gcp-account", 
            "text": "In order to launch Cloudbreak on GCP, you must log in to your GCP account. If you don't have an account, you can create one at  https://console.cloud.google.com .  Once you log in to your GCP account, you must either create a project or use an existing project.", 
            "title": "GCP Account"
        }, 
        {
            "location": "/gcp-launch/index.html#service-account", 
            "text": "In order to launch clusters on GCP via Cloudbreak, you must have a Service Account that Cloudbreak can use to create resources. In addition, you must also have a P12 key associated with the account. If you need to create these, refer to  GCP documentation  on how to create a service account and generate a P12 key.   Once you have the service account that you want to use for Cloudbreak, make sure that your service account fulfills one of the following APIs are enabled for your service account:   Compute Image User     Compute Instance Admin (v1)    Compute Network Admin    Compute Security Admin     A user with an \"Owner\" role can assign roles or access rules to service accounts from  IAM   Admin     IAM . For example:", 
            "title": "Service Account"
        }, 
        {
            "location": "/gcp-launch/index.html#vpc-network", 
            "text": "When launching Cloudbreak, you will be required to select an existing network in which Cloudbreak can be placed. The following ports must be open on the security group: 22 (SSH) and 443 (HTTPS). You may use the  default  network as long as the aforementioned ports are open.   You can manage networks under  Networking     VPC Networks . To edit ports, click on the network name and then click on  Add firewall rules .", 
            "title": "VPC Network"
        }, 
        {
            "location": "/gcp-launch/index.html#region-and-zone", 
            "text": "Decide in which region and zone you would like to launch Cloudbreak. You can launch Cloudbreak and provision your clusters in all regions  supported by GCP .    Clusters created via Cloudbreak can be in the same or different region as Cloudbreak; when you launch a cluster, you select the region in which to launch it.", 
            "title": "Region and Zone"
        }, 
        {
            "location": "/gcp-launch/index.html#launch-the-vm", 
            "text": "Steps    Log in to Google Cloud Platform.    Open the  Google Cloud Shell  by clicking on the    icon in the top-right corner:       Import the Cloudbreak deployer image by executing the following command:   gcloud compute images create cloudbreak-deployer-1164-2017-08-29 --source-uri gs://sequenceiqimage/cloudbreak-deployer-1164-2017-08-29.tar.gz  [comment]:   (TO-DO: This should be generated automatically.)     In the GCP UI, from the  Products and services  menu, select  Compute Engine     Images .    In the search bar, type the name of the Cloudbreak deployer image that you imported earlier.    Select the image and then select  Create Instance :          You will be redirected to  VM instances     Create an instance  form. Provide the following parameters for your VM:         Parameter  Description      Name  Enter a name for the VM.    Zone  Select the zone in which to launch the VM.    Machine type  The minimum instance type suitable for Cloudbreak is  n1-standard-2 . The minimum requirements are 4GB RAM, 10GB disk, 2 cores.    Boot disk  Verify that the Cloudbreak deployer disk which you imported earlier is pre-selected.       Click on  Management, disks, networking, SSH keys  to view the options.    Under  Networking     Network interfaces , select the network in which you want to launch Cloudbreak.     Under  SSH Keys , check  Block project-wise SSH keys  and paste your public SSH key.    Click  Create .", 
            "title": "Launch the VM"
        }, 
        {
            "location": "/gcp-launch/index.html#ssh-to-the-vm", 
            "text": "Now that your VM is ready, access it via SSH:    Use a private key matching the public key that you added to your  project.  The SSH user is called \"cloudbreak\".  You can obtain the VM's IP address from  Compute Engine     VM Instances , the  External IP  column.   On Mac OS, you can SSH to the VM by running the following from the Terminal app:  ssh -i \"your-private-key.pem\" cloudnreak@instance_IP  where \"your-private-key.pem\" points to the location of your private key and \"instance_IP\" is the public IP address of the VM.  On Windows, you can use  PuTTy .", 
            "title": "SSH to the VM"
        }, 
        {
            "location": "/gcp-launch/index.html#launch-cloudbreak-deployer", 
            "text": "After accessing the VM via SSH, launch Cloudbreak deployer using the following steps.  Steps      Navigate to the cloudbreak-deployment directory:  cd /var/lib/cloudbreak-deployment/  This directory contains configuration files and the supporting binaries for Cloudbreak deployer.    Initialize your profile by creating a new file called  Profile  and adding the following content:  export UAA_DEFAULT_SECRET=MY-SECRET\nexport UAA_DEFAULT_USER_PW=MY-PASSWORDP     For example:   export UAA_DEFAULT_SECRET=MySecret123\nexport UAA_DEFAULT_USER_PW=MySecurePassword123     You will need to provide the password when logging in to the Cloudbreak web UI and when using the Cloudbreak Shell. The secret will be used by Cloudbreak for authentication.       Start the Cloudbreak application by using the following command:  cbd start  This will start the Docker containers and initialize the application. The first time you start the Coudbreak app, the process will take longer than usual due to the download of all the necessary docker images.  The  cbd start  command includes the  cbd generate  command which applies the following steps:   Creates the  docker-compose.yml  file, which describes the configuration of all the Docker containers needed for the Cloudbreak deployment.  Creates the  uaa.yml  file, which holds the configuration of the identity server used to authenticate users with Cloudbreak.    Once the  cbd start  has finished, it returns the \"Uluwatu (Cloudbreak UI) url\" which you can later paste in your browser and log in to Cloudbreak web UI.     Check Cloudbreak deployer version and health:   cbd doctor    Next, check Cloudbreak Application logs:   cbd logs cloudbreak  You should see a message like this in the log:  Started CloudbreakApplication in 36.823 seconds.  Cloudbreak normally takes less than a minute to start.", 
            "title": "Launch Cloudbreak Deployer"
        }, 
        {
            "location": "/gcp-launch/index.html#access-cloudbreak-ui", 
            "text": "Log in to the Cloudbreak UI using the following steps.  Steps    You can log into the Cloudbreak application at   https://IP_Address . For example  https://34.212.141.253 .  You can obtain the VM's IP address from  Compute Engine     VM Instances , the  External IP  column.    Confirm the security exception to proceed to the Cloudbreak web UI.  The first time you access Cloudbreak UI, Cloudbreak will automatically generate a self-signed certificate, due to which your browser will warn you about an untrusted connection and will ask you to confirm a security exception.    The login page is displayed:        Log in to the Cloudbreak web UI:    The default username is  admin@example.com  but you should sign up with your own email address.      The password is the value of the  UAA_DEFAULT_USER_PW  variable that you configured in your  Profile  file when  launching Cloudbreak deployer .", 
            "title": "Access Cloudbreak UI"
        }, 
        {
            "location": "/gcp-launch/index.html#create-cloudbreak-credential", 
            "text": "Cloudbreak works by connecting your GCP account through this credential, and then uses it to create resources on your behalf. Before you can start provisioning cluster using Cloudbreak, you must create a Cloudbreak credential.  Steps    In the Cloudbreak web UI, select  Credentials  from the left pane.     Click  Create Credential .     Under  Cloud provider , select \"Google Cloud Platform\":        Provide the following information:     Parameter  Description      Name  Enter a name for your credential.    Description  (Optional) Enter a description.    Project Id  Enter the project ID. You can obtain it from your GCP account by clicking on the name of your project at the top of the page and copying the  ID .    Service Account Email Address  \"Service account ID\" value for your service account created in prerequisites. You can find it on GCP at  IAM   Admin     Service accounts .    Service Account Private (p12) Key  Upload the P12 key that you created in the prerequisites when creating a service account.       Click  Create .    Your credential should now be displayed in the  Credentials  pane.  Congratulations! You've successfully launched and configured Cloudbreak. Now you can use Cloudbreak to  create clusters .      Next: Create a Cluster", 
            "title": "Create Cloudbreak Credential"
        }, 
        {
            "location": "/gcp-create/index.html", 
            "text": "Create a Cluster on GCP\n\n\nOptional Prerequisites\n\n\nIf you are just getting started with Cloudbreak, follow the steps below to create a cluster using the default blueprints. \n\n\nIf you are already familiar with Cloudbreak and you want to use custom blueprints, refer to the \nBlueprints\n documentation to customize your blueprints.\n\n\nBasic Options\n\n\nUse these steps to create a cluster.\n\n\nSteps\n\n\n\n\n\n\nLog in to the Cloudbreak UI.\n\n\n\n\n\n\nClick \nCreate Cluster\n and the \nCreate Cluster\n form is displayed.\n\n\n\n\n\n\nOn the \nGeneral Configuration\n page, provide the following parameters:\n\n\n\n\nTo view advanced options, click \nAdvanced\n. To learn about advanced options, refer to \nAdvanced Options\n.\n\n\n\n\n\n\n\n\n\n\nParameter\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nSelect Credential\n\n\nSelect a previously created credential.\n\n\n\n\n\n\nCluster Name\n\n\nEnter a name for your cluster. The name must be between 5 and 40 characters, must start with a letter, must only include lowercase letters, numbers, and hyphens.\n\n\n\n\n\n\nTags\n\n\n(Optional) You can optionally add tags, which will help you find your cluster-related resources, such as VMs, in your cloud provider account.\n\n\n\n\n\n\nRegion\n\n\nSelect the region in which you would like to launch your cluster.\n\n\n\n\n\n\nAvailability Zone\n\n\nSelect the availability zone in which you would like to launch your cluster.\n\n\n\n\n\n\nSend Email When Cluster is Ready\n\n\n(Optional) Check this to receive an email each time the cluster status changes.\n\n\n\n\n\n\n\n\n\n\nBy default, Ambari Username and Ambari Password are set to \nadmin\n. You can override it in the \"\nConfigure Cluster\n\" tab.\n\n\n\n\n\n\n\n\nOn the \nHardware and Storage\n page, select the blueprint that you would like to use for your cluster. You can either choose one of the pre-configured blueprints, or add your own in the \nBlueprints\n tab.\n\n\nFor each host group you must provide the following:\n\n\n\n\n\n\n\n\nParameter\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nGroup Size\n\n\nEnter a number defining how many nodes to create per host group. Default is 1. The \"Group Size\" for that host group on which Ambari Server is installed must be set to \"1\".\n\n\n\n\n\n\nTemplate\n\n\nIf you have previously created a template for VMs and storage, you can select it here. If you don't make a selection, default will be used.\n\n\n\n\n\n\nSecurity Group\n\n\nIf you have previously created a template for a security group, you can select it here. If you don't make a selection, default will be used.\n\n\n\n\n\n\nAmbari Server\n\n\nYou must select one node for Ambari Server. The \"Group Size\" for that host group must be set to \"1\".\n\n\n\n\n\n\nRecipes\n\n\nYou can select a previously added recipe (custom script) to be executed on all nodes of the host group. Refer to \nRecipes\n.\n\n\n\n\n\n\n\n\n\n\n\n\nOn the \nFile System\n page, select to use one of the following filesystems:\n\n\n\n\nLocal HDFS\n: No external storage outside of HDFS will be used\n\n\n\n\nGCS file system\n: If you select to use Google Cloud Storage option, you must provide:\n\n\n\n\n\n\n\n\nParameter\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nProject Id\n\n\nThe project ID registered when creating a credential should be pre-populated.\n\n\n\n\n\n\nService Account Email Address\n\n\nThe email address registered when creating a credential should be pre-populated.\n\n\n\n\n\n\nDefault Bucket Name\n\n\n(Deprecated) The name of an existing Google Cloud Storage bucket. This is an optional and deprecated configuration parameter (mapped to \"fs.gs.system.bucket\" in core-site.xml) to set the GCS bucket as a default bucket for URIs without having to specify the \"gs:\" prefix.  For more information about the \nGCS file system\n and \nbucket naming\n, refer to  GCP documentation.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nOn the \nNetwork\n page, provide the following parameters:\n\n\n\n\n\n\n\n\nParameter\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nNetwork\n\n\nSelect the virtual network in which you would like your cluster to be provisioned. You can define custom network configurations or use default network configurations.\n\n\n\n\n\n\nEnable Knox Gateway\n\n\n(Optional) Select this option to enable secure access to Ambari web UI and other cluster UIs via Knox gateway.\n\n\n\n\n\n\nEnable Kerberos Security\n\n\n(Optional) Select this option to enable Kerberos for your cluster. You will have an option to create a new kerberos or use an existing one. For more information refer to Kerberos \ndocumentation\n.\n\n\n\n\n\n\n\n\n\n\n\n\nOn the \nSecurity\n page, provide the following parameters:\n\n\n\n\n\n\nClick on \nCreate Cluster\n to create a cluster.\n\n\n\n\n\n\nYou will be redirected to the Cloudbreak dashboard, and a new tile representing your cluster will appear at the top of the page.\n\n\n\n\n\n\nAdvanced Options\n\n\nClick on \nShow Advanced Options\n to enter additional configuration options.\n\n\nGeneral Configuration\n\n\nYou can optionally configure the following advanced parameters:\n\n\n\n\n\n\n\n\nParameter\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nAmbari Username\n\n\nYou can log in to the Ambari UI using this username. By default, this is set to \nadmin\n.\n\n\n\n\n\n\nAmbari Password\n\n\nYou can log in to the Ambari UI using this password. By default, this is set to \nadmin\n.\n\n\n\n\n\n\nProvision Cluster\n\n\nSALT\n is pre-selected to provision your cluster.\n\n\n\n\n\n\nEnable Lifetime Management\n\n\nCheck this option if you would like your cluster to be automatically terminated after a specific amount of time (defined as \"Time to Live\" in minuter) has passed.\n\n\n\n\n\n\nFlex Subscription\n\n\nThis option will appear if you have configured your deployment for a \nFlex Subscription\n.\n\n\n\n\n\n\n\n\nHardware and Storage\n\n\nAfter selecting a blueprint, you can optionally configure the following advanced parameters:\n\n\n\n\n\n\n\n\nParameter\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nConfig Recommendation Strategy (Stack Advisor)\n\n\nSelect how configuration recommendations generated by stack advisor will be applied. Select one of \nALWAYS_APPLY: Configuration recommendations will be applied automatically.\nALWAYS_APPLY_DONT_OVERRIDE_CUSTOM_VALUES: Configuration recommendations will be applied automatically, but they will be ignored for custom configurations.\nNEVER_APPLY: Configuration recommendations will be ignored.\nONLY_STACK_DEFAULTS_APPLY: Configuration recommendations will be applied only on the default configurations for all included services.\n\n\n\n\n\n\nValidate Blueprint\n\n\nSelect to validate the blueprint.\n\n\n\n\n\n\n\n\nChoose Failure Action\n\n\nYou can optionally select what to do if cluster creation fails or if there aren't enough instances available to create all requested nodes:\n\n\n\n\n\n\n\n\nParameter\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nFailure Action\n\n\nSelect one of: \ndo NOT rollback resources\n (default) or \nrollback resources\n. \nBy default, if creating a cluster fails, the Azure resources that were created up to that point will not be rolled back. This means that they will remain accessible for troubleshooting and you will need to to delete them manually.\n\n\n\n\n\n\nMinimum Cluster Size\n\n\nThis defines the provisioning strategy in case the cloud provider cannot allocate all the requested nodes. Select \nbest effort\n or \nexact\n.\n\n\n\n\n\n\n\n\nConfigure Ambari Repos\n\n\nYou can optionally configure a different version of Ambari than the default by providing the following information:\n\n\n\n\n\n\n\n\nParameter\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nAmbari Version\n\n\nEnter Ambari version.\n\n\n\n\n\n\nAmbari Repo URL\n\n\nEnter Ambari repo URL.\n\n\n\n\n\n\nAmbari Repo Gpg Key URL\n\n\nEnter gpgkey URL.\n\n\n\n\n\n\n\n\nConfigure HDP Repos\n\n\nYou can optionally configure a different version of HDP than the default by providing the following information:\n\n\n\n\n\n\n\n\nParameter\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nStack\n\n\nEnter stack name.\n\n\n\n\n\n\nVersion\n\n\nEnter stack version.\n\n\n\n\n\n\nStack Repo ID\n\n\nEnter stack repo ID.\n\n\n\n\n\n\nBase URL\n\n\nEner stack repo base URL.\n\n\n\n\n\n\nUtils Repo ID\n\n\nEnter Utils repo ID.\n\n\n\n\n\n\nUtils Base URL\n\n\nEnter Utils repo base URL.\n\n\n\n\n\n\nVerify\n\n\nSelect to verify the repo information.\n\n\n\n\n\n\n\n\nConfigure Ambari Database\n\n\nBy default, Ambari stores data on an embedded database, which is sufficient for ephemeral or test clusters. However, as Ambari and Cloudbreak don't perform backups of this database, it is insufficient for long-running production clusters, and you may need to configure a remote database for Ambari and Cloudbreak.\n\n\n\n\n\n\n\n\nParameter\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nVendor\n\n\nSelect database vendor from the list.\n\n\n\n\n\n\nHost\n\n\nEnter database host IP.\n\n\n\n\n\n\nPort\n\n\nEnter port number.\n\n\n\n\n\n\nName\n\n\nEnter database name.\n\n\n\n\n\n\nUser Name\n\n\nEnter database user name.\n\n\n\n\n\n\nPassword\n\n\nEnter database password.\n\n\n\n\n\n\n\n\n\n\nNext: Access Cluster", 
            "title": "Create a Cluster"
        }, 
        {
            "location": "/gcp-create/index.html#create-a-cluster-on-gcp", 
            "text": "", 
            "title": "Create a Cluster on GCP"
        }, 
        {
            "location": "/gcp-create/index.html#optional-prerequisites", 
            "text": "If you are just getting started with Cloudbreak, follow the steps below to create a cluster using the default blueprints.   If you are already familiar with Cloudbreak and you want to use custom blueprints, refer to the  Blueprints  documentation to customize your blueprints.", 
            "title": "Optional Prerequisites"
        }, 
        {
            "location": "/gcp-create/index.html#basic-options", 
            "text": "Use these steps to create a cluster.  Steps    Log in to the Cloudbreak UI.    Click  Create Cluster  and the  Create Cluster  form is displayed.    On the  General Configuration  page, provide the following parameters:   To view advanced options, click  Advanced . To learn about advanced options, refer to  Advanced Options .      Parameter  Description      Select Credential  Select a previously created credential.    Cluster Name  Enter a name for your cluster. The name must be between 5 and 40 characters, must start with a letter, must only include lowercase letters, numbers, and hyphens.    Tags  (Optional) You can optionally add tags, which will help you find your cluster-related resources, such as VMs, in your cloud provider account.    Region  Select the region in which you would like to launch your cluster.    Availability Zone  Select the availability zone in which you would like to launch your cluster.    Send Email When Cluster is Ready  (Optional) Check this to receive an email each time the cluster status changes.      By default, Ambari Username and Ambari Password are set to  admin . You can override it in the \" Configure Cluster \" tab.     On the  Hardware and Storage  page, select the blueprint that you would like to use for your cluster. You can either choose one of the pre-configured blueprints, or add your own in the  Blueprints  tab.  For each host group you must provide the following:     Parameter  Description      Group Size  Enter a number defining how many nodes to create per host group. Default is 1. The \"Group Size\" for that host group on which Ambari Server is installed must be set to \"1\".    Template  If you have previously created a template for VMs and storage, you can select it here. If you don't make a selection, default will be used.    Security Group  If you have previously created a template for a security group, you can select it here. If you don't make a selection, default will be used.    Ambari Server  You must select one node for Ambari Server. The \"Group Size\" for that host group must be set to \"1\".    Recipes  You can select a previously added recipe (custom script) to be executed on all nodes of the host group. Refer to  Recipes .       On the  File System  page, select to use one of the following filesystems:   Local HDFS : No external storage outside of HDFS will be used   GCS file system : If you select to use Google Cloud Storage option, you must provide:     Parameter  Description      Project Id  The project ID registered when creating a credential should be pre-populated.    Service Account Email Address  The email address registered when creating a credential should be pre-populated.    Default Bucket Name  (Deprecated) The name of an existing Google Cloud Storage bucket. This is an optional and deprecated configuration parameter (mapped to \"fs.gs.system.bucket\" in core-site.xml) to set the GCS bucket as a default bucket for URIs without having to specify the \"gs:\" prefix.  For more information about the  GCS file system  and  bucket naming , refer to  GCP documentation.         On the  Network  page, provide the following parameters:     Parameter  Description      Network  Select the virtual network in which you would like your cluster to be provisioned. You can define custom network configurations or use default network configurations.    Enable Knox Gateway  (Optional) Select this option to enable secure access to Ambari web UI and other cluster UIs via Knox gateway.    Enable Kerberos Security  (Optional) Select this option to enable Kerberos for your cluster. You will have an option to create a new kerberos or use an existing one. For more information refer to Kerberos  documentation .       On the  Security  page, provide the following parameters:    Click on  Create Cluster  to create a cluster.    You will be redirected to the Cloudbreak dashboard, and a new tile representing your cluster will appear at the top of the page.", 
            "title": "Basic Options"
        }, 
        {
            "location": "/gcp-create/index.html#advanced-options", 
            "text": "Click on  Show Advanced Options  to enter additional configuration options.", 
            "title": "Advanced Options"
        }, 
        {
            "location": "/gcp-create/index.html#general-configuration", 
            "text": "You can optionally configure the following advanced parameters:     Parameter  Description      Ambari Username  You can log in to the Ambari UI using this username. By default, this is set to  admin .    Ambari Password  You can log in to the Ambari UI using this password. By default, this is set to  admin .    Provision Cluster  SALT  is pre-selected to provision your cluster.    Enable Lifetime Management  Check this option if you would like your cluster to be automatically terminated after a specific amount of time (defined as \"Time to Live\" in minuter) has passed.    Flex Subscription  This option will appear if you have configured your deployment for a  Flex Subscription .", 
            "title": "General Configuration"
        }, 
        {
            "location": "/gcp-create/index.html#hardware-and-storage", 
            "text": "After selecting a blueprint, you can optionally configure the following advanced parameters:     Parameter  Description      Config Recommendation Strategy (Stack Advisor)  Select how configuration recommendations generated by stack advisor will be applied. Select one of  ALWAYS_APPLY: Configuration recommendations will be applied automatically. ALWAYS_APPLY_DONT_OVERRIDE_CUSTOM_VALUES: Configuration recommendations will be applied automatically, but they will be ignored for custom configurations. NEVER_APPLY: Configuration recommendations will be ignored. ONLY_STACK_DEFAULTS_APPLY: Configuration recommendations will be applied only on the default configurations for all included services.    Validate Blueprint  Select to validate the blueprint.", 
            "title": "Hardware and Storage"
        }, 
        {
            "location": "/gcp-create/index.html#choose-failure-action", 
            "text": "You can optionally select what to do if cluster creation fails or if there aren't enough instances available to create all requested nodes:     Parameter  Description      Failure Action  Select one of:  do NOT rollback resources  (default) or  rollback resources .  By default, if creating a cluster fails, the Azure resources that were created up to that point will not be rolled back. This means that they will remain accessible for troubleshooting and you will need to to delete them manually.    Minimum Cluster Size  This defines the provisioning strategy in case the cloud provider cannot allocate all the requested nodes. Select  best effort  or  exact .", 
            "title": "Choose Failure Action"
        }, 
        {
            "location": "/gcp-create/index.html#configure-ambari-repos", 
            "text": "You can optionally configure a different version of Ambari than the default by providing the following information:     Parameter  Description      Ambari Version  Enter Ambari version.    Ambari Repo URL  Enter Ambari repo URL.    Ambari Repo Gpg Key URL  Enter gpgkey URL.", 
            "title": "Configure Ambari Repos"
        }, 
        {
            "location": "/gcp-create/index.html#configure-hdp-repos", 
            "text": "You can optionally configure a different version of HDP than the default by providing the following information:     Parameter  Description      Stack  Enter stack name.    Version  Enter stack version.    Stack Repo ID  Enter stack repo ID.    Base URL  Ener stack repo base URL.    Utils Repo ID  Enter Utils repo ID.    Utils Base URL  Enter Utils repo base URL.    Verify  Select to verify the repo information.", 
            "title": "Configure HDP Repos"
        }, 
        {
            "location": "/gcp-create/index.html#configure-ambari-database", 
            "text": "By default, Ambari stores data on an embedded database, which is sufficient for ephemeral or test clusters. However, as Ambari and Cloudbreak don't perform backups of this database, it is insufficient for long-running production clusters, and you may need to configure a remote database for Ambari and Cloudbreak.     Parameter  Description      Vendor  Select database vendor from the list.    Host  Enter database host IP.    Port  Enter port number.    Name  Enter database name.    User Name  Enter database user name.    Password  Enter database password.      Next: Access Cluster", 
            "title": "Configure Ambari Database"
        }, 
        {
            "location": "/gcp-clusters-access/index.html", 
            "text": "Accessing Your Cluster\n\n\nThe following section describes how to access the various services in the cluster.\n\n\nFinding Cluster Details\n\n\nOnce your cluster is up and running, you can find information about the cluster on the cluster details page in the Cloudbreak UI. To access cluster details page, click on the tile representing your cluster in the Cloudbreak UI. The information presented includes:\n\n\n\n\nCluster status\n\n\nEvent history \n\n\nCluster instance public IP addresses\n\n\nUI links \n\n\n\n\nAccess Cluster via SSH\n\n\nIf you plan to access the cluster via the command line clients, SSH into the master node instance in the cluster. \n\n\n\n\nIn order to use SSH, you must generate an SSH key pair or use an existing SSH keypair.  \n\n\nYou can find the cluster instance public IP addresses on the cluster details page.  \n\n\nWhen accessing instances via SSH use the \ncloudbreak\n user. \n\n\n\n\nOn Mac OS, you can use the following syntax to SSH to the VM:\n\nssh -i \"privatekey.pem\" cloudbreak@publicIP\n\nFor example:\n\nssh -i \"dominika-kp.pem\" cloudbreak@p52.25.169.132\n\n\nOn Windows, you can SSH using an SSH client such as PuTTY.", 
            "title": "Access Cluster"
        }, 
        {
            "location": "/gcp-clusters-access/index.html#accessing-your-cluster", 
            "text": "The following section describes how to access the various services in the cluster.", 
            "title": "Accessing Your Cluster"
        }, 
        {
            "location": "/gcp-clusters-access/index.html#finding-cluster-details", 
            "text": "Once your cluster is up and running, you can find information about the cluster on the cluster details page in the Cloudbreak UI. To access cluster details page, click on the tile representing your cluster in the Cloudbreak UI. The information presented includes:   Cluster status  Event history   Cluster instance public IP addresses  UI links", 
            "title": "Finding Cluster Details"
        }, 
        {
            "location": "/gcp-clusters-access/index.html#access-cluster-via-ssh", 
            "text": "If you plan to access the cluster via the command line clients, SSH into the master node instance in the cluster.    In order to use SSH, you must generate an SSH key pair or use an existing SSH keypair.    You can find the cluster instance public IP addresses on the cluster details page.    When accessing instances via SSH use the  cloudbreak  user.    On Mac OS, you can use the following syntax to SSH to the VM: ssh -i \"privatekey.pem\" cloudbreak@publicIP \nFor example: ssh -i \"dominika-kp.pem\" cloudbreak@p52.25.169.132  On Windows, you can SSH using an SSH client such as PuTTY.", 
            "title": "Access Cluster via SSH"
        }, 
        {
            "location": "/os-launch/index.html", 
            "text": "Launching Cloudbreak on OpenStack\n\n\nBefore launching Cloudbreak on OpenStack, review and meet the prerequisites. Next, import Cloudbreak image, launch a VM, SSH to the VM, and start Cloudbreak. Once Cloudbreak is running, log in to the Cloudbreak UI and create a Cloudbreak credential. \n\n\nMeet Minimum System Requirements\n\n\nBefore launching Cloudbreak on your OpenStack, make sure that your OpenStack deployment fulfills the following requirements.\n\n\nSupported Linux Distributions\n\n\nThe following versions of the \nRed Hat Distribution of OpenStack\n (RDO) are supported:\n\n\n\n\nJuno\n\n\nKilo\n\n\nLiberty\n\n\nMitaka\n\n\n\n\nStandard Modules\n\n\nCloudbreak requires that the following standard modules are installed and configured on OpenStack:\n\n\n\n\nKeystone V2 or Keystone V3  \n\n\nNeutron (Self-service and provider networking)  \n\n\nNova (KVM or Xen hypervisor)  \n\n\nGlance  \n\n\nCinder (Optional)  \n\n\nHeat (Optional but highly recommended, since provisioning through native API calls will be deprecated in the future)  \n\n\n\n\nMeet the Prerequisites\n\n\nBefore launching Cloudbreak on OpenStack, you must meet the following prerequisites.\n\n\nSSH Key Pair\n\n\nCreate a new SSH key pair or import an existing SSH key pair. \n\n\nSecurity Group\n\n\nIn order to launch Cloudbreak, you must have an existing security group with the following ports open: 22 (SSH) and 443 (HTTPS). \n\n\nFor information about OpenStack security groups, refer to the \nOpenStack Operations Guide\n.\n\n\nImport Images to OpenStack\n\n\nAn OpenStack administrator must perform these steps to add the Cloudbreak deployer and HDP images to your OpenStack deployment.\n\n\nImport Cloudbreak Deployer Image\n\n\nImport Cloudbreak deployer image using the following steps.\n\n\nSteps\n\n\n\n\n\n\nDownload the latest Cloudbreak deployer image to your local machine: \n\n\ncurl -O https://public-repo-1.hortonworks.com/HDP/cloudbreak/cloudbreak-deployer-1164-2017-08-25.img\n\n\n\n\n\n\nSet the following environment variables for the OpenStack image import: \n\n\nexport CBD_LATEST_IMAGE=cloudbreak-deployer-1164-2017-08-25.img\nexport OS_IMAGE_NAME=cloudbreak-deployer-1161-2017-06-15.img\nexport OS_USERNAME=your_os_user_name\nexport OS_AUTH_URL=your_authentication_url\nexport OS_TENANT_NAME=your_os_tenant_name\n\n\n\n\n\n\nImport the new image into your OpenStack:\n\n\nglance image-create --name \"$OS_IMAGE_NAME\" --file \"$CBD_LATEST_IMAGE\" --disk-format qcow2 --container-format bare --progress\n \n\n\n\n\n\n\nAfter performing the import, you should be able to see the Cloudbreak deployer image among your other OpenStack images. \n\n\nImport HDP Image\n\n\nImport HDP image using the following steps.\n\n\nSteps\n\n\n\n\n\n\nDownload the latest HDP image to your local machine: \n\n\ncurl -O https://public-repo-1.hortonworks.com/HDP/cloudbreak/hdc-hdp--1706141444.img\n\n\n\n\n\n\nSet the following environment variables for the OpenStack image import: \n\n\nexport CB_LATEST_IMAGE=hdc-hdp--1706141444.img \nexport CB_LATEST_IMAGE_NAME=hdc-hdp--1705081316.img\nexport OS_USERNAME=your_os_user_name\nexport OS_AUTH_URL=your_authentication_url\nexport OS_TENANT_NAME=your_os_tenant_name\n\n\n\n\n\n\nImport the new image into your OpenStack:\n\n\nglance image-create --name \"$CB_LATEST_IMAGE_NAME\" --file \"$CB_LATEST_IMAGE\" --disk-format qcow2 --container-format bare --progress\n\n\n\n\n\n\nAfter performing the import, you should be able to see the Cloudbreak image among your OpenStack images. \n\n\nLaunch the VM\n\n\nIn your OpenStack, launch and instance providing the following parameters:\n\n\n\n\nSelect a VM flavor which meets the following minimum requirements: 4GB RAM, 10GB disk, 2 cores.  \n\n\nSelect the Cloudbreak deployer image that you imported earlier and launch an instance using that image. \n\n\nSelect your SSH key pair.  \n\n\nSelect the security group which has the following ports open: 22 (SSH) and 443 (HTTPS). \n\n\nSelect your preconfigured network.  \n\n\n\n\nSSH to the VM\n\n\nNow that your VM is ready, access it via SSH: \n\n\n\n\nUse a private key matching the public key that you added to your OpenStack project.\n\n\nThe SSH user is called \"cloudbreak\".\n\n\nYou can obtain the VM's IP address from the details of your instance.\n\n\n\n\nOn Mac OS, you can SSH to the VM by running the following from the Terminal app: \nssh -i \"your-private-key.pem\" cloudnreak@instance_IP\n where \"your-private-key.pem\" points to the location of your private key and \"instance_IP\" is the public IP address of the VM.\n\n\nOn Windows, you can use \nPuTTy\n.\n\n\nInitialize Your Profile\n\n\nAfter accessing the VM via SSH, you must initialize your Profile.\n\n\nSteps\n \n\n\n\n\n\n\nNavigate to the cloudbreak-deployment directory:\n\n\ncd /var/lib/cloudbreak-deployment/\n\n\nThis directory contains configuration files and the supporting binaries for Cloudbreak deployer.\n\n\n\n\n\n\nInitialize your profile by creating a new file called \nProfile\n and adding the following content:\n\n\nexport UAA_DEFAULT_SECRET=MY-SECRET\nexport UAA_DEFAULT_USER_PW=MY-PASSWORD\nexport PUBLIC_IP=VM-PUBLIC-IP\n  \n\n\nFor example: \n\n\nexport UAA_DEFAULT_SECRET=MySecret123\nexport UAA_DEFAULT_USER_PW=MySecurePassword123\nexport PUBLIC_IP=34.212.141.253\n \n\n\n\n\nYou will need to provide the password when logging in to the Cloudbreak web UI and when using the Cloudbreak Shell. The secret will be used by Cloudbreak for authentication.  \n\n\n\n\n\n\n\n\nPerform Optional Configurations\n\n\n\n\nThese configurations are optional. \n\n\n\n\nConfiguring a Self-Signed Certificate\n\n\nIf your OpenStack is secured with a self-signed certificate, you need to import that certificate into Cloudbreak, or else Cloudbreak won't be able to communicate with your OpenStack. \n\n\nTo import the certificate, place the certificate file in the \n/certs/trusted/\n directory, follow these steps.\n\n\nSteps\n\n\n\n\nNavigate to the \ncerts\n directory (automatically generated).\n\n\nCreate the \ntrusted\n directory.\n\n\nCopy the certificate to the \ntrusted\n directory. \n\n\n\n\nCloudbreak will automatically pick up the certificate and import it into its truststore upon start.\n\n\nConfiguring Availability Zone and Region\n\n\nBy default, Cloudbreak uses \nRegionOne\n region with \nnova\n availability zone, but you can customize Cloudbreak deployment and enable multiple regions and availability zones by creating an \nopenstack-zone.json\n file in the \netc\n directory of Cloudbreak deployment (that is\n/var/lib/cloudbreak-deployment/etc/openstack-zone.json\n). If the etc directory does not exist in the Cloudbreak deployment directory, then create it. \n\n\nThe following is an example of \nopenstack-zone.json\n containing two regions and four availability zones:\n\n\n{\n  \"items\": [\n    {\n      \"name\": \"MyRegionOne\",\n      \"zones\": [ \"az1\", \"az2\", \"az3\"]\n    },\n    {\n      \"name\": \"MyRegionTwo\",\n      \"zones\": [ \"myaz\"]\n    }\n  ]\n}\n\n\n\n\n\n\nIf you are performing this after you have started cbd, perform \ncbd restart\n.  \n\n\n\n\nLaunch Cloudbreak Deployer\n\n\nLaunch Cloudbreak deployer using the following steps.\n\n\nSteps\n\n\n\n\n\n\nStart the Cloudbreak application by using the following command:\n\n\ncbd start\n\n\nThis will start the Docker containers and initialize the application. The first time you start the Coudbreak app, the process will take longer than usual due to the download of all the necessary docker images.\n\n\nThe \ncbd start\n command includes the \ncbd generate\n command which applies the following steps:\n\n\n\n\nCreates the \ndocker-compose.yml\n file, which describes the configuration of all the Docker containers needed for the Cloudbreak deployment.\n\n\nCreates the \nuaa.yml\n file, which holds the configuration of the identity server used to authenticate users with Cloudbreak.\n\n\n\n\n\n\nOnce the \ncbd start\n has finished, it returns the \"Uluwatu (Cloudbreak UI) url\" which you can later paste in your browser and log in to Cloudbreak web UI.\n\n\n\n\n\n\n\n\nCheck Cloudbreak deployer version and health: \n\n\ncbd doctor\n\n\n\n\n\n\nNext, check Cloudbreak Application logs: \n\n\ncbd logs cloudbreak\n\n\nYou should see a message like this in the log: \nStarted CloudbreakApplication in 36.823 seconds.\n Cloudbreak normally takes less than a minute to start.\n\n\n\n\n\n\nAccess Cloudbreak UI\n\n\nLog in to the Cloudbreak UI using the following steps.\n\n\nSteps\n\n\n\n\n\n\nYou can log into the Cloudbreak application at \nhttps://IP_Address\n where \"IP_Address\" if the public IP of your OpenStack VM. For example \nhttps://34.212.141.253\n.\n\n\n\n\n\n\nConfirm the security exception to proceed to the Cloudbreak web UI.\n\n\nThe first time you access Cloudbreak UI, Cloudbreak will automatically generate a self-signed certificate, due to which your browser will warn you about an untrusted connection and will ask you to confirm a security exception.\n\n\n\n\n\n\nThe login page is displayed:\n\n\n  \n\n\n\n\n\n\nLog in to the Cloudbreak web UI: \n\n\n\n\nThe default username is \nadmin@example.com\n but you should sign up with your own email address.    \n\n\nThe password is the value of the \nUAA_DEFAULT_USER_PW\n variable that you configured in your \nProfile\n file when \nlaunching Cloudbreak deployer\n.\n\n\n\n\n\n\n\n\nCreate Cloudbreak Credential\n\n\nCloudbreak works by connecting your OpenStack account through this credential, and then uses it to create resources on your behalf. Before you can start provisioning cluster using Cloudbreak, you must create a Cloudbreak credential.\n\n\nSteps\n\n\n\n\n\n\nIn the Cloudbreak web UI, select \nCredentials\n from the left pane. \n\n\n\n\n\n\nClick \nCreate Credential\n. \n\n\n\n\n\n\nUnder \nCloud provider\n, select \"Google Cloud Platform\".\n\n\n \n\n\n\n\n\n\nProvide the following information:\n\n\n\n\n\n\n\n\nParameter\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nKeystone Version\n\n\nSelect the keystone version.\n\n\n\n\n\n\nName\n\n\nEnter a name for your credential.\n\n\n\n\n\n\nDescription\n\n\n(Optional) Enter a description.\n\n\n\n\n\n\nUser\n\n\nEnter your OpenStack user name.\n\n\n\n\n\n\nPassword\n\n\nEnter your OpenStack password.\n\n\n\n\n\n\nTenant Name\n\n\nEnter the OpenStack tenant name.\n\n\n\n\n\n\nEndpoint\n\n\nEnter the OpenStack endpoint.\n\n\n\n\n\n\nAPI Facing\n\n\nSelect \npublic\n or \nprivate\n.\n\n\n\n\n\n\n\n\n\n\n\n\nClick \nCreate\n.\n\n\n\n\n\n\nYour credential should now be displayed in the \nCredentials\n pane.\n\n\nCongratulations! You've successfully launched and configured Cloudbreak. Now you can use Cloudbreak to \ncreate clusters\n. \n\n\n\n\n\n\n\n\nNext: Create a Cluster", 
            "title": "Launch on Open Stack"
        }, 
        {
            "location": "/os-launch/index.html#launching-cloudbreak-on-openstack", 
            "text": "Before launching Cloudbreak on OpenStack, review and meet the prerequisites. Next, import Cloudbreak image, launch a VM, SSH to the VM, and start Cloudbreak. Once Cloudbreak is running, log in to the Cloudbreak UI and create a Cloudbreak credential.", 
            "title": "Launching Cloudbreak on OpenStack"
        }, 
        {
            "location": "/os-launch/index.html#meet-minimum-system-requirements", 
            "text": "Before launching Cloudbreak on your OpenStack, make sure that your OpenStack deployment fulfills the following requirements.", 
            "title": "Meet Minimum System Requirements"
        }, 
        {
            "location": "/os-launch/index.html#supported-linux-distributions", 
            "text": "The following versions of the  Red Hat Distribution of OpenStack  (RDO) are supported:   Juno  Kilo  Liberty  Mitaka", 
            "title": "Supported Linux Distributions"
        }, 
        {
            "location": "/os-launch/index.html#standard-modules", 
            "text": "Cloudbreak requires that the following standard modules are installed and configured on OpenStack:   Keystone V2 or Keystone V3    Neutron (Self-service and provider networking)    Nova (KVM or Xen hypervisor)    Glance    Cinder (Optional)    Heat (Optional but highly recommended, since provisioning through native API calls will be deprecated in the future)", 
            "title": "Standard Modules"
        }, 
        {
            "location": "/os-launch/index.html#meet-the-prerequisites", 
            "text": "Before launching Cloudbreak on OpenStack, you must meet the following prerequisites.", 
            "title": "Meet the Prerequisites"
        }, 
        {
            "location": "/os-launch/index.html#ssh-key-pair", 
            "text": "Create a new SSH key pair or import an existing SSH key pair.", 
            "title": "SSH Key Pair"
        }, 
        {
            "location": "/os-launch/index.html#security-group", 
            "text": "In order to launch Cloudbreak, you must have an existing security group with the following ports open: 22 (SSH) and 443 (HTTPS).   For information about OpenStack security groups, refer to the  OpenStack Operations Guide .", 
            "title": "Security Group"
        }, 
        {
            "location": "/os-launch/index.html#import-images-to-openstack", 
            "text": "An OpenStack administrator must perform these steps to add the Cloudbreak deployer and HDP images to your OpenStack deployment.", 
            "title": "Import Images to OpenStack"
        }, 
        {
            "location": "/os-launch/index.html#import-cloudbreak-deployer-image", 
            "text": "Import Cloudbreak deployer image using the following steps.  Steps    Download the latest Cloudbreak deployer image to your local machine:   curl -O https://public-repo-1.hortonworks.com/HDP/cloudbreak/cloudbreak-deployer-1164-2017-08-25.img    Set the following environment variables for the OpenStack image import:   export CBD_LATEST_IMAGE=cloudbreak-deployer-1164-2017-08-25.img\nexport OS_IMAGE_NAME=cloudbreak-deployer-1161-2017-06-15.img\nexport OS_USERNAME=your_os_user_name\nexport OS_AUTH_URL=your_authentication_url\nexport OS_TENANT_NAME=your_os_tenant_name    Import the new image into your OpenStack:  glance image-create --name \"$OS_IMAGE_NAME\" --file \"$CBD_LATEST_IMAGE\" --disk-format qcow2 --container-format bare --progress      After performing the import, you should be able to see the Cloudbreak deployer image among your other OpenStack images.", 
            "title": "Import Cloudbreak Deployer Image"
        }, 
        {
            "location": "/os-launch/index.html#import-hdp-image", 
            "text": "Import HDP image using the following steps.  Steps    Download the latest HDP image to your local machine:   curl -O https://public-repo-1.hortonworks.com/HDP/cloudbreak/hdc-hdp--1706141444.img    Set the following environment variables for the OpenStack image import:   export CB_LATEST_IMAGE=hdc-hdp--1706141444.img \nexport CB_LATEST_IMAGE_NAME=hdc-hdp--1705081316.img\nexport OS_USERNAME=your_os_user_name\nexport OS_AUTH_URL=your_authentication_url\nexport OS_TENANT_NAME=your_os_tenant_name    Import the new image into your OpenStack:  glance image-create --name \"$CB_LATEST_IMAGE_NAME\" --file \"$CB_LATEST_IMAGE\" --disk-format qcow2 --container-format bare --progress    After performing the import, you should be able to see the Cloudbreak image among your OpenStack images.", 
            "title": "Import HDP Image"
        }, 
        {
            "location": "/os-launch/index.html#launch-the-vm", 
            "text": "In your OpenStack, launch and instance providing the following parameters:   Select a VM flavor which meets the following minimum requirements: 4GB RAM, 10GB disk, 2 cores.    Select the Cloudbreak deployer image that you imported earlier and launch an instance using that image.   Select your SSH key pair.    Select the security group which has the following ports open: 22 (SSH) and 443 (HTTPS).   Select your preconfigured network.", 
            "title": "Launch the VM"
        }, 
        {
            "location": "/os-launch/index.html#ssh-to-the-vm", 
            "text": "Now that your VM is ready, access it via SSH:    Use a private key matching the public key that you added to your OpenStack project.  The SSH user is called \"cloudbreak\".  You can obtain the VM's IP address from the details of your instance.   On Mac OS, you can SSH to the VM by running the following from the Terminal app:  ssh -i \"your-private-key.pem\" cloudnreak@instance_IP  where \"your-private-key.pem\" points to the location of your private key and \"instance_IP\" is the public IP address of the VM.  On Windows, you can use  PuTTy .", 
            "title": "SSH to the VM"
        }, 
        {
            "location": "/os-launch/index.html#initialize-your-profile", 
            "text": "After accessing the VM via SSH, you must initialize your Profile.  Steps      Navigate to the cloudbreak-deployment directory:  cd /var/lib/cloudbreak-deployment/  This directory contains configuration files and the supporting binaries for Cloudbreak deployer.    Initialize your profile by creating a new file called  Profile  and adding the following content:  export UAA_DEFAULT_SECRET=MY-SECRET\nexport UAA_DEFAULT_USER_PW=MY-PASSWORD\nexport PUBLIC_IP=VM-PUBLIC-IP     For example:   export UAA_DEFAULT_SECRET=MySecret123\nexport UAA_DEFAULT_USER_PW=MySecurePassword123\nexport PUBLIC_IP=34.212.141.253     You will need to provide the password when logging in to the Cloudbreak web UI and when using the Cloudbreak Shell. The secret will be used by Cloudbreak for authentication.", 
            "title": "Initialize Your Profile"
        }, 
        {
            "location": "/os-launch/index.html#perform-optional-configurations", 
            "text": "These configurations are optional.", 
            "title": "Perform Optional Configurations"
        }, 
        {
            "location": "/os-launch/index.html#configuring-a-self-signed-certificate", 
            "text": "If your OpenStack is secured with a self-signed certificate, you need to import that certificate into Cloudbreak, or else Cloudbreak won't be able to communicate with your OpenStack.   To import the certificate, place the certificate file in the  /certs/trusted/  directory, follow these steps.  Steps   Navigate to the  certs  directory (automatically generated).  Create the  trusted  directory.  Copy the certificate to the  trusted  directory.    Cloudbreak will automatically pick up the certificate and import it into its truststore upon start.", 
            "title": "Configuring a Self-Signed Certificate"
        }, 
        {
            "location": "/os-launch/index.html#configuring-availability-zone-and-region", 
            "text": "By default, Cloudbreak uses  RegionOne  region with  nova  availability zone, but you can customize Cloudbreak deployment and enable multiple regions and availability zones by creating an  openstack-zone.json  file in the  etc  directory of Cloudbreak deployment (that is /var/lib/cloudbreak-deployment/etc/openstack-zone.json ). If the etc directory does not exist in the Cloudbreak deployment directory, then create it.   The following is an example of  openstack-zone.json  containing two regions and four availability zones:  {\n  \"items\": [\n    {\n      \"name\": \"MyRegionOne\",\n      \"zones\": [ \"az1\", \"az2\", \"az3\"]\n    },\n    {\n      \"name\": \"MyRegionTwo\",\n      \"zones\": [ \"myaz\"]\n    }\n  ]\n}   If you are performing this after you have started cbd, perform  cbd restart .", 
            "title": "Configuring Availability Zone and Region"
        }, 
        {
            "location": "/os-launch/index.html#launch-cloudbreak-deployer", 
            "text": "Launch Cloudbreak deployer using the following steps.  Steps    Start the Cloudbreak application by using the following command:  cbd start  This will start the Docker containers and initialize the application. The first time you start the Coudbreak app, the process will take longer than usual due to the download of all the necessary docker images.  The  cbd start  command includes the  cbd generate  command which applies the following steps:   Creates the  docker-compose.yml  file, which describes the configuration of all the Docker containers needed for the Cloudbreak deployment.  Creates the  uaa.yml  file, which holds the configuration of the identity server used to authenticate users with Cloudbreak.    Once the  cbd start  has finished, it returns the \"Uluwatu (Cloudbreak UI) url\" which you can later paste in your browser and log in to Cloudbreak web UI.     Check Cloudbreak deployer version and health:   cbd doctor    Next, check Cloudbreak Application logs:   cbd logs cloudbreak  You should see a message like this in the log:  Started CloudbreakApplication in 36.823 seconds.  Cloudbreak normally takes less than a minute to start.", 
            "title": "Launch Cloudbreak Deployer"
        }, 
        {
            "location": "/os-launch/index.html#access-cloudbreak-ui", 
            "text": "Log in to the Cloudbreak UI using the following steps.  Steps    You can log into the Cloudbreak application at  https://IP_Address  where \"IP_Address\" if the public IP of your OpenStack VM. For example  https://34.212.141.253 .    Confirm the security exception to proceed to the Cloudbreak web UI.  The first time you access Cloudbreak UI, Cloudbreak will automatically generate a self-signed certificate, due to which your browser will warn you about an untrusted connection and will ask you to confirm a security exception.    The login page is displayed:        Log in to the Cloudbreak web UI:    The default username is  admin@example.com  but you should sign up with your own email address.      The password is the value of the  UAA_DEFAULT_USER_PW  variable that you configured in your  Profile  file when  launching Cloudbreak deployer .", 
            "title": "Access Cloudbreak UI"
        }, 
        {
            "location": "/os-launch/index.html#create-cloudbreak-credential", 
            "text": "Cloudbreak works by connecting your OpenStack account through this credential, and then uses it to create resources on your behalf. Before you can start provisioning cluster using Cloudbreak, you must create a Cloudbreak credential.  Steps    In the Cloudbreak web UI, select  Credentials  from the left pane.     Click  Create Credential .     Under  Cloud provider , select \"Google Cloud Platform\".       Provide the following information:     Parameter  Description      Keystone Version  Select the keystone version.    Name  Enter a name for your credential.    Description  (Optional) Enter a description.    User  Enter your OpenStack user name.    Password  Enter your OpenStack password.    Tenant Name  Enter the OpenStack tenant name.    Endpoint  Enter the OpenStack endpoint.    API Facing  Select  public  or  private .       Click  Create .    Your credential should now be displayed in the  Credentials  pane.  Congratulations! You've successfully launched and configured Cloudbreak. Now you can use Cloudbreak to  create clusters .      Next: Create a Cluster", 
            "title": "Create Cloudbreak Credential"
        }, 
        {
            "location": "/os-create/index.html", 
            "text": "Create a Cluster on OpenStack\n\n\nOptional Prerequisites\n\n\nIf you are just getting started with Cloudbreak, follow the steps below to create a cluster using the default blueprints. \n\n\nIf you are already familiar with Cloudbreak and you want to use custom blueprints, refer to the \nBlueprints\n documentation to customize your blueprints. \n\n\nBasic Options\n\n\nUse these steps to create a cluster.\n\n\nSteps\n\n\n\n\n\n\nLog in to the Cloudbreak UI.\n\n\n\n\n\n\nClick \nCreate Cluster\n and the \nCreate Cluster\n form is displayed.\n\n\n\n\n\n\nOn the \nGeneral Configuration\n page, provide the following parameters:\n\n\n\n\nTo view advanced options, click \nAdvanced\n. To learn about advanced options, refer to \nAdvanced Options\n.\n\n\n\n\n\n\n\n\n\n\nParameter\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nSelect Credential\n\n\nSelect a previously created credential.\n\n\n\n\n\n\nCluster Name\n\n\nEnter a name for your cluster. The name must be between 5 and 40 characters, must start with a letter, must only include lowercase letters, numbers, and hyphens.\n\n\n\n\n\n\nTags\n\n\n(Optional) You can optionally add tags, which will help you find your cluster-related resources, such as VMs, in your cloud provider account.\n\n\n\n\n\n\nRegion\n\n\nSelect the region in which you would like to launch your cluster.\n\n\n\n\n\n\nAvailability Zone\n\n\nSelect the availability zone in which you would like to launch your cluster.\n\n\n\n\n\n\nConnector Variant\n\n\nSelect \"HEAT\" or \"NATIVE\".\n\n\n\n\n\n\nSend Email When Cluster is Ready\n\n\n(Optional) Check this to receive an email each time the cluster status changes.\n\n\n\n\n\n\n\n\n\n\nBy default, Ambari Username and Ambari Password are set to \nadmin\n. You can override it in the \"\nConfigure Cluster\n\" tab.\n\n\n\n\n\n\n\n\nOn the \nHardware and Storage\n page, select the blueprint that you would like to use for your cluster. You can either choose one of the pre-configured blueprints, or add your own in the \nBlueprints\n tab.\n\n\nFor each host group you must provide the following:\n\n\n\n\n\n\n\n\nParameter\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nGroup Size\n\n\nEnter a number defining how many nodes to create per host group. Default is 1. The \"Group Size\" for that host group on which Ambari Server is installed must be set to \"1\".\n\n\n\n\n\n\nTemplate\n\n\nIf you have previously created a template for VMs and storage, you can select it here. If you don't make a selection, default will be used.\n\n\n\n\n\n\nSecurity Group\n\n\nIf you have previously created a template for a security group, you can select it here. If you don't make a selection, default will be used.\n\n\n\n\n\n\nAmbari Server\n\n\nYou must select one node for Ambari Server. The \"Group Size\" for that host group must be set to \"1\".\n\n\n\n\n\n\nRecipes\n\n\nYou can select a previously added recipe (custom script) to be executed on all nodes of the host group. Refer to \nRecipes\n.\n\n\n\n\n\n\n\n\n\n\n\n\nOn the \nNetwork\n page, provide the following parameters:\n\n\n\n\n\n\n\n\nParameter\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nNetwork\n\n\nSelect the virtual network in which you would like your cluster to be provisioned. You can define custom network configurations or use default network configurations.\n\n\n\n\n\n\nEnable Knox Gateway\n\n\n(Optional) Select this option to enable secure access to Ambari web UI and other cluster UIs via Knox gateway.\n\n\n\n\n\n\nEnable Kerberos Security\n\n\n(Optional) Select this option to enable Kerberos for your cluster. You will have an option to create a new kerberos or use an existing one. For more information refer to Kerberos \ndocumentation\n.\n\n\n\n\n\n\n\n\n\n\n\n\nOn the \nSecurity\n page, provide the following parameters:\n\n\n\n\n\n\nClick on \nCreate Cluster\n to create a cluster.\n\n\n\n\n\n\nYou will be redirected to the Cloudbreak dashboard, and a new tile representing your cluster will appear at the top of the page.\n\n\n\n\n\n\nAdvanced Options\n\n\nClick on \nShow Advanced Options\n to enter additional configuration options.\n\n\nGeneral Configuration\n\n\nYou can optionally configure the following advanced parameters:\n\n\n\n\n\n\n\n\nParameter\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nAmbari Username\n\n\nYou can log in to the Ambari UI using this username. By default, this is set to \nadmin\n.\n\n\n\n\n\n\nAmbari Password\n\n\nYou can log in to the Ambari UI using this password. By default, this is set to \nadmin\n.\n\n\n\n\n\n\nProvision Cluster\n\n\nSALT\n is pre-selected to provision your cluster.\n\n\n\n\n\n\nEnable Lifetime Management\n\n\nCheck this option if you would like your cluster to be automatically terminated after a specific amount of time (defined as \"Time to Live\" in minuter) has passed.\n\n\n\n\n\n\nFlex Subscription\n\n\nThis option will appear if you have configured your deployment for a \nFlex Subscription\n.\n\n\n\n\n\n\n\n\nHardware and Storage\n\n\nAfter selecting a blueprint, you can optionally configure the following advanced parameters:\n\n\n\n\n\n\n\n\nParameter\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nConfig Recommendation Strategy (Stack Advisor)\n\n\nSelect how configuration recommendations generated by stack advisor will be applied. Select one of \nALWAYS_APPLY: Configuration recommendations will be applied automatically.\nALWAYS_APPLY_DONT_OVERRIDE_CUSTOM_VALUES: Configuration recommendations will be applied automatically, but they will be ignored for custom configurations.\nNEVER_APPLY: Configuration recommendations will be ignored.\nONLY_STACK_DEFAULTS_APPLY: Configuration recommendations will be applied only on the default configurations for all included services.\n\n\n\n\n\n\nValidate Blueprint\n\n\nSelect to validate the blueprint.\n\n\n\n\n\n\n\n\nChoose Failure Action\n\n\nYou can optionally select what to do if cluster creation fails or if there aren't enough instances available to create all requested nodes:\n\n\n\n\n\n\n\n\nParameter\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nFailure Action\n\n\nSelect one of: \ndo NOT rollback resources\n (default) or \nrollback resources\n. \nBy default, if creating a cluster fails, the Azure resources that were created up to that point will not be rolled back. This means that they will remain accessible for troubleshooting and you will need to to delete them manually.\n\n\n\n\n\n\nMinimum Cluster Size\n\n\nThis defines the provisioning strategy in case the cloud provider cannot allocate all the requested nodes. Select \nbest effort\n or \nexact\n.\n\n\n\n\n\n\n\n\nConfigure Ambari Repos\n\n\nYou can optionally configure a different version of Ambari than the default by providing the following information:\n\n\n\n\n\n\n\n\nParameter\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nAmbari Version\n\n\nEnter Ambari version.\n\n\n\n\n\n\nAmbari Repo URL\n\n\nEnter Ambari repo URL.\n\n\n\n\n\n\nAmbari Repo Gpg Key URL\n\n\nEnter gpgkey URL.\n\n\n\n\n\n\n\n\nConfigure HDP Repos\n\n\nYou can optionally configure a different version of HDP than the default by providing the following information:\n\n\n\n\n\n\n\n\nParameter\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nStack\n\n\nEnter stack name.\n\n\n\n\n\n\nVersion\n\n\nEnter stack version.\n\n\n\n\n\n\nStack Repo ID\n\n\nEnter stack repo ID.\n\n\n\n\n\n\nBase URL\n\n\nEner stack repo base URL.\n\n\n\n\n\n\nUtils Repo ID\n\n\nEnter Utils repo ID.\n\n\n\n\n\n\nUtils Base URL\n\n\nEnter Utils repo base URL.\n\n\n\n\n\n\nVerify\n\n\nSelect to verify the repo information.\n\n\n\n\n\n\n\n\nConfigure Ambari Database\n\n\nBy default, Ambari stores data on an embedded database, which is sufficient for ephemeral or test clusters. However, as Ambari and Cloudbreak don't perform backups of this database, it is insufficient for long-running production clusters, and you may need to configure a remote database for Ambari and Cloudbreak.\n\n\n\n\n\n\n\n\nParameter\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nVendor\n\n\nSelect database vendor from the list.\n\n\n\n\n\n\nHost\n\n\nEnter database host IP.\n\n\n\n\n\n\nPort\n\n\nEnter port number.\n\n\n\n\n\n\nName\n\n\nEnter database name.\n\n\n\n\n\n\nUser Name\n\n\nEnter database user name.\n\n\n\n\n\n\nPassword\n\n\nEnter database password.\n\n\n\n\n\n\n\n\n\n\nNext: Access Cluster", 
            "title": "Create a Cluster"
        }, 
        {
            "location": "/os-create/index.html#create-a-cluster-on-openstack", 
            "text": "", 
            "title": "Create a Cluster on OpenStack"
        }, 
        {
            "location": "/os-create/index.html#optional-prerequisites", 
            "text": "If you are just getting started with Cloudbreak, follow the steps below to create a cluster using the default blueprints.   If you are already familiar with Cloudbreak and you want to use custom blueprints, refer to the  Blueprints  documentation to customize your blueprints.", 
            "title": "Optional Prerequisites"
        }, 
        {
            "location": "/os-create/index.html#basic-options", 
            "text": "Use these steps to create a cluster.  Steps    Log in to the Cloudbreak UI.    Click  Create Cluster  and the  Create Cluster  form is displayed.    On the  General Configuration  page, provide the following parameters:   To view advanced options, click  Advanced . To learn about advanced options, refer to  Advanced Options .      Parameter  Description      Select Credential  Select a previously created credential.    Cluster Name  Enter a name for your cluster. The name must be between 5 and 40 characters, must start with a letter, must only include lowercase letters, numbers, and hyphens.    Tags  (Optional) You can optionally add tags, which will help you find your cluster-related resources, such as VMs, in your cloud provider account.    Region  Select the region in which you would like to launch your cluster.    Availability Zone  Select the availability zone in which you would like to launch your cluster.    Connector Variant  Select \"HEAT\" or \"NATIVE\".    Send Email When Cluster is Ready  (Optional) Check this to receive an email each time the cluster status changes.      By default, Ambari Username and Ambari Password are set to  admin . You can override it in the \" Configure Cluster \" tab.     On the  Hardware and Storage  page, select the blueprint that you would like to use for your cluster. You can either choose one of the pre-configured blueprints, or add your own in the  Blueprints  tab.  For each host group you must provide the following:     Parameter  Description      Group Size  Enter a number defining how many nodes to create per host group. Default is 1. The \"Group Size\" for that host group on which Ambari Server is installed must be set to \"1\".    Template  If you have previously created a template for VMs and storage, you can select it here. If you don't make a selection, default will be used.    Security Group  If you have previously created a template for a security group, you can select it here. If you don't make a selection, default will be used.    Ambari Server  You must select one node for Ambari Server. The \"Group Size\" for that host group must be set to \"1\".    Recipes  You can select a previously added recipe (custom script) to be executed on all nodes of the host group. Refer to  Recipes .       On the  Network  page, provide the following parameters:     Parameter  Description      Network  Select the virtual network in which you would like your cluster to be provisioned. You can define custom network configurations or use default network configurations.    Enable Knox Gateway  (Optional) Select this option to enable secure access to Ambari web UI and other cluster UIs via Knox gateway.    Enable Kerberos Security  (Optional) Select this option to enable Kerberos for your cluster. You will have an option to create a new kerberos or use an existing one. For more information refer to Kerberos  documentation .       On the  Security  page, provide the following parameters:    Click on  Create Cluster  to create a cluster.    You will be redirected to the Cloudbreak dashboard, and a new tile representing your cluster will appear at the top of the page.", 
            "title": "Basic Options"
        }, 
        {
            "location": "/os-create/index.html#advanced-options", 
            "text": "Click on  Show Advanced Options  to enter additional configuration options.", 
            "title": "Advanced Options"
        }, 
        {
            "location": "/os-create/index.html#general-configuration", 
            "text": "You can optionally configure the following advanced parameters:     Parameter  Description      Ambari Username  You can log in to the Ambari UI using this username. By default, this is set to  admin .    Ambari Password  You can log in to the Ambari UI using this password. By default, this is set to  admin .    Provision Cluster  SALT  is pre-selected to provision your cluster.    Enable Lifetime Management  Check this option if you would like your cluster to be automatically terminated after a specific amount of time (defined as \"Time to Live\" in minuter) has passed.    Flex Subscription  This option will appear if you have configured your deployment for a  Flex Subscription .", 
            "title": "General Configuration"
        }, 
        {
            "location": "/os-create/index.html#hardware-and-storage", 
            "text": "After selecting a blueprint, you can optionally configure the following advanced parameters:     Parameter  Description      Config Recommendation Strategy (Stack Advisor)  Select how configuration recommendations generated by stack advisor will be applied. Select one of  ALWAYS_APPLY: Configuration recommendations will be applied automatically. ALWAYS_APPLY_DONT_OVERRIDE_CUSTOM_VALUES: Configuration recommendations will be applied automatically, but they will be ignored for custom configurations. NEVER_APPLY: Configuration recommendations will be ignored. ONLY_STACK_DEFAULTS_APPLY: Configuration recommendations will be applied only on the default configurations for all included services.    Validate Blueprint  Select to validate the blueprint.", 
            "title": "Hardware and Storage"
        }, 
        {
            "location": "/os-create/index.html#choose-failure-action", 
            "text": "You can optionally select what to do if cluster creation fails or if there aren't enough instances available to create all requested nodes:     Parameter  Description      Failure Action  Select one of:  do NOT rollback resources  (default) or  rollback resources .  By default, if creating a cluster fails, the Azure resources that were created up to that point will not be rolled back. This means that they will remain accessible for troubleshooting and you will need to to delete them manually.    Minimum Cluster Size  This defines the provisioning strategy in case the cloud provider cannot allocate all the requested nodes. Select  best effort  or  exact .", 
            "title": "Choose Failure Action"
        }, 
        {
            "location": "/os-create/index.html#configure-ambari-repos", 
            "text": "You can optionally configure a different version of Ambari than the default by providing the following information:     Parameter  Description      Ambari Version  Enter Ambari version.    Ambari Repo URL  Enter Ambari repo URL.    Ambari Repo Gpg Key URL  Enter gpgkey URL.", 
            "title": "Configure Ambari Repos"
        }, 
        {
            "location": "/os-create/index.html#configure-hdp-repos", 
            "text": "You can optionally configure a different version of HDP than the default by providing the following information:     Parameter  Description      Stack  Enter stack name.    Version  Enter stack version.    Stack Repo ID  Enter stack repo ID.    Base URL  Ener stack repo base URL.    Utils Repo ID  Enter Utils repo ID.    Utils Base URL  Enter Utils repo base URL.    Verify  Select to verify the repo information.", 
            "title": "Configure HDP Repos"
        }, 
        {
            "location": "/os-create/index.html#configure-ambari-database", 
            "text": "By default, Ambari stores data on an embedded database, which is sufficient for ephemeral or test clusters. However, as Ambari and Cloudbreak don't perform backups of this database, it is insufficient for long-running production clusters, and you may need to configure a remote database for Ambari and Cloudbreak.     Parameter  Description      Vendor  Select database vendor from the list.    Host  Enter database host IP.    Port  Enter port number.    Name  Enter database name.    User Name  Enter database user name.    Password  Enter database password.      Next: Access Cluster", 
            "title": "Configure Ambari Database"
        }, 
        {
            "location": "/os-clusters-access/index.html", 
            "text": "Accessing Your Cluster\n\n\nThe following section describes how to access the various services in the cluster.\n\n\nFinding Cluster Details\n\n\nOnce your cluster is up and running, you can find information about the cluster on the cluster details page in the Cloudbreak UI. To access cluster details page, click on the tile representing your cluster in the Cloudbreak UI. The information presented includes:\n\n\n\n\nCluster status\n\n\nEvent history \n\n\nCluster instance public IP addresses\n\n\nUI links \n\n\n\n\nAccess Cluster via SSH\n\n\nIf you plan to access the cluster via the command line clients, SSH into the master node instance in the cluster. \n\n\n\n\nIn order to use SSH, you must generate an SSH key pair or use an existing SSH keypair.  \n\n\nYou can find the cluster instance public IP addresses on the cluster details page.  \n\n\nWhen accessing instances via SSH use the \ncloudbreak\n user. \n\n\n\n\nOn Mac OS, you can use the following syntax to SSH to the VM:\n\nssh -i \"privatekey.pem\" cloudbreak@publicIP\n\nFor example:\n\nssh -i \"dominika-kp.pem\" cloudbreak@p52.25.169.132\n\n\nOn Windows, you can SSH using an SSH client such as PuTTY.", 
            "title": "Access Cluster"
        }, 
        {
            "location": "/os-clusters-access/index.html#accessing-your-cluster", 
            "text": "The following section describes how to access the various services in the cluster.", 
            "title": "Accessing Your Cluster"
        }, 
        {
            "location": "/os-clusters-access/index.html#finding-cluster-details", 
            "text": "Once your cluster is up and running, you can find information about the cluster on the cluster details page in the Cloudbreak UI. To access cluster details page, click on the tile representing your cluster in the Cloudbreak UI. The information presented includes:   Cluster status  Event history   Cluster instance public IP addresses  UI links", 
            "title": "Finding Cluster Details"
        }, 
        {
            "location": "/os-clusters-access/index.html#access-cluster-via-ssh", 
            "text": "If you plan to access the cluster via the command line clients, SSH into the master node instance in the cluster.    In order to use SSH, you must generate an SSH key pair or use an existing SSH keypair.    You can find the cluster instance public IP addresses on the cluster details page.    When accessing instances via SSH use the  cloudbreak  user.    On Mac OS, you can use the following syntax to SSH to the VM: ssh -i \"privatekey.pem\" cloudbreak@publicIP \nFor example: ssh -i \"dominika-kp.pem\" cloudbreak@p52.25.169.132  On Windows, you can SSH using an SSH client such as PuTTY.", 
            "title": "Access Cluster via SSH"
        }, 
        {
            "location": "/blueprints/index.html", 
            "text": "Blueprints\n\n\nAmbari blueprints\n are your declarative definition of a Hadoop cluster, defining the host groups and which components to install on which host group. Ambari uses them as a base for your clusters. \n\n\nYou have three options concerning using blueprints with Cloudbreak:\n\n\n\n\nUse one of the pre-defined blueprints.    \n\n\nAdd your custom blueprint by uploading a JSON file or pasting the JSON text. \n\n\nCopy and edit one of the pre-defined blueprints. \n\n\n\n\nWe recommend that you review the default blueprints to check if they meet your requirements. You can do this by expanding  the \nmanage bluerints\n pane in the Cloudbreak web UI or by reading the documentation below.\n\n\nUse Default Blueprints\n\n\nTo use one of the default blueprints, simply select them when creating a cluster. The option is available on the \nGeneral Configuration\n page. First select the \nStack Version\n and then select your chosen blueprint under \nCluster Type\n. \n\n\nTo review default blueprints, refer to \nDefault Blueprints\n. \n\n\nAdd Custom Blueprint\n\n\nThis option allows you to save your custom blueprints. For correct blueprint layout and other useful information about Ambari blueprints, refer to the \nAmbari cwiki\n page.\n\n\nCreating a Blueprint\n\n\nAmbari blueprints are specified in the JSON format. After you provide the blueprint to Cloudbreak, the host groups in the JSON will be mapped to a set of instances when starting the cluster, and the specified services and components will be installed on the corresponding nodes. It is not necessary to define a complete configuration in the blueprint. If a configuration is missing, Ambari will fill that with a default value. \nFurthermore, a blueprint can be modified later from the Ambari UI.\n\n\nA blueprint can be exported from a running Ambari cluster and can be reused in Cloudbreak after slight modifications. When a blueprint is exported, some configurations are hardcoded for example domain names, memory configurations, and so on, that won't be applicable to the Cloudbreak cluster. There is no automatic way to modify an exported blueprint and make it instantly usable in Cloudbreak, the modifications have to be done manually.\n\n\nAlso see \nExample Blueprints\n and \nBlueprint Components\n.\n\n\nUpload a Blueprint\n\n\nOnce you have your blueprint ready, perform these steps.\n\n\nSteps\n\n\n\n\n\n\nNavigate to the \nmanage blueprints\n tab. To add your own blueprint, click \n+create blueprint\n and enter the following parameters:\n\n\n\n\n\n\n\n\nParameter\n\n\nValue\n\n\n\n\n\n\n\n\n\n\nName\n\n\nEnter a name for your blueprint.\n\n\n\n\n\n\nDescription\n\n\n(Optional) Enter a description for your blueprint.\n\n\n\n\n\n\nBlueprint Source\n\n\nSelect one of: \nText\n: Paste blueprint in JSON format.\n \nFile\n: Upload a file that contains the blueprint.\n \nURL\n: Specify the URL for your blueprint.\n\n\n\n\n\n\nPublic In Account\n\n\n(Optional) If this option is checked, all the users belonging to your account will be able to use this blueprint to create clusters, but they cannot delete it.\n\n\n\n\n\n\n\n\n\n\n\n\nTo use the uploaded blueprints, simply select it when creating a cluster. The option is available on the \nGeneral Configuration\n page. First select the \nStack Version\n and then select your chosen blueprint under \nCluster Type\n. \n\n\n\n\n\n\nCopy and Edit Existing Blueprint\n\n\nYou can reuse default and previously added blueprints by using the \ncopy \n edit\n option, which allows you to clone and edit an existing blueprint without changing or deleting the original blueprint. \n\n\nSteps\n\n\n\n\n\n\nNavigate to the \nmanage blueprints\n tab, expand the blueprint entry, and then click \ncopy \n edit\n.\n\n\n\n\n\n\nMake updates in your blueprint and save them as a new blueprint.\n\n\n\n\n\n\nModifying Existing Blueprint\n\n\nTo modify existing an blueprint without keeping the original:\n\n\n\n\nIf you pasted or uploaded your blueprint in the Cloudbreak UI, in order to modify it you must delete the entry and add the blueprint again in a new entry.     \n\n\nIf you provided a URL to the location where the blueprint is stored, in order to modify the blueprint simply update it in the location to which the URL is pointing. \n\n\n\n\nYou can also \ncopy and edit an existing blueprint\n.\n\n\nDelete Blueprint\n\n\nYou can delete previously added items by selecting and item and using the \ndelete\n option. \n\n\nExample Blueprints\n\n\nHere are blueprint examples:  \n\n\n\n\nSmallest Possible HDP 2.4\n\n\nSmall HDP 2.4\n\n\nSmall HDP 2.4 Streaming\n\n\nSmall HDP 2.4 Spark\n\n\n\n\nBlueprint Components\n\n\nAmbari supports the concept of stacks and associated services in a stack definition. By leveraging the stack definition, Ambari has a consistent and defined interface to install, manage, and monitor a set of services, and provides extensibility model for new stacks and services to be introduced.\n\n\nAt a high level, the supported components can be grouped into two main categories: master and slave. The components are bundled together, forming specific services:\n\n\n\n\n\n\n\n\nService\n\n\nComponets\n\n\n\n\n\n\n\n\n\n\nHDFS\n\n\nDATANODE, HDFS_CLIENT, JOURNALNODE, NAMENODE, SECONDARY_NAMENODE, ZKFC\n\n\n\n\n\n\nYARN\n\n\nAPP_TIMELINE_SERVER, NODEMANAGER, RESOURCEMANAGER, YARN_CLIENT\n\n\n\n\n\n\nMAPREDUCE2\n\n\nHISTORYSERVER, MAPREDUCE2_CLIENT\n\n\n\n\n\n\nHBASE\n\n\nHBASE_CLIENT, HBASE_MASTER, HBASE_REGIONSERVER\n\n\n\n\n\n\nHIVE\n\n\nHIVE_CLIENT, HIVE_METASTORE, HIVE_SERVER, MYSQL_SERVER\n\n\n\n\n\n\nHCATALOG\n\n\nHCAT\n\n\n\n\n\n\nWEBHCAT\n\n\nWEBHCAT_SERVER\n\n\n\n\n\n\nOOZIE\n\n\nOOZIE_CLIENT, OOZIE_SERVER\n\n\n\n\n\n\nPIG\n\n\nPIG\n\n\n\n\n\n\nSQOOP\n\n\nSQOOP\n\n\n\n\n\n\nSTORM\n\n\nDRPC_SERVER, NIMBUS, STORM_REST_API, STORM_UI_SERVER, SUPERVISOR\n\n\n\n\n\n\nTEZ\n\n\nTEZ_CLIENT\n\n\n\n\n\n\nFALCON\n\n\nFALCON_CLIENT, FALCON_SERVER\n\n\n\n\n\n\nZOOKEEPER\n\n\nZOOKEEPER_CLIENT, ZOOKEEPER_SERVER\n\n\n\n\n\n\nSPARK\n\n\nSPARK_JOBHISTORYSERVER, SPARK_CLIENT\n\n\n\n\n\n\nRANGER\n\n\nRANGER_USERSYNC, RANGER_ADMIN\n\n\n\n\n\n\nAMBARI_METRICS\n\n\nAMBARI_METRICS, METRICS_COLLECTOR, METRICS_MONITOR\n\n\n\n\n\n\nKERBEROS\n\n\nKERBEROS_CLIENT\n\n\n\n\n\n\nFLUME\n\n\nFLUME_HANDLER\n\n\n\n\n\n\nKAFKA\n\n\nKAFKA_BROKER\n\n\n\n\n\n\nKNOX\n\n\nKNOX_GATEWAY\n\n\n\n\n\n\nATLAS\n\n\nATLAS\n\n\n\n\n\n\nCLOUDBREAK\n\n\nCLOUDBREAK\n\n\n\n\n\n\n\n\nDefault Blueprints\n\n\nCloudbreak includes the following default HDP cluster blueprints:\n\n\nHDP Version: \nHDP 2.6\n\n\n\n\n\n\n\n\nCluster Type\n\n\nServices\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nData Science\n\n\n Spark 1.6,\nZeppelin 0.7.0\n\n\nThis cluster configuration includes Spark 1.6 with Zeppelin.\n\n\n\n\n\n\nData Science\n\n\n Spark 2.1,\nZeppelin 0.7.0\n\n\nThis cluster configuration includes Spark 2.1 with Zeppelin.\n\n\n\n\n\n\nEDW - Analytics\n\n\n Hive 2 LLAP\n,\nZeppelin 0.7.0\n\n\nThis cluster configuration includes Hive 2 LLAP.\n\n\n\n\n\n\nEDW - ETL\n\n\n Hive 1.2.1,\nSpark 1.6\n\n\nThis cluster configuration includes Hive and Spark 1.6.\n\n\n\n\n\n\nEDW - ETL\n\n\n Hive 1.2.1,\n Spark 2.1\n\n\nThis cluster configuration includes Hive and Spark 2.1.\n\n\n\n\n\n\nBI\n\n\n Druid 0.9.2\n\n\nThis cluster configuration includes a Technical Preview of Druid.\n\n\n\n\n\n\n\n\nHDP Version: \nHDP 2.5\n\n\n\n\n\n\n\n\nCluster Type\n\n\nServices\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nData Science\n\n\n Spark 1.6,\nZeppelin 0.6.0\n\n\nThis cluster configuration includes Spark 1.6 and Zeppelin.\n\n\n\n\n\n\nEDW - ETL\n\n\n Hive 1.2.1,\nSpark 1.6\n\n\nThis cluster configuration includes Hive and Spark 1.6.\n\n\n\n\n\n\nEDW - ETL\n\n\n Hive 1.2.1,\n Spark 2.0\n\n\nThis cluster configuration includes a Technical Preview of Spark 2.0.\n\n\n\n\n\n\nEDW - Analytics\n\n\n Hive 2 LLAP\n,\nZeppelin 0.6.0\n\n\nThis cluster configuration includes a Technical Preview of Hive 2 LLAP.\n\n\n\n\n\n\n\n\n\n    \nChoosing Your Configuration\n\n    \n\nWhen creating a cluster, you can choose a more stable cluster configuration for a predicable experience.\nAlternatively, you can try the latest capabilities by choosing a cluster configuration\nthat is much more experimental. The following configuration classification applies:\n\n\n\n Stable configurations are the best choice if you want to avoid issues and other problems with launching and using clusters.\n\n\n If you want to use a Technical Preview version of a component in a release of HDP, use these configurations.\n\n\n These are the most cutting edge of the configurations, including Technical Preview components in a Technical Preview HDP release.\n\n\n\n\n\n\n\n\n\nThe following services are included in the respective blueprints:\n\n\nHDP Version: \nHDP 2.6\n\n\n\n\n\n\n\n\nService\n\n\nData Science\n(Spark 1.6)\n\n\nData Science\n(Spark 2.1)\n\n\nEDW-ETL\n(Spark 1.6)\n\n\nEDW-ETL\n(Spark 2.1)\n\n\nEDW-Analytics\n\n\nBI-Druid\n\n\n\n\n\n\n\n\n\n\nHDFS\n\n\nx\n\n\nx\n\n\nx\n\n\nx\n\n\nx\n\n\nx\n\n\n\n\n\n\nYARN\n\n\nx\n\n\nx\n\n\nx\n\n\nx\n\n\nx\n\n\nx\n\n\n\n\n\n\nMapReduce2\n\n\nx\n\n\nx\n\n\nx\n\n\nx\n\n\nx\n\n\nx\n\n\n\n\n\n\nTez\n\n\nx\n\n\nx\n\n\nx\n\n\nx\n\n\nx\n\n\nx\n\n\n\n\n\n\nHive 1.2.1\n\n\nx\n\n\nx\n\n\nx\n\n\nx\n\n\n\n\n\n\n\n\n\n\nHive 2 LLAP\n\n\n\n\n\n\n\n\n\n\nx\n\n\n\n\n\n\n\n\nDruid\n\n\n\n\n\n\n\n\n\n\n\n\nx\n\n\n\n\n\n\nPig\n\n\nx\n\n\nx\n\n\nx\n\n\nx\n\n\nx\n\n\n\n\n\n\n\n\nSqoop\n\n\nx\n\n\nx\n\n\nx\n\n\n\n\n\n\nx\n\n\n\n\n\n\nZooKeeper\n\n\nx\n\n\nx\n\n\nx\n\n\nx\n\n\nx\n\n\nx\n\n\n\n\n\n\nAmbari Metrics\n\n\nx\n\n\nx\n\n\nx\n\n\nx\n\n\nx\n\n\nx\n\n\n\n\n\n\nSpark 1.6\n\n\nx\n\n\n\n\nx\n\n\n\n\nx\n\n\n\n\n\n\n\n\nSpark 2.1\n\n\n\n\nx\n\n\n\n\nx\n\n\n\n\n\n\n\n\n\n\nZeppelin 0.7.0\n\n\nx\n\n\nx\n\n\n\n\n\n\nx\n\n\n\n\n\n\n\n\nSlider\n\n\n\n\n\n\n\n\n\n\nx\n\n\n\n\n\n\n\n\n\n\nHDP Version: \nHDP 2.5\n\n\n\n\n\n\n\n\nService\n\n\nData Science\n\n\nEDW-ETL (Spark 1.6)\n\n\nEDW-ETL (Spark 2.0)\n\n\nEDW-Analytics\n\n\n\n\n\n\n\n\n\n\nHDFS\n\n\nx\n\n\nx\n\n\nx\n\n\nx\n\n\n\n\n\n\nYARN\n\n\nx\n\n\nx\n\n\nx\n\n\nx\n\n\n\n\n\n\nMapReduce2\n\n\nx\n\n\nx\n\n\nx\n\n\nx\n\n\n\n\n\n\nTez\n\n\nx\n\n\nx\n\n\nx\n\n\nx\n\n\n\n\n\n\nHive 1.2.1\n\n\nx\n\n\nx\n\n\nx\n\n\n\n\n\n\n\n\nHive 2 LLAP\n\n\n\n\n\n\n\n\nx\n\n\n\n\n\n\nPig\n\n\nx\n\n\nx\n\n\nx\n\n\nx\n\n\n\n\n\n\nSqoop\n\n\nx\n\n\nx\n\n\n\n\n\n\n\n\n\n\nZooKeeper\n\n\nx\n\n\nx\n\n\nx\n\n\nx\n\n\n\n\n\n\nAmbari Metrics\n\n\nx\n\n\nx\n\n\nx\n\n\nx\n\n\n\n\n\n\nSpark 1.6\n\n\nx\n\n\nx\n\n\n\n\nx\n\n\n\n\n\n\nSpark 2.0\n\n\n\n\n\n\nx\n\n\n\n\n\n\n\n\nZeppelin 0.6.0\n\n\nx\n\n\n\n\n\n\nx\n\n\n\n\n\n\nSlider\n\n\n\n\n\n\n\n\nx", 
            "title": "Blueprints"
        }, 
        {
            "location": "/blueprints/index.html#blueprints", 
            "text": "Ambari blueprints  are your declarative definition of a Hadoop cluster, defining the host groups and which components to install on which host group. Ambari uses them as a base for your clusters.   You have three options concerning using blueprints with Cloudbreak:   Use one of the pre-defined blueprints.      Add your custom blueprint by uploading a JSON file or pasting the JSON text.   Copy and edit one of the pre-defined blueprints.    We recommend that you review the default blueprints to check if they meet your requirements. You can do this by expanding  the  manage bluerints  pane in the Cloudbreak web UI or by reading the documentation below.", 
            "title": "Blueprints"
        }, 
        {
            "location": "/blueprints/index.html#use-default-blueprints", 
            "text": "To use one of the default blueprints, simply select them when creating a cluster. The option is available on the  General Configuration  page. First select the  Stack Version  and then select your chosen blueprint under  Cluster Type .   To review default blueprints, refer to  Default Blueprints .", 
            "title": "Use Default Blueprints"
        }, 
        {
            "location": "/blueprints/index.html#add-custom-blueprint", 
            "text": "This option allows you to save your custom blueprints. For correct blueprint layout and other useful information about Ambari blueprints, refer to the  Ambari cwiki  page.", 
            "title": "Add Custom Blueprint"
        }, 
        {
            "location": "/blueprints/index.html#creating-a-blueprint", 
            "text": "Ambari blueprints are specified in the JSON format. After you provide the blueprint to Cloudbreak, the host groups in the JSON will be mapped to a set of instances when starting the cluster, and the specified services and components will be installed on the corresponding nodes. It is not necessary to define a complete configuration in the blueprint. If a configuration is missing, Ambari will fill that with a default value. \nFurthermore, a blueprint can be modified later from the Ambari UI.  A blueprint can be exported from a running Ambari cluster and can be reused in Cloudbreak after slight modifications. When a blueprint is exported, some configurations are hardcoded for example domain names, memory configurations, and so on, that won't be applicable to the Cloudbreak cluster. There is no automatic way to modify an exported blueprint and make it instantly usable in Cloudbreak, the modifications have to be done manually.  Also see  Example Blueprints  and  Blueprint Components .", 
            "title": "Creating a Blueprint"
        }, 
        {
            "location": "/blueprints/index.html#upload-a-blueprint", 
            "text": "Once you have your blueprint ready, perform these steps.  Steps    Navigate to the  manage blueprints  tab. To add your own blueprint, click  +create blueprint  and enter the following parameters:     Parameter  Value      Name  Enter a name for your blueprint.    Description  (Optional) Enter a description for your blueprint.    Blueprint Source  Select one of:  Text : Paste blueprint in JSON format.   File : Upload a file that contains the blueprint.   URL : Specify the URL for your blueprint.    Public In Account  (Optional) If this option is checked, all the users belonging to your account will be able to use this blueprint to create clusters, but they cannot delete it.       To use the uploaded blueprints, simply select it when creating a cluster. The option is available on the  General Configuration  page. First select the  Stack Version  and then select your chosen blueprint under  Cluster Type .", 
            "title": "Upload a Blueprint"
        }, 
        {
            "location": "/blueprints/index.html#copy-and-edit-existing-blueprint", 
            "text": "You can reuse default and previously added blueprints by using the  copy   edit  option, which allows you to clone and edit an existing blueprint without changing or deleting the original blueprint.   Steps    Navigate to the  manage blueprints  tab, expand the blueprint entry, and then click  copy   edit .    Make updates in your blueprint and save them as a new blueprint.", 
            "title": "Copy and Edit Existing Blueprint"
        }, 
        {
            "location": "/blueprints/index.html#modifying-existing-blueprint", 
            "text": "To modify existing an blueprint without keeping the original:   If you pasted or uploaded your blueprint in the Cloudbreak UI, in order to modify it you must delete the entry and add the blueprint again in a new entry.       If you provided a URL to the location where the blueprint is stored, in order to modify the blueprint simply update it in the location to which the URL is pointing.    You can also  copy and edit an existing blueprint .", 
            "title": "Modifying Existing Blueprint"
        }, 
        {
            "location": "/blueprints/index.html#delete-blueprint", 
            "text": "You can delete previously added items by selecting and item and using the  delete  option.", 
            "title": "Delete Blueprint"
        }, 
        {
            "location": "/blueprints/index.html#example-blueprints", 
            "text": "Here are blueprint examples:     Smallest Possible HDP 2.4  Small HDP 2.4  Small HDP 2.4 Streaming  Small HDP 2.4 Spark", 
            "title": "Example Blueprints"
        }, 
        {
            "location": "/blueprints/index.html#blueprint-components", 
            "text": "Ambari supports the concept of stacks and associated services in a stack definition. By leveraging the stack definition, Ambari has a consistent and defined interface to install, manage, and monitor a set of services, and provides extensibility model for new stacks and services to be introduced.  At a high level, the supported components can be grouped into two main categories: master and slave. The components are bundled together, forming specific services:     Service  Componets      HDFS  DATANODE, HDFS_CLIENT, JOURNALNODE, NAMENODE, SECONDARY_NAMENODE, ZKFC    YARN  APP_TIMELINE_SERVER, NODEMANAGER, RESOURCEMANAGER, YARN_CLIENT    MAPREDUCE2  HISTORYSERVER, MAPREDUCE2_CLIENT    HBASE  HBASE_CLIENT, HBASE_MASTER, HBASE_REGIONSERVER    HIVE  HIVE_CLIENT, HIVE_METASTORE, HIVE_SERVER, MYSQL_SERVER    HCATALOG  HCAT    WEBHCAT  WEBHCAT_SERVER    OOZIE  OOZIE_CLIENT, OOZIE_SERVER    PIG  PIG    SQOOP  SQOOP    STORM  DRPC_SERVER, NIMBUS, STORM_REST_API, STORM_UI_SERVER, SUPERVISOR    TEZ  TEZ_CLIENT    FALCON  FALCON_CLIENT, FALCON_SERVER    ZOOKEEPER  ZOOKEEPER_CLIENT, ZOOKEEPER_SERVER    SPARK  SPARK_JOBHISTORYSERVER, SPARK_CLIENT    RANGER  RANGER_USERSYNC, RANGER_ADMIN    AMBARI_METRICS  AMBARI_METRICS, METRICS_COLLECTOR, METRICS_MONITOR    KERBEROS  KERBEROS_CLIENT    FLUME  FLUME_HANDLER    KAFKA  KAFKA_BROKER    KNOX  KNOX_GATEWAY    ATLAS  ATLAS    CLOUDBREAK  CLOUDBREAK", 
            "title": "Blueprint Components"
        }, 
        {
            "location": "/blueprints/index.html#default-blueprints", 
            "text": "Cloudbreak includes the following default HDP cluster blueprints:  HDP Version:  HDP 2.6     Cluster Type  Services  Description      Data Science   Spark 1.6, Zeppelin 0.7.0  This cluster configuration includes Spark 1.6 with Zeppelin.    Data Science   Spark 2.1, Zeppelin 0.7.0  This cluster configuration includes Spark 2.1 with Zeppelin.    EDW - Analytics   Hive 2 LLAP , Zeppelin 0.7.0  This cluster configuration includes Hive 2 LLAP.    EDW - ETL   Hive 1.2.1, Spark 1.6  This cluster configuration includes Hive and Spark 1.6.    EDW - ETL   Hive 1.2.1,  Spark 2.1  This cluster configuration includes Hive and Spark 2.1.    BI   Druid 0.9.2  This cluster configuration includes a Technical Preview of Druid.     HDP Version:  HDP 2.5     Cluster Type  Services  Description      Data Science   Spark 1.6, Zeppelin 0.6.0  This cluster configuration includes Spark 1.6 and Zeppelin.    EDW - ETL   Hive 1.2.1, Spark 1.6  This cluster configuration includes Hive and Spark 1.6.    EDW - ETL   Hive 1.2.1,  Spark 2.0  This cluster configuration includes a Technical Preview of Spark 2.0.    EDW - Analytics   Hive 2 LLAP , Zeppelin 0.6.0  This cluster configuration includes a Technical Preview of Hive 2 LLAP.     \n     Choosing Your Configuration \n     \nWhen creating a cluster, you can choose a more stable cluster configuration for a predicable experience.\nAlternatively, you can try the latest capabilities by choosing a cluster configuration\nthat is much more experimental. The following configuration classification applies:   Stable configurations are the best choice if you want to avoid issues and other problems with launching and using clusters.   If you want to use a Technical Preview version of a component in a release of HDP, use these configurations.   These are the most cutting edge of the configurations, including Technical Preview components in a Technical Preview HDP release.     The following services are included in the respective blueprints:  HDP Version:  HDP 2.6     Service  Data Science (Spark 1.6)  Data Science (Spark 2.1)  EDW-ETL (Spark 1.6)  EDW-ETL (Spark 2.1)  EDW-Analytics  BI-Druid      HDFS  x  x  x  x  x  x    YARN  x  x  x  x  x  x    MapReduce2  x  x  x  x  x  x    Tez  x  x  x  x  x  x    Hive 1.2.1  x  x  x  x      Hive 2 LLAP      x     Druid       x    Pig  x  x  x  x  x     Sqoop  x  x  x    x    ZooKeeper  x  x  x  x  x  x    Ambari Metrics  x  x  x  x  x  x    Spark 1.6  x   x   x     Spark 2.1   x   x      Zeppelin 0.7.0  x  x    x     Slider      x      HDP Version:  HDP 2.5     Service  Data Science  EDW-ETL (Spark 1.6)  EDW-ETL (Spark 2.0)  EDW-Analytics      HDFS  x  x  x  x    YARN  x  x  x  x    MapReduce2  x  x  x  x    Tez  x  x  x  x    Hive 1.2.1  x  x  x     Hive 2 LLAP     x    Pig  x  x  x  x    Sqoop  x  x      ZooKeeper  x  x  x  x    Ambari Metrics  x  x  x  x    Spark 1.6  x  x   x    Spark 2.0    x     Zeppelin 0.6.0  x    x    Slider     x", 
            "title": "Default Blueprints"
        }, 
        {
            "location": "/recipes/index.html", 
            "text": "Recipes\n\n\nAlthough Cloudbreak lets you provision HDP clusters in the cloud based on custom Ambari blueprints, Cloudbreak provisioning options don't consider all possible use cases. For that reason, we introduced recipes. \n\n\nA recipe is a script that runs on all nodes of a selected node group before or after the Ambari cluster installation. You can use recipes for tasks such as installing additional software or performing advanced cluster configuration. For example, you can use a recipe to put a JAR file on the Hadoop classpath.\n\n\nWhen creating a cluster, you can optionally upload one or more \"recipes\" (custom scripts) and they will be executed on a specific host group before or after the cluster installation. \n\n\nWriting Recipes\n\n\nWhen using recipes, consider the following:\n\n\n\n\nThe scripts will be executed on the node types you specify (such as \"master\", \"worker\", \"compute\"). If you want to run a a script on all nodes, define the recipe one per node type.  \n\n\nThe script will execute on all of the nodes of that type as root.  \n\n\nIn order to be executed, your script must be in a network location which is accessible from the cloud controller and cluster instances VPC.  \n\n\nMake sure to follow Linux best practices when creating your scripts. For example, don't forget to script \"Yes\" auto-answers where needed.  \n\n\nDo not execute yum update \u2013y since it may update other components on the instances (such as salt), which can create unintended or unstable behavior.   \n\n\nThe scripts will be executed as root. The recipe output is written to \n/var/log/recipes\n on each node on which it was executed.\n\n\n\n\nSample Recipe for Yum Proxy Setting\n\n\n#!/bin/bash\ncat \n /etc/yum.conf \nENDOF\nproxy=http://10.0.0.133:3128\nENDOF\n\n\n\n\nAdding Recipes\n\n\nTo add a recipe, perform these steps.\n\n\nSteps\n\n\n\n\n\n\nPlace your script in a network location accessible from Cloudbreak and cluster instances virtual network. \n\n\n\n\n\n\nDefine the recipe when creating a cluster using the Cloudbreak UI or Cloudbreak Shell. You must provide:\n\n\n\n\n\n\n\n\nParameter\n\n\nValue\n\n\n\n\n\n\n\n\n\n\nName\n\n\nEnter a name for your recipe.\n\n\n\n\n\n\nDescription\n\n\n(Optional) Enter a description for your recipe.\n\n\n\n\n\n\nExecution Type\n\n\nSelect \nPRE\n or \nPOST\n, depending on whether you want the script to be executed prior to or post Ambari cluster deployment.\n\n\n\n\n\n\nScript\n\n\nSelect one of: \nScript\n: Paste the script.\n \nFile\n: Point to a file on your machine that contains the recipe.\n \nURL\n: Specify the URL for your recipe.\n\n\n\n\n\n\nPublic In Account\n\n\n(Optional) If this option is checked, all the users belonging to your account will be able to use this recipe to create clusters, but they cannot delete it.\n\n\n\n\n\n\n\n\nExample: \n\n\n \n\n\n\n\n\n\nWhen creating a cluster, select \nShow Advanced Options\n \n \nChoose Blueprint\n and specify which recipe you want to execute on which host group. \n\n\nExample: \n\n\n \n\n\n\n\n\n\nDeleting Recipes\n\n\nYou can delete previously added items by selecting and item and using the \ndelete\n option. \n\n\nModifying Recipes\n\n\nTo modify previously added recipes:\n\n\n\n\nIf you pasted or uploaded your recipe in the Cloudbreak UI, in order to modify it you must delete the entry and add the recipe again in a new entry.   \n\n\nIf you provided a URL to the location where the recipe is stored, in order to modify the recipe simply update it in the location to which the URL is pointing.", 
            "title": "Recipes"
        }, 
        {
            "location": "/recipes/index.html#recipes", 
            "text": "Although Cloudbreak lets you provision HDP clusters in the cloud based on custom Ambari blueprints, Cloudbreak provisioning options don't consider all possible use cases. For that reason, we introduced recipes.   A recipe is a script that runs on all nodes of a selected node group before or after the Ambari cluster installation. You can use recipes for tasks such as installing additional software or performing advanced cluster configuration. For example, you can use a recipe to put a JAR file on the Hadoop classpath.  When creating a cluster, you can optionally upload one or more \"recipes\" (custom scripts) and they will be executed on a specific host group before or after the cluster installation.", 
            "title": "Recipes"
        }, 
        {
            "location": "/recipes/index.html#writing-recipes", 
            "text": "When using recipes, consider the following:   The scripts will be executed on the node types you specify (such as \"master\", \"worker\", \"compute\"). If you want to run a a script on all nodes, define the recipe one per node type.    The script will execute on all of the nodes of that type as root.    In order to be executed, your script must be in a network location which is accessible from the cloud controller and cluster instances VPC.    Make sure to follow Linux best practices when creating your scripts. For example, don't forget to script \"Yes\" auto-answers where needed.    Do not execute yum update \u2013y since it may update other components on the instances (such as salt), which can create unintended or unstable behavior.     The scripts will be executed as root. The recipe output is written to  /var/log/recipes  on each node on which it was executed.", 
            "title": "Writing Recipes"
        }, 
        {
            "location": "/recipes/index.html#sample-recipe-for-yum-proxy-setting", 
            "text": "#!/bin/bash\ncat   /etc/yum.conf  ENDOF\nproxy=http://10.0.0.133:3128\nENDOF", 
            "title": "Sample Recipe for Yum Proxy Setting"
        }, 
        {
            "location": "/recipes/index.html#adding-recipes", 
            "text": "To add a recipe, perform these steps.  Steps    Place your script in a network location accessible from Cloudbreak and cluster instances virtual network.     Define the recipe when creating a cluster using the Cloudbreak UI or Cloudbreak Shell. You must provide:     Parameter  Value      Name  Enter a name for your recipe.    Description  (Optional) Enter a description for your recipe.    Execution Type  Select  PRE  or  POST , depending on whether you want the script to be executed prior to or post Ambari cluster deployment.    Script  Select one of:  Script : Paste the script.   File : Point to a file on your machine that contains the recipe.   URL : Specify the URL for your recipe.    Public In Account  (Optional) If this option is checked, all the users belonging to your account will be able to use this recipe to create clusters, but they cannot delete it.     Example:        When creating a cluster, select  Show Advanced Options     Choose Blueprint  and specify which recipe you want to execute on which host group.   Example:", 
            "title": "Adding Recipes"
        }, 
        {
            "location": "/recipes/index.html#deleting-recipes", 
            "text": "You can delete previously added items by selecting and item and using the  delete  option.", 
            "title": "Deleting Recipes"
        }, 
        {
            "location": "/recipes/index.html#modifying-recipes", 
            "text": "To modify previously added recipes:   If you pasted or uploaded your recipe in the Cloudbreak UI, in order to modify it you must delete the entry and add the recipe again in a new entry.     If you provided a URL to the location where the recipe is stored, in order to modify the recipe simply update it in the location to which the URL is pointing.", 
            "title": "Modifying Recipes"
        }, 
        {
            "location": "/tags/index.html", 
            "text": "Resource Tagging\n\n\nWhen you manually create resources (such as VMs) in the cloud, you have an option to add custom tags that help you track these resources. Likewise, when creating clusters, you can instruct Cloudbreak to tag the cloud resources that it creates on your behalf.\n\n\nThe tags added during cluster creation will be displayed on your cloud account, allowing you to track your resources. \n\n\nYou can use tags to categorize your cloud resources by purpose, owner, and so on. Tags come in especially handy when you are using a corporate AWS account and you want to quickly identify which resources belong to your cluster(s). In fact, your corporate cloud account admin may require you to tag all the resources that you create, in particular resources, such as VMs, which incur charges.\n\n\nAdd Tags When Creating a Cluster\n\n\nYou can tag the cloud resources used for a cluster by providing custom tag names and values when creating a cluster via UI or CLI. In the UI, this option is available on the \nConfigure Cluster\n page \n \nTags\n.\n\n\nNote that:\n\n\n\n\nIt is not possible to add tags after your cluster has been created.  \n\n\nWhen you clone your cluster, all tags associated with the source cluster will be added to the template of the clone.  \n\n\nWhen you save a cluster template, all tags will be saved as part of the template, and they will be listed on the cluster template page.  \n\n\n\n\nAdd Tags in Profile (AWS)\n\n\nIn order to differentiate launched instances, you can optionally define custom tags for your AWS resources deployed by Cloudbreak. \n\n\n\n\n\n\nIf you want just one custom tag for your Cloudformation resources, set this variable in the \nProfile\n:\n\n\nexport CB_AWS_DEFAULT_CF_TAG=mytagcontent\n\n\nIn this example, the name of the tag will be \nCloudbreakId\n and the value will be \nmytagcontent\n.\n\n\n\n\n\n\nIf you prefer to customize the tag name, set this variable:\n\n\nexport CB_AWS_CUSTOM_CF_TAGS=mytagname:mytagvalue\n\n\nIn this example the name of the tag will be \nmytagname\n and the value will be \nmytagvalue\n. \n\n\n\n\n\n\nYou can specify a list of tags with a comma separated list: \n\n\nexport CB_AWS_CUSTOM_CF_TAGS=tag1:value1,tag2:value2,tag3:value3\n\n\n\n\n\n\nCloud Provider Documentation\n\n\nTo learn more about tags and their restrictions, refer to the cloud provider documentation:\n\n\n\n\nTags on AWS\n    \n\n\nTags on Azure\n  \n\n\nLabels on GCP\n  \n\n\nTags on OpenStack", 
            "title": "Resource Tagging"
        }, 
        {
            "location": "/tags/index.html#resource-tagging", 
            "text": "When you manually create resources (such as VMs) in the cloud, you have an option to add custom tags that help you track these resources. Likewise, when creating clusters, you can instruct Cloudbreak to tag the cloud resources that it creates on your behalf.  The tags added during cluster creation will be displayed on your cloud account, allowing you to track your resources.   You can use tags to categorize your cloud resources by purpose, owner, and so on. Tags come in especially handy when you are using a corporate AWS account and you want to quickly identify which resources belong to your cluster(s). In fact, your corporate cloud account admin may require you to tag all the resources that you create, in particular resources, such as VMs, which incur charges.", 
            "title": "Resource Tagging"
        }, 
        {
            "location": "/tags/index.html#add-tags-when-creating-a-cluster", 
            "text": "You can tag the cloud resources used for a cluster by providing custom tag names and values when creating a cluster via UI or CLI. In the UI, this option is available on the  Configure Cluster  page    Tags .  Note that:   It is not possible to add tags after your cluster has been created.    When you clone your cluster, all tags associated with the source cluster will be added to the template of the clone.    When you save a cluster template, all tags will be saved as part of the template, and they will be listed on the cluster template page.", 
            "title": "Add Tags When Creating a Cluster"
        }, 
        {
            "location": "/tags/index.html#add-tags-in-profile-aws", 
            "text": "In order to differentiate launched instances, you can optionally define custom tags for your AWS resources deployed by Cloudbreak.     If you want just one custom tag for your Cloudformation resources, set this variable in the  Profile :  export CB_AWS_DEFAULT_CF_TAG=mytagcontent  In this example, the name of the tag will be  CloudbreakId  and the value will be  mytagcontent .    If you prefer to customize the tag name, set this variable:  export CB_AWS_CUSTOM_CF_TAGS=mytagname:mytagvalue  In this example the name of the tag will be  mytagname  and the value will be  mytagvalue .     You can specify a list of tags with a comma separated list:   export CB_AWS_CUSTOM_CF_TAGS=tag1:value1,tag2:value2,tag3:value3", 
            "title": "Add Tags in Profile (AWS)"
        }, 
        {
            "location": "/tags/index.html#cloud-provider-documentation", 
            "text": "To learn more about tags and their restrictions, refer to the cloud provider documentation:   Tags on AWS       Tags on Azure     Labels on GCP     Tags on OpenStack", 
            "title": "Cloud Provider Documentation"
        }, 
        {
            "location": "/images/index.html", 
            "text": "Custom Images\n\n\n\n\nThis feature is \nTECHNICAL PREVIEW\n.\n\n\n\n\nBy default, Cloudbreak launches clusters from an image that includes default configuration and default tooling for provisioning. These are considered the Standard default images.\n\n\nIn some cases, these default images might not fit the requirements of users: for example when they need custom OS hardening, libraries, tooling, and so on. In such cases, the user would like to start their clusters from their own custom image.\n\n\nBuilding Custom Images\n\n\nRefer to \nCustom Images for Cloudbreak\n. This repository includes instructions and scripts to help you build custom images. Once you have the images, refer to the documentation below for information on how to register and use these images with Cloudbreak.\n\n\nRegistering Custom Images\n\n\nRegister your custom images in Cloudbreak by placing \nyml\n files that declare your custom images in the \n/var/lib/cloudbreak-deployment/etc\n directory on the Cloudbreak host. The \netc\n directory does not exist by default, so you need to create it.\n\n\nThe format of the \nyml\n files is cloud provider specific and described in the following sections.  \n\n\n\n\nIf you register the images after Cloudbreak has been started, you need to restart Cloudbreak after updating the images.\n\n\n\n\nRegister Images for AWS\n\n\nTo override the default images, perform these steps.\n\n\nSteps\n\n\n\n\nNavigate to the \n/var/lib/cloudbreak-deployment/\n directory and create a new directory called \netc\n.\n\n\nNavigate to \n/var/lib/cloudbreak-deployment/etc/\n and create a new file called \naws-images.yml\n. Use the content below as base content for \naws-images.yml\n but replace the images listed with your custom images for each region that you want to use:\n\n\n\n\n\naws:\n  ap-northeast-1: ami-76729917\n  ap-northeast-2: ami-7c1ad112\n  ap-southeast-1: ami-a7ac7fc4\n  ap-southeast-2: ami-acf7decf\n  eu-central-1: ami-71da331e\n  eu-west-1: ami-cba43bb8\n  sa-east-1: ami-f8901a94\n  us-east-1: ami-48ba9d5e\n  us-west-1: ami-b76421d7\n  us-west-2: ami-d541bbb5\n\n\n\n\nRegister Images for Azure\n\n\nTo override the default images, perform these steps.\n\n\nSteps\n\n\n\n\nNavigate to the \n/var/lib/cloudbreak-deployment/\n directory and create a new directory called \netc\n.\n\n\nNavigate to \n/var/lib/cloudbreak-deployment/etc/\n and create a new file called \narm-images.yml\n. Use the content below as base content for \narm-images.yml\n but replace the images listed with your custom images for each region that you want to use:\n\n\n\n\nazure_rm:\n  East Asia: https://sequenceiqeastasia2.blob.core.windows.net/images/cb-2016-06-14-03-27.vhd\n  East US: https://sequenceiqeastus2.blob.core.windows.net/images/cb-2016-06-14-03-27.vhd\n  Central US: https://sequenceiqcentralus2.blob.core.windows.net/images/cb-2016-06-14-03-27.vhd\n  North Europe: https://sequenceiqnortheurope2.blob.core.windows.net/images/cb-2016-06-14-03-27.vhd\n  South Central US: https://sequenceiqouthcentralus2.blob.core.windows.net/images/cb-2016-06-14-03-27.vhd\n  North Central US: https://sequenceiqorthcentralus2.blob.core.windows.net/images/cb-2016-06-14-03-27.vhd\n  East US 2: https://sequenceiqeastus22.blob.core.windows.net/images/cb-2016-06-14-03-27.vhd\n  Japan East: https://sequenceiqjapaneast2.blob.core.windows.net/images/cb-2016-06-14-03-27.vhd\n  Japan West: https://sequenceiqjapanwest2.blob.core.windows.net/images/cb-2016-06-14-03-27.vhd\n  Southeast Asia: https://sequenceiqsoutheastasia2.blob.core.windows.net/images/cb-2016-06-14-03-27.vhd\n  West US: https://sequenceiqwestus2.blob.core.windows.net/images/cb-2016-06-14-03-27.vhd\n  West Europe: https://sequenceiqwesteurope2.blob.core.windows.net/images/cb-2016-06-14-03-27.vhd\n  Brazil South: https://sequenceiqbrazilsouth2.blob.core.windows.net/images/cb-2016-06-14-03-27.vhd\n\n\n\n\nRegister Images for GCP\n\n\nTo override the default images, perform these steps.\n\n\nSteps\n\n\n\n\nNavigate to the \n/var/lib/cloudbreak-deployment/\n directory and create a new directory called \netc\n.\n\n\nNavigate to \n/var/lib/cloudbreak-deployment/etc/\n and create a new file called \ngcp-images.yml\n. Use the content below as base content for \ngcp-images.yml\n but replace the images listed with your custom images for each region that you want to use:\n\n\n\n\n\ngcp:\n  default: sequenceiqimage/cb-2016-06-14-03-27.tar.gz\n\n\n\n\nRegister Images for OpenStack\n\n\nTo override the default images, perform these steps.\n\n\nSteps\n\n\n\n\nNavigate to the \n/var/lib/cloudbreak-deployment/\n directory and create a new directory called \netc\n.\n\n\nNavigate to \n/var/lib/cloudbreak-deployment/etc/\n and create a new file called \nos-images.yml\n. Use the content below as base content for \nos-images.yml\n but replace the images listed with your custom images for each region that you want to use:\n\n\n\n\n\nopenstack:\n  default: cloudbreak-2016-06-14-10-58", 
            "title": "Custom Images"
        }, 
        {
            "location": "/images/index.html#custom-images", 
            "text": "This feature is  TECHNICAL PREVIEW .   By default, Cloudbreak launches clusters from an image that includes default configuration and default tooling for provisioning. These are considered the Standard default images.  In some cases, these default images might not fit the requirements of users: for example when they need custom OS hardening, libraries, tooling, and so on. In such cases, the user would like to start their clusters from their own custom image.", 
            "title": "Custom Images"
        }, 
        {
            "location": "/images/index.html#building-custom-images", 
            "text": "Refer to  Custom Images for Cloudbreak . This repository includes instructions and scripts to help you build custom images. Once you have the images, refer to the documentation below for information on how to register and use these images with Cloudbreak.", 
            "title": "Building Custom Images"
        }, 
        {
            "location": "/images/index.html#registering-custom-images", 
            "text": "Register your custom images in Cloudbreak by placing  yml  files that declare your custom images in the  /var/lib/cloudbreak-deployment/etc  directory on the Cloudbreak host. The  etc  directory does not exist by default, so you need to create it.  The format of the  yml  files is cloud provider specific and described in the following sections.     If you register the images after Cloudbreak has been started, you need to restart Cloudbreak after updating the images.", 
            "title": "Registering Custom Images"
        }, 
        {
            "location": "/images/index.html#register-images-for-aws", 
            "text": "To override the default images, perform these steps.  Steps   Navigate to the  /var/lib/cloudbreak-deployment/  directory and create a new directory called  etc .  Navigate to  /var/lib/cloudbreak-deployment/etc/  and create a new file called  aws-images.yml . Use the content below as base content for  aws-images.yml  but replace the images listed with your custom images for each region that you want to use:   \naws:\n  ap-northeast-1: ami-76729917\n  ap-northeast-2: ami-7c1ad112\n  ap-southeast-1: ami-a7ac7fc4\n  ap-southeast-2: ami-acf7decf\n  eu-central-1: ami-71da331e\n  eu-west-1: ami-cba43bb8\n  sa-east-1: ami-f8901a94\n  us-east-1: ami-48ba9d5e\n  us-west-1: ami-b76421d7\n  us-west-2: ami-d541bbb5", 
            "title": "Register Images for AWS"
        }, 
        {
            "location": "/images/index.html#register-images-for-azure", 
            "text": "To override the default images, perform these steps.  Steps   Navigate to the  /var/lib/cloudbreak-deployment/  directory and create a new directory called  etc .  Navigate to  /var/lib/cloudbreak-deployment/etc/  and create a new file called  arm-images.yml . Use the content below as base content for  arm-images.yml  but replace the images listed with your custom images for each region that you want to use:   azure_rm:\n  East Asia: https://sequenceiqeastasia2.blob.core.windows.net/images/cb-2016-06-14-03-27.vhd\n  East US: https://sequenceiqeastus2.blob.core.windows.net/images/cb-2016-06-14-03-27.vhd\n  Central US: https://sequenceiqcentralus2.blob.core.windows.net/images/cb-2016-06-14-03-27.vhd\n  North Europe: https://sequenceiqnortheurope2.blob.core.windows.net/images/cb-2016-06-14-03-27.vhd\n  South Central US: https://sequenceiqouthcentralus2.blob.core.windows.net/images/cb-2016-06-14-03-27.vhd\n  North Central US: https://sequenceiqorthcentralus2.blob.core.windows.net/images/cb-2016-06-14-03-27.vhd\n  East US 2: https://sequenceiqeastus22.blob.core.windows.net/images/cb-2016-06-14-03-27.vhd\n  Japan East: https://sequenceiqjapaneast2.blob.core.windows.net/images/cb-2016-06-14-03-27.vhd\n  Japan West: https://sequenceiqjapanwest2.blob.core.windows.net/images/cb-2016-06-14-03-27.vhd\n  Southeast Asia: https://sequenceiqsoutheastasia2.blob.core.windows.net/images/cb-2016-06-14-03-27.vhd\n  West US: https://sequenceiqwestus2.blob.core.windows.net/images/cb-2016-06-14-03-27.vhd\n  West Europe: https://sequenceiqwesteurope2.blob.core.windows.net/images/cb-2016-06-14-03-27.vhd\n  Brazil South: https://sequenceiqbrazilsouth2.blob.core.windows.net/images/cb-2016-06-14-03-27.vhd", 
            "title": "Register Images for Azure"
        }, 
        {
            "location": "/images/index.html#register-images-for-gcp", 
            "text": "To override the default images, perform these steps.  Steps   Navigate to the  /var/lib/cloudbreak-deployment/  directory and create a new directory called  etc .  Navigate to  /var/lib/cloudbreak-deployment/etc/  and create a new file called  gcp-images.yml . Use the content below as base content for  gcp-images.yml  but replace the images listed with your custom images for each region that you want to use:   \ngcp:\n  default: sequenceiqimage/cb-2016-06-14-03-27.tar.gz", 
            "title": "Register Images for GCP"
        }, 
        {
            "location": "/images/index.html#register-images-for-openstack", 
            "text": "To override the default images, perform these steps.  Steps   Navigate to the  /var/lib/cloudbreak-deployment/  directory and create a new directory called  etc .  Navigate to  /var/lib/cloudbreak-deployment/etc/  and create a new file called  os-images.yml . Use the content below as base content for  os-images.yml  but replace the images listed with your custom images for each region that you want to use:   \nopenstack:\n  default: cloudbreak-2016-06-14-10-58", 
            "title": "Register Images for OpenStack"
        }, 
        {
            "location": "/ambari-db/index.html", 
            "text": "Ambari Database\n\n\n\n\nAmbari Database support is technical preview and may not be suitable for production use.\n\n\n\n\nBy default, Ambari uses an embedded database to store data. Ambari and Cloudbreak don't perform backups of this database, so although this database is sufficient for ephemeral or test clusters, it is not sufficient for long-running production clusters. Therefore, you may need to configure a remote database for Ambari in Cloudbreak.\n\n\nThe overall steps are:  \n\n\n\n\nConfigure a remote database. You have two options for configuring a remote database: you can set up a supported database on your own or use a cloud provider database service.  \n\n\nNext, you must pass the details to Cloudbreak during cluster creation, and Cloudbreak will configure Ambari to connect to that remote database. \n\n\n\n\nCloudbreak supports out-of-the-box PostgreSQL, MariaDB, and MySQL. This means that if you are using any of these databases, you only need to create the database itself and configure user permissions to create the cluster. Cloudbreak will initialize database tables, relations, and default values, and will download JDBC driver for Ambari. For other databases, you have to execute create SQL on your database and deliver JDBC driver to \n/opt/jdbc-drivers\n directory on Ambari server node.\n\n\nCreating and Configuring a Remote Datatabase\n\n\nConsider these constraints when setting up your remote datatabase:   \n\n\n\n\nCloudbreak doesn't validate the database connection, so wrong connection parameters will cause the cluster installation to fail.  \n\n\nYour database must be available to the Ambari server. This means that:  \n\n\nThe database must be located in the same region as the Ambari cluster. Slow database connection will cause cluster installation to fail.  \n\n\nThe database could be on a public server with firewall protection, but for security reasons we suggest that you use a private virtual network with subnet, and configure the Cloudbreak network to use existing resources.  \n\n\n\n\n\n\nFor the supported out-of-the-box databases, Cloudbreak creates the tables and upgrades Ambari if necessary, but performing any other operations on the database is your responsibility.  \n\n\nIf you selected PostgreSQL, you must use the \npublic\n schema. It is not possible to use a different schema for Ambari.  \n\n\nDatabase name, username, and password should not contain the \n'\n character. Other special characters are allowed.  \n\n\n\n\nRegister the Database in Cloudbreak UI\n\n\nYou can configure Ambari database during cluster creation. The option is available in advanced options, in the \nConfigure Ambari Database\n tab. You must provide the following information:\n\n\n\n\n\n\n\n\nParameter\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nVendor\n\n\nSelect database vendor from the list.\n\n\n\n\n\n\nHost\n\n\nEnter database host IP.\n\n\n\n\n\n\nPort\n\n\nEnter port number.\n\n\n\n\n\n\nName\n\n\nEnter database name.\n\n\n\n\n\n\nUser Name\n\n\nEnter database user name.\n\n\n\n\n\n\nPassword\n\n\nEnter database password.", 
            "title": "Ambari Database"
        }, 
        {
            "location": "/ambari-db/index.html#ambari-database", 
            "text": "Ambari Database support is technical preview and may not be suitable for production use.   By default, Ambari uses an embedded database to store data. Ambari and Cloudbreak don't perform backups of this database, so although this database is sufficient for ephemeral or test clusters, it is not sufficient for long-running production clusters. Therefore, you may need to configure a remote database for Ambari in Cloudbreak.  The overall steps are:     Configure a remote database. You have two options for configuring a remote database: you can set up a supported database on your own or use a cloud provider database service.    Next, you must pass the details to Cloudbreak during cluster creation, and Cloudbreak will configure Ambari to connect to that remote database.    Cloudbreak supports out-of-the-box PostgreSQL, MariaDB, and MySQL. This means that if you are using any of these databases, you only need to create the database itself and configure user permissions to create the cluster. Cloudbreak will initialize database tables, relations, and default values, and will download JDBC driver for Ambari. For other databases, you have to execute create SQL on your database and deliver JDBC driver to  /opt/jdbc-drivers  directory on Ambari server node.", 
            "title": "Ambari Database"
        }, 
        {
            "location": "/ambari-db/index.html#creating-and-configuring-a-remote-datatabase", 
            "text": "Consider these constraints when setting up your remote datatabase:      Cloudbreak doesn't validate the database connection, so wrong connection parameters will cause the cluster installation to fail.    Your database must be available to the Ambari server. This means that:    The database must be located in the same region as the Ambari cluster. Slow database connection will cause cluster installation to fail.    The database could be on a public server with firewall protection, but for security reasons we suggest that you use a private virtual network with subnet, and configure the Cloudbreak network to use existing resources.      For the supported out-of-the-box databases, Cloudbreak creates the tables and upgrades Ambari if necessary, but performing any other operations on the database is your responsibility.    If you selected PostgreSQL, you must use the  public  schema. It is not possible to use a different schema for Ambari.    Database name, username, and password should not contain the  '  character. Other special characters are allowed.", 
            "title": "Creating and Configuring a Remote Datatabase"
        }, 
        {
            "location": "/ambari-db/index.html#register-the-database-in-cloudbreak-ui", 
            "text": "You can configure Ambari database during cluster creation. The option is available in advanced options, in the  Configure Ambari Database  tab. You must provide the following information:     Parameter  Description      Vendor  Select database vendor from the list.    Host  Enter database host IP.    Port  Enter port number.    Name  Enter database name.    User Name  Enter database user name.    Password  Enter database password.", 
            "title": "Register the Database in Cloudbreak UI"
        }, 
        {
            "location": "/clusters-manage/index.html", 
            "text": "Managing and Monitoring Your Clusters\n\n\nYou can manage monitor your clusters from the Cloudbreak UI. To do that, click on the tile representing the cluster that you want to access: \n\n\n \n\n\nRepairing Your Cluster\n\n\nTo trigger repair process for your cluster, click \nrepair\n. Faulty nodes will be deleted from the cluster and new ones will be added in their place.\n\n\nSynchronizing with Cloud Provider\n\n\nTBD\n\n\nCloning Your Cluster\n\n\nTBD\n\n\nResizing Your Cluster\n\n\nTBD\n\n\nAuto Scaling\n\n\nTBD\n\n\nStopping and Restarting\n\n\nTBD\n\n\nTerminating Your Cluster\n\n\nTo terminate your cluster, click \nterminate\n. All cluster-related resources will be deleted, unless the network is used by other VMs, in which case it will not be deleted. \n\n\nViewing Cluster History\n\n\nFrom the navigation menu in the Cloudbreak UI, you can access the History page that allows you to generate a report showing basic information related to the clusters that were running within the specified range of dates.\n\n\nTo generate a report, follow these steps.\n\n\nSteps\n\n\n\n\n\n\nFrom the Cloudbreak UI navigation menu, select \nHistory\n.\n\n\n\n\n\n\nOn the History page, select the range of dates and click \nShow history\n to generate a report for the selected period.\n\n\n\n\n\n\nHistory Report Content\n\n\n\n\n\n\n\n\n\n\nTO-DO: How are these entries broken down? Is there one entry per instance group?\n\n\n\n\n\n\n\n\n\n\nEach entry in the report represents one cluster instance group. For each entry, the report includes the following information:\n\n\n\n\nCreated\n - The date when your cluster was created (YYYY-MM-DD).\n\n\nProvider\n - The name of the cloud provider (AWS, Azure, Google, or OpenStack) on which the cluster instances are/were running.\n\n\nCluster Name\n - The name that you selected for the cluster.\n\n\nWorker Count\n - The number of worker nodes in the cluster. This number may be a decimal if a cluster has been resized.\n\n\nInstance Type\n - Provider-specific VM type of the cluster instances.\n\n\nInstance Group\n - The name of the instance group.  \n\n\nRegion\n - The AWS region in which your cluster is running.\n\n\nRunning Time (hours)\n - The sum of the running times for all the nodes in the instance group.\n\n\n\n\nThe \nAGGREGATE RUNNING TIME\n is the sum of the Running Times, adjusted for the selected time range.\n\n\nTo learn about how your cloud provider bills you for the VMs, refer to their documentation:\n\n\n\n\nAWS\n      \n\n\nAzure\n     \n\n\nGCP", 
            "title": "Manage and Monitor Clusters"
        }, 
        {
            "location": "/clusters-manage/index.html#managing-and-monitoring-your-clusters", 
            "text": "You can manage monitor your clusters from the Cloudbreak UI. To do that, click on the tile representing the cluster that you want to access:", 
            "title": "Managing and Monitoring Your Clusters"
        }, 
        {
            "location": "/clusters-manage/index.html#repairing-your-cluster", 
            "text": "To trigger repair process for your cluster, click  repair . Faulty nodes will be deleted from the cluster and new ones will be added in their place.", 
            "title": "Repairing Your Cluster"
        }, 
        {
            "location": "/clusters-manage/index.html#synchronizing-with-cloud-provider", 
            "text": "TBD", 
            "title": "Synchronizing with Cloud Provider"
        }, 
        {
            "location": "/clusters-manage/index.html#cloning-your-cluster", 
            "text": "TBD", 
            "title": "Cloning Your Cluster"
        }, 
        {
            "location": "/clusters-manage/index.html#resizing-your-cluster", 
            "text": "TBD", 
            "title": "Resizing Your Cluster"
        }, 
        {
            "location": "/clusters-manage/index.html#auto-scaling", 
            "text": "TBD", 
            "title": "Auto Scaling"
        }, 
        {
            "location": "/clusters-manage/index.html#stopping-and-restarting", 
            "text": "TBD", 
            "title": "Stopping and Restarting"
        }, 
        {
            "location": "/clusters-manage/index.html#terminating-your-cluster", 
            "text": "To terminate your cluster, click  terminate . All cluster-related resources will be deleted, unless the network is used by other VMs, in which case it will not be deleted.", 
            "title": "Terminating Your Cluster"
        }, 
        {
            "location": "/clusters-manage/index.html#viewing-cluster-history", 
            "text": "From the navigation menu in the Cloudbreak UI, you can access the History page that allows you to generate a report showing basic information related to the clusters that were running within the specified range of dates.  To generate a report, follow these steps.  Steps    From the Cloudbreak UI navigation menu, select  History .    On the History page, select the range of dates and click  Show history  to generate a report for the selected period.", 
            "title": "Viewing Cluster History"
        }, 
        {
            "location": "/clusters-manage/index.html#history-report-content", 
            "text": "TO-DO: How are these entries broken down? Is there one entry per instance group?      Each entry in the report represents one cluster instance group. For each entry, the report includes the following information:   Created  - The date when your cluster was created (YYYY-MM-DD).  Provider  - The name of the cloud provider (AWS, Azure, Google, or OpenStack) on which the cluster instances are/were running.  Cluster Name  - The name that you selected for the cluster.  Worker Count  - The number of worker nodes in the cluster. This number may be a decimal if a cluster has been resized.  Instance Type  - Provider-specific VM type of the cluster instances.  Instance Group  - The name of the instance group.    Region  - The AWS region in which your cluster is running.  Running Time (hours)  - The sum of the running times for all the nodes in the instance group.   The  AGGREGATE RUNNING TIME  is the sum of the Running Times, adjusted for the selected time range.  To learn about how your cloud provider bills you for the VMs, refer to their documentation:   AWS         Azure        GCP", 
            "title": "History Report Content"
        }, 
        {
            "location": "/vm-launch/index.html", 
            "text": "Install Cloudbreak in Your Own VM\n\n\nThis is an advanced deployment option. Select this option if you have custom VM requirements. Otherwise, you should use one of the pre-built images and follow these instructions:\n\n\n\n\nLaunch on AWS\n  \n\n\nLaunch on Azure\n  \n\n\nLaunch on GCP\n  \n\n\nLaunch on OpenStack\n   \n\n\n\n\nSystem Requirements\n\n\nTo launch the Cloudbreak deployer and install the Cloudbreak application, your system must meet the following requirements:\n\n\n\n\nMinimum VM requirements: 8GB RAM, 10GB disk, 2 cores\n\n\nSupported operating systems: RHEL, CentOS, and Oracle Linux 7 (64-bit)\n\n\nDocker 1.9.1 must be installed \n\n\n\n\n\n\nYou can install Cloudbreak on Mac OS X for evaluation purposes only. Mac OS X is not supported for a production deployment of Cloudbreak.\n\n\n\n\nPrerequisites\n\n\nTo launch the Cloudbreak deployer and install the Cloudbreak application, you must first meet the following prerequisites:\n\n\nPorts\n\n\nPorts 22 (SSH) and 443 (HTTPS) must be open.\n\n\nRoot Access\n\n\nEvery command must be executed as root. In order to get root privileges execute: \n\n\nsudo -i\n\n\n\nSystem Updates\n\n\nEnsure that your system is up-to-date by executing:\n\n\nyum -y update\n\n\n\nReboot it if necessary.\n\n\nIptables\n\n\nInstall iptables-services:\n\n\nyum -y install iptables-services net-tools\n\n\n\nWithout iptables-services installed the \niptables save\n command will not be available.\n\n\nNext, configure permissive iptables on your machine:\n\n\n\niptables --flush INPUT \n&\n&\n \\\niptables --flush FORWARD \n&\n&\n \\\nservice iptables save\n\n\n\n\nInstall Cloudbreak on Your Own VM\n\n\nInstall Cloudbreak using the following steps.\n\n\nSteps\n\n\n\n\n\n\nInstall the Cloudbreak deployer and unzip the platform-specific single binary to your PATH. For example:\n\n\nyum -y install unzip tar\ncurl -Ls s3.amazonaws.com/public-repo-1.hortonworks.com/HDP/cloudbreak/cloudbreak-deployer_1.16.1_$(uname)_x86_64.tgz | sudo tar -xz -C /bin cbd\ncbd --version\n\n\nOnce the Cloudbreak Deployer is installed, you can set up the Cloudbreak application.\n\n\n\n\n\n\nCreate a Cloudbreak deployment directory and navigate to it:\n\n\nmkdir cloudbreak-deployment\ncd cloudbreak-deployment\n\n\n\n\n\n\nIn the directory, create a file called \nProfile\n with the following content:\n\n\nexport UAA_DEFAULT_SECRET=MY-SECRET\nexport UAA_DEFAULT_USER_PW=MY-PASSWORD\n\n\nFor example:\n\n\nexport UAA_DEFAULT_SECRET=MySecret123\nexport UAA_DEFAULT_USER_PW=MySecurePassword123\n\n\n\n\nYou will need to provide the password when logging in to the Cloudbreak web UI and when using the Cloudbreak Shell. The secret will be used by Cloudbreak for authentication.\n\n\n\n\n\n\n\n\nGenerate configurations by executing:\n\n\nrm *.yml\ncbd generate\n   \n\n\nThe cbd start command includes the cbd generate command which applies the following steps:\n\n\n\n\nCreates the \ndocker-compose.yml\n file, which describes the configuration of all the Docker containers required for the Cloudbreak deployment.  \n\n\nCreates the \nuaa.yml\n file, which holds the configuration of the identity server used to authenticate users with Cloudbreak.   \n\n\n\n\n\n\n\n\nStart the Cloudbreak application by using the following commands:\n\n\ncbd pull\ncbd start\n\n\nThis will start the Docker containers and initialize the application. The first time you start the Coudbreak app, the process will take longer than usual due to the download of all the necessary docker images.\n\n\n\n\n\n\nNext, check Cloudbreak Application logs: \n\n\ncbd logs cloudbreak\n\n\nYou should see a message like this in the log: \nStarted CloudbreakApplication in 36.823 seconds.\n Cloudbreak normally takes less than a minute to start.\n\n\n\n\n\n\nTroubleshooting\n\n\nCbd Cannot Get VM's Public IP\n\n\nBy default the \ncbd\n tool tries to get the VM's public IP to bind Cloudbreak UI to it. But if \ncbd\n cannot get the IP address during the initialization, you must set it manually. Check your \nProfile\n and if \nPUBLIC_IP\n is not set, add the \nPUBLIC_IP\n variable and set it to the public IP of the VM. For example: \n\n\nexport PUBLIC_IP=192.134.23.10\n\n\n\nPermission or Connection Problems\n\n\nIf you face permission or connection issues, disable SELinux:\n\n\n\n\nSet \nSELINUX=disabled\n in \n/etc/selinux/config\n.  \n\n\nReboot the machine.  \n\n\n\n\nEnsure the SELinux is not turned on afterwards:\n\n\n\n\n\n\n\n\nNext Steps\n\n\nFollow the platform-specific instructions. Make sure to review the prerequisites for creating a Cloudbreak credential and then log in to the Cloudbreak web UI and create a credential for Cloubdreak.\n\n\n\n\nLaunch on AWS\n\n\nLaunch on Azure\n\n\nLaunch on GCP\n\n\nLaunch on OpenStack", 
            "title": "Install on Your Own VM"
        }, 
        {
            "location": "/vm-launch/index.html#install-cloudbreak-in-your-own-vm", 
            "text": "This is an advanced deployment option. Select this option if you have custom VM requirements. Otherwise, you should use one of the pre-built images and follow these instructions:   Launch on AWS     Launch on Azure     Launch on GCP     Launch on OpenStack", 
            "title": "Install Cloudbreak in Your Own VM"
        }, 
        {
            "location": "/vm-launch/index.html#system-requirements", 
            "text": "To launch the Cloudbreak deployer and install the Cloudbreak application, your system must meet the following requirements:   Minimum VM requirements: 8GB RAM, 10GB disk, 2 cores  Supported operating systems: RHEL, CentOS, and Oracle Linux 7 (64-bit)  Docker 1.9.1 must be installed     You can install Cloudbreak on Mac OS X for evaluation purposes only. Mac OS X is not supported for a production deployment of Cloudbreak.", 
            "title": "System Requirements"
        }, 
        {
            "location": "/vm-launch/index.html#prerequisites", 
            "text": "To launch the Cloudbreak deployer and install the Cloudbreak application, you must first meet the following prerequisites:", 
            "title": "Prerequisites"
        }, 
        {
            "location": "/vm-launch/index.html#ports", 
            "text": "Ports 22 (SSH) and 443 (HTTPS) must be open.", 
            "title": "Ports"
        }, 
        {
            "location": "/vm-launch/index.html#root-access", 
            "text": "Every command must be executed as root. In order to get root privileges execute:   sudo -i", 
            "title": "Root Access"
        }, 
        {
            "location": "/vm-launch/index.html#system-updates", 
            "text": "Ensure that your system is up-to-date by executing:  yum -y update  Reboot it if necessary.", 
            "title": "System Updates"
        }, 
        {
            "location": "/vm-launch/index.html#iptables", 
            "text": "Install iptables-services:  yum -y install iptables-services net-tools  Without iptables-services installed the  iptables save  command will not be available.  Next, configure permissive iptables on your machine:  \niptables --flush INPUT  & &  \\\niptables --flush FORWARD  & &  \\\nservice iptables save", 
            "title": "Iptables"
        }, 
        {
            "location": "/vm-launch/index.html#install-cloudbreak-on-your-own-vm", 
            "text": "Install Cloudbreak using the following steps.  Steps    Install the Cloudbreak deployer and unzip the platform-specific single binary to your PATH. For example:  yum -y install unzip tar\ncurl -Ls s3.amazonaws.com/public-repo-1.hortonworks.com/HDP/cloudbreak/cloudbreak-deployer_1.16.1_$(uname)_x86_64.tgz | sudo tar -xz -C /bin cbd\ncbd --version  Once the Cloudbreak Deployer is installed, you can set up the Cloudbreak application.    Create a Cloudbreak deployment directory and navigate to it:  mkdir cloudbreak-deployment\ncd cloudbreak-deployment    In the directory, create a file called  Profile  with the following content:  export UAA_DEFAULT_SECRET=MY-SECRET\nexport UAA_DEFAULT_USER_PW=MY-PASSWORD  For example:  export UAA_DEFAULT_SECRET=MySecret123\nexport UAA_DEFAULT_USER_PW=MySecurePassword123   You will need to provide the password when logging in to the Cloudbreak web UI and when using the Cloudbreak Shell. The secret will be used by Cloudbreak for authentication.     Generate configurations by executing:  rm *.yml\ncbd generate      The cbd start command includes the cbd generate command which applies the following steps:   Creates the  docker-compose.yml  file, which describes the configuration of all the Docker containers required for the Cloudbreak deployment.    Creates the  uaa.yml  file, which holds the configuration of the identity server used to authenticate users with Cloudbreak.        Start the Cloudbreak application by using the following commands:  cbd pull\ncbd start  This will start the Docker containers and initialize the application. The first time you start the Coudbreak app, the process will take longer than usual due to the download of all the necessary docker images.    Next, check Cloudbreak Application logs:   cbd logs cloudbreak  You should see a message like this in the log:  Started CloudbreakApplication in 36.823 seconds.  Cloudbreak normally takes less than a minute to start.", 
            "title": "Install Cloudbreak on Your Own VM"
        }, 
        {
            "location": "/vm-launch/index.html#troubleshooting", 
            "text": "", 
            "title": "Troubleshooting"
        }, 
        {
            "location": "/vm-launch/index.html#cbd-cannot-get-vms-public-ip", 
            "text": "By default the  cbd  tool tries to get the VM's public IP to bind Cloudbreak UI to it. But if  cbd  cannot get the IP address during the initialization, you must set it manually. Check your  Profile  and if  PUBLIC_IP  is not set, add the  PUBLIC_IP  variable and set it to the public IP of the VM. For example:   export PUBLIC_IP=192.134.23.10", 
            "title": "Cbd Cannot Get VM's Public IP"
        }, 
        {
            "location": "/vm-launch/index.html#permission-or-connection-problems", 
            "text": "If you face permission or connection issues, disable SELinux:   Set  SELINUX=disabled  in  /etc/selinux/config .    Reboot the machine.     Ensure the SELinux is not turned on afterwards:", 
            "title": "Permission or Connection Problems"
        }, 
        {
            "location": "/vm-launch/index.html#next-steps", 
            "text": "Follow the platform-specific instructions. Make sure to review the prerequisites for creating a Cloudbreak credential and then log in to the Cloudbreak web UI and create a credential for Cloubdreak.   Launch on AWS  Launch on Azure  Launch on GCP  Launch on OpenStack", 
            "title": "Next Steps"
        }, 
        {
            "location": "/credentials/index.html", 
            "text": "Credentials\n\n\nYou can manage Cloudbreak credentials in the \nmanage credentials\n tab by clicking on \n+create credential\n and providing required parameters. You must create at least one credential in order to be able to create a cluster. \n\n\nCreating Cloudbreak Credental\n\n\nFor steps, refer to:\n\n\n\n\nCreate Credential on AWS\n  \n\n\nCreate Credential on Azure\n  \n\n\nCreate Credential on GCP\n \n\n\nCreate Credential on OpenStack\n\n\n\n\nManaging Cloudbrek Credentials\n\n\nYou can manage (add and delete) your credentials from the \nmanage credentials\n tab. \n\n\nAll credentials that was cerated with \"Public In Account\" unchecked (which is the default behavior) are only visible to the user who created them. \n\n\nAll credentials that were cerated with \"Public In Account\" checked are visible to all users of the Cloudbreak instance, but only the user who created them can delete them.", 
            "title": "Manage Cloudbreak Credentials"
        }, 
        {
            "location": "/credentials/index.html#credentials", 
            "text": "You can manage Cloudbreak credentials in the  manage credentials  tab by clicking on  +create credential  and providing required parameters. You must create at least one credential in order to be able to create a cluster.", 
            "title": "Credentials"
        }, 
        {
            "location": "/credentials/index.html#creating-cloudbreak-credental", 
            "text": "For steps, refer to:   Create Credential on AWS     Create Credential on Azure     Create Credential on GCP    Create Credential on OpenStack", 
            "title": "Creating Cloudbreak Credental"
        }, 
        {
            "location": "/credentials/index.html#managing-cloudbrek-credentials", 
            "text": "You can manage (add and delete) your credentials from the  manage credentials  tab.   All credentials that was cerated with \"Public In Account\" unchecked (which is the default behavior) are only visible to the user who created them.   All credentials that were cerated with \"Public In Account\" checked are visible to all users of the Cloudbreak instance, but only the user who created them can delete them.", 
            "title": "Managing Cloudbrek Credentials"
        }, 
        {
            "location": "/upgrade/index.html", 
            "text": "Upgrade Cloudbreak\n\n\nUpdate Cloudbreak Deployer\n\n\nTo upgrade Cloudbreak to the newest version, perform the following steps.\n\n\nSteps\n\n\n\n\n\n\nOn the VM where Cloudbreak is running, navigate to the directory where your Profile file is located:\n\n\ncd /var/lib/cloudbreak-deployment/\n\n\n\n\n\n\nStop all of the running Cloudbreak components:\n\n\ncbd kill\n\n\n\n\n\n\nUpdate Cloudbreak deployer:\n\n\ncbd update\n\n\n\n\n\n\nUpdate the \ndocker-compose.yml\n file with new Docker containers needed for the cbd:\n\n\ncbd regenerate\n\n\n\n\n\n\nIf there are no other Cloudbreak instances that still use old Cloudbreak versions, remove the obsolete containers:\n\n\ncbd util cleanup\n\n\n\n\n\n\nCheck the health and version of the updated cbd:\n\n\ncbd doctor\n\n\n\n\n\n\nStart the new version of the cbd:\n\n\ncbd start\n\n\nCloudbreak needs to download updated docker images for the new version, so this step may take a while.\n\n\n\n\n\n\nIn addition, if you have any clusters running, you must update them using the folloing steps. \n\n\nUpdate Existing Clusters\n\n\nUpgrading from version 1.4.0 to the newest version does not require any manual modification from the users.\n\n\nUpgrading from version 1.3.0 to the newest version requires that you update existing clusters. To update existing clusters, run the following commands on the \ncbgateway\n node of the cluster.\n\n\nSteps\n\n\n\n\n\n\nUpdate the version of the Salt-Bootsrap tool on the nodes:\n    \nsalt '*' cmd.run 'curl -Ls https://github.com/sequenceiq/salt-bootstrap/releases/download/v0.1.2/salt-bootstrap_0.1.2_Linux_x86_64.tgz | tar -zx -C /usr/sbin/ salt-bootstrap'\n\n\n\n\n\n\nTrigger restart of the tool on the nodes:\n\n\nsalt '*' service.dead salt-bootstrap\n\n\n\n\nTo check the version of the Salt-Bootsrap on the nodes, use \nsalt '*' cmd.run 'salt-bootstrap --version'", 
            "title": "Upgrade Clodbreak"
        }, 
        {
            "location": "/upgrade/index.html#upgrade-cloudbreak", 
            "text": "", 
            "title": "Upgrade Cloudbreak"
        }, 
        {
            "location": "/upgrade/index.html#update-cloudbreak-deployer", 
            "text": "To upgrade Cloudbreak to the newest version, perform the following steps.  Steps    On the VM where Cloudbreak is running, navigate to the directory where your Profile file is located:  cd /var/lib/cloudbreak-deployment/    Stop all of the running Cloudbreak components:  cbd kill    Update Cloudbreak deployer:  cbd update    Update the  docker-compose.yml  file with new Docker containers needed for the cbd:  cbd regenerate    If there are no other Cloudbreak instances that still use old Cloudbreak versions, remove the obsolete containers:  cbd util cleanup    Check the health and version of the updated cbd:  cbd doctor    Start the new version of the cbd:  cbd start  Cloudbreak needs to download updated docker images for the new version, so this step may take a while.    In addition, if you have any clusters running, you must update them using the folloing steps.", 
            "title": "Update Cloudbreak Deployer"
        }, 
        {
            "location": "/upgrade/index.html#update-existing-clusters", 
            "text": "Upgrading from version 1.4.0 to the newest version does not require any manual modification from the users.  Upgrading from version 1.3.0 to the newest version requires that you update existing clusters. To update existing clusters, run the following commands on the  cbgateway  node of the cluster.  Steps    Update the version of the Salt-Bootsrap tool on the nodes:\n     salt '*' cmd.run 'curl -Ls https://github.com/sequenceiq/salt-bootstrap/releases/download/v0.1.2/salt-bootstrap_0.1.2_Linux_x86_64.tgz | tar -zx -C /usr/sbin/ salt-bootstrap'    Trigger restart of the tool on the nodes:  salt '*' service.dead salt-bootstrap   To check the version of the Salt-Bootsrap on the nodes, use  salt '*' cmd.run 'salt-bootstrap --version'", 
            "title": "Update Existing Clusters"
        }, 
        {
            "location": "/delete/index.html", 
            "text": "Deleting Resources\n\n\nIn general, you can delete clusters from the Cloudbreak UI. If needed, you can also delete the cluster resources manually via the cloud provider tools. \n\n\nDeleting Resources on AWS\n\n\nTBD\n\n\nDeleting Resources on Azure\n\n\nDeleting Clusters on Azure\n\n\nYou can delete clusters from the Cloudbreak UI. If needed, you can also delete the cluster manually by deleting the whole resource group created when the cluster was deployed. \n\n\nThe name of the resource group, under which the cluster-related resources are organized always includes the name of the cluster, so you should be able to find the group by searching for that name in the \nResource groups\n.\n\n\nDelete Cloudbreak on Azure\n\n\nTo delete Cloudbreak, delete the whole related resource group.\n\n\nSteps\n\n\n\n\nFrom the Microsoft Azure Portal dashboard, select \n.\n\n\n\n\nFind the resource group that you want to delete, click on \n...\n and select \nDelete\n:\n\n\n  \n\n\n\n\n\n\nType the name of the resource group to delete and click \nDelete\n.\n\n\n\n\n\n\nDeleting Resources on GCP\n\n\nTBD\n\n\nDeleting Resources on OpenStack\n\n\nTBD", 
            "title": "Delete Cloudbreak"
        }, 
        {
            "location": "/delete/index.html#deleting-resources", 
            "text": "In general, you can delete clusters from the Cloudbreak UI. If needed, you can also delete the cluster resources manually via the cloud provider tools.", 
            "title": "Deleting Resources"
        }, 
        {
            "location": "/delete/index.html#deleting-resources-on-aws", 
            "text": "TBD", 
            "title": "Deleting Resources on AWS"
        }, 
        {
            "location": "/delete/index.html#deleting-resources-on-azure", 
            "text": "", 
            "title": "Deleting Resources on Azure"
        }, 
        {
            "location": "/delete/index.html#deleting-clusters-on-azure", 
            "text": "You can delete clusters from the Cloudbreak UI. If needed, you can also delete the cluster manually by deleting the whole resource group created when the cluster was deployed.   The name of the resource group, under which the cluster-related resources are organized always includes the name of the cluster, so you should be able to find the group by searching for that name in the  Resource groups .", 
            "title": "Deleting Clusters on Azure"
        }, 
        {
            "location": "/delete/index.html#delete-cloudbreak-on-azure", 
            "text": "To delete Cloudbreak, delete the whole related resource group.  Steps   From the Microsoft Azure Portal dashboard, select  .   Find the resource group that you want to delete, click on  ...  and select  Delete :        Type the name of the resource group to delete and click  Delete .", 
            "title": "Delete Cloudbreak on Azure"
        }, 
        {
            "location": "/delete/index.html#deleting-resources-on-gcp", 
            "text": "TBD", 
            "title": "Deleting Resources on GCP"
        }, 
        {
            "location": "/delete/index.html#deleting-resources-on-openstack", 
            "text": "TBD", 
            "title": "Deleting Resources on OpenStack"
        }, 
        {
            "location": "/profile/index.html", 
            "text": "Configure Cloudbreak via Profile Variables\n\n\nCloudbreak deployer configuration is based on environment variables.  \n\n\nDuring startup, Cloudbreak deployer tries to determine the underlying infrastructure and then sets required environment variables with appropriate default values. If these environment variables are not sufficient for your use case, you can set additional environment variables in your \nProfile\n file. \n\n\nCloudbreak deployer always opens a new bash subprocess without inheriting environment variables. Only the following environment variables are inherited:\n\n\n\n\nHOME\n  \n\n\nDEBUG\n  \n\n\nTRACE\n  \n\n\nCBD_DEFAULT_PROFILE\n  \n\n\nall \nDOCKER_XXX\n \n\n\n\n\nSet Profile Variables\n\n\nTo set environment variables relevant for Cloudbreak Deployer, add them to a file called \nProfile\n located in the Cloudbreak deployment directory (typically \n/var/lib/cloudbreak-deployment\n).\n\n\nThe \nProfile\n file is sourced, so you can use the usual syntax to set configuration values:\n\n\nexport MY_VAR=some_value\nexport MY_OTHER_VAR=another_value \n\n\n\n\nCheck Available Profile Variables\n\n\nTo see all available environment variables with their default values, use:\n\n\ncbd env show\n\n\n\n\nProfile Variables\n\n\nCloudbreak Variables\n\n\nRefer to this list for available environment variables. The variables are listed with their default values. If default is unset, no value is listed. \n\n\n\n\n\n\n\n\nVariable Name\n\n\nDefault Value\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nADDRESS_RESOLVING_TIMEOUT\n\n\n120000\n\n\nDNS lookup timeout for internal service discovery\n\n\n\n\n\n\nCAPTURE_CRON_EXPRESSION\n\n\n\n\nSmartSense bundle generation time interval in cron format\n\n\n\n\n\n\nCBD_CERT_ROOT_PATH\n\n\n\"${PWD}/certs\"\n\n\nPath where deployer stores Cloudbreak certificates. ${PWD} refers to the Cloudbreak deployment directory\n\n\n\n\n\n\nCBD_LOG_NAME\n\n\ncbreak\n\n\nName of the Cloudbreak log file\n\n\n\n\n\n\nCBD_TRAEFIK_TLS\n\n\n\"/certs/traefik/client.pem,/certs/traefik/client-key.pem\"\n\n\nPath inside of the Traefik container where TLS files are located\n\n\n\n\n\n\nCB_BLUEPRINT_DEFAULTS\n\n\n\"hdp-small-default;hdp-spark-cluster;hdp-streaming-cluster\"\n\n\nComma separated list of the default blueprints that Cloudbreak initializes in its database\n\n\n\n\n\n\nCB_BYOS_DFS_DATA_DIR\n\n\n\"/hadoop/hdfs/data\"\n\n\n(Deprecated) Default data directory for BYOS orchestrators\n\n\n\n\n\n\nCB_COMPONENT_CLUSTER_ID\n\n\n\n\nSmartSense component cluster ID\n\n\n\n\n\n\nCB_COMPONENT_ID\n\n\n\n\nSmartSense component ID\n\n\n\n\n\n\nCB_COMPOSE_PROJECT\n\n\ncbreak\n\n\nName of the Docker Compose project; it will appear in container names\n\n\n\n\n\n\nCB_DB_ENV_DB\n\n\n\"cbdb\"\n\n\nName of the Cloudbreak database\n\n\n\n\n\n\nCB_DB_ENV_PASS\n\n\n\"\"\n\n\nPassword for the Cloudbreak database authentication\n\n\n\n\n\n\nCB_DB_ENV_SCHEMA\n\n\n\"public\"\n\n\nSchema used in the Cloudbreak database\n\n\n\n\n\n\nCB_DB_ENV_USER\n\n\n\"postgres\"\n\n\nUser for the Cloudbreak database authentication\n\n\n\n\n\n\nCB_DB_ROOT_PATH\n\n\n\"/var/lib/cloudbreak\"\n\n\n(Deprecated) Location of the database volume on Cloudbreak host\n\n\n\n\n\n\nCB_DEFAULT_SUBSCRIPTION_ADDRESS\n\n\nhttp://uluwatu.service.consul:3000/notifications\n\n\nURL of the default subscription for Cloudbreak notifications\n\n\n\n\n\n\nCB_ENABLEDPLATFORMS\n\n\n\n\nSet this to disable Cloudbreak resource called Platform\n\n\n\n\n\n\nCB_ENABLE_CUSTOM_IMAGE\n\n\n\"false\"\n\n\nSet to \"true\" to enable custom cloud images\n\n\n\n\n\n\nCBD_FORCE_START\n\n\n\n\nSet this to disable docker-compose.yml and uaa.yml validation\n\n\n\n\n\n\nCB_HBM2DDL_STRATEGY\n\n\n\"validate\"\n\n\nConfigures hibernate.hbm2ddl.auto in Cloudbreak\n\n\n\n\n\n\nCB_HOST_DISCOVERY_CUSTOM_DOMAIN\n\n\n\"\"\n\n\nCustom domain of the provisioned cluster\n\n\n\n\n\n\nCB_HTTPS_PROXY\n\n\n\"\"\n\n\nHTTPS proxy URL\n\n\n\n\n\n\nCB_HTTP_PROXY\n\n\n\"\"\n\n\nHTTP proxy URL\n\n\n\n\n\n\nCB_IMAGE_CATALOG_URL\n\n\n\"https://s3-eu-west-1.amazonaws.com/cloudbreak-info/cb-image-catalog.json\"\n\n\nImage catalog URL\n\n\n\n\n\n\nCB_INSTANCE_NODE_ID\n\n\n\n\nUnique identifier of the Cloudbreak node\n\n\n\n\n\n\nCB_INSTANCE_PROVIDER\n\n\n\n\nCloud provider of the Cloudbreak instance\n\n\n\n\n\n\nCB_INSTANCE_REGION\n\n\n\n\nCloud region of the Cloudbreak instance\n\n\n\n\n\n\nCB_INSTANCE_UUID\n\n\n\n\nUnique identifier of Cloudbreak deployment\n\n\n\n\n\n\nCB_JAVA_OPTS\n\n\n\"\"\n\n\nExtra Java options for Autoscale and Cloudbreak\n\n\n\n\n\n\nCB_LOG_LEVEL\n\n\n\"INFO\"\n\n\nLog level of the Cloudbreak service\n\n\n\n\n\n\nCB_MAX_SALT_NEW_SERVICE_RETRY\n\n\n90\n\n\nSalt orchestrator max retry count\n\n\n\n\n\n\nCB_MAX_SALT_RECIPE_EXECUTION_RETRY\n\n\n90\n\n\nSalt orchestrator max retry count for recipes\n\n\n\n\n\n\nCB_PLATFORM_DEFAULT_REGIONS\n\n\n\n\nComma separated list of default regions by platform. For example: \nAWS:eu-west-1\n.\n\n\n\n\n\n\nCB_PRODUCT_ID\n\n\n\n\nSmartSense product ID\n\n\n\n\n\n\nCB_SCHEMA_MIGRATION_AUTO\n\n\ntrue\n\n\nWhen set to true, enables Cloudbreak automatic database schema update\n\n\n\n\n\n\nCB_SMARTSENSE_CONFIGURE\n\n\n\"false\"\n\n\nSet to \u201ctrue\u201d to install and configure SmartSense on cluster nodes\n\n\n\n\n\n\nCB_SMARTSENSE_CLUSTER_NAME_PREFIX\n\n\n\n\nSmartSense Cloudbreak cluster name prefix\n\n\n\n\n\n\nCB_SMARTSENSE_ID\n\n\n\"\"\n\n\nSmartSense subscription ID\n\n\n\n\n\n\nCB_TEMPLATE_DEFAULTS\n\n\n\"minviable-gcp,minviable-azure,minviable-aws\"\n\n\nComma separated list of the default templates that Cloudbreak initializes in its database\n\n\n\n\n\n\nCB_UI_MAX_WAIT\n\n\n400\n\n\nWait timeout for \ncbd start-wait\n command\n\n\n\n\n\n\nCERT_VALIDATION\n\n\n\"true\"\n\n\nWhen set to \"true\", enables cert validation in Cloudbreak and Autoscale\n\n\n\n\n\n\nCLOUDBREAK_SMTP_AUTH\n\n\n\"true\"\n\n\nWhen set to \"true\", configures mail.smtp.auth in Cloudbreak\n\n\n\n\n\n\nCLOUDBREAK_SMTP_SENDER_FROM\n\n\n\"noreply@hortonworks.com\"\n\n\nEmail address of the sender\n\n\n\n\n\n\nCLOUDBREAK_SMTP_SENDER_HOST\n\n\n\"smtp.service.consul\"\n\n\nSMTP server address of the hostname\n\n\n\n\n\n\nCLOUDBREAK_SMTP_SENDER_PASSWORD\n\n\n\"$LOCAL_SMTP_PASSWORD\"\n\n\nSMTP server password\n\n\n\n\n\n\nCLOUDBREAK_SMTP_SENDER_PORT\n\n\n25\n\n\nPort of the SMTP server\n\n\n\n\n\n\nCLOUDBREAK_SMTP_SENDER_USERNAME\n\n\n\"admin\"\n\n\nUsername for SMTP authentication\n\n\n\n\n\n\nCLOUDBREAK_SMTP_STARTTLS_ENABLE\n\n\n\"false\"\n\n\nSet to \"true\" to configure mail.smtp.starttls.enable in Cloudbreak\n\n\n\n\n\n\nCLOUDBREAK_SMTP_TYPE\n\n\n\"smtp\"\n\n\nDefines mail.transport.protocol in CLoudbreak\n\n\n\n\n\n\nCOMMON_DB\n\n\ncommondb\n\n\nName of the database container\n\n\n\n\n\n\nCOMMON_DB_VOL\n\n\ncommon\n\n\nName of the database volume\n\n\n\n\n\n\nCOMPOSE_HTTP_TIMEOUT\n\n\n120\n\n\nDocker Compose execution timeout\n\n\n\n\n\n\nDB_DUMP_VOLUME\n\n\ncbreak_dump\n\n\nName of the database dump volume\n\n\n\n\n\n\nDB_MIGRATION_LOG\n\n\n\"db_migration.log\"\n\n\nDatabase migration log file\n\n\n\n\n\n\nDEFAULT_INBOUND_ACCESS_IP\n\n\n\"\"\n\n\nOpens default ports on AWS instances for Cloudbreak\n\n\n\n\n\n\nDOCKER_CONSUL_OPTIONS\n\n\n\"\"\n\n\nExtra options for Consul\n\n\n\n\n\n\nDOCKER_IMAGE_CBD_SMARTSENSE\n\n\nhortonworks/cbd-smartsense\n\n\nSmartSense Docker image name\n\n\n\n\n\n\nDOCKER_IMAGE_CLOUDBREAK\n\n\nhortonworks/cloudbreak\n\n\nCloudbreak Docker image name\n\n\n\n\n\n\nDOCKER_IMAGE_CLOUDBREAK_AUTH\n\n\nhortonworks/cloudbreak-auth\n\n\nAuthentication service Docker image name\n\n\n\n\n\n\nDOCKER_IMAGE_CLOUDBREAK_PERISCOPE\n\n\nhortonworks/cloudbreak-autoscale\n\n\nAutoscale Docker image name\n\n\n\n\n\n\nDOCKER_IMAGE_CLOUDBREAK_SHELL\n\n\nhortonworks/cloudbreak-shell\n\n\nCloudbreak Shell Docker image name\n\n\n\n\n\n\nDOCKER_IMAGE_CLOUDBREAK_WEB\n\n\nhortonworks/cloudbreak-web\n\n\nWeb UI Docker image name\n\n\n\n\n\n\nDOCKER_TAG_ALPINE\n\n\n3.1\n\n\nAlpine container version\n\n\n\n\n\n\nDOCKER_TAG_CBD_SMARTSENSE\n\n\n0.10.0\n\n\nSmartSense container version\n\n\n\n\n\n\nDOCKER_TAG_CERT_TOOL\n\n\n0.2.0\n\n\nCert tool container version\n\n\n\n\n\n\nDOCKER_TAG_CLOUDBREAK\n\n\n2.1.0-dev.70\n\n\nCloudbreak container version\n\n\n\n\n\n\nDOCKER_TAG_CLOUDBREAK_SHELL\n\n\n2.1.0-dev.70\n\n\nCloudbreak Shell container version\n\n\n\n\n\n\nDOCKER_TAG_CONSUL\n\n\n0.5\n\n\nConsul container version\n\n\n\n\n\n\nDOCKER_TAG_HAVEGED\n\n\n1.1.0\n\n\nHaveged container version\n\n\n\n\n\n\nDOCKER_TAG_LOGROTATE\n\n\n1.0.0\n\n\nLogrotate container version\n\n\n\n\n\n\nDOCKER_TAG_MIGRATION\n\n\n1.0.0\n\n\nMigration container version\n\n\n\n\n\n\nDOCKER_TAG_PERISCOPE\n\n\n2.1.0-dev.70\n\n\nAutoscale container version\n\n\n\n\n\n\nDOCKER_TAG_POSTFIX\n\n\nlatest\n\n\nPostfix container version\n\n\n\n\n\n\nDOCKER_TAG_POSTGRES\n\n\n9.6.1-alpine\n\n\nPostgresql container version\n\n\n\n\n\n\nDOCKER_TAG_REGISTRATOR\n\n\nv5\n\n\nRegistrator container version\n\n\n\n\n\n\nDOCKER_TAG_SULTANS\n\n\n2.1.0-dev.70\n\n\nAuthentication service container version\n\n\n\n\n\n\nDOCKER_TAG_TRAEFIK\n\n\nv1.2.0\n\n\nTraefik container version\n\n\n\n\n\n\nDOCKER_TAG_UAA\n\n\n3.6.5\n\n\nIdentity container version\n\n\n\n\n\n\nDOCKER_TAG_ULUWATU\n\n\n2.1.0-dev.70\n\n\nWeb UI container version\n\n\n\n\n\n\nIDENTITY_DB_NAME\n\n\n\"uaadb\"\n\n\nName of the Identity database\n\n\n\n\n\n\nIDENTITY_DB_PASS\n\n\n\"\"\n\n\nPassword for the Identity database authentication\n\n\n\n\n\n\nIDENTITY_DB_URL\n\n\n\"${COMMON_DB}.service.consul:5432\"\n\n\nURL for the Identity database connection, including the port number\n\n\n\n\n\n\nIDENTITY_DB_USER\n\n\n\"postgres\"\n\n\nUser for the Identity database authentication\n\n\n\n\n\n\nLOCAL_SMTP_PASSWORD\n\n\n\"$UAA_DEFAULT_USER_PW\"\n\n\nDefault password for the internal mail server\n\n\n\n\n\n\nPERISCOPE_DB_HBM2DDL_STRATEGY\n\n\n\"validate\"\n\n\nConfigures hibernate.hbm2ddl.auto in Autoscale\n\n\n\n\n\n\nPERISCOPE_DB_NAME\n\n\n\"periscopedb\"\n\n\nName of the Autoscale database\n\n\n\n\n\n\nPERISCOPE_DB_PASS\n\n\n\"\"\n\n\nPassword for the Autoscale database authentication\n\n\n\n\n\n\nPERISCOPE_DB_SCHEMA_NAME\n\n\n\"public\"\n\n\nSchema used in the Autoscale database\n\n\n\n\n\n\nPERISCOPE_DB_USER\n\n\n\"postgres\"\n\n\nUser for the Autoscale database authentication\n\n\n\n\n\n\nPERISCOPE_DB_TCP_ADDR\n\n\n\n\nAddress of the Autoscale database\n\n\n\n\n\n\nPERISCOPE_DB_TCP_PORT\n\n\n\n\nPort number of the Autoscale database\n\n\n\n\n\n\nPERISCOPE_LOG_LEVEL\n\n\n\"INFO\"\n\n\nLog level of the Autoscale service\n\n\n\n\n\n\nPERISCOPE_SCHEMA_MIGRATION_AUTO\n\n\ntrue\n\n\nWhen set to \"true\", enables Autoscale automatic database schema update\n\n\n\n\n\n\nPUBLIC_IP\n\n\n\n\nIP address or hostname of the public interface\n\n\n\n\n\n\nREST_DEBUG\n\n\n\"false\"\n\n\nSet to \"true\" to enable REST call debug level in Cloudbreak and Autoscale\n\n\n\n\n\n\nSL_ADDRESS_RESOLVING_TIMEOUT\n\n\n\n\nDNS lookup timeout of Authentication service for internal service discovery\n\n\n\n\n\n\nSL_NODE_TLS_REJECT_UNAUTHORIZED\n\n\n\"0\"\n\n\nWhen set to \"0\", enables self-signed certifications in Authentication service\n\n\n\n\n\n\nSULTANS_CONTAINER_PATH\n\n\n/sultans\n\n\nDefault project location in Authentication service container\n\n\n\n\n\n\nTRAEFIK_MAX_IDLE_CONNECTION\n\n\n100\n\n\nSets --maxidleconnsperhost for Traefik to the value entered\n\n\n\n\n\n\nUAA_CLOUDBREAK_ID\n\n\ncloudbreak\n\n\nIdentity of the Cloudbreak scope in Identity\n\n\n\n\n\n\nUAA_CLOUDBREAK_SECRET\n\n\n$UAA_DEFAULT_SECRET\n\n\nSecret of the Cloudbreak scope in Identity\n\n\n\n\n\n\nUAA_CLOUDBREAK_SHELL_ID\n\n\ncloudbreak_shell\n\n\nIdentity of the Cloudbreak Shell scope in Identity\n\n\n\n\n\n\nUAA_DEFAULT_ACCOUNT\n\n\n\"seq1234567.SequenceIQ\"\n\n\nDefault account for users as an Identity group\n\n\n\n\n\n\nUAA_DEFAULT_SECRET\n\n\n\n\nDefault secret for all the scopes and encryptions\n\n\n\n\n\n\nUAA_DEFAULT_USER_EMAIL\n\n\nadmin@example.com\n\n\nEmail address of default admin user\n\n\n\n\n\n\nUAA_DEFAULT_USER_FIRSTNAME\n\n\nJoe\n\n\nFirst name of default admin user\n\n\n\n\n\n\nUAA_DEFAULT_USER_GROUPS\n\n\nSee \nhere\n\n\nDefault user groups of the users\n\n\n\n\n\n\nUAA_DEFAULT_USER_LASTNAME\n\n\nAdmin\n\n\nLast name of default admin user\n\n\n\n\n\n\nUAA_DEFAULT_USER_PW\n\n\n\n\nPassword of default admin user\n\n\n\n\n\n\nUAA_FLEX_USAGE_CLIENT_ID\n\n\nflex_usage_client\n\n\nIdentity of the Flex usage generator scope in Identity\n\n\n\n\n\n\nUAA_FLEX_USAGE_CLIENT_SECRET\n\n\n$UAA_DEFAULT_SECRET\n\n\nSecret of the Flex usage generator scope in Identity\n\n\n\n\n\n\nUAA_PERISCOPE_ID\n\n\nperiscope\n\n\nIdentity of the Autoscale scope in Identity\n\n\n\n\n\n\nUAA_PERISCOPE_SECRET\n\n\n$UAA_DEFAULT_SECRET\n\n\nSecret of the Autoscale scope in Identity\n\n\n\n\n\n\nUAA_PORT\n\n\n8089\n\n\nIdentity service public port\n\n\n\n\n\n\nUAA_SULTANS_ID\n\n\nsultans\n\n\nIdentity of the Authentication service scope in Identity\n\n\n\n\n\n\nUAA_SULTANS_SECRET\n\n\n$UAA_DEFAULT_SECRET\n\n\nSecret of the Authentication service scope in Identity\n\n\n\n\n\n\nUAA_ULUWATU_ID\n\n\nuluwatu\n\n\nIdentity of the Web UI scope in Identity\n\n\n\n\n\n\nUAA_ULUWATU_SECRET\n\n\n$UAA_DEFAULT_SECRET\n\n\nSecret of the Web UI scope in Identity\n\n\n\n\n\n\nUAA_ZONE_DOMAIN\n\n\nexample.com\n\n\nExternal domain name for zone in Identity\n\n\n\n\n\n\nULUWATU_CONTAINER_PATH\n\n\n/uluwatu\n\n\nDefault project location in the Web UI container\n\n\n\n\n\n\nULU_DEFAULT_SSH_KEY\n\n\n\"\"\n\n\nDefault SSH key for the credentials in Cloudbreak\n\n\n\n\n\n\nULU_HOST_ADDRESS\n\n\n\"https://$PUBLIC_IP\"\n\n\nURL for the Web UI host\n\n\n\n\n\n\nULU_NODE_TLS_REJECT_UNAUTHORIZED\n\n\n\"0\"\n\n\nWhen set to \"0\", enables self-signed certifications in Web UI\n\n\n\n\n\n\nULU_OAUTH_REDIRECT_URI\n\n\n\"$ULU_HOST_ADDRESS/authorize\"\n\n\nAuthorization page on Web UI\n\n\n\n\n\n\nULU_SUBSCRIBE_TO_NOTIFICATIONS\n\n\n\"false\"\n\n\nSet to \u201ctrue\u201d to enable email notifications for Cloudbreak events\n\n\n\n\n\n\nULU_SULTANS_ADDRESS\n\n\n\"https://$PUBLIC_IP/sl\"\n\n\nAuthentication service URL\n\n\n\n\n\n\nVERBOSE_MIGRATION\n\n\nfalse\n\n\nWhen set to true, enables verbose database migration\n\n\n\n\n\n\n\n\nAWS Variables\n\n\n\n\n\n\n\n\nVariable Name\n\n\nDefault Value\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nAWS_ACCESS_KEY_ID\n\n\n\"\"\n\n\nAccess key of the AWS account\n\n\n\n\n\n\nAWS_ROLE_NAME\n\n\ncbreak-deployer\n\n\nName of the AWS role for the \ncbd aws [generate-rol, show role]\n commands\n\n\n\n\n\n\nAWS_SECRET_ACCESS_KEY\n\n\n\"\"\n\n\nSecret access key of the AWS account\n\n\n\n\n\n\nCB_AWS_CUSTOM_CF_TAGS\n\n\n\"\"\n\n\nComma separated list of AWS CloudFormation stack tags\n\n\n\n\n\n\nCB_AWS_DEFAULT_CF_TAG\n\n\n\"\"\n\n\nDefault tag for AWS CloudFormation stack\n\n\n\n\n\n\nCB_AWS_DEFAULT_INBOUND_SECURITY_GROUP\n\n\n\"\"\n\n\nDefault inbound policy name for AWS CloudFormation stack\n\n\n\n\n\n\nCB_AWS_EXTERNAL_ID\n\n\nprovision-ambari\n\n\nExternal ID of the assume role policy\n\n\n\n\n\n\nCB_AWS_HOSTKEY_VERIFY\n\n\n\"false\"\n\n\nEnables host fingerprint verification on AWS\n\n\n\n\n\n\nCB_AWS_VPC\n\n\n\"\"\n\n\nConfigures the VPC ID on AWS. Set this variable if you are provisioning cluster to the same VPC where Cloudbreak is deployed on AWS.\n\n\n\n\n\n\nCERTS_BUCKET\n\n\n\"\"\n\n\nS3 bucket name for backup and restore certificates via \ncbd aws [certs-restore-s3  certs-upload-s3]\n commands\n\n\n\n\n\n\n\n\nAzure Variables\n\n\n\n\n\n\n\n\nVariable Name\n\n\nDefault Value\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nAZURE_SUBSCRIPTION_ID\n\n\n\n\nAzure subscription ID for interactive login in the web UI\n\n\n\n\n\n\nAZURE_TENANT_ID\n\n\n\n\nAzure tenant ID for interactive login in the web UI\n\n\n\n\n\n\n\n\nGCP Variables\n\n\n\n\n\n\n\n\nVariable Name\n\n\nDefault Value\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nCB_GCP_HOSTKEY_VERIFY\n\n\n\"false\"\n\n\nWhen set to \"true\", enables host fingerprint verification on GCP\n\n\n\n\n\n\n\n\nLocal Development Variables\n\n\n\n\n\n\n\n\nVariable Name\n\n\nDefault Value\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nCB_LOCAL_DEV_BIND_ADDR\n\n\n\"192.168.64.1\"\n\n\nAmbassador external address for local development of Cloudbreak and Autoscale\n\n\n\n\n\n\nCB_SCHEMA_SCRIPTS_LOCATION\n\n\n\"container\"\n\n\nLocation of Cloudbreak schema update files\n\n\n\n\n\n\nDOCKER_TAG_AMBASSADOR\n\n\n0.5.0\n\n\nAmbassador container version for local development\n\n\n\n\n\n\nPERISCOPE_SCHEMA_SCRIPTS_LOCATION\n\n\n\"container\"\n\n\nLocation of Cloudbreak schema update files\n\n\n\n\n\n\nPRIVATE_IP\n\n\n$BRIDGE_IP\n\n\nIP address or hostname of the private interface\n\n\n\n\n\n\nREMOVE_CONTAINER\n\n\n\"--rm\"\n\n\nWhen set to \"--rm\" (default), removes side effect containers for debug purpose. Set to \" \" to keep side effect containers for debug purpose\n\n\n\n\n\n\nSULTANS_VOLUME_HOST\n\n\n/dev/null\n\n\nLocation of the locally developed Authentication service project\n\n\n\n\n\n\nUAA_SCHEMA_SCRIPTS_LOCATION\n\n\n\"container\"\n\n\nLocation of Identity schema update files\n\n\n\n\n\n\nULUWATU_VOLUME_HOST\n\n\n/dev/null\n\n\nLocation of the locally developed web UI project\n\n\n\n\n\n\n\n\nMacOS Variables\n\n\n\n\n\n\n\n\nVariable Name\n\n\nDefault Value\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nDOCKER_MACHINE\n\n\n\"\"\n\n\nName of the Docker Machine where Cloudbreak runs\n\n\n\n\n\n\nDOCKER_PROFILE\n\n\nProfile\n\n\nProfile file for environment variables related to Docker Machine\n\n\n\n\n\n\nMACHINE_CPU\n\n\n2\n\n\nNumber of the CPU cores on the Docker Machine instance\n\n\n\n\n\n\nMACHINE_MEM\n\n\n4096\n\n\nAmount of RAM on the Docker Machine instance\n\n\n\n\n\n\nMACHINE_NAME\n\n\ncbd\n\n\nName of the Docker Machine instance\n\n\n\n\n\n\nMACHINE_OPTS\n\n\n\"--xhyve-virtio-9p\"\n\n\nExtra options for Docker Machine instance\n\n\n\n\n\n\nMACHINE_STORAGE_PATH\n\n\n$HOME/.docker/machine\n\n\nDocker Machine storage path\n\n\n\n\n\n\n\n\nUAA_DEFAULT_USER_GROUPS\n\n\nDefault value fro \nUAA_DEFAULT_USER_GROUPS\n is:\n\n\n\"openid,cloudbreak.networks,cloudbreak.securitygroups,cloudbreak.templates,cloudbreak.blueprints,cloudbreak.credentials,cloudbreak.stacks,sequenceiq.cloudbreak.admin,sequenceiq.cloudbreak.user,sequenceiq.account.${UAA_DEFAULT_ACCOUNT},cloudbreak.events,cloudbreak.usages.global,cloudbreak.usages.account,cloudbreak.usages.user,periscope.cluster,cloudbreak.recipes,cloudbreak.blueprints.read,cloudbreak.templates.read,cloudbreak.credentials.read,cloudbreak.recipes.read,cloudbreak.networks.read,cloudbreak.securitygroups.read,cloudbreak.stacks.read,cloudbreak.sssdconfigs,cloudbreak.sssdconfigs.read,cloudbreak.platforms,cloudbreak.platforms.read\"\n\n\n\nChange SMTP Parameters\n\n\nIf you want to change SMTP parameters, add them your \nProfile\n.\n\n\nThe default values of the SMTP parameters are:\n\n\nexport CLOUDBREAK_SMTP_SENDER_USERNAME=\nexport CLOUDBREAK_SMTP_SENDER_PASSWORD=\nexport CLOUDBREAK_SMTP_SENDER_HOST=\nexport CLOUDBREAK_SMTP_SENDER_PORT=25\nexport CLOUDBREAK_SMTP_SENDER_FROM=\nexport CLOUDBREAK_SMTP_AUTH=true\nexport CLOUDBREAK_SMTP_STARTTLS_ENABLE=true\nexport CLOUDBREAK_SMTP_TYPE=smtp\n\n\n\n\nIf your SMTP server uses SMTPS, you must set the protocol in your \nProfile\n to smtps:\n\n\nexport CLOUDBREAK_SMTP_TYPE=smtps\n\n\n\n\nConfigure Consul\n\n\nCloudbreak uses \nConsul\n for DNS resolution. All Cloudbreak related services are registered as someservice.service.consul.\n\n\nConsul\u2019s built-in DNS server is able to fallback on another DNS server. This option is called \n-recursor\n. Clodbreak Deployer first tries to discover the DNS settings of the host by looking for nameserver entry in the \n/etc/resolv.conf\n file. If it finds one, consul will use it as a recursor. Otherwise, it will use \n8.8.8.8\n.\n\n\nFor a full list of available consul config options, refer to \nConsul documentation\n.\n\n\nTo pass any additional Consul configuration, define the \nDOCKER_CONSUL_OPTIONS\n variable in the Profile file.\n\n\nCreate Environment Specific Profiles\n\n\nIf you would like to use a different versions of Cloudbreak for prod and qa profile, you must create two environment specific configurations that can be sourced. For example:\n\n\n\n\nProfile.prod  \n\n\nProfile.qa   \n\n\n\n\nFor example, to create and use a prod profile, you need to:\n\n\n\n\nCreate a file called \nProfile.prod\n  \n\n\nWrite the environment-specific \nexport DOCKER_TAG_CLOUDBREAK=0.3.99\n into \nProfile.prod\n to specify Docker image.  \n\n\nSet the environment variable: \nCBD_DEFAULT_PROFILE=prod\n  \n\n\n\n\nTo use the prod specific profile once, set:  \n\n\nCBD_DEFAULT_PROFILE=prod cbd some_commands\n\n\n\nTo permanently use the prod profile, set \nexport CBD_DEFAULT_PROFILE=prod\n in your \n.bash_profile\n.", 
            "title": "Configure Cloudbreak"
        }, 
        {
            "location": "/profile/index.html#configure-cloudbreak-via-profile-variables", 
            "text": "Cloudbreak deployer configuration is based on environment variables.    During startup, Cloudbreak deployer tries to determine the underlying infrastructure and then sets required environment variables with appropriate default values. If these environment variables are not sufficient for your use case, you can set additional environment variables in your  Profile  file.   Cloudbreak deployer always opens a new bash subprocess without inheriting environment variables. Only the following environment variables are inherited:   HOME     DEBUG     TRACE     CBD_DEFAULT_PROFILE     all  DOCKER_XXX", 
            "title": "Configure Cloudbreak via Profile Variables"
        }, 
        {
            "location": "/profile/index.html#set-profile-variables", 
            "text": "To set environment variables relevant for Cloudbreak Deployer, add them to a file called  Profile  located in the Cloudbreak deployment directory (typically  /var/lib/cloudbreak-deployment ).  The  Profile  file is sourced, so you can use the usual syntax to set configuration values:  export MY_VAR=some_value\nexport MY_OTHER_VAR=another_value", 
            "title": "Set Profile Variables"
        }, 
        {
            "location": "/profile/index.html#check-available-profile-variables", 
            "text": "To see all available environment variables with their default values, use:  cbd env show", 
            "title": "Check Available Profile Variables"
        }, 
        {
            "location": "/profile/index.html#profile-variables", 
            "text": "", 
            "title": "Profile Variables"
        }, 
        {
            "location": "/profile/index.html#cloudbreak-variables", 
            "text": "Refer to this list for available environment variables. The variables are listed with their default values. If default is unset, no value is listed.      Variable Name  Default Value  Description      ADDRESS_RESOLVING_TIMEOUT  120000  DNS lookup timeout for internal service discovery    CAPTURE_CRON_EXPRESSION   SmartSense bundle generation time interval in cron format    CBD_CERT_ROOT_PATH  \"${PWD}/certs\"  Path where deployer stores Cloudbreak certificates. ${PWD} refers to the Cloudbreak deployment directory    CBD_LOG_NAME  cbreak  Name of the Cloudbreak log file    CBD_TRAEFIK_TLS  \"/certs/traefik/client.pem,/certs/traefik/client-key.pem\"  Path inside of the Traefik container where TLS files are located    CB_BLUEPRINT_DEFAULTS  \"hdp-small-default;hdp-spark-cluster;hdp-streaming-cluster\"  Comma separated list of the default blueprints that Cloudbreak initializes in its database    CB_BYOS_DFS_DATA_DIR  \"/hadoop/hdfs/data\"  (Deprecated) Default data directory for BYOS orchestrators    CB_COMPONENT_CLUSTER_ID   SmartSense component cluster ID    CB_COMPONENT_ID   SmartSense component ID    CB_COMPOSE_PROJECT  cbreak  Name of the Docker Compose project; it will appear in container names    CB_DB_ENV_DB  \"cbdb\"  Name of the Cloudbreak database    CB_DB_ENV_PASS  \"\"  Password for the Cloudbreak database authentication    CB_DB_ENV_SCHEMA  \"public\"  Schema used in the Cloudbreak database    CB_DB_ENV_USER  \"postgres\"  User for the Cloudbreak database authentication    CB_DB_ROOT_PATH  \"/var/lib/cloudbreak\"  (Deprecated) Location of the database volume on Cloudbreak host    CB_DEFAULT_SUBSCRIPTION_ADDRESS  http://uluwatu.service.consul:3000/notifications  URL of the default subscription for Cloudbreak notifications    CB_ENABLEDPLATFORMS   Set this to disable Cloudbreak resource called Platform    CB_ENABLE_CUSTOM_IMAGE  \"false\"  Set to \"true\" to enable custom cloud images    CBD_FORCE_START   Set this to disable docker-compose.yml and uaa.yml validation    CB_HBM2DDL_STRATEGY  \"validate\"  Configures hibernate.hbm2ddl.auto in Cloudbreak    CB_HOST_DISCOVERY_CUSTOM_DOMAIN  \"\"  Custom domain of the provisioned cluster    CB_HTTPS_PROXY  \"\"  HTTPS proxy URL    CB_HTTP_PROXY  \"\"  HTTP proxy URL    CB_IMAGE_CATALOG_URL  \"https://s3-eu-west-1.amazonaws.com/cloudbreak-info/cb-image-catalog.json\"  Image catalog URL    CB_INSTANCE_NODE_ID   Unique identifier of the Cloudbreak node    CB_INSTANCE_PROVIDER   Cloud provider of the Cloudbreak instance    CB_INSTANCE_REGION   Cloud region of the Cloudbreak instance    CB_INSTANCE_UUID   Unique identifier of Cloudbreak deployment    CB_JAVA_OPTS  \"\"  Extra Java options for Autoscale and Cloudbreak    CB_LOG_LEVEL  \"INFO\"  Log level of the Cloudbreak service    CB_MAX_SALT_NEW_SERVICE_RETRY  90  Salt orchestrator max retry count    CB_MAX_SALT_RECIPE_EXECUTION_RETRY  90  Salt orchestrator max retry count for recipes    CB_PLATFORM_DEFAULT_REGIONS   Comma separated list of default regions by platform. For example:  AWS:eu-west-1 .    CB_PRODUCT_ID   SmartSense product ID    CB_SCHEMA_MIGRATION_AUTO  true  When set to true, enables Cloudbreak automatic database schema update    CB_SMARTSENSE_CONFIGURE  \"false\"  Set to \u201ctrue\u201d to install and configure SmartSense on cluster nodes    CB_SMARTSENSE_CLUSTER_NAME_PREFIX   SmartSense Cloudbreak cluster name prefix    CB_SMARTSENSE_ID  \"\"  SmartSense subscription ID    CB_TEMPLATE_DEFAULTS  \"minviable-gcp,minviable-azure,minviable-aws\"  Comma separated list of the default templates that Cloudbreak initializes in its database    CB_UI_MAX_WAIT  400  Wait timeout for  cbd start-wait  command    CERT_VALIDATION  \"true\"  When set to \"true\", enables cert validation in Cloudbreak and Autoscale    CLOUDBREAK_SMTP_AUTH  \"true\"  When set to \"true\", configures mail.smtp.auth in Cloudbreak    CLOUDBREAK_SMTP_SENDER_FROM  \"noreply@hortonworks.com\"  Email address of the sender    CLOUDBREAK_SMTP_SENDER_HOST  \"smtp.service.consul\"  SMTP server address of the hostname    CLOUDBREAK_SMTP_SENDER_PASSWORD  \"$LOCAL_SMTP_PASSWORD\"  SMTP server password    CLOUDBREAK_SMTP_SENDER_PORT  25  Port of the SMTP server    CLOUDBREAK_SMTP_SENDER_USERNAME  \"admin\"  Username for SMTP authentication    CLOUDBREAK_SMTP_STARTTLS_ENABLE  \"false\"  Set to \"true\" to configure mail.smtp.starttls.enable in Cloudbreak    CLOUDBREAK_SMTP_TYPE  \"smtp\"  Defines mail.transport.protocol in CLoudbreak    COMMON_DB  commondb  Name of the database container    COMMON_DB_VOL  common  Name of the database volume    COMPOSE_HTTP_TIMEOUT  120  Docker Compose execution timeout    DB_DUMP_VOLUME  cbreak_dump  Name of the database dump volume    DB_MIGRATION_LOG  \"db_migration.log\"  Database migration log file    DEFAULT_INBOUND_ACCESS_IP  \"\"  Opens default ports on AWS instances for Cloudbreak    DOCKER_CONSUL_OPTIONS  \"\"  Extra options for Consul    DOCKER_IMAGE_CBD_SMARTSENSE  hortonworks/cbd-smartsense  SmartSense Docker image name    DOCKER_IMAGE_CLOUDBREAK  hortonworks/cloudbreak  Cloudbreak Docker image name    DOCKER_IMAGE_CLOUDBREAK_AUTH  hortonworks/cloudbreak-auth  Authentication service Docker image name    DOCKER_IMAGE_CLOUDBREAK_PERISCOPE  hortonworks/cloudbreak-autoscale  Autoscale Docker image name    DOCKER_IMAGE_CLOUDBREAK_SHELL  hortonworks/cloudbreak-shell  Cloudbreak Shell Docker image name    DOCKER_IMAGE_CLOUDBREAK_WEB  hortonworks/cloudbreak-web  Web UI Docker image name    DOCKER_TAG_ALPINE  3.1  Alpine container version    DOCKER_TAG_CBD_SMARTSENSE  0.10.0  SmartSense container version    DOCKER_TAG_CERT_TOOL  0.2.0  Cert tool container version    DOCKER_TAG_CLOUDBREAK  2.1.0-dev.70  Cloudbreak container version    DOCKER_TAG_CLOUDBREAK_SHELL  2.1.0-dev.70  Cloudbreak Shell container version    DOCKER_TAG_CONSUL  0.5  Consul container version    DOCKER_TAG_HAVEGED  1.1.0  Haveged container version    DOCKER_TAG_LOGROTATE  1.0.0  Logrotate container version    DOCKER_TAG_MIGRATION  1.0.0  Migration container version    DOCKER_TAG_PERISCOPE  2.1.0-dev.70  Autoscale container version    DOCKER_TAG_POSTFIX  latest  Postfix container version    DOCKER_TAG_POSTGRES  9.6.1-alpine  Postgresql container version    DOCKER_TAG_REGISTRATOR  v5  Registrator container version    DOCKER_TAG_SULTANS  2.1.0-dev.70  Authentication service container version    DOCKER_TAG_TRAEFIK  v1.2.0  Traefik container version    DOCKER_TAG_UAA  3.6.5  Identity container version    DOCKER_TAG_ULUWATU  2.1.0-dev.70  Web UI container version    IDENTITY_DB_NAME  \"uaadb\"  Name of the Identity database    IDENTITY_DB_PASS  \"\"  Password for the Identity database authentication    IDENTITY_DB_URL  \"${COMMON_DB}.service.consul:5432\"  URL for the Identity database connection, including the port number    IDENTITY_DB_USER  \"postgres\"  User for the Identity database authentication    LOCAL_SMTP_PASSWORD  \"$UAA_DEFAULT_USER_PW\"  Default password for the internal mail server    PERISCOPE_DB_HBM2DDL_STRATEGY  \"validate\"  Configures hibernate.hbm2ddl.auto in Autoscale    PERISCOPE_DB_NAME  \"periscopedb\"  Name of the Autoscale database    PERISCOPE_DB_PASS  \"\"  Password for the Autoscale database authentication    PERISCOPE_DB_SCHEMA_NAME  \"public\"  Schema used in the Autoscale database    PERISCOPE_DB_USER  \"postgres\"  User for the Autoscale database authentication    PERISCOPE_DB_TCP_ADDR   Address of the Autoscale database    PERISCOPE_DB_TCP_PORT   Port number of the Autoscale database    PERISCOPE_LOG_LEVEL  \"INFO\"  Log level of the Autoscale service    PERISCOPE_SCHEMA_MIGRATION_AUTO  true  When set to \"true\", enables Autoscale automatic database schema update    PUBLIC_IP   IP address or hostname of the public interface    REST_DEBUG  \"false\"  Set to \"true\" to enable REST call debug level in Cloudbreak and Autoscale    SL_ADDRESS_RESOLVING_TIMEOUT   DNS lookup timeout of Authentication service for internal service discovery    SL_NODE_TLS_REJECT_UNAUTHORIZED  \"0\"  When set to \"0\", enables self-signed certifications in Authentication service    SULTANS_CONTAINER_PATH  /sultans  Default project location in Authentication service container    TRAEFIK_MAX_IDLE_CONNECTION  100  Sets --maxidleconnsperhost for Traefik to the value entered    UAA_CLOUDBREAK_ID  cloudbreak  Identity of the Cloudbreak scope in Identity    UAA_CLOUDBREAK_SECRET  $UAA_DEFAULT_SECRET  Secret of the Cloudbreak scope in Identity    UAA_CLOUDBREAK_SHELL_ID  cloudbreak_shell  Identity of the Cloudbreak Shell scope in Identity    UAA_DEFAULT_ACCOUNT  \"seq1234567.SequenceIQ\"  Default account for users as an Identity group    UAA_DEFAULT_SECRET   Default secret for all the scopes and encryptions    UAA_DEFAULT_USER_EMAIL  admin@example.com  Email address of default admin user    UAA_DEFAULT_USER_FIRSTNAME  Joe  First name of default admin user    UAA_DEFAULT_USER_GROUPS  See  here  Default user groups of the users    UAA_DEFAULT_USER_LASTNAME  Admin  Last name of default admin user    UAA_DEFAULT_USER_PW   Password of default admin user    UAA_FLEX_USAGE_CLIENT_ID  flex_usage_client  Identity of the Flex usage generator scope in Identity    UAA_FLEX_USAGE_CLIENT_SECRET  $UAA_DEFAULT_SECRET  Secret of the Flex usage generator scope in Identity    UAA_PERISCOPE_ID  periscope  Identity of the Autoscale scope in Identity    UAA_PERISCOPE_SECRET  $UAA_DEFAULT_SECRET  Secret of the Autoscale scope in Identity    UAA_PORT  8089  Identity service public port    UAA_SULTANS_ID  sultans  Identity of the Authentication service scope in Identity    UAA_SULTANS_SECRET  $UAA_DEFAULT_SECRET  Secret of the Authentication service scope in Identity    UAA_ULUWATU_ID  uluwatu  Identity of the Web UI scope in Identity    UAA_ULUWATU_SECRET  $UAA_DEFAULT_SECRET  Secret of the Web UI scope in Identity    UAA_ZONE_DOMAIN  example.com  External domain name for zone in Identity    ULUWATU_CONTAINER_PATH  /uluwatu  Default project location in the Web UI container    ULU_DEFAULT_SSH_KEY  \"\"  Default SSH key for the credentials in Cloudbreak    ULU_HOST_ADDRESS  \"https://$PUBLIC_IP\"  URL for the Web UI host    ULU_NODE_TLS_REJECT_UNAUTHORIZED  \"0\"  When set to \"0\", enables self-signed certifications in Web UI    ULU_OAUTH_REDIRECT_URI  \"$ULU_HOST_ADDRESS/authorize\"  Authorization page on Web UI    ULU_SUBSCRIBE_TO_NOTIFICATIONS  \"false\"  Set to \u201ctrue\u201d to enable email notifications for Cloudbreak events    ULU_SULTANS_ADDRESS  \"https://$PUBLIC_IP/sl\"  Authentication service URL    VERBOSE_MIGRATION  false  When set to true, enables verbose database migration", 
            "title": "Cloudbreak Variables"
        }, 
        {
            "location": "/profile/index.html#aws-variables", 
            "text": "Variable Name  Default Value  Description      AWS_ACCESS_KEY_ID  \"\"  Access key of the AWS account    AWS_ROLE_NAME  cbreak-deployer  Name of the AWS role for the  cbd aws [generate-rol, show role]  commands    AWS_SECRET_ACCESS_KEY  \"\"  Secret access key of the AWS account    CB_AWS_CUSTOM_CF_TAGS  \"\"  Comma separated list of AWS CloudFormation stack tags    CB_AWS_DEFAULT_CF_TAG  \"\"  Default tag for AWS CloudFormation stack    CB_AWS_DEFAULT_INBOUND_SECURITY_GROUP  \"\"  Default inbound policy name for AWS CloudFormation stack    CB_AWS_EXTERNAL_ID  provision-ambari  External ID of the assume role policy    CB_AWS_HOSTKEY_VERIFY  \"false\"  Enables host fingerprint verification on AWS    CB_AWS_VPC  \"\"  Configures the VPC ID on AWS. Set this variable if you are provisioning cluster to the same VPC where Cloudbreak is deployed on AWS.    CERTS_BUCKET  \"\"  S3 bucket name for backup and restore certificates via  cbd aws [certs-restore-s3  certs-upload-s3]  commands", 
            "title": "AWS Variables"
        }, 
        {
            "location": "/profile/index.html#azure-variables", 
            "text": "Variable Name  Default Value  Description      AZURE_SUBSCRIPTION_ID   Azure subscription ID for interactive login in the web UI    AZURE_TENANT_ID   Azure tenant ID for interactive login in the web UI", 
            "title": "Azure Variables"
        }, 
        {
            "location": "/profile/index.html#gcp-variables", 
            "text": "Variable Name  Default Value  Description      CB_GCP_HOSTKEY_VERIFY  \"false\"  When set to \"true\", enables host fingerprint verification on GCP", 
            "title": "GCP Variables"
        }, 
        {
            "location": "/profile/index.html#local-development-variables", 
            "text": "Variable Name  Default Value  Description      CB_LOCAL_DEV_BIND_ADDR  \"192.168.64.1\"  Ambassador external address for local development of Cloudbreak and Autoscale    CB_SCHEMA_SCRIPTS_LOCATION  \"container\"  Location of Cloudbreak schema update files    DOCKER_TAG_AMBASSADOR  0.5.0  Ambassador container version for local development    PERISCOPE_SCHEMA_SCRIPTS_LOCATION  \"container\"  Location of Cloudbreak schema update files    PRIVATE_IP  $BRIDGE_IP  IP address or hostname of the private interface    REMOVE_CONTAINER  \"--rm\"  When set to \"--rm\" (default), removes side effect containers for debug purpose. Set to \" \" to keep side effect containers for debug purpose    SULTANS_VOLUME_HOST  /dev/null  Location of the locally developed Authentication service project    UAA_SCHEMA_SCRIPTS_LOCATION  \"container\"  Location of Identity schema update files    ULUWATU_VOLUME_HOST  /dev/null  Location of the locally developed web UI project", 
            "title": "Local Development Variables"
        }, 
        {
            "location": "/profile/index.html#macos-variables", 
            "text": "Variable Name  Default Value  Description      DOCKER_MACHINE  \"\"  Name of the Docker Machine where Cloudbreak runs    DOCKER_PROFILE  Profile  Profile file for environment variables related to Docker Machine    MACHINE_CPU  2  Number of the CPU cores on the Docker Machine instance    MACHINE_MEM  4096  Amount of RAM on the Docker Machine instance    MACHINE_NAME  cbd  Name of the Docker Machine instance    MACHINE_OPTS  \"--xhyve-virtio-9p\"  Extra options for Docker Machine instance    MACHINE_STORAGE_PATH  $HOME/.docker/machine  Docker Machine storage path", 
            "title": "MacOS Variables"
        }, 
        {
            "location": "/profile/index.html#uaa_default_user_groups", 
            "text": "Default value fro  UAA_DEFAULT_USER_GROUPS  is:  \"openid,cloudbreak.networks,cloudbreak.securitygroups,cloudbreak.templates,cloudbreak.blueprints,cloudbreak.credentials,cloudbreak.stacks,sequenceiq.cloudbreak.admin,sequenceiq.cloudbreak.user,sequenceiq.account.${UAA_DEFAULT_ACCOUNT},cloudbreak.events,cloudbreak.usages.global,cloudbreak.usages.account,cloudbreak.usages.user,periscope.cluster,cloudbreak.recipes,cloudbreak.blueprints.read,cloudbreak.templates.read,cloudbreak.credentials.read,cloudbreak.recipes.read,cloudbreak.networks.read,cloudbreak.securitygroups.read,cloudbreak.stacks.read,cloudbreak.sssdconfigs,cloudbreak.sssdconfigs.read,cloudbreak.platforms,cloudbreak.platforms.read\"", 
            "title": "UAA_DEFAULT_USER_GROUPS"
        }, 
        {
            "location": "/profile/index.html#change-smtp-parameters", 
            "text": "If you want to change SMTP parameters, add them your  Profile .  The default values of the SMTP parameters are:  export CLOUDBREAK_SMTP_SENDER_USERNAME=\nexport CLOUDBREAK_SMTP_SENDER_PASSWORD=\nexport CLOUDBREAK_SMTP_SENDER_HOST=\nexport CLOUDBREAK_SMTP_SENDER_PORT=25\nexport CLOUDBREAK_SMTP_SENDER_FROM=\nexport CLOUDBREAK_SMTP_AUTH=true\nexport CLOUDBREAK_SMTP_STARTTLS_ENABLE=true\nexport CLOUDBREAK_SMTP_TYPE=smtp  If your SMTP server uses SMTPS, you must set the protocol in your  Profile  to smtps:  export CLOUDBREAK_SMTP_TYPE=smtps", 
            "title": "Change SMTP Parameters"
        }, 
        {
            "location": "/profile/index.html#configure-consul", 
            "text": "Cloudbreak uses  Consul  for DNS resolution. All Cloudbreak related services are registered as someservice.service.consul.  Consul\u2019s built-in DNS server is able to fallback on another DNS server. This option is called  -recursor . Clodbreak Deployer first tries to discover the DNS settings of the host by looking for nameserver entry in the  /etc/resolv.conf  file. If it finds one, consul will use it as a recursor. Otherwise, it will use  8.8.8.8 .  For a full list of available consul config options, refer to  Consul documentation .  To pass any additional Consul configuration, define the  DOCKER_CONSUL_OPTIONS  variable in the Profile file.", 
            "title": "Configure Consul"
        }, 
        {
            "location": "/profile/index.html#create-environment-specific-profiles", 
            "text": "If you would like to use a different versions of Cloudbreak for prod and qa profile, you must create two environment specific configurations that can be sourced. For example:   Profile.prod    Profile.qa      For example, to create and use a prod profile, you need to:   Create a file called  Profile.prod     Write the environment-specific  export DOCKER_TAG_CLOUDBREAK=0.3.99  into  Profile.prod  to specify Docker image.    Set the environment variable:  CBD_DEFAULT_PROFILE=prod      To use the prod specific profile once, set:    CBD_DEFAULT_PROFILE=prod cbd some_commands  To permanently use the prod profile, set  export CBD_DEFAULT_PROFILE=prod  in your  .bash_profile .", 
            "title": "Create Environment Specific Profiles"
        }, 
        {
            "location": "/cb-db/index.html", 
            "text": "Manage Cloudbreak Database\n\n\nBy default, Cloudbreak uses a built-in PostgreSQL database to persist data. For production environments, we suggest that you use an external database, an RDS served by your cloud provider. However, if you choose to use the default database, you should know that Cloudbreak deployer includes features for dumping and restoring built-in databases.\n\n\nDump and Restore Database\n\n\nCloudbreak deployer uses Docker for the underlying infrastructure and uses Docker volume for storing data. There are two separate volumes: \n\n\n\n\na volume called \ncommon\n for storing live data  \n\n\na volume called \ncbreak_dump\n for database dumps \n\n\n\n\nYou can override default live data volume any time by extending your \nProfile\n with the following variable:\n\n\nexport COMMON_DB_VOL=\nmy-live-data-volume\n\n\n\n\n\nTo create database dumps, execute the following commands:\n\n\ncbd db dump common cbdb\ncbd db dump common uaadb\ncbd db dump common periscopedb\n\n\n\n\nThe dump command has an optional third parameter, the \nname\n of the dump. If you give your dump a name, Cloudbreak deployer will create a symbolic link which points to the SQL dump. For example: \n\n\ncbd db dump common cbdb name-of-the-dump\n\n\n\n\nTo list existing dumps, execute the \ncbd db list-dumps\n command. Each kind of database dump (cbdb, uaadb, periscopedb) has a link to the latest dump on the \ncbreak_dump\n volume. During the restore process, Cloudbreak deployer restores from latest dump. To check which dump is the latest, execute:\n\n\ndocker run --rm -v cbreak_dump:/dump -it alpine ls -lsa /dump/cbdb/latest\n\n\n\n\nTo set any of the existing dumps as latest use the \nset-dump\n command. You can set both regular or named dumps. For example: \n\n\ncbd db set-dump cbdb 20170628_1805\n\n\n\n\nor\n\n\ncbd db set-dump cbdb name-of-the-dump\n\n\n\n\nTo remove the existing \ncommon\n volume, stop all the related Cloudbreak containers with \ncbd kill\n command, and then remove the volume:\n\n\ndocker volume rm common\n\n\n\n\nTo restore databases from dumps, execute:\n\n\ncbd db restore-volume-from-dump common cbdb\ncbd db restore-volume-from-dump common uaadb\ncbd db restore-volume-from-dump common periscopedb\n\n\n\n\nTo save your dumps to the host machine, execute:\n\n\ndocker run --rm -v cbreak_dump:/dump -it alpine cat /dump/cbdb/latest/dump.sql \n cbdb.sql\ndocker run --rm -v cbreak_dump:/dump -it alpine cat /dump/uaadb/latest/dump.sql \n uaadb.sql\ndocker run --rm -v cbreak_dump:/dump -it alpine cat /dump/periscopedb/latest/dump.sql \n periscopedb.sql", 
            "title": "Manage Cloudbreak Database"
        }, 
        {
            "location": "/cb-db/index.html#manage-cloudbreak-database", 
            "text": "By default, Cloudbreak uses a built-in PostgreSQL database to persist data. For production environments, we suggest that you use an external database, an RDS served by your cloud provider. However, if you choose to use the default database, you should know that Cloudbreak deployer includes features for dumping and restoring built-in databases.", 
            "title": "Manage Cloudbreak Database"
        }, 
        {
            "location": "/cb-db/index.html#dump-and-restore-database", 
            "text": "Cloudbreak deployer uses Docker for the underlying infrastructure and uses Docker volume for storing data. There are two separate volumes:    a volume called  common  for storing live data    a volume called  cbreak_dump  for database dumps    You can override default live data volume any time by extending your  Profile  with the following variable:  export COMMON_DB_VOL= my-live-data-volume   To create database dumps, execute the following commands:  cbd db dump common cbdb\ncbd db dump common uaadb\ncbd db dump common periscopedb  The dump command has an optional third parameter, the  name  of the dump. If you give your dump a name, Cloudbreak deployer will create a symbolic link which points to the SQL dump. For example:   cbd db dump common cbdb name-of-the-dump  To list existing dumps, execute the  cbd db list-dumps  command. Each kind of database dump (cbdb, uaadb, periscopedb) has a link to the latest dump on the  cbreak_dump  volume. During the restore process, Cloudbreak deployer restores from latest dump. To check which dump is the latest, execute:  docker run --rm -v cbreak_dump:/dump -it alpine ls -lsa /dump/cbdb/latest  To set any of the existing dumps as latest use the  set-dump  command. You can set both regular or named dumps. For example:   cbd db set-dump cbdb 20170628_1805  or  cbd db set-dump cbdb name-of-the-dump  To remove the existing  common  volume, stop all the related Cloudbreak containers with  cbd kill  command, and then remove the volume:  docker volume rm common  To restore databases from dumps, execute:  cbd db restore-volume-from-dump common cbdb\ncbd db restore-volume-from-dump common uaadb\ncbd db restore-volume-from-dump common periscopedb  To save your dumps to the host machine, execute:  docker run --rm -v cbreak_dump:/dump -it alpine cat /dump/cbdb/latest/dump.sql   cbdb.sql\ndocker run --rm -v cbreak_dump:/dump -it alpine cat /dump/uaadb/latest/dump.sql   uaadb.sql\ndocker run --rm -v cbreak_dump:/dump -it alpine cat /dump/periscopedb/latest/dump.sql   periscopedb.sql", 
            "title": "Dump and Restore Database"
        }, 
        {
            "location": "/security/index.html", 
            "text": "Network and Security\n\n\nVirtual Network\n\n\nAzure uses Virtual network (VNet) service to create virtual networks that resembles a traditional networks. Your Cloudbreak controller and clusters are launched into the virtual network infrastructure, with a new VNet created for each resource (Cloudbreak controller, cluster1, cluster2, and so on).\n\n\nNetwork Security Groups\n\n\nNetwork security groups are set up to control network traffic to the VMs in the system. By default, the system is configured to restrict inbound network traffic to the minimal set of ports. You can add or modify rules to each security group that allow traffic to or from its associated instances.\n\n\nThis section describes the default security group configuration for the various components in the system.\n\n\n\n\nThe inbound and outbound rules (protocols, port and IP ranges) for the security groups can be modified later using the Network Security Groups dashboard.\n\n\n\n\nThe naming convention for the security groups that are automatically created is:\n\n\n\n\nCloudbreak controller VM: cbdeployerNsg\n\n\nCluster node VMs: {host_group_name}{cluster_name}sg\n\n\n\n\nThe following security groups are created automatically:\n\n\nCloudbreak Security Group\n\n\nThe \ncbdeployerNsg\n security group is created when launching  Cloudbreak and is associated with the Cloudbreak VM. The following table lists the security group port configuration for the cloud controller instance.\nThe security group \nSource\n for these ports is set to the \nRemote Access\n CIDR IP specified when\nlaunching Cloudbreak.\n\n\n\n\n\n\n\n\nInbound Port\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\n22\n\n\nSSH access to the Cloudbreak VM.\n\n\n\n\n\n\n80\n\n\nHTTP access to the Cloudbreak UI. This is automatically redirected to the HTTPS (443) port.\n\n\n\n\n\n\n443\n\n\nHTTPS access to the Cloudbreak UI.\n\n\n\n\n\n\n\n\nCluster Security Groups\n\n\nMultiple security groups are created when you create a cluster, one for each host group. The\nsecurity group \nSource\n for these ports is set to the \nRemote Access\n CIDR IP specified when creating the cluster.\n\n\nThe following table lists the \nmaster node\n security group port configuration.\n\n\n\n\n\n\n\n\nInbound Port\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\n22\n\n\nSSH access to the node instance.\n\n\n\n\n\n\n9443\n\n\nInternal management port, used by the cloud controller to communicate with the cluster master node.\n\n\n\n\n\n\n8443\n1\n\n\nSecured HTTPS gateway access to the Ambari, Zeppelin, Hive JDBC, and other Cluster Components.\n\n\n\n\n\n\n\n\n\n\n1\n Port 8443 is only opened on the master node if when you create cluster, you check the checkbox under \nSetup Network and Security \n Enable Knox Gateway\n (to Ambari and Zeppelin Web UIs, Hive JDBC and/or Cluster Components UIs).\n\n\n\n\nThe following table lists the security group port configuration for other host groups:\n\n\n\n\n\n\n\n\nInbound Port\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\n22\n\n\nSSH access to the node instance.\n\n\n\n\n\n\n\n\n\n    \nLearn More\n\n    \n\nRefer to the Azure network security groups \n documentation\n\nfor more information about viewing and modifying network security group rules for VMs.", 
            "title": "Security Overview"
        }, 
        {
            "location": "/security/index.html#network-and-security", 
            "text": "", 
            "title": "Network and Security"
        }, 
        {
            "location": "/security/index.html#virtual-network", 
            "text": "Azure uses Virtual network (VNet) service to create virtual networks that resembles a traditional networks. Your Cloudbreak controller and clusters are launched into the virtual network infrastructure, with a new VNet created for each resource (Cloudbreak controller, cluster1, cluster2, and so on).", 
            "title": "Virtual Network"
        }, 
        {
            "location": "/security/index.html#network-security-groups", 
            "text": "Network security groups are set up to control network traffic to the VMs in the system. By default, the system is configured to restrict inbound network traffic to the minimal set of ports. You can add or modify rules to each security group that allow traffic to or from its associated instances.  This section describes the default security group configuration for the various components in the system.   The inbound and outbound rules (protocols, port and IP ranges) for the security groups can be modified later using the Network Security Groups dashboard.   The naming convention for the security groups that are automatically created is:   Cloudbreak controller VM: cbdeployerNsg  Cluster node VMs: {host_group_name}{cluster_name}sg   The following security groups are created automatically:", 
            "title": "Network Security Groups"
        }, 
        {
            "location": "/security/index.html#cloudbreak-security-group", 
            "text": "The  cbdeployerNsg  security group is created when launching  Cloudbreak and is associated with the Cloudbreak VM. The following table lists the security group port configuration for the cloud controller instance.\nThe security group  Source  for these ports is set to the  Remote Access  CIDR IP specified when\nlaunching Cloudbreak.     Inbound Port  Description      22  SSH access to the Cloudbreak VM.    80  HTTP access to the Cloudbreak UI. This is automatically redirected to the HTTPS (443) port.    443  HTTPS access to the Cloudbreak UI.", 
            "title": "Cloudbreak Security Group"
        }, 
        {
            "location": "/security/index.html#cluster-security-groups", 
            "text": "Multiple security groups are created when you create a cluster, one for each host group. The\nsecurity group  Source  for these ports is set to the  Remote Access  CIDR IP specified when creating the cluster.  The following table lists the  master node  security group port configuration.     Inbound Port  Description      22  SSH access to the node instance.    9443  Internal management port, used by the cloud controller to communicate with the cluster master node.    8443 1  Secured HTTPS gateway access to the Ambari, Zeppelin, Hive JDBC, and other Cluster Components.      1  Port 8443 is only opened on the master node if when you create cluster, you check the checkbox under  Setup Network and Security   Enable Knox Gateway  (to Ambari and Zeppelin Web UIs, Hive JDBC and/or Cluster Components UIs).   The following table lists the security group port configuration for other host groups:     Inbound Port  Description      22  SSH access to the node instance.     \n     Learn More \n     \nRefer to the Azure network security groups   documentation \nfor more information about viewing and modifying network security group rules for VMs.", 
            "title": "Cluster Security Groups"
        }, 
        {
            "location": "/security-cb/index.html", 
            "text": "Securing Cloudbreak After Launch\n\n\nCloudbreak comes with default settings designed for easy first experience rather than strict security. To secure Cloudbreak, follow these recommendations. \n\n\nRestricting Inbound Access\n\n\nWe recommend that you block all communication ports except 443 on the firewall or security group (depending on the provider). \n\n\nIf you have to log in to the Cloudbreak host remotely, use the SSH port (usually 22).\n\n\nConfiguring the Profile\n\n\nBefore starting Cloudbreak for the first time, configure the Profile file as directed below. Changes are applied during startup so a restart (\ncbd restart\n) is required after each change.\n\n\n\n\n\n\nExecute the following command in the directory where you want to store Cloudbreak-related files:\n\n\n\necho export PUBLIC_IP=[the ip or hostname to bind] \n Profile\n\n\n\ncomment\n: \n (TO-DO: Do you mean that this needs to be executed in the deployment directory? Or?)\n\n\n\n\n\n\nAfter you have a base Profile file, add the following custom properties to it:\n\n\n\nexport UAA_DEFAULT_SECRET='[custom secret]'\nexport UAA_DEFAULT_USER_EMAIL='[default admin email address]'\nexport UAA_DEFAULT_USER_PW='[default admin password]'\nexport UAA_DEFAULT_USER_FIRSTNAME='[default admin first name]'\nexport UAA_DEFAULT_USER_LASTNAME='[default admin last name]'\n\n\n\nCloudbreak has additional secrets which by default inherit their values from \nUAA_DEFAULT_SECRET\n. Instead of using the default, you can define different values in the Profile for each of these service clients:\n\n\n\nexport UAA_CLOUDBREAK_SECRET='[cloudbreak secret]'\nexport UAA_PERISCOPE_SECRET='[auto scaling secret]'\nexport UAA_ULUWATU_SECRET='[web ui secret]'\nexport UAA_SULTANS_SECRET='[authenticator secret]'\n\n\n\nYou can change these secrets at any time, except \nUAA_CLOUDBREAK_SECRET\n which is used to encrypt sensitive information at database level. \n\n\ncomment\n: \n (TO-DO: The info below is explained in a way that is confusing. Can you rephrase?) \n\n\nUAA_DEFAULT_USER_PW\n is stored in plain text format, but if \nUAA_DEFAULT_USER_PW\n is missing from the Profile, it gets a default value. Because default password is not an option, if you set an empty password explicitly in the Profile Cloudbreak deployer will ask for password all the time when it is needed for the operation.\n\n\n\nexport UAA_DEFAULT_USER_PW=''\n\n\n\nIn this case, Cloudbreak deployer wouldn't be able to add the default user, so you have to do it manually by executing the following command:\n\n\n\ncbd util add-default-user\n\n\n\n\n\n\n\nFor more information about setting environment variables in Profile, refer to \nConfigure Profile Variables\n.\n\n\nAdding SSL Certificate for Cloudbreak UI\n\n\nBy default Cloudbreak has been configured with a self-signed certificate for access via HTTPS. This is sufficient for many deployments such as trials, development, testing, or staging. However, for production deployments, a trusted certificate is preferred and can be configured in the controller. Follow these steps to configure the cloud controller to use your own trusted certificate. \n\n\nPrerequisites\n\n\nTo use your own certificate, you must have:\n\n\n\n\nA resolvable fully qualified domain name (FQDN) for the controller host IP address. For example, this can be set up in \nAmazon Route 53\n.  \n\n\nA valid SSL certificate for this fully qualified domain name. The certificate can be obtained from a number of certificate providers.  \n\n\n\n\nSteps\n\n\n\n\n\n\nSSH to the Cloudbreak host instance:\n\n\nssh -i mykeypair.pem cloudbreak@[CONTROLLER-IP-ADDRESS]\n\n\n\n\n\n\nMake sure that the target fully qualified domain name (FQDN) which you plan to use for Cloudbreak is resolvable:\n\n\nnslookup [TARGET-CONTROLLER-FQDN]\n\n\nFor example:\n\n\nnslookup hdcloud.example.com\n\n\n\n\n\n\nBrowse to the Cloudbreak deployment directory and edit the \nProfile\n file:\n\n\nvi /var/lib/cloudbreak-deployment/Profile\n\n\n\n\n\n\nReplace the value of the \nPUBLIC_IP\n variable with the \nTARGET-CONTROLLER-FQDN\n value:\n\n\nPUBLIC_IP=[TARGET-CONTROLLER-FQDN]\n\n\n\n\n\n\nCopy your private key and certificate files for the FQDN onto the Cloudbreak host. These files must be placed under \n/var/lib/cloudbreak-deployment/certs/traefik/\n directory.\n\n\n\n\nFile permissions for the private key and certificate files can be set to 600.\n\n\n\n\n\n\n\n\n\n\nFile\n\n\nExample\n\n\n\n\n\n\n\n\n\n\nPRIV-KEY-LOCATION\n\n\n/var/lib/cloudbreak-deployment/certs/traefik/hdcloud.example.com.key\n\n\n\n\n\n\nCERT-LOCATION\n\n\n/var/lib/cloudbreak-deployment/certs/traefik/hdcloud.example.com.crt\n\n\n\n\n\n\n\n\n\n\n\n\nConfigure TLS details in your \nProfile\n by adding the following line at the end of the file.\n\n\n\n\nNotice that \nCERT-LOCATION\n and \nPRIV-KEY-LOCATION\n are file locations from Step 5, starting at the \n/certs/...\n path.\n\n\n\n\nexport CBD_TRAEFIK_TLS=\u201d[CERT-LOCATION],[PRIV-KEY-LOCATION]\u201d\n\n\nFor example:\n\n\nexport CBD_TRAEFIK_TLS=\"/certs/traefik/hdcloud.example.com.crt,/certs/traefik/hdcloud.example.com.key\"\n\n\n\n\n\n\nRestart Cloudbreak deployer:\n\n\ncbd restart\n\n\n\n\n\n\nUsing your web browser, access to the Cloudbreak UI using the new resolvable fully qualified domain name.\n\n\n\n\n\n\nConfirm that the connection is SSL-protected and that the certificate used is the certificate that you provided to Cloudbreak.\n\n\n\n\n\n\nConfigure Access from Custom Domains\n\n\nCloudbreak Deployer, which uses UAA as an identity provider, supports multitenancy. In UAA, multitenancy is managed through identity zones. An identity zone is accessed through a unique subdomain. For example, if the standard UAA responds to \nhttps://uaa.10.244.0.34.xip.io\n, a zone on this UAA can be accessed through a unique subdomain \nhttps://testzone1.uaa.10.244.0.34.xip.io\n.\n\n\nIf you want to use a custom domain for your identity or deployment, add the \nUAA_ZONE_DOMAIN\n line to your \nProfile\n:\n\n\nexport UAA_ZONE_DOMAIN=my-subdomain.example.com\n\n\n\nThis variable is necessary for UAA to identify which zone provider should handle the requests that arrive to that domain.", 
            "title": "Configure Basic Security"
        }, 
        {
            "location": "/security-cb/index.html#securing-cloudbreak-after-launch", 
            "text": "Cloudbreak comes with default settings designed for easy first experience rather than strict security. To secure Cloudbreak, follow these recommendations.", 
            "title": "Securing Cloudbreak After Launch"
        }, 
        {
            "location": "/security-cb/index.html#restricting-inbound-access", 
            "text": "We recommend that you block all communication ports except 443 on the firewall or security group (depending on the provider).   If you have to log in to the Cloudbreak host remotely, use the SSH port (usually 22).", 
            "title": "Restricting Inbound Access"
        }, 
        {
            "location": "/security-cb/index.html#configuring-the-profile", 
            "text": "Before starting Cloudbreak for the first time, configure the Profile file as directed below. Changes are applied during startup so a restart ( cbd restart ) is required after each change.    Execute the following command in the directory where you want to store Cloudbreak-related files:  \necho export PUBLIC_IP=[the ip or hostname to bind]   Profile  comment :   (TO-DO: Do you mean that this needs to be executed in the deployment directory? Or?)    After you have a base Profile file, add the following custom properties to it:  \nexport UAA_DEFAULT_SECRET='[custom secret]'\nexport UAA_DEFAULT_USER_EMAIL='[default admin email address]'\nexport UAA_DEFAULT_USER_PW='[default admin password]'\nexport UAA_DEFAULT_USER_FIRSTNAME='[default admin first name]'\nexport UAA_DEFAULT_USER_LASTNAME='[default admin last name]'  Cloudbreak has additional secrets which by default inherit their values from  UAA_DEFAULT_SECRET . Instead of using the default, you can define different values in the Profile for each of these service clients:  \nexport UAA_CLOUDBREAK_SECRET='[cloudbreak secret]'\nexport UAA_PERISCOPE_SECRET='[auto scaling secret]'\nexport UAA_ULUWATU_SECRET='[web ui secret]'\nexport UAA_SULTANS_SECRET='[authenticator secret]'  You can change these secrets at any time, except  UAA_CLOUDBREAK_SECRET  which is used to encrypt sensitive information at database level.   comment :   (TO-DO: The info below is explained in a way that is confusing. Can you rephrase?)   UAA_DEFAULT_USER_PW  is stored in plain text format, but if  UAA_DEFAULT_USER_PW  is missing from the Profile, it gets a default value. Because default password is not an option, if you set an empty password explicitly in the Profile Cloudbreak deployer will ask for password all the time when it is needed for the operation.  \nexport UAA_DEFAULT_USER_PW=''  In this case, Cloudbreak deployer wouldn't be able to add the default user, so you have to do it manually by executing the following command:  \ncbd util add-default-user    For more information about setting environment variables in Profile, refer to  Configure Profile Variables .", 
            "title": "Configuring the Profile"
        }, 
        {
            "location": "/security-cb/index.html#adding-ssl-certificate-for-cloudbreak-ui", 
            "text": "By default Cloudbreak has been configured with a self-signed certificate for access via HTTPS. This is sufficient for many deployments such as trials, development, testing, or staging. However, for production deployments, a trusted certificate is preferred and can be configured in the controller. Follow these steps to configure the cloud controller to use your own trusted certificate.   Prerequisites  To use your own certificate, you must have:   A resolvable fully qualified domain name (FQDN) for the controller host IP address. For example, this can be set up in  Amazon Route 53 .    A valid SSL certificate for this fully qualified domain name. The certificate can be obtained from a number of certificate providers.     Steps    SSH to the Cloudbreak host instance:  ssh -i mykeypair.pem cloudbreak@[CONTROLLER-IP-ADDRESS]    Make sure that the target fully qualified domain name (FQDN) which you plan to use for Cloudbreak is resolvable:  nslookup [TARGET-CONTROLLER-FQDN]  For example:  nslookup hdcloud.example.com    Browse to the Cloudbreak deployment directory and edit the  Profile  file:  vi /var/lib/cloudbreak-deployment/Profile    Replace the value of the  PUBLIC_IP  variable with the  TARGET-CONTROLLER-FQDN  value:  PUBLIC_IP=[TARGET-CONTROLLER-FQDN]    Copy your private key and certificate files for the FQDN onto the Cloudbreak host. These files must be placed under  /var/lib/cloudbreak-deployment/certs/traefik/  directory.   File permissions for the private key and certificate files can be set to 600.      File  Example      PRIV-KEY-LOCATION  /var/lib/cloudbreak-deployment/certs/traefik/hdcloud.example.com.key    CERT-LOCATION  /var/lib/cloudbreak-deployment/certs/traefik/hdcloud.example.com.crt       Configure TLS details in your  Profile  by adding the following line at the end of the file.   Notice that  CERT-LOCATION  and  PRIV-KEY-LOCATION  are file locations from Step 5, starting at the  /certs/...  path.   export CBD_TRAEFIK_TLS=\u201d[CERT-LOCATION],[PRIV-KEY-LOCATION]\u201d  For example:  export CBD_TRAEFIK_TLS=\"/certs/traefik/hdcloud.example.com.crt,/certs/traefik/hdcloud.example.com.key\"    Restart Cloudbreak deployer:  cbd restart    Using your web browser, access to the Cloudbreak UI using the new resolvable fully qualified domain name.    Confirm that the connection is SSL-protected and that the certificate used is the certificate that you provided to Cloudbreak.", 
            "title": "Adding SSL Certificate for Cloudbreak UI"
        }, 
        {
            "location": "/security-cb/index.html#configure-access-from-custom-domains", 
            "text": "Cloudbreak Deployer, which uses UAA as an identity provider, supports multitenancy. In UAA, multitenancy is managed through identity zones. An identity zone is accessed through a unique subdomain. For example, if the standard UAA responds to  https://uaa.10.244.0.34.xip.io , a zone on this UAA can be accessed through a unique subdomain  https://testzone1.uaa.10.244.0.34.xip.io .  If you want to use a custom domain for your identity or deployment, add the  UAA_ZONE_DOMAIN  line to your  Profile :  export UAA_ZONE_DOMAIN=my-subdomain.example.com  This variable is necessary for UAA to identify which zone provider should handle the requests that arrive to that domain.", 
            "title": "Configure Access from Custom Domains"
        }, 
        {
            "location": "/security-kerberos/index.html", 
            "text": "Configuring Kerberos\n\n\n\n\nThis feature is \nTECHNICAL PREVIEW\n.\n\n\n\n\nCloudbreak supports using Kerberos security for its clusters. It supports three ways of provisioning Kerberos-enabled clusters:\n\n\n\n\nCreate new MIT Kerberos at cluster provisioning time  \n\n\nUse your existing MIT Kerberos server with a Cloudbreak provisioned cluster  \n\n\nUse your existing Active Directory with a Cloudbreak provisioned cluster  \n\n\n\n\nCreate New MIT Kerberos at Cluster Provisioning\n\n\nWhen creating a cluster, you can provide your Kerberos configuration details and Cloudbreak will install an MIT KDC and enable Kerberos on the cluster.\n\n\nTo enable Kerberos on a cluster, follow these steps when creating your cluster via Cloudbreak web UI.\n\n\nSteps\n\n\n\n\n\n\nIn the \nCreate cluster\n wizard, in the \nSetup Network and Security\n tab, check the \nEnable security\n option and select \nCreate New MIT Kerberos\n.\n\n\n\n\n\n\nProvide the following information for the KDC:\n\n\n\n\n\n\n\n\n\n\n\n\nField\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nKerberos master key\n\n\nThe master key to use for the KDC.\n\n\n\n\n\n\nKerberos admin\n\n\nThe KDC admin username to use for the KDC.\n\n\n\n\n\n\nKerberos password\n\n\nThe KDC admin password to use for the KDC.\n\n\n\n\n\n\n\n\nTesting Kerberos\n\n\nTo run a job on the cluster, you can use one of the default Hadoop users, like \nambari-qa\n.\n\n\nOnce kerberos is enabled, you need a \nticket\n to execute any job on the cluster. Here's an example of how to get a ticket:\n\n\nkinit -V -kt /etc/security/keytabs/smokeuser.headless.keytab ambari-qa-sparktest-rec@NODE.DC1.CONSUL\n\n\n\n\nHere is an example job:\n\n\nexport HADOOP_LIBS=/usr/hdp/current/hadoop-mapreduce-client\nexport JAR_EXAMPLES=$HADOOP_LIBS/hadoop-mapreduce-examples.jar\nexport JAR_JOBCLIENT=$HADOOP_LIBS/hadoop-mapreduce-client-jobclient.jar\n\nhadoop jar $JAR_EXAMPLES teragen 10000000 /user/ambari-qa/terasort-input\n\nhadoop jar $JAR_JOBCLIENT mrbench -baseDir /user/ambari-qa/smallJobsBenchmark -numRuns 5 -maps 10 -reduces 5 -inputLines 10 -inputType ascending\n\n\n\n\nUse Existing MIT Kerberos Server with a Cloudbreak Provisioned Cluster\n\n\nCloudbreak supports using Kerberos security on the cluster with an existing MIT Kerberos. When creating a cluster, provide your Kerberos configuration details, and Cloudbreak will automatically extend your blueprint configuration with the defined properties. \nSetup an exiting MIT KDC\n\n\nTo enable Kerberos on a cluster, perform the following steps when creating your cluster via Cloudbreak web UI.\n\n\nSteps\n\n\n\n\nIn the \nCreate cluster\n wizard, in the \nSetup Network and Security\n tab, check the \nEnable security\n option and select \nUse Existing MIT Kerberos\n.\n\n\nFill in the following fields:\n\n\n\n\n\n\n\n\n\n\nField\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nKerberos Password\n\n\nThe KDC admin password to use for the KDC.\n\n\n\n\n\n\nExisting Kerberos Principal\n\n\nThe KDC principal in your existing MIT KDC.\n\n\n\n\n\n\nExisting Kerberos URL\n\n\nThe location of your existing MIT KDC.\n\n\n\n\n\n\nUse Tcp Connection\n\n\nThe connection type for your existing MIT KDC (default is \nUDP\n).\n\n\n\n\n\n\nExisting Kerberos Realm\n\n\nThe realm in your existing MIT KDC.\n\n\n\n\n\n\n\n\nTo enable Kerberos on a cluster, do the following when creating your cluster via Cloudbreak shell:\n\n\n\n\n\n\n\n\nParameter\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\n--kerberosPassword\n\n\nThe KDC admin password to use for the KDC.\n\n\n\n\n\n\n--kerberosPrincipal\n\n\nThe KDC principal in your existing MIT KDC.\n\n\n\n\n\n\n--kerberosUrl\n\n\nThe location of your existing MIT KDC.\n\n\n\n\n\n\n--kerberosTcpAllowed\n\n\nThe connection type for your existing MIT KDC (default is \nUDP\n).\n\n\n\n\n\n\n--kerberosRealm\n\n\nThe realm in your existing MIT KDC.\n\n\n\n\n\n\n\n\nUse Existing Active Directory with a Cloudbreak Provisioned Cluster\n\n\nCloudbreak supports using Kerberos security on the cluster with an existing Active Directory. When creating a cluster, provide your Kerberos configuration details, and Cloudbreak will automatically extend your blueprint configuration with the defined properties. \nSetup an Active Directory for Kerberos\n\n\nTo enable Kerberos on a cluster, perform these steps when creating your cluster via Cloudbreak UI.\n\n\nSteps\n\n\n\n\nIn the \nCreate cluster\n wizard, in the \nSetup Network and Security\n tab, check the \nEnable security\n option and select \nUse Existing Active Directory\n.\n\n\nProvide the following informstion:\n\n\n\n\n\n\n\n\n\n\nField\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nKerberos Password\n\n\nThe KDC admin password to use for the KDC.\n\n\n\n\n\n\nExisting Kerberos Principal\n\n\nThe KDC principal in your existing MIT KDC.\n\n\n\n\n\n\nExisting Kerberos URL\n\n\nThe location of your existing MIT KDC.\n\n\n\n\n\n\nUse Tcp Connection\n\n\nThe connection type for your existing MIT KDC (default is \nUDP\n).\n\n\n\n\n\n\nExisting Kerberos Realm\n\n\nThe realm in your existing MIT KDC.\n\n\n\n\n\n\nExisting Kerberos Ldap AD Url\n\n\nThe url of the existing secure ldap (eg. ldaps://10.1.1.5).\n\n\n\n\n\n\nExisting Kerberos AD Container DN\n\n\nActive Directory User container for principals. For example \"OU=Hadoop,OU=People,dc=apache,dc=org\".\n\n\n\n\n\n\n\n\nCreate Hadoop Users\n\n\nTo create Hadoop users, follow these steps.\n\n\nSteps\n\n\n\n\n\n\nLog in via SSH to the node where the Ambari Server is (IP address is the same as the Ambari UI) and run:\n\n\n\nkadmin -p [admin_user]/[admin_user]@NODE.DC1.CONSUL (type admin password)\naddprinc custom-user (type user password twice)\n\n\n\n\n\n\n\nLog in via SSH to all other nodes and, on each node, run:\n\n\n\nuseradd custom-user\n\n\n\n\n\n\n\nLog in via SSH to one of the nodes and run:\n\n\n\nsu custom-user\nkinit -p custom-user (type user password)\nhdfs dfs -mkdir input\nhdfs dfs -put /tmp/wait-for-host-number.sh input\nyarn jar $(find /usr/hdp -name hadoop-mapreduce-examples.jar) wordcount input output\nhdfs dfs -cat output/*", 
            "title": "Configure Kerberos"
        }, 
        {
            "location": "/security-kerberos/index.html#configuring-kerberos", 
            "text": "This feature is  TECHNICAL PREVIEW .   Cloudbreak supports using Kerberos security for its clusters. It supports three ways of provisioning Kerberos-enabled clusters:   Create new MIT Kerberos at cluster provisioning time    Use your existing MIT Kerberos server with a Cloudbreak provisioned cluster    Use your existing Active Directory with a Cloudbreak provisioned cluster", 
            "title": "Configuring Kerberos"
        }, 
        {
            "location": "/security-kerberos/index.html#create-new-mit-kerberos-at-cluster-provisioning", 
            "text": "When creating a cluster, you can provide your Kerberos configuration details and Cloudbreak will install an MIT KDC and enable Kerberos on the cluster.  To enable Kerberos on a cluster, follow these steps when creating your cluster via Cloudbreak web UI.  Steps    In the  Create cluster  wizard, in the  Setup Network and Security  tab, check the  Enable security  option and select  Create New MIT Kerberos .    Provide the following information for the KDC:       Field  Description      Kerberos master key  The master key to use for the KDC.    Kerberos admin  The KDC admin username to use for the KDC.    Kerberos password  The KDC admin password to use for the KDC.", 
            "title": "Create New MIT Kerberos at Cluster Provisioning"
        }, 
        {
            "location": "/security-kerberos/index.html#testing-kerberos", 
            "text": "To run a job on the cluster, you can use one of the default Hadoop users, like  ambari-qa .  Once kerberos is enabled, you need a  ticket  to execute any job on the cluster. Here's an example of how to get a ticket:  kinit -V -kt /etc/security/keytabs/smokeuser.headless.keytab ambari-qa-sparktest-rec@NODE.DC1.CONSUL  Here is an example job:  export HADOOP_LIBS=/usr/hdp/current/hadoop-mapreduce-client\nexport JAR_EXAMPLES=$HADOOP_LIBS/hadoop-mapreduce-examples.jar\nexport JAR_JOBCLIENT=$HADOOP_LIBS/hadoop-mapreduce-client-jobclient.jar\n\nhadoop jar $JAR_EXAMPLES teragen 10000000 /user/ambari-qa/terasort-input\n\nhadoop jar $JAR_JOBCLIENT mrbench -baseDir /user/ambari-qa/smallJobsBenchmark -numRuns 5 -maps 10 -reduces 5 -inputLines 10 -inputType ascending", 
            "title": "Testing Kerberos"
        }, 
        {
            "location": "/security-kerberos/index.html#use-existing-mit-kerberos-server-with-a-cloudbreak-provisioned-cluster", 
            "text": "Cloudbreak supports using Kerberos security on the cluster with an existing MIT Kerberos. When creating a cluster, provide your Kerberos configuration details, and Cloudbreak will automatically extend your blueprint configuration with the defined properties.  Setup an exiting MIT KDC  To enable Kerberos on a cluster, perform the following steps when creating your cluster via Cloudbreak web UI.  Steps   In the  Create cluster  wizard, in the  Setup Network and Security  tab, check the  Enable security  option and select  Use Existing MIT Kerberos .  Fill in the following fields:      Field  Description      Kerberos Password  The KDC admin password to use for the KDC.    Existing Kerberos Principal  The KDC principal in your existing MIT KDC.    Existing Kerberos URL  The location of your existing MIT KDC.    Use Tcp Connection  The connection type for your existing MIT KDC (default is  UDP ).    Existing Kerberos Realm  The realm in your existing MIT KDC.     To enable Kerberos on a cluster, do the following when creating your cluster via Cloudbreak shell:     Parameter  Description      --kerberosPassword  The KDC admin password to use for the KDC.    --kerberosPrincipal  The KDC principal in your existing MIT KDC.    --kerberosUrl  The location of your existing MIT KDC.    --kerberosTcpAllowed  The connection type for your existing MIT KDC (default is  UDP ).    --kerberosRealm  The realm in your existing MIT KDC.", 
            "title": "Use Existing MIT Kerberos Server with a Cloudbreak Provisioned Cluster"
        }, 
        {
            "location": "/security-kerberos/index.html#use-existing-active-directory-with-a-cloudbreak-provisioned-cluster", 
            "text": "Cloudbreak supports using Kerberos security on the cluster with an existing Active Directory. When creating a cluster, provide your Kerberos configuration details, and Cloudbreak will automatically extend your blueprint configuration with the defined properties.  Setup an Active Directory for Kerberos  To enable Kerberos on a cluster, perform these steps when creating your cluster via Cloudbreak UI.  Steps   In the  Create cluster  wizard, in the  Setup Network and Security  tab, check the  Enable security  option and select  Use Existing Active Directory .  Provide the following informstion:      Field  Description      Kerberos Password  The KDC admin password to use for the KDC.    Existing Kerberos Principal  The KDC principal in your existing MIT KDC.    Existing Kerberos URL  The location of your existing MIT KDC.    Use Tcp Connection  The connection type for your existing MIT KDC (default is  UDP ).    Existing Kerberos Realm  The realm in your existing MIT KDC.    Existing Kerberos Ldap AD Url  The url of the existing secure ldap (eg. ldaps://10.1.1.5).    Existing Kerberos AD Container DN  Active Directory User container for principals. For example \"OU=Hadoop,OU=People,dc=apache,dc=org\".", 
            "title": "Use Existing Active Directory with a Cloudbreak Provisioned Cluster"
        }, 
        {
            "location": "/security-kerberos/index.html#create-hadoop-users", 
            "text": "To create Hadoop users, follow these steps.  Steps    Log in via SSH to the node where the Ambari Server is (IP address is the same as the Ambari UI) and run:  \nkadmin -p [admin_user]/[admin_user]@NODE.DC1.CONSUL (type admin password)\naddprinc custom-user (type user password twice)    Log in via SSH to all other nodes and, on each node, run:  \nuseradd custom-user    Log in via SSH to one of the nodes and run:  \nsu custom-user\nkinit -p custom-user (type user password)\nhdfs dfs -mkdir input\nhdfs dfs -put /tmp/wait-for-host-number.sh input\nyarn jar $(find /usr/hdp -name hadoop-mapreduce-examples.jar) wordcount input output\nhdfs dfs -cat output/*", 
            "title": "Create Hadoop Users"
        }, 
        {
            "location": "/cli/index.html", 
            "text": "Cloudbreak Shell\n\n\nComing soon...", 
            "title": "CLI"
        }, 
        {
            "location": "/cli/index.html#cloudbreak-shell", 
            "text": "Coming soon...", 
            "title": "Cloudbreak Shell"
        }, 
        {
            "location": "/dev/index.html", 
            "text": "Developer Documentation\n\n\nThe following table includes links to Cloudbreak developer documentation: \n\n\n\n\n\n\n\n\nDocumentation Link\n\n\nDescripriton\n\n\n\n\n\n\n\n\n\n\nSet Up Local Development\n\n\nThis documentation will help you set up your local development environment.\n\n\n\n\n\n\nSPI Reference\n\n\nThis is Cloudbreak SPI reference documentation.\n\n\n\n\n\n\nAPI Reference\n\n\nThis is Cloudbreak API reference documentation.\n\n\n\n\n\n\nFlow Diagrams\n\n\nThis is Cloudbreak flow diagrams reference documentation.", 
            "title": "Developer Docs"
        }, 
        {
            "location": "/dev/index.html#developer-documentation", 
            "text": "The following table includes links to Cloudbreak developer documentation:      Documentation Link  Descripriton      Set Up Local Development  This documentation will help you set up your local development environment.    SPI Reference  This is Cloudbreak SPI reference documentation.    API Reference  This is Cloudbreak API reference documentation.    Flow Diagrams  This is Cloudbreak flow diagrams reference documentation.", 
            "title": "Developer Documentation"
        }, 
        {
            "location": "/dev-spi/index.html", 
            "text": "Cloudbreak Service Provider Interface (SPI)\n\n\nIn addition to supporting multiple cloud platforms, Cloudbreak provides an easy way to integrate a new provider trough its \nService Provider Interface (SPI)\n, a plugin mechanism that enables seamless integration with any cloud provider. \n\n\nThis SPI plugin mechanism has been used to integrate all currently supported providers with Cloudbreak. The following links point to the Cloudbreak SPI implementations for AWS, Azure, Google Cloud, and OpenStack. You can use these implementations as a reference:\n\n\n\n\nThe \ncloud-aws\n module integrates Amazon Web Services\n\n\nThe \ncloud-arm\n module integrates Microsoft Azure\n\n\nThe \ncloud-gcp\n module integrates Google Cloud Platform  \n\n\nThe \ncloud-openstack\n module integrates OpenStack\n\n\n\n\nThe Cloubdreak SPI interface is event-based, scalable, and decoupled from Cloudbreak. The core of Cloudbreak uses \nEventBus\n to communicate with the providers, but the complexity of event handling is hidden from the provider implementation.\n\n\nSupported Resource Management Methods\n\n\nCloud providers support two kinds of deployment and resource management methods:\n\n\n\n\nTemplate based deployments\n\n\nIndividual resource based deployments\n\n\n\n\nCloudbreak's SPI supports both of these methods. It provides a well-defined interface, abstract classes, and helper classes, scheduling and polling of resources to aid the integration and to avoid any boilerplate code in the module of cloud provider.\n\n\nTemplate Based Deployments\n\n\nProviders with template-based deployments such as \nAWS CloudFormation\n, \nAzure ARM\n or \nOpenStack Heat\n have the ability to create and manage a collection of related cloud resources, provisioning and updating them in an orderly and predictable fashion. \n\n\nWhen working with providers that use template-based deployments, Cloudbreak needs to be provided with a reference to the template, because every change in the infrastructure (for example, creating a new instance or deleting one) is managed through this templating mechanism.\n\n\nIf a provider has templating support, then the provider's \ngradle\n module depends on the \ncloud-api\n module:\n\n\napply plugin: 'java'\n\nsourceCompatibility = 1.7\n\nrepositories {\n    mavenCentral()\n}\n\njar {\n    baseName = 'cloud-new-provider'\n}\n\ndependencies {\n\n    compile project(':cloud-api')\n\n}\n\n\n\n\nThe entry point for the provider is the  \nCloudConnector\n interface and every interface that needs to be implemented is reachable trough this interface.\n\n\nIndividual Resource Based Deployments\n\n\nThere are providers such as GCP that do not support a templating mechanism, and customizable providers such as OpenStack where the Heat Orchestration (templating) component is optional and individual resources need to be handled separately. \n\n\nWhen working with such providers, resources such as networks, discs, and compute instances need to be created and managed with an ordered sequence of API calls, and Cloudbreak needs to provide a solution to manage the collection of related cloud resources as a whole.\n\n\nIf the provider has no templating support, then the provider's \ngradle\n module typically depends on the \ncloud-template\n module, which includes Cloudbreak-defined abstract template. This template is a set of abstract and utility classes to support provisioning and updating related resources in an orderly and predictable manner trough ordered sequences of cloud API calls:\n\n\napply plugin: 'java'\n\nsourceCompatibility = 1.7\n\nrepositories {\n    mavenCentral()\n}\n\njar {\n    baseName = 'cloud-new-provider'\n}\n\ndependencies {\n\n    compile project(':cloud-template')\n\n}\n\n\n\n\nSupport for Modularity\n\n\nCloudbreak uses \nvariants\n to deal with highly modular providers such as OpenStack, which allows you to install different components (for volume storage, networking, and so on) and to entirely exclude certain components. For example, Nova or Neutron can be used for networking in OpenStack and some components such as Heat may not installed at all in some deployment scenarios. \n\n\nCloudbreak SPI interface uses variants to support this flexibility: if some part of the cloud provider uses a different component, you don't need to re-implement the complete stack, but just use a different variant and re-implement the part that is different.\n\n\nAn example implementation for this feature can be found in the \ncloud-openstack\n module which supports a HEAT and NATIVE variants. While the HEAT variant utilizes the Heat templating to launch a stack, the NATIVE variant starts the cluster by using a sequence of API calls without Heat to achieve the same result. Both of them use the same authentication and credential management.", 
            "title": "Cloudbreak SPI"
        }, 
        {
            "location": "/dev-spi/index.html#cloudbreak-service-provider-interface-spi", 
            "text": "In addition to supporting multiple cloud platforms, Cloudbreak provides an easy way to integrate a new provider trough its  Service Provider Interface (SPI) , a plugin mechanism that enables seamless integration with any cloud provider.   This SPI plugin mechanism has been used to integrate all currently supported providers with Cloudbreak. The following links point to the Cloudbreak SPI implementations for AWS, Azure, Google Cloud, and OpenStack. You can use these implementations as a reference:   The  cloud-aws  module integrates Amazon Web Services  The  cloud-arm  module integrates Microsoft Azure  The  cloud-gcp  module integrates Google Cloud Platform    The  cloud-openstack  module integrates OpenStack   The Cloubdreak SPI interface is event-based, scalable, and decoupled from Cloudbreak. The core of Cloudbreak uses  EventBus  to communicate with the providers, but the complexity of event handling is hidden from the provider implementation.", 
            "title": "Cloudbreak Service Provider Interface (SPI)"
        }, 
        {
            "location": "/dev-spi/index.html#supported-resource-management-methods", 
            "text": "Cloud providers support two kinds of deployment and resource management methods:   Template based deployments  Individual resource based deployments   Cloudbreak's SPI supports both of these methods. It provides a well-defined interface, abstract classes, and helper classes, scheduling and polling of resources to aid the integration and to avoid any boilerplate code in the module of cloud provider.", 
            "title": "Supported Resource Management Methods"
        }, 
        {
            "location": "/dev-spi/index.html#template-based-deployments", 
            "text": "Providers with template-based deployments such as  AWS CloudFormation ,  Azure ARM  or  OpenStack Heat  have the ability to create and manage a collection of related cloud resources, provisioning and updating them in an orderly and predictable fashion.   When working with providers that use template-based deployments, Cloudbreak needs to be provided with a reference to the template, because every change in the infrastructure (for example, creating a new instance or deleting one) is managed through this templating mechanism.  If a provider has templating support, then the provider's  gradle  module depends on the  cloud-api  module:  apply plugin: 'java'\n\nsourceCompatibility = 1.7\n\nrepositories {\n    mavenCentral()\n}\n\njar {\n    baseName = 'cloud-new-provider'\n}\n\ndependencies {\n\n    compile project(':cloud-api')\n\n}  The entry point for the provider is the   CloudConnector  interface and every interface that needs to be implemented is reachable trough this interface.", 
            "title": "Template Based Deployments"
        }, 
        {
            "location": "/dev-spi/index.html#individual-resource-based-deployments", 
            "text": "There are providers such as GCP that do not support a templating mechanism, and customizable providers such as OpenStack where the Heat Orchestration (templating) component is optional and individual resources need to be handled separately.   When working with such providers, resources such as networks, discs, and compute instances need to be created and managed with an ordered sequence of API calls, and Cloudbreak needs to provide a solution to manage the collection of related cloud resources as a whole.  If the provider has no templating support, then the provider's  gradle  module typically depends on the  cloud-template  module, which includes Cloudbreak-defined abstract template. This template is a set of abstract and utility classes to support provisioning and updating related resources in an orderly and predictable manner trough ordered sequences of cloud API calls:  apply plugin: 'java'\n\nsourceCompatibility = 1.7\n\nrepositories {\n    mavenCentral()\n}\n\njar {\n    baseName = 'cloud-new-provider'\n}\n\ndependencies {\n\n    compile project(':cloud-template')\n\n}", 
            "title": "Individual Resource Based Deployments"
        }, 
        {
            "location": "/dev-spi/index.html#support-for-modularity", 
            "text": "Cloudbreak uses  variants  to deal with highly modular providers such as OpenStack, which allows you to install different components (for volume storage, networking, and so on) and to entirely exclude certain components. For example, Nova or Neutron can be used for networking in OpenStack and some components such as Heat may not installed at all in some deployment scenarios.   Cloudbreak SPI interface uses variants to support this flexibility: if some part of the cloud provider uses a different component, you don't need to re-implement the complete stack, but just use a different variant and re-implement the part that is different.  An example implementation for this feature can be found in the  cloud-openstack  module which supports a HEAT and NATIVE variants. While the HEAT variant utilizes the Heat templating to launch a stack, the NATIVE variant starts the cluster by using a sequence of API calls without Heat to achieve the same result. Both of them use the same authentication and credential management.", 
            "title": "Support for Modularity"
        }, 
        {
            "location": "/dev-api/index.html", 
            "text": "Cloudbreak APIs\n\n\nCloudbreak is a RESTful application development platform whose goal is to help developers deploy HDP clusters in various cloud environments. Once Cloudbreak is deployed in your favorite servlet container, it exposes REST APIs, allowing you to spin up Hadoop clusters of any size with your chosen cloud provider.\n\n\nAPI Documentation\n\n\nThe Cloudbreak API documentation is available \nhere\n. \n\n\n\n\nThis documentation was generated from the code using \nSwagger\n.", 
            "title": "Cloudbreak APIs"
        }, 
        {
            "location": "/dev-api/index.html#cloudbreak-apis", 
            "text": "Cloudbreak is a RESTful application development platform whose goal is to help developers deploy HDP clusters in various cloud environments. Once Cloudbreak is deployed in your favorite servlet container, it exposes REST APIs, allowing you to spin up Hadoop clusters of any size with your chosen cloud provider.", 
            "title": "Cloudbreak APIs"
        }, 
        {
            "location": "/dev-api/index.html#api-documentation", 
            "text": "The Cloudbreak API documentation is available  here .    This documentation was generated from the code using  Swagger .", 
            "title": "API Documentation"
        }, 
        {
            "location": "/dev-flows/index.html", 
            "text": "Flow Diagrams\n\n\nRefer to \nhttp://hortonworks.github.io/cloudbreak-docs/release-1.16.4/flow/\n.", 
            "title": "Flow Diagrams"
        }, 
        {
            "location": "/dev-flows/index.html#flow-diagrams", 
            "text": "Refer to  http://hortonworks.github.io/cloudbreak-docs/release-1.16.4/flow/ .", 
            "title": "Flow Diagrams"
        }, 
        {
            "location": "/trouble-cb/index.html", 
            "text": "Troubleshooting Cloudbreak\n\n\nChecking the Logs\n\n\nWhen troubleshooting, you can access the following Cloudbreak logs.\n\n\nCloudbreak Logs\n\n\nWhen installing Cloudbreak using a pre-built cloud image, the  Cloudbreak Deployer location and the cbd root folder is \n/var/lib/cloudbreak-deployment\n. You must execute all cbd actions from the cbd root folder as a cloudbreak user. \n\n\n\n\nYour cbd root directory may be different if you installed Cloudbreak on your own VM. \n\n\n\n\nCloudbreak consists of multiple microservices deployed into Docker containers. \n\n\nAggregated Logs\n\n\nTo check aggregated service logs, use the following commands:\n\n\ncbd logs\n shows all service logs.\n\n\ncbd logs | tee cloudbreak.log\n allows you to redirect the input into a file for sharing these logs.\n\n\nIndividual Service Logs\n\n\nTo check individual service logs, use the following commands:\n\n\ncbd logs cloudbreak\n shows Cloudbreak logs. This service is the backend service that handles all deployments.\n\n\ncbd logs uluwatu\n shows Cloudbreak UI logs. Uluwatu is the UI component of Cloudbreak.\n\n\ncbd logs identity\n shows Identity logs. Identity is responsible for authentication and authorization.\n\n\ncbd logs periscope\n shows Periscope logs. Periscope is responsible for triggering autoscaling rules.\n\n\nDocker Logs\n\n\nThe same logs can be accessed via Docker commands:\n\n\ndocker logs cbreak_cloudbreak_1\n shows the same logs as \ncbd logs cloudbreak\n.\n\n\nCloudbreak logs are rotated and can be accessed later from the Cloudbreak deployment folder. Each time you restart the application via cbd restart a new log file is created with a timestamp in the name (for example, cbreak-20170821-105900.log). \n\n\n\n\nThere is a symlink called \ncbreak.log\n which points to the latest log file. Sharing this symlink does not share the log itself.\n\n\n\n\nSaltstack Logs\n\n\nCloudbreak uses Saltstack to install Ambari and the necessary packages for the HDP provisioning. Salt Master always runs alongside the Ambari Server node. Each instance in the cluster runs a Salt Minion, which connects to the Salt Master. There can be multiple Salt Masters if the cluster is configured to run in HA (High Availability) mode and in this case each Salt Minion connects to each Salt Master.\n\n\nCloudbreak also uses SaltStack to execute user-provided customization scripts called \"recipes\". \n\n\nSalt Master and Salt Minion logs can be found at the following location: \n/var/log/salt\n\n\nAmbari Logs\n\n\nCloudbreak uses Ambari to orchestrate the installation of the different HDP components. Each instance in the cluster runs an Ambari agent which connects to the Ambari server. Ambari server is declared by the user during the cluster installation wizard. \n\n\nAmbari Server Logs\n\n\nAmbari server logs can be found on the nodes where Ambari server is installed in the following locations:\n\n\n/var/log/ambari-server/ambari-server.log\n\n\n/var/log/ambari-server/ambari-server.out\n\n\nBoth files contain important information about the root cause of a certain issue so it is advised to check both.\n\n\nAmbari Agent Logs\n\n\nAmbari agent logs can be found on the nodes where Ambari agent is installed in the following locations:\n\n\n/var/log/ambari-agent/ambari-agent.log\n\n\nRecipe Logs\n\n\nCloudbreak supports \"recipes\" - user-provided customization scripts that can be run prior to or after cluster installtion. It is the user\u2019s responsibility to provide an idempotent well tested script. If the execution fails, the recipe logs can be found at \n/var/log/recipes\n on the nodes on which the recipes were executed.\n\n\nIt is advised, but not required to have an advanced logging mechanism in the script, as Cloudbreak always logs every script that are run. Recipes are often the sources of installation failures as users might try to remove necessary packages or reconfigure services.\n\n\nCommon Errors\n\n\nQuota Limitations\n\n\nEach cloud provider has quota limitations on various cloud resources, and these quotas can usually be increased on request. If there is an error message in Cloudbreak saying that there are no more available EIPs (Elastic IP Address) or VPCs, you need to request more of these resources. \n\n\nTo see the limitations visit the cloud provider\u2019s site:\n\n\n\n\nAWS Service Limits\n \n\n\nAzure subscription and service limits, quotas, and constraints\n\n\nGCP Resource Quotas\n \n\n\n\n\nConnection Timeout: Ports Not Open\n\n\nIn the cluster installation wizard, you must specify on which node you want to run the Ambari server. Cloudbreak communicates with this node to orchestrate the installation.\n\n\nA common reason for connection timeout is security group misconfiguration. Cloudbreak allows configuring different security groups for the different instance groups; however, there are certain requirements for the Ambari server node. Specifically, the following ports must be open in order to communicate with that node:\n\n\n\n\n22 (SSH)  \n\n\n9443 (two-way-ssl through nginx) \n\n\n\n\nBlueprints: Invalid Services and Configurations\n\n\nAmbari blueprints are a declarative definition of a cluster. With a blueprint, you specify a stack, the component layout, and the configurations to materialize a Hadoop cluster instance via a REST API without having to use the Ambari cluster install wizard. \n\n\nCloudbreak supports any type of blueprints, which is a common source of errors. These errors are only visible once the core infrastructure is up and running and Cloudbreak tries to initiate the cluster installation through Ambari. Ambari validates the blueprint and  rejects it if it's invalid. \n\n\nFor example, if there are configurations for a certain service like Hive but Hive as a service is not mapped to any host group, the blueprint is invalid.\n\n\nTo fix these type of issues, edit your blueprint and then reinstall your cluster. Cloudbreak UI has support for this so the infrastructure does not have to be terminated.\n\n\nThere are some cases when Ambari cannot validate your blueprint beforehand. In these cases, the issues are only visible in the Ambari server logs. To trubleshoot, check Ambari server logs.\n\n\nBlueprints: High Availability\n\n\nCloudbreak always tries to validate that a blueprint not to include multiple master services into different host groups. However, this exact setup is required for HA clusters. To overcome this, you can disable blueprint validation in the UI (using an advanced option in the Create Cluster wizard \n Choose Blueprint), but you must include the necessary configurations.\n\n\nBlueprints: Wrong HDP Version\n\n\nIn the blueprint, only the major and minor HDP version should be defined (for example, \"2.6\"). If wrong version number is provided, the following error can be found in the logs:\n\n\n5/15/2017 12:23:19 PM testcluster26 - create failed: Cannot use the specified Ambari stack: HDPRepo\n{stack='null'; utils='null'}\n. Error: org.apache.ambari.server.controller.spi.NoSuchResourceException: The specified resource doesn't exist: Stack data, Stack HDP 2.6.0.3 is not found in Ambari metainfo\n\n\n\n\nFor correct blueprint layout, refer to the \nAmbari cwiki\n page.\n\n\nRecipes: Recipe Execution Times Out\n\n\nIf the scripts are taking too much time to execute, the processes will time out, as the threshold for all recipes is set to 15 minutes. To change this threshold, you must override the default value by adding the following to the cbd Profile file:\n\n\nexport CB_JAVA_OPTS=\u201d -Dcb.max.salt.recipe.execution.retry=90\u201d\n\n\n\n\nThis property indicates the number of tries for checking if the scripts have finished with a sleep time (i.e. the wait time between two polling attempts) of 10 seconds. The default value is 90. To increase the threshold, provide a number greater than 90. You must restart Cloudbreak after changing properties in the Profile file.\n\n\nRecipes: Recipe Execution Fails\n\n\nIt often happens that a script cannot be executed successfully because there are typos or errors in the script. To verify this you can check the recipe logs at\n\n/var/log/recipes\n. For each script, there will be a separate log file with the name of the script that you provided on the Cloudbreak UI.\n\n\nInvalid PUBLIC_IP in CBD Profile\n\n\nThe \nPUBLIC_IP\n property must be set in the cbd Profile file or else you won\u2019t be able to log in on the Cloudbreak UI. \n\n\nIf you are migrating your instance, make sure that after the start the IP remains valid. If you need to edit the \nPUBLIC_IP\n property in Profile, make sure to restart Cloudbreak using \ncbd restart\n.\n\n\nChanging Properties in the Cloudbreak Profile File\n\n\nThere are many properties that can be changed in the Cloudbreak application. These values must be changed in the Cloudbreak \nProfil\ne file. To see all possible options, use the following command:\n\ncbd env show\n.\n\n\nAfter changing a property, you must regenerate the config file and restart the application. There are two ways to do this:\n\n\nIn version 1.4.0 and newer of the cbd command line, you can regenerate the config file and restart the application with a single command:\n\n\ncbd restart\n - same as cbd regenerate/kill/start.\n\n\nIn versions earlier than 1.4.0, you must run the following three commands:\n\n\ncbd regenerate\n regenerates the Docker compose file\n\ncbd kill\n removes all Docker containers (there is no stop command for this).\n\ncbd start\n starts the application with the new compose file.\n\n\nChanging Amari Credentials\n\n\nIn Cloudbreak 1.14 and later, Cloudbreak creates a new admin user in Ambari, so you can change the credentials of the admin user in the Ambari web UI. You can also set the admin user credentials in the cluster installation wizard in the Cloudbreak UI.\n\n\nIn Cloudbreak versions earlier than 1.14 it is not possible to change the password in the Ambari UI.  If you change the admin credentials in the Ambari UI, Cloudbreak will no longer be able to orchestrate Ambari. To change the password, you must use the option available on the cluster details page in the Cloudbreak UI.", 
            "title": "General Troubleshooting"
        }, 
        {
            "location": "/trouble-cb/index.html#troubleshooting-cloudbreak", 
            "text": "", 
            "title": "Troubleshooting Cloudbreak"
        }, 
        {
            "location": "/trouble-cb/index.html#checking-the-logs", 
            "text": "When troubleshooting, you can access the following Cloudbreak logs.", 
            "title": "Checking the Logs"
        }, 
        {
            "location": "/trouble-cb/index.html#cloudbreak-logs", 
            "text": "When installing Cloudbreak using a pre-built cloud image, the  Cloudbreak Deployer location and the cbd root folder is  /var/lib/cloudbreak-deployment . You must execute all cbd actions from the cbd root folder as a cloudbreak user.    Your cbd root directory may be different if you installed Cloudbreak on your own VM.    Cloudbreak consists of multiple microservices deployed into Docker containers.   Aggregated Logs  To check aggregated service logs, use the following commands:  cbd logs  shows all service logs.  cbd logs | tee cloudbreak.log  allows you to redirect the input into a file for sharing these logs.  Individual Service Logs  To check individual service logs, use the following commands:  cbd logs cloudbreak  shows Cloudbreak logs. This service is the backend service that handles all deployments.  cbd logs uluwatu  shows Cloudbreak UI logs. Uluwatu is the UI component of Cloudbreak.  cbd logs identity  shows Identity logs. Identity is responsible for authentication and authorization.  cbd logs periscope  shows Periscope logs. Periscope is responsible for triggering autoscaling rules.  Docker Logs  The same logs can be accessed via Docker commands:  docker logs cbreak_cloudbreak_1  shows the same logs as  cbd logs cloudbreak .  Cloudbreak logs are rotated and can be accessed later from the Cloudbreak deployment folder. Each time you restart the application via cbd restart a new log file is created with a timestamp in the name (for example, cbreak-20170821-105900.log).    There is a symlink called  cbreak.log  which points to the latest log file. Sharing this symlink does not share the log itself.", 
            "title": "Cloudbreak Logs"
        }, 
        {
            "location": "/trouble-cb/index.html#saltstack-logs", 
            "text": "Cloudbreak uses Saltstack to install Ambari and the necessary packages for the HDP provisioning. Salt Master always runs alongside the Ambari Server node. Each instance in the cluster runs a Salt Minion, which connects to the Salt Master. There can be multiple Salt Masters if the cluster is configured to run in HA (High Availability) mode and in this case each Salt Minion connects to each Salt Master.  Cloudbreak also uses SaltStack to execute user-provided customization scripts called \"recipes\".   Salt Master and Salt Minion logs can be found at the following location:  /var/log/salt", 
            "title": "Saltstack Logs"
        }, 
        {
            "location": "/trouble-cb/index.html#ambari-logs", 
            "text": "Cloudbreak uses Ambari to orchestrate the installation of the different HDP components. Each instance in the cluster runs an Ambari agent which connects to the Ambari server. Ambari server is declared by the user during the cluster installation wizard.   Ambari Server Logs  Ambari server logs can be found on the nodes where Ambari server is installed in the following locations:  /var/log/ambari-server/ambari-server.log  /var/log/ambari-server/ambari-server.out  Both files contain important information about the root cause of a certain issue so it is advised to check both.  Ambari Agent Logs  Ambari agent logs can be found on the nodes where Ambari agent is installed in the following locations:  /var/log/ambari-agent/ambari-agent.log", 
            "title": "Ambari Logs"
        }, 
        {
            "location": "/trouble-cb/index.html#recipe-logs", 
            "text": "Cloudbreak supports \"recipes\" - user-provided customization scripts that can be run prior to or after cluster installtion. It is the user\u2019s responsibility to provide an idempotent well tested script. If the execution fails, the recipe logs can be found at  /var/log/recipes  on the nodes on which the recipes were executed.  It is advised, but not required to have an advanced logging mechanism in the script, as Cloudbreak always logs every script that are run. Recipes are often the sources of installation failures as users might try to remove necessary packages or reconfigure services.", 
            "title": "Recipe Logs"
        }, 
        {
            "location": "/trouble-cb/index.html#common-errors", 
            "text": "", 
            "title": "Common Errors"
        }, 
        {
            "location": "/trouble-cb/index.html#quota-limitations", 
            "text": "Each cloud provider has quota limitations on various cloud resources, and these quotas can usually be increased on request. If there is an error message in Cloudbreak saying that there are no more available EIPs (Elastic IP Address) or VPCs, you need to request more of these resources.   To see the limitations visit the cloud provider\u2019s site:   AWS Service Limits    Azure subscription and service limits, quotas, and constraints  GCP Resource Quotas", 
            "title": "Quota Limitations"
        }, 
        {
            "location": "/trouble-cb/index.html#connection-timeout-ports-not-open", 
            "text": "In the cluster installation wizard, you must specify on which node you want to run the Ambari server. Cloudbreak communicates with this node to orchestrate the installation.  A common reason for connection timeout is security group misconfiguration. Cloudbreak allows configuring different security groups for the different instance groups; however, there are certain requirements for the Ambari server node. Specifically, the following ports must be open in order to communicate with that node:   22 (SSH)    9443 (two-way-ssl through nginx)", 
            "title": "Connection Timeout: Ports Not Open"
        }, 
        {
            "location": "/trouble-cb/index.html#blueprints-invalid-services-and-configurations", 
            "text": "Ambari blueprints are a declarative definition of a cluster. With a blueprint, you specify a stack, the component layout, and the configurations to materialize a Hadoop cluster instance via a REST API without having to use the Ambari cluster install wizard.   Cloudbreak supports any type of blueprints, which is a common source of errors. These errors are only visible once the core infrastructure is up and running and Cloudbreak tries to initiate the cluster installation through Ambari. Ambari validates the blueprint and  rejects it if it's invalid.   For example, if there are configurations for a certain service like Hive but Hive as a service is not mapped to any host group, the blueprint is invalid.  To fix these type of issues, edit your blueprint and then reinstall your cluster. Cloudbreak UI has support for this so the infrastructure does not have to be terminated.  There are some cases when Ambari cannot validate your blueprint beforehand. In these cases, the issues are only visible in the Ambari server logs. To trubleshoot, check Ambari server logs.", 
            "title": "Blueprints: Invalid Services and Configurations"
        }, 
        {
            "location": "/trouble-cb/index.html#blueprints-high-availability", 
            "text": "Cloudbreak always tries to validate that a blueprint not to include multiple master services into different host groups. However, this exact setup is required for HA clusters. To overcome this, you can disable blueprint validation in the UI (using an advanced option in the Create Cluster wizard   Choose Blueprint), but you must include the necessary configurations.", 
            "title": "Blueprints: High Availability"
        }, 
        {
            "location": "/trouble-cb/index.html#blueprints-wrong-hdp-version", 
            "text": "In the blueprint, only the major and minor HDP version should be defined (for example, \"2.6\"). If wrong version number is provided, the following error can be found in the logs:  5/15/2017 12:23:19 PM testcluster26 - create failed: Cannot use the specified Ambari stack: HDPRepo\n{stack='null'; utils='null'}\n. Error: org.apache.ambari.server.controller.spi.NoSuchResourceException: The specified resource doesn't exist: Stack data, Stack HDP 2.6.0.3 is not found in Ambari metainfo  For correct blueprint layout, refer to the  Ambari cwiki  page.", 
            "title": "Blueprints: Wrong HDP Version"
        }, 
        {
            "location": "/trouble-cb/index.html#recipes-recipe-execution-times-out", 
            "text": "If the scripts are taking too much time to execute, the processes will time out, as the threshold for all recipes is set to 15 minutes. To change this threshold, you must override the default value by adding the following to the cbd Profile file:  export CB_JAVA_OPTS=\u201d -Dcb.max.salt.recipe.execution.retry=90\u201d  This property indicates the number of tries for checking if the scripts have finished with a sleep time (i.e. the wait time between two polling attempts) of 10 seconds. The default value is 90. To increase the threshold, provide a number greater than 90. You must restart Cloudbreak after changing properties in the Profile file.", 
            "title": "Recipes: Recipe Execution Times Out"
        }, 
        {
            "location": "/trouble-cb/index.html#recipes-recipe-execution-fails", 
            "text": "It often happens that a script cannot be executed successfully because there are typos or errors in the script. To verify this you can check the recipe logs at /var/log/recipes . For each script, there will be a separate log file with the name of the script that you provided on the Cloudbreak UI.", 
            "title": "Recipes: Recipe Execution Fails"
        }, 
        {
            "location": "/trouble-cb/index.html#invalid-public_ip-in-cbd-profile", 
            "text": "The  PUBLIC_IP  property must be set in the cbd Profile file or else you won\u2019t be able to log in on the Cloudbreak UI.   If you are migrating your instance, make sure that after the start the IP remains valid. If you need to edit the  PUBLIC_IP  property in Profile, make sure to restart Cloudbreak using  cbd restart .", 
            "title": "Invalid PUBLIC_IP in CBD Profile"
        }, 
        {
            "location": "/trouble-cb/index.html#changing-properties-in-the-cloudbreak-profile-file", 
            "text": "There are many properties that can be changed in the Cloudbreak application. These values must be changed in the Cloudbreak  Profil e file. To see all possible options, use the following command: cbd env show .  After changing a property, you must regenerate the config file and restart the application. There are two ways to do this:  In version 1.4.0 and newer of the cbd command line, you can regenerate the config file and restart the application with a single command:  cbd restart  - same as cbd regenerate/kill/start.  In versions earlier than 1.4.0, you must run the following three commands:  cbd regenerate  regenerates the Docker compose file cbd kill  removes all Docker containers (there is no stop command for this). cbd start  starts the application with the new compose file.", 
            "title": "Changing Properties in the Cloudbreak Profile File"
        }, 
        {
            "location": "/trouble-cb/index.html#changing-amari-credentials", 
            "text": "In Cloudbreak 1.14 and later, Cloudbreak creates a new admin user in Ambari, so you can change the credentials of the admin user in the Ambari web UI. You can also set the admin user credentials in the cluster installation wizard in the Cloudbreak UI.  In Cloudbreak versions earlier than 1.14 it is not possible to change the password in the Ambari UI.  If you change the admin credentials in the Ambari UI, Cloudbreak will no longer be able to orchestrate Ambari. To change the password, you must use the option available on the cluster details page in the Cloudbreak UI.", 
            "title": "Changing Amari Credentials"
        }, 
        {
            "location": "/trouble-aws/index.html", 
            "text": "Troubleshooting Cloudbreak on AWS\n\n\nComing soon! \n\n\nMeanwhile, check out \nHDCloud\n troubleshooting docs.", 
            "title": "Troubleshooting AWS"
        }, 
        {
            "location": "/trouble-aws/index.html#troubleshooting-cloudbreak-on-aws", 
            "text": "Coming soon!   Meanwhile, check out  HDCloud  troubleshooting docs.", 
            "title": "Troubleshooting Cloudbreak on AWS"
        }, 
        {
            "location": "/trouble-azure/index.html", 
            "text": "Troubleshooting Cloudbreak on Azure\n\n\nCredential Creation Errors\n\n\nRole already exists\n\n\nExample error message: \nRole already exists in Azure with the name: CloudbreakCustom50\n\n\nSymptom\n: You specified that you want to create a new role for Cloudbreak credential, but an existing role with the same name already exists in Azure. \n\n\nSolution\n: You should either rename the role during credential creation or select the \nReuse existing custom role\n option. \n\n\nRole does not exist\n\n\nExample error message: \nRole does not exist in Azure with the name: CloudbreakCustom60\n\n\nSymptom\n: You specified that you want to reuse an existing role for your Cloudbreak credential, but that particular role does not exist in Azure.\n\n\nSolution\n: You should either rename the new role during the credential creation to match the existing role's name or select the \nLet Cloudbreak create a custom role\n option. \n\n\nRole does not have enough privileges\n\n\nExample error message: \nCloudbreakCustom 50 role does not have enough privileges to be used by Cloudbreak!\n\n\nPlease contact the documentaion for more information!\n\n\nSymptom\n: You specified that you want to reuse an  existing role for your Cloudbreak credential, but that particular role does not have the necessary privileges for Cloudbreak cluster management.\n\n\nSolution\n: You should either select an existing role with enough privileges or select the \nLet Cloudbreak create a custom role\n option.\n\n\nThe necessary action set for Cloudbreak to be able to manage the clusters includes:\n        \n\"Microsoft.Compute/*\",\n        \"Microsoft.Network/*\",\n        \"Microsoft.Storage/*\",\n        \"Microsoft.Resources/*\"\n\n\nCloud not validate publickey certificate\n\n\nExample error message: \nCould not validate publickey certificate [certificate: 'fdfdsf'], detailed message: Corrupt or unknown public key file format\n\n\nSymptom\n: The syntax of your SSH public key is incorrect.\n\n\nSolution\n: You must correct the syntax of your SSH key. For information about the correct syntax, refer to \nthis\n page.", 
            "title": "Troubleshooting Azure"
        }, 
        {
            "location": "/trouble-azure/index.html#troubleshooting-cloudbreak-on-azure", 
            "text": "", 
            "title": "Troubleshooting Cloudbreak on Azure"
        }, 
        {
            "location": "/trouble-azure/index.html#credential-creation-errors", 
            "text": "", 
            "title": "Credential Creation Errors"
        }, 
        {
            "location": "/trouble-azure/index.html#role-already-exists", 
            "text": "Example error message:  Role already exists in Azure with the name: CloudbreakCustom50  Symptom : You specified that you want to create a new role for Cloudbreak credential, but an existing role with the same name already exists in Azure.   Solution : You should either rename the role during credential creation or select the  Reuse existing custom role  option.", 
            "title": "Role already exists"
        }, 
        {
            "location": "/trouble-azure/index.html#role-does-not-exist", 
            "text": "Example error message:  Role does not exist in Azure with the name: CloudbreakCustom60  Symptom : You specified that you want to reuse an existing role for your Cloudbreak credential, but that particular role does not exist in Azure.  Solution : You should either rename the new role during the credential creation to match the existing role's name or select the  Let Cloudbreak create a custom role  option.", 
            "title": "Role does not exist"
        }, 
        {
            "location": "/trouble-azure/index.html#role-does-not-have-enough-privileges", 
            "text": "Example error message:  CloudbreakCustom 50 role does not have enough privileges to be used by Cloudbreak!  Please contact the documentaion for more information!  Symptom : You specified that you want to reuse an  existing role for your Cloudbreak credential, but that particular role does not have the necessary privileges for Cloudbreak cluster management.  Solution : You should either select an existing role with enough privileges or select the  Let Cloudbreak create a custom role  option.  The necessary action set for Cloudbreak to be able to manage the clusters includes:\n         \"Microsoft.Compute/*\",\n        \"Microsoft.Network/*\",\n        \"Microsoft.Storage/*\",\n        \"Microsoft.Resources/*\"", 
            "title": "Role does not have enough privileges"
        }, 
        {
            "location": "/trouble-azure/index.html#cloud-not-validate-publickey-certificate", 
            "text": "Example error message:  Could not validate publickey certificate [certificate: 'fdfdsf'], detailed message: Corrupt or unknown public key file format  Symptom : The syntax of your SSH public key is incorrect.  Solution : You must correct the syntax of your SSH key. For information about the correct syntax, refer to  this  page.", 
            "title": "Cloud not validate publickey certificate"
        }, 
        {
            "location": "/trouble-gcp/index.html", 
            "text": "Troubleshooting Cloudbreak on GCP\n\n\nComing soon!", 
            "title": "Troubleshooting GCP"
        }, 
        {
            "location": "/trouble-gcp/index.html#troubleshooting-cloudbreak-on-gcp", 
            "text": "Coming soon!", 
            "title": "Troubleshooting Cloudbreak on GCP"
        }, 
        {
            "location": "/trouble-os/index.html", 
            "text": "Troubleshooting Cloudbreak on OpenStack", 
            "title": "Troubleshooting OpenStack"
        }, 
        {
            "location": "/trouble-os/index.html#troubleshooting-cloudbreak-on-openstack", 
            "text": "", 
            "title": "Troubleshooting Cloudbreak on OpenStack"
        }, 
        {
            "location": "/faq/index.html", 
            "text": "FAQs\n\n\nHow to Generate SSH Key Pair\n\n\nAll the instances created by Cloudbreak are configured to allow key-based SSH, so you'll need to provide an SSH public key that can be used later to SSH onto the instances in the clusters you'll create with Cloudbreak. You can use one of your existing keys or you can generate a new one.\n\n\nTo generate a new SSH key pair, execute:\n\n\nssh-keygen -t rsa -b 4096 -C \"your_email@example.com\"\n# Creates a new ssh key, using the provided email as a label\n# Generating public/private rsa key pair.\n# Enter file in which to save the key (/Users/you/.ssh/id_rsa): [Press enter]\n\n\n\nYou'll be asked to enter a passphrase, but you can leave it empty:\n\n\n# Enter passphrase (empty for no passphrase): [Type a passphrase]\n# Enter same passphrase again: [Type passphrase again]\n\n\n\nAfter you enter (or not) a passphrase, the key pair is generated. The output should look similar to:\n\n\n# Your identification has been saved in /Users/you/.ssh/id_rsa.\n# Your public key has been saved in /Users/you/.ssh/id_rsa.pub.\n# The key fingerprint is:\n# 01:0f:f4:3b:ca:85:sd:17:sd:7d:sd:68:9d:sd:a2:sd your_email@example.com\n\n\n\nLater you'll need to pass the content of the \n.pub\n file to Cloudbreak and use the private key file to SSH to the instances. \n\n\nHow to Recover Public SSH Key\n\n\nThe \n-y\n option of \nssh-keygen\n outputs the public key. For example:\n\n\nssh-keygen -y -f ~/.ssh/id_rsa > ~/.ssh/id_rsa.pub\n\n\n\nHow to SSH to the Hosts\n\n\nTo connect to a running VM through SSH, you need to know its public IP address and have your private key available. \n\n\nThe private key that you must use to access the VM is the counterpart of the public key that you specified when creating a Cloudbreak credential.\n\n\nYou can find the IP addresses of all the running VMs in the Cloudbreak UI, on the cluster details page. Only key-based authentication is supported. \n\n\nCloudbreak creates a cloudbreak user which can be used to ssh into the box. This user has passwordless sudo rights.\n\n\nFor example:\n\n\nssh -i ~/.ssh/your-private-key.pem cloudbreak@\npublic-ip\n\n\n\n\n\nHow to Check Cloudbreak Version\n\n\nTo check Cloudbreak version, navigate to the Cloudbreak home directory and execute the \ncbd doctor\n command.\n\n\nHow to Check Available Environment Variables\n\n\nTo see all available environment variables with their default values, use:\n\n\ncbd env show\n\n\n\n\nHow to Access Cloudbreak Logs\n\n\nRefer to \nTroubleshooting\n.\n\n\nHow to Debug in Cloudbreak Shell\n\n\nTo get more detailed command prompt output, set the DEBUG environment variable to non-zero:\n\n\nDEBUG=1 cbd \nsome_command\n\n\n\n\n\nHow to Configure and Test Proxy Settings\n\n\nFor cbd\n\n\nTo configure proxy settings for Cloudbreak Deployer, add the following configs to your Profile:\n\n\nexport http_proxy=\nhttp://YOUR_PROXY_ADDRESS:YOUR_PROXY_PORT/\n\nexport https_proxy=\nhttp(s)://YOUR_PROXY_ADDRESS:YOUR_PROXY_PORT/\n\nexport CB_HTTP_PROXY=\nhttp://YOUR_PROXY_ADDRESS:YOUR_PROXY_PORT/\n\nexport CB_HTTPS_PROXY=\nhttp(s)://YOUR_PROXY_ADDRESS:YOUR_PROXY_PORT/\n\nexport CB_JAVA_OPTS=\n-Dhttp.proxyHost=YOUR_PROXY_ADDRESS -Dhttp.proxyPort=YOUR_PROXY_PORT -Dhttps.proxyHost=YOUR_PROXY_ADDRESS -Dhttps.proxyPort=YOUR_PROXY_PORT -Dhttp.nonProxyHosts=172.17.0.1|*.service.consul|*.node.dc1.consul\n\n\n\n\n\nFor Docker\n\n\nTo download newer Docker images from the official repository, you need to configure proxy settings for the Docker service. You can do this by configuring the 'HTTP_PROXY' variable in your environment. Next, restart the docker service. For more information, refer to \nDocker documentation\n.\n\n\nFor Provisioned Clusters\n\n\nFor a cluster to be provisioned to a (virtual) network that is behind a proxy, the yum on the provisioned machines needs to be configured to use that proxy. This is important because the Ambari install needs access to public repositories. You can configure yum proxy settings by using the recipe functionality of Cloudbreak. Use the following bash script to create a 'pre' recipe that will run on all of the nodes before the Ambari install:\n\n\n#!/bin/bash\ncat \n /etc/yum.conf \nENDOF\n\nproxy=http://YOUR_PROXY_ADDRESS:YOUR_PROXY_PORT\n\nENDOF\n\n\n\n\nTest Your Proxy Settings\n\n\nYou can use the following CURL command to test your proxy settings:\n\n\nhttps_proxy=\nYOUR_PROXY_ADDRESS:YOUR_PROXY_PORT\n curl -X GET -I --insecure https://cloudbreak-api.sequenceiq.com/info\n\n\n\n\nIts output should start with:\n\n\nHTTP/1.1 200 OK\n\n\n\n\nWhere are Data Volumes Mounted\n\n\nThe disks that are attached to the instances serving as cluster nodes are automatically mounted to \n/hadoopfs/fs1\n, \n/hadoopfs/fs2\n, ... , \n/hadoopfs/fsN\n respectively.", 
            "title": "FAQs"
        }, 
        {
            "location": "/faq/index.html#faqs", 
            "text": "", 
            "title": "FAQs"
        }, 
        {
            "location": "/faq/index.html#how-to-generate-ssh-key-pair", 
            "text": "All the instances created by Cloudbreak are configured to allow key-based SSH, so you'll need to provide an SSH public key that can be used later to SSH onto the instances in the clusters you'll create with Cloudbreak. You can use one of your existing keys or you can generate a new one.  To generate a new SSH key pair, execute:  ssh-keygen -t rsa -b 4096 -C \"your_email@example.com\"\n# Creates a new ssh key, using the provided email as a label\n# Generating public/private rsa key pair.\n# Enter file in which to save the key (/Users/you/.ssh/id_rsa): [Press enter]  You'll be asked to enter a passphrase, but you can leave it empty:  # Enter passphrase (empty for no passphrase): [Type a passphrase]\n# Enter same passphrase again: [Type passphrase again]  After you enter (or not) a passphrase, the key pair is generated. The output should look similar to:  # Your identification has been saved in /Users/you/.ssh/id_rsa.\n# Your public key has been saved in /Users/you/.ssh/id_rsa.pub.\n# The key fingerprint is:\n# 01:0f:f4:3b:ca:85:sd:17:sd:7d:sd:68:9d:sd:a2:sd your_email@example.com  Later you'll need to pass the content of the  .pub  file to Cloudbreak and use the private key file to SSH to the instances.", 
            "title": "How to Generate SSH Key Pair"
        }, 
        {
            "location": "/faq/index.html#how-to-recover-public-ssh-key", 
            "text": "The  -y  option of  ssh-keygen  outputs the public key. For example:  ssh-keygen -y -f ~/.ssh/id_rsa > ~/.ssh/id_rsa.pub", 
            "title": "How to Recover Public SSH Key"
        }, 
        {
            "location": "/faq/index.html#how-to-ssh-to-the-hosts", 
            "text": "To connect to a running VM through SSH, you need to know its public IP address and have your private key available.   The private key that you must use to access the VM is the counterpart of the public key that you specified when creating a Cloudbreak credential.  You can find the IP addresses of all the running VMs in the Cloudbreak UI, on the cluster details page. Only key-based authentication is supported.   Cloudbreak creates a cloudbreak user which can be used to ssh into the box. This user has passwordless sudo rights.  For example:  ssh -i ~/.ssh/your-private-key.pem cloudbreak@ public-ip", 
            "title": "How to SSH to the Hosts"
        }, 
        {
            "location": "/faq/index.html#how-to-check-cloudbreak-version", 
            "text": "To check Cloudbreak version, navigate to the Cloudbreak home directory and execute the  cbd doctor  command.", 
            "title": "How to Check Cloudbreak Version"
        }, 
        {
            "location": "/faq/index.html#how-to-check-available-environment-variables", 
            "text": "To see all available environment variables with their default values, use:  cbd env show", 
            "title": "How to Check Available Environment Variables"
        }, 
        {
            "location": "/faq/index.html#how-to-access-cloudbreak-logs", 
            "text": "Refer to  Troubleshooting .", 
            "title": "How to Access Cloudbreak Logs"
        }, 
        {
            "location": "/faq/index.html#how-to-debug-in-cloudbreak-shell", 
            "text": "To get more detailed command prompt output, set the DEBUG environment variable to non-zero:  DEBUG=1 cbd  some_command", 
            "title": "How to Debug in Cloudbreak Shell"
        }, 
        {
            "location": "/faq/index.html#how-to-configure-and-test-proxy-settings", 
            "text": "For cbd  To configure proxy settings for Cloudbreak Deployer, add the following configs to your Profile:  export http_proxy= http://YOUR_PROXY_ADDRESS:YOUR_PROXY_PORT/ \nexport https_proxy= http(s)://YOUR_PROXY_ADDRESS:YOUR_PROXY_PORT/ \nexport CB_HTTP_PROXY= http://YOUR_PROXY_ADDRESS:YOUR_PROXY_PORT/ \nexport CB_HTTPS_PROXY= http(s)://YOUR_PROXY_ADDRESS:YOUR_PROXY_PORT/ \nexport CB_JAVA_OPTS= -Dhttp.proxyHost=YOUR_PROXY_ADDRESS -Dhttp.proxyPort=YOUR_PROXY_PORT -Dhttps.proxyHost=YOUR_PROXY_ADDRESS -Dhttps.proxyPort=YOUR_PROXY_PORT -Dhttp.nonProxyHosts=172.17.0.1|*.service.consul|*.node.dc1.consul   For Docker  To download newer Docker images from the official repository, you need to configure proxy settings for the Docker service. You can do this by configuring the 'HTTP_PROXY' variable in your environment. Next, restart the docker service. For more information, refer to  Docker documentation .  For Provisioned Clusters  For a cluster to be provisioned to a (virtual) network that is behind a proxy, the yum on the provisioned machines needs to be configured to use that proxy. This is important because the Ambari install needs access to public repositories. You can configure yum proxy settings by using the recipe functionality of Cloudbreak. Use the following bash script to create a 'pre' recipe that will run on all of the nodes before the Ambari install:  #!/bin/bash\ncat   /etc/yum.conf  ENDOF\n\nproxy=http://YOUR_PROXY_ADDRESS:YOUR_PROXY_PORT\n\nENDOF  Test Your Proxy Settings  You can use the following CURL command to test your proxy settings:  https_proxy= YOUR_PROXY_ADDRESS:YOUR_PROXY_PORT  curl -X GET -I --insecure https://cloudbreak-api.sequenceiq.com/info  Its output should start with:  HTTP/1.1 200 OK", 
            "title": "How to Configure and Test Proxy Settings"
        }, 
        {
            "location": "/faq/index.html#where-are-data-volumes-mounted", 
            "text": "The disks that are attached to the instances serving as cluster nodes are automatically mounted to  /hadoopfs/fs1 ,  /hadoopfs/fs2 , ... ,  /hadoopfs/fsN  respectively.", 
            "title": "Where are Data Volumes Mounted"
        }, 
        {
            "location": "/get-help/index.html", 
            "text": "Get Help\n\n\nIf you need help with Cloudbreak, you have two options:\n\n\n\n\n\n\n\n\nOption\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nHortonworks Community Connection\n\n\nThis is free optional support via Hortonworks Community Connection (HCC).\n\n\n\n\n\n\nHortonworks Flex Support Subscription\n\n\nThis is paid Hortonworks enterprise support.\n\n\n\n\n\n\n\n\nHCC\n\n\nYou can optionally register for optional free community support at \nHortonworks Community Connection\n where you can browse articles and previously answered questions, and ask questions of your own. When posting questions related to Cloudbreak, make sure to use the \"Cloudbreak\" tag.\n\n\nFlex Subscription\n\n\nYou can optionally use your existing Hortonworks \nFlex subscription(s)\n to cover the Cloudbreak node and all clusters created. \n\n\nPrerequisites\n: You must have an existing SmartSense ID and a Flex subscription. For general information about the Hortonworks Flex Support Subscription, visit the Hortonworks Support page at \nhttps://hortonworks.com/services/support/enterprise/\n.\n\n\nThe general steps are:\n\n\n\n\nConfigure Smart Sense in your \nProfile\n file.   \n\n\nRegister your Flex subscription in the Cloudbreak web UI in the the \nmanage flex subscriptions\n pane. You can register and manage multiple Flex subscriptions.   \n\n\n\n\n\n\nAlternatively, you can perform these steps using the Cloudbreak Shell. \n\n\n\n\nConfiguring SmartSense\n\n\nTo configure SmartSense in Cloudbreak, enable SmartSense and add your SmartSense ID to the \nProfile\n by adding the following variables:\n\n\nexport CB_SMARTSENSE_CONFIGURE=true\nexport CB_SMARTSENSE_ID=YOUR-SMARTSENSE-ID\n\n\n\nFor example:\n\n\nexport CB_SMARTSENSE_CONFIGURE=true\nexport CB_SMARTSENSE_ID=A-00000000-C-00000000\n\n\n\nYou can do this in one of the two ways:\n\n\n\n\nWhen initiating Cloudbreak Deployer  \n\n\nAfter you've already initiated Cloudbreak Deployer. If you choose this option, you must restart Cloudbreak using \ncbd restart\n.\n\n\n\n\n\n\nSmartSense ID defined in the \nProfile\n file always overrides the ID registered via Cloudbreak Shell.\n\n\n\n\nManaging Flex Subscriptions\n\n\nOnce you log in to the Cloudbreak web UI, you can manage your Flex subscriptions from the \nmanage flex subscriptions\n pane. You can:\n\n\n\n\nRegister a new Flex subscription.  \n\n\nSet a default Flex subscription.  \n\n\nSelect a Flex subscription to be used for cloud controller.  \n\n\nDelete a Flex subscription.  \n\n\nCheck which clusters are connected to a specific subscription.  \n\n\n\n\nWhen creating a cluster using the advanced options, in the \nCONFIGURE CLUSTER\n \n \nFlex Subscriptions\n, you can select the Flex subscription that you want to use.\n\n\nMore Resources\n\n\nCheck out the following documentation to learn more:\n\n\n\n\n Resource \nDescription\n\n\nHortonworks documentation \n\n\nDuring cluster create process, Hortonworks Data Cloud automatically installs Ambari and sets up a cluster for you. After this deployment is complete, refer to the \nAmbari documentation\n and \nHDP documentation\n for help.\n\n\n\n\n\n\nHortonworks tutorials\n\n\n\n\nUse Hortonworks tutorials to get started with Apache Spark, Apache Hive, Apache Zeppelin, and more.\n\n\nApache documentation\n\n\n\n\n In addition to Hortonworks documentation, refer to the Apache Software Foundation documentation to get information on specific Hadoop services. \n\n\n\n\n\nAmbari Blueprints\nLearn about Ambari Bleuprints. Ambari Blueprints are a declarative definition of a Hadoop cluster that Ambari can use to create Hadoop clusters.\n\n\nCloudbreak Project\nVisit the Hortonworks website to see Cloudbreak-related news and updates.\n\n\nApache Ambari Project\nLearn about the Apache Ambari Project. Apache Ambari is an operational platform for provisioning, managing, and monitoring Apache Hadoop clusters. Ambari exposes a robust set of REST APIs and a rich web interface for cluster management.", 
            "title": "Getting Help"
        }, 
        {
            "location": "/get-help/index.html#get-help", 
            "text": "If you need help with Cloudbreak, you have two options:     Option  Description      Hortonworks Community Connection  This is free optional support via Hortonworks Community Connection (HCC).    Hortonworks Flex Support Subscription  This is paid Hortonworks enterprise support.", 
            "title": "Get Help"
        }, 
        {
            "location": "/get-help/index.html#hcc", 
            "text": "You can optionally register for optional free community support at  Hortonworks Community Connection  where you can browse articles and previously answered questions, and ask questions of your own. When posting questions related to Cloudbreak, make sure to use the \"Cloudbreak\" tag.", 
            "title": "HCC"
        }, 
        {
            "location": "/get-help/index.html#flex-subscription", 
            "text": "You can optionally use your existing Hortonworks  Flex subscription(s)  to cover the Cloudbreak node and all clusters created.   Prerequisites : You must have an existing SmartSense ID and a Flex subscription. For general information about the Hortonworks Flex Support Subscription, visit the Hortonworks Support page at  https://hortonworks.com/services/support/enterprise/ .  The general steps are:   Configure Smart Sense in your  Profile  file.     Register your Flex subscription in the Cloudbreak web UI in the the  manage flex subscriptions  pane. You can register and manage multiple Flex subscriptions.       Alternatively, you can perform these steps using the Cloudbreak Shell.", 
            "title": "Flex Subscription"
        }, 
        {
            "location": "/get-help/index.html#configuring-smartsense", 
            "text": "To configure SmartSense in Cloudbreak, enable SmartSense and add your SmartSense ID to the  Profile  by adding the following variables:  export CB_SMARTSENSE_CONFIGURE=true\nexport CB_SMARTSENSE_ID=YOUR-SMARTSENSE-ID  For example:  export CB_SMARTSENSE_CONFIGURE=true\nexport CB_SMARTSENSE_ID=A-00000000-C-00000000  You can do this in one of the two ways:   When initiating Cloudbreak Deployer    After you've already initiated Cloudbreak Deployer. If you choose this option, you must restart Cloudbreak using  cbd restart .    SmartSense ID defined in the  Profile  file always overrides the ID registered via Cloudbreak Shell.", 
            "title": "Configuring SmartSense"
        }, 
        {
            "location": "/get-help/index.html#managing-flex-subscriptions", 
            "text": "Once you log in to the Cloudbreak web UI, you can manage your Flex subscriptions from the  manage flex subscriptions  pane. You can:   Register a new Flex subscription.    Set a default Flex subscription.    Select a Flex subscription to be used for cloud controller.    Delete a Flex subscription.    Check which clusters are connected to a specific subscription.     When creating a cluster using the advanced options, in the  CONFIGURE CLUSTER     Flex Subscriptions , you can select the Flex subscription that you want to use.", 
            "title": "Managing Flex Subscriptions"
        }, 
        {
            "location": "/get-help/index.html#more-resources", 
            "text": "Check out the following documentation to learn more:    Resource  Description  Hortonworks documentation   During cluster create process, Hortonworks Data Cloud automatically installs Ambari and sets up a cluster for you. After this deployment is complete, refer to the  Ambari documentation  and  HDP documentation  for help.    Hortonworks tutorials   Use Hortonworks tutorials to get started with Apache Spark, Apache Hive, Apache Zeppelin, and more.  Apache documentation    In addition to Hortonworks documentation, refer to the Apache Software Foundation documentation to get information on specific Hadoop services.    Ambari Blueprints Learn about Ambari Bleuprints. Ambari Blueprints are a declarative definition of a Hadoop cluster that Ambari can use to create Hadoop clusters.  Cloudbreak Project Visit the Hortonworks website to see Cloudbreak-related news and updates.  Apache Ambari Project Learn about the Apache Ambari Project. Apache Ambari is an operational platform for provisioning, managing, and monitoring Apache Hadoop clusters. Ambari exposes a robust set of REST APIs and a rich web interface for cluster management.", 
            "title": "More Resources"
        }, 
        {
            "location": "/smartsense/index.html", 
            "text": "SmartSense Telemetry\n\n\nHelp us make a better product by opt'ing in to automatically send information to Hortonworks. This includes enabling\nHortonworks SmartSense and sending performance and usage info. As you use the product,\nSmartSense measures and collects information and then sends these information bundles to Hortonworks.\n\n\nHow to Disable\n\n\nDisable Bundle Upload for the Cloud Controller and New Clusters\n\n\n\n    \nImportant\n\n    \n\n    Do not perform these steps when you have clusters currently in the process of being deployed.\n    Wait for all clusters to be deployed.\n\n\n\n\n\n\n\n\n\nSSH into the cloud controller host.\n\n\n\n\n\n\nEdit \n/var/lib/cloudbreak-deployment/Profile\n.\n\n\n\n\n\n\nChange \nCB_SMARTSENSE_CONFIGURE\n to \nfalse\n:\n\n    \nexport CB_SMARTSENSE_CONFIGURE=false\n\n\n\n\n\n\nRestart the cloud controller:\n\n    \ncd /var/lib/cloudbreak-deployment\ncbd restart\n\n\n\n\n\n\nDisable Bundle Upload for an Existing Cluster\n\n\n\n\n\n\nSSH into the Master node for the cluster.\n\n\n\n\n\n\nEdit \n/etc/hst/conf/hst-server.ini\n.\n\n\n\n\n\n\nChange \n[gateway]\n configuration to \nfalse\n:\n\n    \n[gateway]\nenabled=false\n\n\n\n\n\n\nRestart the SmartSense Server:\n    \nhst restart\n\n\n\n\n\n\n(Optional) Disable SmartSense daily bundle capture:\n\n\n\n\nSmartSense is scheduled to capture a telemetry bundle daily. With the bundle upload disabled, the bundle will still\nbe captured but just saved locally (i.e. not uploaded).\n\n\nTo disable the bundle capture, execute the following:\n\nhst capture-schedule -a pause\n\n\n\n\n\n\n\n\nRepeat on all existing clusters.", 
            "title": "SmartSense"
        }, 
        {
            "location": "/smartsense/index.html#smartsense-telemetry", 
            "text": "Help us make a better product by opt'ing in to automatically send information to Hortonworks. This includes enabling\nHortonworks SmartSense and sending performance and usage info. As you use the product,\nSmartSense measures and collects information and then sends these information bundles to Hortonworks.", 
            "title": "SmartSense Telemetry"
        }, 
        {
            "location": "/smartsense/index.html#how-to-disable", 
            "text": "", 
            "title": "How to Disable"
        }, 
        {
            "location": "/smartsense/index.html#disable-bundle-upload-for-the-cloud-controller-and-new-clusters", 
            "text": "Important \n     \n    Do not perform these steps when you have clusters currently in the process of being deployed.\n    Wait for all clusters to be deployed.     SSH into the cloud controller host.    Edit  /var/lib/cloudbreak-deployment/Profile .    Change  CB_SMARTSENSE_CONFIGURE  to  false : \n     export CB_SMARTSENSE_CONFIGURE=false    Restart the cloud controller: \n     cd /var/lib/cloudbreak-deployment\ncbd restart", 
            "title": "Disable Bundle Upload for the Cloud Controller and New Clusters"
        }, 
        {
            "location": "/smartsense/index.html#disable-bundle-upload-for-an-existing-cluster", 
            "text": "SSH into the Master node for the cluster.    Edit  /etc/hst/conf/hst-server.ini .    Change  [gateway]  configuration to  false : \n     [gateway]\nenabled=false    Restart the SmartSense Server:\n     hst restart    (Optional) Disable SmartSense daily bundle capture:   SmartSense is scheduled to capture a telemetry bundle daily. With the bundle upload disabled, the bundle will still\nbe captured but just saved locally (i.e. not uploaded).  To disable the bundle capture, execute the following: hst capture-schedule -a pause     Repeat on all existing clusters.", 
            "title": "Disable Bundle Upload for an Existing Cluster"
        }
    ]
}