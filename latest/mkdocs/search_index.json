{
    "docs": [
        {
            "location": "/index.html", 
            "text": "Introduction\n\n\nWelcome to the \nCloudbreak 2.4\n documentation!\n\n\nCloudbreak simplifies the provisioning, management, and monitoring of on-demand HDP clusters in virtual and cloud environments. It leverages cloud infrastructure to create host instances, and uses Apache Ambari via Ambari blueprints to provision and manage HDP clusters. \n\n\nCloudbreak allows you to create clusters using the Cloudbreak web UI, Cloudbreak CLI, and Cloudbreak REST API. Clusters can be launched on public cloud infrastructure platforms \nMicrosoft Azure\n, \nAmazon Web Services (AWS)\n, and \nGoogle Cloud Platform (GCP)\n, and on the private cloud infrastructure platform \nOpenStack\n.\n\n\n   \n\n\nUse Cases\n\n\nCloudbreak allows you to create, manage, and monitor your clusters on your chosen cloud platform:\n\n\n\n\nQuickly create a cluster using one of the default cluster blueprints and infrastructure settings.  \n\n\nCreate a cluster based on the requirements of your workloads and provision infrastructure based on your IT requirements.\n\n\nSecure your cluster by enabling Kerberos.\n\n\nAutomate cluster creation using the Cloudbreak CLI. \n\n\nDevelop your application using Cloudbreak API.\n\n\n\n\nArchitecture\n\n\nRefer to \nArchitecture\n.\n\n\nGet Started\n\n\nTo get started with Cloudbreak:\n\n\n\n\nSelect the \ncloud platform\n on which you would like to launch Cloudbreak.   \n\n\nSelect the \ndeployment option\n that you would like to use. \n\n\nLaunch Cloudbreak\n. \n\n\n\n\nSelect Cloud Platform\n\n\nYou can deploy and use Cloudbreak on the following cloud platforms:\n\n\n\n\nAmazon Web Services (AWS)\n\n\nMicrosoft Azure\n\n\nGoogle Cloud Platform (GCP)\n\n\nOpenStack\n\n\n\n\nSelect Deployment Option\n\n\nThere are two basic deployment options:\n\n\n\n\n\n\n\n\nDeployment option\n\n\nWhen to use\n\n\n\n\n\n\n\n\n\n\nInstantiate one of the pre-built cloud images\n\n\nThis is the recommended basic deployment option.\n The cloud images include Cloudbreak deployer pre-installed on a CentOS VM.\n\n\n\n\n\n\nInstall the Cloudbreak deployer on your own VM\n\n\nThis is an advanced deployment option.\n \nSelect this option if you have custom VM requirements. The supported operating systems are RHEL, CentOS, and Oracle Linux 7 (64-bit).\n\n\n\n\n\n\n\n\nLaunch Cloudbreak\n\n\n(Option 1) You can launch Cloudbreak from one of the pre-built images:  \n\n\n\n\nLaunch on AWS\n  \n\n\nLaunch on Azure\n  \n\n\nLaunch on GCP\n   \n\n\nLaunch on OpenStack\n    \n\n\n\n\n(Option 2) Or you can launch Cloudbreak \non your own VM\n on one of these cloud platforms. This is an advanced deployment option that you should only use if you have custom VM requirements. \n\n\nIn general, the steps include meeting the prerequisites, launching Cloudbreak on a VM, and creating the Cloudbreak credential. After performing these steps, you can create a cluster based on one of the default blueprints or upload your own blueprint and then create a cluster. \n\n\n\n    \nNote\n\n    \nThe Cloudbreak software runs in your cloud environment. You are responsible for cloud infrastructure related charges while running Cloudbreak and the clusters being managed by Cloudbreak.", 
            "title": "Introduction"
        }, 
        {
            "location": "/index.html#introduction", 
            "text": "Welcome to the  Cloudbreak 2.4  documentation!  Cloudbreak simplifies the provisioning, management, and monitoring of on-demand HDP clusters in virtual and cloud environments. It leverages cloud infrastructure to create host instances, and uses Apache Ambari via Ambari blueprints to provision and manage HDP clusters.   Cloudbreak allows you to create clusters using the Cloudbreak web UI, Cloudbreak CLI, and Cloudbreak REST API. Clusters can be launched on public cloud infrastructure platforms  Microsoft Azure ,  Amazon Web Services (AWS) , and  Google Cloud Platform (GCP) , and on the private cloud infrastructure platform  OpenStack .", 
            "title": "Introduction"
        }, 
        {
            "location": "/index.html#use-cases", 
            "text": "Cloudbreak allows you to create, manage, and monitor your clusters on your chosen cloud platform:   Quickly create a cluster using one of the default cluster blueprints and infrastructure settings.    Create a cluster based on the requirements of your workloads and provision infrastructure based on your IT requirements.  Secure your cluster by enabling Kerberos.  Automate cluster creation using the Cloudbreak CLI.   Develop your application using Cloudbreak API.", 
            "title": "Use Cases"
        }, 
        {
            "location": "/index.html#architecture", 
            "text": "Refer to  Architecture .", 
            "title": "Architecture"
        }, 
        {
            "location": "/index.html#get-started", 
            "text": "To get started with Cloudbreak:   Select the  cloud platform  on which you would like to launch Cloudbreak.     Select the  deployment option  that you would like to use.   Launch Cloudbreak .", 
            "title": "Get Started"
        }, 
        {
            "location": "/index.html#select-cloud-platform", 
            "text": "You can deploy and use Cloudbreak on the following cloud platforms:   Amazon Web Services (AWS)  Microsoft Azure  Google Cloud Platform (GCP)  OpenStack", 
            "title": "Select Cloud Platform"
        }, 
        {
            "location": "/index.html#select-deployment-option", 
            "text": "There are two basic deployment options:     Deployment option  When to use      Instantiate one of the pre-built cloud images  This is the recommended basic deployment option.  The cloud images include Cloudbreak deployer pre-installed on a CentOS VM.    Install the Cloudbreak deployer on your own VM  This is an advanced deployment option.   Select this option if you have custom VM requirements. The supported operating systems are RHEL, CentOS, and Oracle Linux 7 (64-bit).", 
            "title": "Select Deployment Option"
        }, 
        {
            "location": "/index.html#launch-cloudbreak", 
            "text": "(Option 1) You can launch Cloudbreak from one of the pre-built images:     Launch on AWS     Launch on Azure     Launch on GCP      Launch on OpenStack        (Option 2) Or you can launch Cloudbreak  on your own VM  on one of these cloud platforms. This is an advanced deployment option that you should only use if you have custom VM requirements.   In general, the steps include meeting the prerequisites, launching Cloudbreak on a VM, and creating the Cloudbreak credential. After performing these steps, you can create a cluster based on one of the default blueprints or upload your own blueprint and then create a cluster.   \n     Note \n     The Cloudbreak software runs in your cloud environment. You are responsible for cloud infrastructure related charges while running Cloudbreak and the clusters being managed by Cloudbreak.", 
            "title": "Launch Cloudbreak"
        }, 
        {
            "location": "/architecture/index.html", 
            "text": "Architecture\n\n\nCloudbreak deployer\n installs Cloudbreak components on a VM. Once these components are deployed, you can use \nCloudbreak application\n or Cloudbreak CLI to create, manage, and monitor clusters. \n\n\nCloudbreak Deployer Architecture\n\n\nCloudbreak deployer\n installs Cloudbreak components on a VM. It includes the following components:\n\n\n\n\n\n\n\n\nComponent\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nCloudbreak Application\n\n\nCloudbreak application is built on the foundation of cloud provider APIs and Apache Ambari.\n\n\n\n\n\n\nUluwatu\n\n\nThis is Cloudbreak web UI, which can be used to create, manage, and monitor clusters.\n\n\n\n\n\n\nCloudbreak CLI\n\n\nThis is Cloudbreak's command line tool, which can be used to create, manage, and monitor clusters.\n\n\n\n\n\n\nIdentity\n\n\nThis is Cloudbreak's OAuth identity server implementation, which utilizes UAA.\n\n\n\n\n\n\nSultans\n\n\nThis is Cloudbreak's user management system.\n\n\n\n\n\n\nPeriscope\n\n\nThis is Cloudbreak's autoscaling application, which is responsible for automatically increasing or decreasing the capacity of the cluster when your pre-defined conditions are met.\n\n\n\n\n\n\n\n\nCloudbreak Application Architecture\n\n\nThe Cloudbreak application is a web application which simplifies HDP cluster provisioning in the cloud. Based on your input, Cloudbreak provisions all required cloud infrastructure and then provisions an HDP cluster on your behalf within your cloud provider account.   \n\n\n \n\n\nCloudbreak application is built on the foundation of cloud provider APIs and Apache Ambari:\n\n\n\n\n\n\nCloudbreak uses \ncloud provider APIs\n to communicate with the cloud providers. \n\n\n\n\n\n\nCloudbreak uses the \nCloudbreak credential\n to authenticate with your cloud provider account and provision cloud resources required for the HDP clusters. \n\n\n\n\n\n\nCloudbreak uses Apache Ambari and \nAmbari blueprints\n to provision, manage, and monitor HDP clusters. Ambari blueprints are a declarative definition of a cluster. With a blueprint, you can specify stack, component layout, and configurations to materialize an HDP cluster instance via Ambari REST API, without having to use the Ambari cluster install wizard.     \n\n\n\n\n\n\nCloudbreak Credential\n\n\nAfter launching Cloudbreak, you must create a Cloudbreak credential for each cloud provider on which you would like to provision clusters. Only after you have completed that step you can start creating clusters. \n\n\nCloudbreak credential allows Cloudbreak to authenticate with the cloud provider and create resources on your behalf. The authentication process varies depending on the cloud provider, but is typically done via assigning a specific IAM role to Cloudbreak which allows Cloudbreak to perform certain actions within your cloud provider account. To learn more, refer to \nIdentity Management\n.  \n\n\n \n\n\nRelated Links\n\n\nIdentity Management\n  \n\n\nAmbari Blueprints\n\n\nAmbari blueprints are a declarative definition of a cluster. A blueprint allows you to specify stack, component layout, and configurations to materialize an HDP cluster instance via Ambari REST API, without having to use the Ambari cluster install wizard.  \n\n\nAmbari blueprints are specified in JSON format. After you provide the blueprint to Cloudbreak, the host groups in the JSON are mapped to a set of instances when starting the cluster, and the specified services and components are installed on the corresponding nodes.\n\n\nCloudbreak includes a few default blueprints and allows you to upload your own blueprints.\n\n\n \n\n\nRelated Links\n\n\nBlueprints\n\n\nApache documentation\n (External)", 
            "title": "Architecture"
        }, 
        {
            "location": "/architecture/index.html#architecture", 
            "text": "Cloudbreak deployer  installs Cloudbreak components on a VM. Once these components are deployed, you can use  Cloudbreak application  or Cloudbreak CLI to create, manage, and monitor clusters.", 
            "title": "Architecture"
        }, 
        {
            "location": "/architecture/index.html#cloudbreak-deployer-architecture", 
            "text": "Cloudbreak deployer  installs Cloudbreak components on a VM. It includes the following components:     Component  Description      Cloudbreak Application  Cloudbreak application is built on the foundation of cloud provider APIs and Apache Ambari.    Uluwatu  This is Cloudbreak web UI, which can be used to create, manage, and monitor clusters.    Cloudbreak CLI  This is Cloudbreak's command line tool, which can be used to create, manage, and monitor clusters.    Identity  This is Cloudbreak's OAuth identity server implementation, which utilizes UAA.    Sultans  This is Cloudbreak's user management system.    Periscope  This is Cloudbreak's autoscaling application, which is responsible for automatically increasing or decreasing the capacity of the cluster when your pre-defined conditions are met.", 
            "title": "Cloudbreak Deployer Architecture"
        }, 
        {
            "location": "/architecture/index.html#cloudbreak-application-architecture", 
            "text": "The Cloudbreak application is a web application which simplifies HDP cluster provisioning in the cloud. Based on your input, Cloudbreak provisions all required cloud infrastructure and then provisions an HDP cluster on your behalf within your cloud provider account.        Cloudbreak application is built on the foundation of cloud provider APIs and Apache Ambari:    Cloudbreak uses  cloud provider APIs  to communicate with the cloud providers.     Cloudbreak uses the  Cloudbreak credential  to authenticate with your cloud provider account and provision cloud resources required for the HDP clusters.     Cloudbreak uses Apache Ambari and  Ambari blueprints  to provision, manage, and monitor HDP clusters. Ambari blueprints are a declarative definition of a cluster. With a blueprint, you can specify stack, component layout, and configurations to materialize an HDP cluster instance via Ambari REST API, without having to use the Ambari cluster install wizard.", 
            "title": "Cloudbreak Application Architecture"
        }, 
        {
            "location": "/architecture/index.html#cloudbreak-credential", 
            "text": "After launching Cloudbreak, you must create a Cloudbreak credential for each cloud provider on which you would like to provision clusters. Only after you have completed that step you can start creating clusters.   Cloudbreak credential allows Cloudbreak to authenticate with the cloud provider and create resources on your behalf. The authentication process varies depending on the cloud provider, but is typically done via assigning a specific IAM role to Cloudbreak which allows Cloudbreak to perform certain actions within your cloud provider account. To learn more, refer to  Identity Management .       Related Links  Identity Management", 
            "title": "Cloudbreak Credential"
        }, 
        {
            "location": "/architecture/index.html#ambari-blueprints", 
            "text": "Ambari blueprints are a declarative definition of a cluster. A blueprint allows you to specify stack, component layout, and configurations to materialize an HDP cluster instance via Ambari REST API, without having to use the Ambari cluster install wizard.    Ambari blueprints are specified in JSON format. After you provide the blueprint to Cloudbreak, the host groups in the JSON are mapped to a set of instances when starting the cluster, and the specified services and components are installed on the corresponding nodes.  Cloudbreak includes a few default blueprints and allows you to upload your own blueprints.     Related Links  Blueprints  Apache documentation  (External)", 
            "title": "Ambari Blueprints"
        }, 
        {
            "location": "/aws-launch/index.html", 
            "text": "Launching Cloudbreak on AWS\n\n\nBefore launching Cloudbreak on AWS, review and meet the prerequisites. Next, launch a VM using a Cloudbreak Amazon Machine Image, access the VM, and then start Cloudbreak. Once Cloudbreak is running, log in to the Cloudbreak UI and create a Cloudbreak credential. \n\n\nMeet the Prerequisites\n\n\nBefore launching Cloudbreak on AWS, you must meet the following prerequisites.\n\n\nAWS Account\n\n\nIn order to launch Cloudbreak on AWS, you must log in to your AWS account. If you don't have an account, you can create one at \nhttps://aws.amazon.com/\n.\n\n\nAWS Region\n\n\nDecide in which AWS region you would like to launch Cloudbreak. The following AWS regions are supported: \n\n\n\n\n\n\n\n\nRegion Name\n\n\nRegion\n\n\n\n\n\n\n\n\n\n\nEU (Ireland)\n\n\neu-west-1\n\n\n\n\n\n\nEU (Frankfurt)\n\n\neu-central-1\n\n\n\n\n\n\nUS East (N. Virginia)\n\n\nus-east-1\n\n\n\n\n\n\nUS West (N. California)\n\n\nus-west-1\n\n\n\n\n\n\nUS West (Oregon)\n\n\nus-west-2\n\n\n\n\n\n\nSouth America (S\u00e3o Paulo)\n\n\nsa-east-1\n\n\n\n\n\n\nAsia Pacific (Tokyo)\n\n\nap-northeast-1\n\n\n\n\n\n\nAsia Pacific (Singapore)\n\n\nap-southeast-1\n\n\n\n\n\n\nAsia Pacific (Sydney)\n\n\nap-southeast-2\n\n\n\n\n\n\n\n\nClusters created via Cloudbreak can be in the same or different region as Cloudbreak; when you launch a cluster, you select the region in which to launch it.\n\n\nRelated Links\n\n\nAWS Regions and Endpoints\n (External)   \n\n\nSSH Key Pair\n\n\nImport an existing key pair or generate a new key pair in the AWS region which you are planning to use for launching Cloudbreak and clusters. You can do this using the following steps.\n\n\nSteps\n \n\n\n\n\nNavigate to the Amazon EC2 console at https://console.aws.amazon.com/ec2/.  \n\n\nCheck the region listed in the top right corner to make sure that you are in the correct region.  \n\n\nIn the left pane, find \nNETWORK AND SECURITY\n and click \nKey Pairs\n.   \n\n\nDo one of the following:\n\n\nClick \nCreate Key Pair\n to create a new key pair. Your private key file will be automatically downloaded onto your computer. Make sure to save it in a secure location. You will need it to SSH to the cluster nodes. You may want to change access settings for the file using \nchmod 400 my-key-pair.pem\n.  \n\n\nClick \nImport Key Pair\n to upload an existing public key and then select it and click \nImport\n. Make sure that you have access to its corresponding private key.    \n\n\n\n\n\n\n\n\nYou need this SSH key pair to SSH to the Cloudbreak instance and start Cloudbreak. \n\n\nRelated Links\n\n\nCreating a Key Pair Using Amazon EC2\n (External)  \n\n\nAuthentication\n\n\nBefore you can start using Cloudbreak for provisioning clusters, you must select a way for Cloudbreak to authenticate with your AWS account and create resources on your behalf. There are two ways to do this: \n\n\n\n\n\n\nKey-based\n: This is a simpler option which does not require additional configuration at this point. It requires that you provide your AWS access key and secret key pair in the Cloudbreak web UI later. All you need to do now is check your AWS account and ensure that you can access this key pair.\n\n\n\n\n\n\nRole-based\n: This requires that you or your AWS admin create an IAM role to allow Cloudbreak to assume AWS roles (the \"AssumeRole\" policy).\n\n\n\n\n\n\nOption 1: Key-based Authentication\n\n\nIf you are using key-based authentication for Cloudbreak on AWS, you must be able to provide your AWS access key and secret key pair. Cloudbreak will use these keys to launch the resources. You must provide the access and secret keys later in the Cloudbreak web UI later when creating a credential. \n\n\nIf you choose this option, all you need to do at this point is check your AWS account and make sure that you can access this key pair. You can generate new access and secret keys from the \nIAM Console\n \n \nUsers\n. Next, select a user and click on the \nSecurity credentials\n tab:\n\n\n \n\n\nIf you choose this option, you can proceed to \nLaunch the VM\n.\n\n\nOption 2: Role-based Authentication\n\n\nIf you are using role-based authentication for Cloudbreak on AWS, you must create two IAM roles: one to grant Cloudbreak access to allow Cloudbreak to assume AWS roles (using the \"AssumeRole\" policy) and the second one to provide Cloudbreak with the capabilities required for cluster creation (using the \"cb-policy\" policy).\n\n\nThe following table provides contextual information about the two roles required: \n\n\n\n\n\n\n\n\nRole\n\n\nPurpose\n\n\nOverview of Steps\n\n\nConfiguration\n\n\n\n\n\n\n\n\n\n\nCloudbreakRole\n\n\nAllows Cloudbreak to assume other IAM roles - specifically the CredentialRole.\n\n\nCreate a role called \"CloudbreakRole\" and attach the \"AssumeRole\" policy. The \"AssumeRole\" policy definition and steps for creating the CloudbreakRole are provided below.\n\n\nWhen launching your Cloudbreak VM, during \nStep 3: Configure Instance Details\n \n \nIAM\n, you will attach the \"CloudbreakRole\" IAM role to the VM.\n\n\n\n\n\n\nCredentialRole\n\n\nAllows Cloudbreak to create AWS resources required for clusters.\n\n\nCreate a new IAM role called \"CredentialRole\" and attach the \"cb-policy\" policy to it. The \"cb-policy\" policy definition and steps for creating the CredentialRole are provided below.\n When creating this role using the AWS Console, make sure that that it is a role for cross-account access and that the trust-relation is set up as follows: 'Account ID' is your own 12-digit AWS account ID and 'External ID' is \u201cprovision-ambari\u201d. See steps below.\n\n\nOnce you log in to the Cloudbreak UI and are ready to create clusters, you will use this role to create the Cloudbreak credential.\n\n\n\n\n\n\n\n\n\n\nAlternatively, instead of attaching the \"CloudbreakRole\" role during the VM launch, you can assign the \"CloudbreakRole\" to an IAM user and then add the access and secret key of that user to your 'Profile'.\n\n\nAlternatively you can generate the \"CredentialRole\" role later once your Cloudbreak VM is running by SSHing to the Cloudbreak VM and running the \ncbd aws generate-role\n command. This command creates a role with the name \"cbreak-deployer\" (equivalent to the \"CredentialRole\"). To customize the name of the role, add \nexport AWS_ROLE_NAME=my-cloudbreak-role-name\n (where \"my-cloudbreak-role-name\" is your custom role name) as a new line to your Profile. If you choose this option, you must make sure that the \"CloudbreakRole\" or the IAM user have a permission not only to assume a role but also to create a role.  \n\n\n\n\nYou can create these roles in the \nIAM console\n, on the \nRoles\n page via the \nCreate Role\n option. Detailed steps are provided below. \n\n\nCreate CloudbreakRole\n\n\nUse these steps to create CloudbreakRole. \n\n\nUse the following \"AssumeRole\" policy definition: \n\n\n{\n  \"Version\": \"2012-10-17\",\n  \"Statement\": {\n    \"Sid\": \"Stmt1400068149000\",\n    \"Effect\": \"Allow\",\n    \"Action\": [\"sts:AssumeRole\"],\n    \"Resource\": \"*\"\n  }\n}\n\n\n\n\nSteps\n\n\n\n\n\n\nNavigate to the \nIAM console\n \n \nRoles\n and click \nCreate Role\n.\n\n\n \n\n\n\n\n\n\nIn the \"Create Role\" wizard, select \nAWS service\n role type and then select any service. \n\n\n \n\n\n\n\n\n\nWhen done, click \nNext: Permissions\n to navigate to the next page in the wizard.\n\n\n\n\n\n\nClick \nCreate policy\n.\n\n\n\n\n\n\n\n\nClick \nSelect\n next to \"Create Your Own Policy\".\n\n\n  \n\n\n\n\n\n\nIn the \nPolicy Name\n field, enter \"AssumeRole\" and in the \nPolicy Document\n paste the policy definition. You can either copy it from the section preceding these steps or download and copy it from \nhere\n.\n\n\n  \n\n\n\n\n\n\nWhen done, click \nCreate Policy\n.\n\n\n\n\n\n\nClick \nRefresh\n. Next, find the \"AsumeRole\" policy that you just created and select it by checking the box.\n\n\n \n\n\n\n\n\n\nWhen done, click \nNext: Review\n.\n\n\n\n\n\n\nIn the \nRoles name\n field, enter role name, for example \"CloudbreakRole\". \n\n\n \n\n\n\n\n\n\nWhen done, click \nCreate role\n to finish the role creation process.\n\n\n\n\n\n\nCreate CredentialRole\n\n\nUse these steps to create CredentialRole.\n\n\nUse the following \"cb-policy\" policy definition: \n\n\n{\n  \"Version\": \"2012-10-17\",\n  \"Statement\": [\n    {\n      \"Effect\": \"Allow\",\n      \"Action\": [\n        \"cloudformation:CreateStack\",\n        \"cloudformation:DeleteStack\",\n        \"cloudformation:DescribeStackEvents\",\n        \"cloudformation:DescribeStackResource\",\n        \"cloudformation:DescribeStacks\"\n      ],\n      \"Resource\": [\n        \"*\"\n      ]\n    },\n    {\n      \"Effect\": \"Allow\",\n      \"Action\": [\n        \"ec2:AllocateAddress\",\n        \"ec2:AssociateAddress\",\n        \"ec2:AssociateRouteTable\",\n        \"ec2:AuthorizeSecurityGroupIngress\",\n        \"ec2:DescribeRegions\",\n        \"ec2:DescribeAvailabilityZones\",\n        \"ec2:CreateRoute\",\n        \"ec2:CreateRouteTable\",\n        \"ec2:CreateSecurityGroup\",\n        \"ec2:CreateSubnet\",\n        \"ec2:CreateTags\",\n        \"ec2:CreateVpc\",\n        \"ec2:ModifyVpcAttribute\",\n        \"ec2:DeleteSubnet\",\n        \"ec2:CreateInternetGateway\",\n        \"ec2:CreateKeyPair\",\n        \"ec2:DisassociateAddress\",\n        \"ec2:DisassociateRouteTable\",\n        \"ec2:ModifySubnetAttribute\",\n        \"ec2:ReleaseAddress\",\n        \"ec2:DescribeAddresses\",\n        \"ec2:DescribeImages\",\n        \"ec2:DescribeInstanceStatus\",\n        \"ec2:DescribeInstances\",\n        \"ec2:DescribeInternetGateways\",\n        \"ec2:DescribeKeyPairs\",\n        \"ec2:DescribeRouteTables\",\n        \"ec2:DescribeSecurityGroups\",\n        \"ec2:DescribeSubnets\",\n        \"ec2:DescribeVpcs\",\n        \"ec2:DescribeSpotInstanceRequests\",\n        \"ec2:DescribeVpcAttribute\",\n        \"ec2:ImportKeyPair\",\n        \"ec2:AttachInternetGateway\",\n        \"ec2:DeleteVpc\",\n        \"ec2:DeleteSecurityGroup\",\n        \"ec2:DeleteRouteTable\",\n        \"ec2:DeleteInternetGateway\",\n        \"ec2:DeleteRouteTable\",\n        \"ec2:DeleteRoute\",\n        \"ec2:DetachInternetGateway\"\n      ],\n      \"Resource\": [\n        \"*\"\n      ]\n    },\n    {\n      \"Effect\": \"Allow\",\n      \"Action\": [\n        \"iam:ListRolePolicies\",\n        \"iam:GetRolePolicy\",\n        \"iam:ListAttachedRolePolicies\",\n        \"iam:ListInstanceProfiles\",\n        \"iam:PutRolePolicy\",\n        \"iam:PassRole\",\n        \"iam:GetRole\"\n      ],\n      \"Resource\": [\n        \"*\"\n      ]\n    },\n    {\n      \"Effect\": \"Allow\",\n      \"Action\": [\n        \"autoscaling:CreateAutoScalingGroup\",\n        \"autoscaling:CreateLaunchConfiguration\",\n        \"autoscaling:DeleteAutoScalingGroup\",\n        \"autoscaling:DeleteLaunchConfiguration\",\n        \"autoscaling:DescribeAutoScalingGroups\",\n        \"autoscaling:DescribeLaunchConfigurations\",\n        \"autoscaling:DescribeScalingActivities\",\n        \"autoscaling:DetachInstances\",\n        \"autoscaling:ResumeProcesses\",\n        \"autoscaling:SuspendProcesses\",\n        \"autoscaling:UpdateAutoScalingGroup\"\n      ],\n      \"Resource\": [\n        \"*\"\n      ]\n    }\n  ]\n}\n\n\n\nSteps\n\n\n\n\n\n\nNavigate to the \nIAM console\n \n \nRoles\n and click \nCreate Role\n.\n\n\n \n\n\n\n\n\n\nIn the \"Create Role\" wizard, select \nAnother AWS account\n role type. Next, provide the following:\n\n\n\n\nIn the \nAccount ID\n field, enter your AWS account ID.\n\n\nUnder \nOptions\n, check \nRequire external ID\n.\n\n\nIn the \nExternal ID\n, enter \"provision-ambari\".\n\n\n\n\n \n\n\n\n\n\n\nWhen done, click \nNext: Permissions\n to navigate to the next page in the wizard.\n\n\n\n\n\n\nClick \nCreate policy\n.\n\n\n\n\n\n\n\n\nClick \nSelect\n next to \"Create Your Own Policy\".\n\n\n \n\n\n\n\n\n\nIn the \nPolicy Name\n field, enter \"cb-policy\" and in the \nPolicy Document\n paste the policy definition.  You can either copy it from the section preceding these steps or download and copy it from \nhere\n.\n\n\n  \n\n\n\n\n\n\nWhen done, click \nCreate Policy\n.\n\n\n\n\n\n\nClick \nRefresh\n. Next, find the \"cb-policy\" that you just created and select it by checking the box.\n\n\n \n\n\n\n\n\n\nWhen done, click \nNext: Review\n.\n\n\n\n\n\n\nIn the \nRoles name\n field, enter role name, for example \"CredentialRole\". \n\n\n \n\n\n\n\n\n\nWhen done, click \nCreate role\n to finish the role creation process. \n\n\n\n\n\n\nOnce you are done, you can proceed to \nLaunch the VM\n.  \n\n\nRelated Links\n\n\nUsing Instance Profiles\n (External)\n\n\nUsing an IAM Role to Grant Permissions to Applications\n (External)   \n\n\nLaunch the VM\n\n\nNow that you've met the prerequisites, you can launch the Cloudbreak deployer VM available as a Community AMI.\n\n\nSteps\n\n\n\n\n\n\nOn AWS, navigate to the EC2 Console.  \n\n\n\n\n\n\nIn the top right corner, select the region in which you want to launch Cloudbreak.  \n\n\n \n\n\n\n\n\n\nFrom the left pane, select \nINSTANCES\n \n \nInstances\n.  \n\n\n\n\n\n\nClick on \nLaunch Instance\n.\n\n\n\n\n\n\nIn \nStep 1: Choose an Amazon Machine Image (AMI)\n, select \nCommunity AMIs\n from the left pane. \n\n\n \n\n\n\n\n\n\nIn the search box, enter the image name. The following Cloudbreak deployer images are available:\n\n\n\n\n\n\n\n\nRegion Name\n\n\nRegion\n\n\nCommunity AMI\n\n\n\n\n\n\n\n\n\n\nEU (Ireland)\n\n\neu-west-1\n\n\nami-6edb5817\n\n\n\n\n\n\nEU (Frankfurt)\n\n\neu-central-1\n\n\nami-665ecb09\n\n\n\n\n\n\nUS East (N. Virginia)\n\n\nus-east-1\n\n\nami-82324cf8\n\n\n\n\n\n\nUS West (N. California)\n\n\nus-west-1\n\n\nami-a53532c5\n\n\n\n\n\n\nUS West (Oregon)\n\n\nus-west-2\n\n\nami-d88725a0\n\n\n\n\n\n\nSouth America (S\u00e3o Paulo)\n\n\nsa-east-1\n\n\nami-a56627c9\n\n\n\n\n\n\nAsia Pacific (Tokyo)\n\n\nap-northeast-1\n\n\nami-d40d84b2\n\n\n\n\n\n\nAsia Pacific (Singapore)\n\n\nap-southeast-1\n\n\nami-02523e7e\n\n\n\n\n\n\nAsia Pacific (Sydney)\n\n\nap-southeast-2\n\n\nami-afd726cd\n\n\n\n\n\n\n\n\n\n\n\n\nClick \nSelect\n.  \n\n\n\n\nThe steps listed below only mention required parameters. You may optionally review and adjust additional parameters. \n\n\n\n\n\n\n\n\nIn \nStep2: Choose Instance Type\n, choose an instance type. The minimum instance type which is suitable for Cloudbreak is \nm3.large\n. Minimum requirements are 8GB RAM, 10GB disk, 2 cores. When done, click \nNext\n.\n\n\n   \n\n\n\n\n\n\n(Perform this step only if you are using role-based authorization) In \nStep 3: Configure Instance Details\n \n \nIAM\n, select the \"CloudbreakRole\" IAM role which you \ncreated earlier\n.\n\n\n\n\n\n\nIn \nStep 6: Configure Security Group\n, open the following ports:\n\n\n\n\n22 (for access via SSH)  \n\n\n80 (for access via HTTP)   \n\n\n443 (for access via HTTPS). \n\n\n\n\n \n\n\nWhen done, click \nReview and Launch\n.\n\n\n\n\n\n\nIn \nStep 7: Review Instance Launch\n, review the information carefully and then click \nLaunch\n. \n\n\n\n\n\n\nWhen prompted select an existing key pair or create a new one. Next, acknowledge that you have access to the private key file and click \nLaunch Instance\n. \n\n\n  \n\n\n\n\n\n\nClick on the instance ID to navigate to the \nInstances\n view in your EC2 console. \n\n\n  \n\n\n\n\n\n\nSSH to the VM\n\n\nNow that your VM is ready, access it via SSH: \n\n\n\n\nUse the private key from the key pair that you selected when launching the instance. \n\n\nThe SSH user is called \"cloudbreak\".\n\n\nYou can obtain the host IP from the EC2 console \n \nInstances\n view by selecting the instance, selecting the \nDescription\n tab, and copying the value of the \nPublic DNS (IPv4) \n or \nIPv4 Public IP\n parameter.\n\n\n\n\nOn Mac OS X, you can SSH to the VM by running the following from the Terminal app: \nssh -i \"your-private-key.pem\" cloudnreak@instance_IP\n where \"your-private-key.pem\" points to the location of your private key and \"instance_IP\" is the public IP address of the VM.\n\n\nOn Windows, you can use \nPuTTy\n.\n\n\nLaunch Cloudbreak Deployer\n\n\nAfter accessing the VM via SSH, launch Cloudbreak deployer using the following steps.  \n\n\nSteps\n  \n\n\n\n\n\n\nNavigate to the cloudbreak-deployment directory:\n\n\ncd /var/lib/cloudbreak-deployment/\n\n\nThis directory contains configuration files and the supporting binaries for Cloudbreak deployer.\n\n\n\n\n\n\nInitialize your profile by creating a new file called \nProfile\n and adding the following content:\n\n\nexport UAA_DEFAULT_SECRET=MY-SECRET\nexport UAA_DEFAULT_USER_PW=MY-PASSWORD\nexport UAA_DEFAULT_USER_EMAIL=MY-EMAIL\n  \n\n\nFor example: \n\n\nexport UAA_DEFAULT_SECRET=MySecret123\nexport UAA_DEFAULT_USER_PW=MySecurePassword123\nexport UAA_DEFAULT_USER_EMAIL=dbialek@hortonworks.com\n \n\n\n\n\nYou will need to provide the email and password when logging in to the Cloudbreak web UI and when using the Cloudbreak CLI. The secret will be used by Cloudbreak for authentication.  \n\n\n\n\n\n\n\n\nStart the Cloudbreak application by using the following command:\n\n\ncbd start\n\n\nThis will start the Docker containers and initialize the application. The first time you start the Coudbreak app, this also downloads of all the necessary docker images.\n\n\nOnce the \ncbd start\n has finished, it returns the \"Uluwatu (Cloudbreak UI) url\" which you can later paste in your browser and log in to Cloudbreak web UI. \n\n\n\n\n\n\nCheck Cloudbreak deployer version and health: \n\n\ncbd doctor\n\n\n\n\n\n\nCheck Cloudbreak Application logs: \n\n\ncbd logs cloudbreak\n\n\nYou should see a message like this in the log: \nStarted CloudbreakApplication in 36.823 seconds.\n Cloudbreak takes less than a minute to start. If you try to access the Cloudbreak UI before Cloudbreak started, you will get a \"Bad Gateway\" error or \"Cannot connect to Cloubdreak\" error.\n\n\n\n\n\n\nAccess Cloudbreak UI\n\n\nLog in to the Cloudbreak UI using the following steps.\n\n\nSteps\n\n\n\n\n\n\nYou can log into the Cloudbreak application at \nhttps://IPv4_Public_IP\n/\n or \nhttps://Public_DNS\n. For example \nhttps://34.212.141.253\n or \nhttps://ec2-34-212-141-253.us-west-2.compute.amazonaws.com\n. \n\n\n\n\n\n\nConfirm the security exception to proceed to the Cloudbreak web UI.\n\n\nThe first time you access Cloudbreak UI, Cloudbreak will automatically generate a self-signed certificate, due to which your browser will warn you about an untrusted connection and will ask you to confirm a security exception.\n\n\n\n\n\n\nThe login page is displayed:\n\n\n  \n\n\n\n\n\n\nLog in to the Cloudbreak web UI using the credential that you configured in your \nProfile\n file when \nlaunching Cloudbreak deployer\n:\n\n\n\n\nThe username is the \nUAA_DEFAULT_USER_EMAIL\n     \n\n\nThe password is the \nUAA_DEFAULT_USER_PW\n \n\n\n\n\n\n\n\n\nUpon a successful login, you are redirected to the dashboard:\n\n\n  \n\n\n\n\n\n\nCreate Cloudbreak Credential\n\n\nBefore you can start creating clusters, you must first create a \nCloudbreak credential\n. Without this credential, you will not be able to create clusters via Cloudbreak. \n\n\nAs part of the \nprerequisites\n, you had two options to allow Cloudbreak to authenticate with AWS and create resources on your behalf: key-based or role-based authentication. Depending on your choice, you must configure a key-based or role-based credential: \n\n\n\n\nCreate Key-Based Credential\n  \n\n\nCreate Role-Based Credential\n\n\n\n\nCreate Key-Based Credential\n\n\nTo perform these steps, you must know your access and secret key. If needed, you or your AWS administrator can generate new access and secret keys from the \nIAM Console\n \n \nUsers\n \n select a user \n \nSecurity credentials\n. \n\n\nSteps\n\n\n\n\n\n\nIn the Cloudbreak web UI, select \nCredentials\n from the navigation pane. \n\n\n\n\n\n\nClick \nCreate Credential\n. \n\n\n\n\n\n\nUnder \nCloud provider\n, select \"Amazon Web Services\":\n\n\n  \n\n\n\n\n\n\nProvide the following information:\n\n\n\n\n\n\n\n\nParameter\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nSelect Credential Type\n\n\nSelect \nKey Based\n.\n\n\n\n\n\n\nName\n\n\nEnter a name for your credential.\n\n\n\n\n\n\nDescription\n\n\n(Optional) Enter a description.\n\n\n\n\n\n\nAccess Key\n\n\nPaste your access key.\n\n\n\n\n\n\nSecret Access Key\n\n\nPaste your secret key.\n\n\n\n\n\n\n\n\n\n\n\n\nClick \nCreate\n.\n\n\n\n\n\n\nYour credential should now be displayed in the \nCredentials\n pane.\n\n\nCongratulations! You've successfully launched Cloudbreak and create a Cloudbreak credential. Now it's time to \ncreate a cluster\n. \n\n\n\n\n\n\nCreate Role-Based Credential\n\n\nTo perform these steps, you must know the \nIAM Role ARN\n corresponding to the \"CredentialRole\" (configured as a \nprerequisite\n).  \n\n\nSteps\n\n\n\n\n\n\nIn the Cloudbreak web UI, select \nCredentials\n from the navigation pane. \n\n\n\n\n\n\nClick \nCreate Credential\n. \n\n\n\n\n\n\nUnder \nCloud provider\n, select \"Amazon Web Services\":\n\n\n  \n\n\n\n\n\n\nProvide the following information:\n\n\n\n\n\n\n\n\nParameter\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nSelect Credential Type\n\n\nSelect \nRole Based\n (default value).\n\n\n\n\n\n\nName\n\n\nEnter a name for your credential.\n\n\n\n\n\n\nDescription\n\n\n(Optional) Enter a description.\n\n\n\n\n\n\nIAM Role ARN\n\n\nPaste the IAM Role ARN corresponding to the \"CredentialRole\" that you created earlier. For example \narn:aws:iam::315627065446:role/CredentialRole\n is a valid IAM Role ARN.\n\n\n\n\n\n\n\n\n\n\n\n\nClick \nCreate\n.\n\n\n\n\n\n\nYour credential should now be displayed in the \nCredentials\n pane.\n\n\nCongratulations! You have successfully launched Cloudbreak and created a Cloudbreak credential. Now you can use Cloubdreak to \ncreate clusters\n. \n\n\n\n\n\n\n\n\nNext: Create a Cluster", 
            "title": "Launch on AWS"
        }, 
        {
            "location": "/aws-launch/index.html#launching-cloudbreak-on-aws", 
            "text": "Before launching Cloudbreak on AWS, review and meet the prerequisites. Next, launch a VM using a Cloudbreak Amazon Machine Image, access the VM, and then start Cloudbreak. Once Cloudbreak is running, log in to the Cloudbreak UI and create a Cloudbreak credential.", 
            "title": "Launching Cloudbreak on AWS"
        }, 
        {
            "location": "/aws-launch/index.html#meet-the-prerequisites", 
            "text": "Before launching Cloudbreak on AWS, you must meet the following prerequisites.", 
            "title": "Meet the Prerequisites"
        }, 
        {
            "location": "/aws-launch/index.html#aws-account", 
            "text": "In order to launch Cloudbreak on AWS, you must log in to your AWS account. If you don't have an account, you can create one at  https://aws.amazon.com/ .", 
            "title": "AWS Account"
        }, 
        {
            "location": "/aws-launch/index.html#aws-region", 
            "text": "Decide in which AWS region you would like to launch Cloudbreak. The following AWS regions are supported:      Region Name  Region      EU (Ireland)  eu-west-1    EU (Frankfurt)  eu-central-1    US East (N. Virginia)  us-east-1    US West (N. California)  us-west-1    US West (Oregon)  us-west-2    South America (S\u00e3o Paulo)  sa-east-1    Asia Pacific (Tokyo)  ap-northeast-1    Asia Pacific (Singapore)  ap-southeast-1    Asia Pacific (Sydney)  ap-southeast-2     Clusters created via Cloudbreak can be in the same or different region as Cloudbreak; when you launch a cluster, you select the region in which to launch it.  Related Links  AWS Regions and Endpoints  (External)", 
            "title": "AWS Region"
        }, 
        {
            "location": "/aws-launch/index.html#ssh-key-pair", 
            "text": "Import an existing key pair or generate a new key pair in the AWS region which you are planning to use for launching Cloudbreak and clusters. You can do this using the following steps.  Steps     Navigate to the Amazon EC2 console at https://console.aws.amazon.com/ec2/.    Check the region listed in the top right corner to make sure that you are in the correct region.    In the left pane, find  NETWORK AND SECURITY  and click  Key Pairs .     Do one of the following:  Click  Create Key Pair  to create a new key pair. Your private key file will be automatically downloaded onto your computer. Make sure to save it in a secure location. You will need it to SSH to the cluster nodes. You may want to change access settings for the file using  chmod 400 my-key-pair.pem .    Click  Import Key Pair  to upload an existing public key and then select it and click  Import . Make sure that you have access to its corresponding private key.         You need this SSH key pair to SSH to the Cloudbreak instance and start Cloudbreak.   Related Links  Creating a Key Pair Using Amazon EC2  (External)", 
            "title": "SSH Key Pair"
        }, 
        {
            "location": "/aws-launch/index.html#authentication", 
            "text": "Before you can start using Cloudbreak for provisioning clusters, you must select a way for Cloudbreak to authenticate with your AWS account and create resources on your behalf. There are two ways to do this:     Key-based : This is a simpler option which does not require additional configuration at this point. It requires that you provide your AWS access key and secret key pair in the Cloudbreak web UI later. All you need to do now is check your AWS account and ensure that you can access this key pair.    Role-based : This requires that you or your AWS admin create an IAM role to allow Cloudbreak to assume AWS roles (the \"AssumeRole\" policy).    Option 1: Key-based Authentication  If you are using key-based authentication for Cloudbreak on AWS, you must be able to provide your AWS access key and secret key pair. Cloudbreak will use these keys to launch the resources. You must provide the access and secret keys later in the Cloudbreak web UI later when creating a credential.   If you choose this option, all you need to do at this point is check your AWS account and make sure that you can access this key pair. You can generate new access and secret keys from the  IAM Console     Users . Next, select a user and click on the  Security credentials  tab:     If you choose this option, you can proceed to  Launch the VM .  Option 2: Role-based Authentication  If you are using role-based authentication for Cloudbreak on AWS, you must create two IAM roles: one to grant Cloudbreak access to allow Cloudbreak to assume AWS roles (using the \"AssumeRole\" policy) and the second one to provide Cloudbreak with the capabilities required for cluster creation (using the \"cb-policy\" policy).  The following table provides contextual information about the two roles required:      Role  Purpose  Overview of Steps  Configuration      CloudbreakRole  Allows Cloudbreak to assume other IAM roles - specifically the CredentialRole.  Create a role called \"CloudbreakRole\" and attach the \"AssumeRole\" policy. The \"AssumeRole\" policy definition and steps for creating the CloudbreakRole are provided below.  When launching your Cloudbreak VM, during  Step 3: Configure Instance Details     IAM , you will attach the \"CloudbreakRole\" IAM role to the VM.    CredentialRole  Allows Cloudbreak to create AWS resources required for clusters.  Create a new IAM role called \"CredentialRole\" and attach the \"cb-policy\" policy to it. The \"cb-policy\" policy definition and steps for creating the CredentialRole are provided below.  When creating this role using the AWS Console, make sure that that it is a role for cross-account access and that the trust-relation is set up as follows: 'Account ID' is your own 12-digit AWS account ID and 'External ID' is \u201cprovision-ambari\u201d. See steps below.  Once you log in to the Cloudbreak UI and are ready to create clusters, you will use this role to create the Cloudbreak credential.      Alternatively, instead of attaching the \"CloudbreakRole\" role during the VM launch, you can assign the \"CloudbreakRole\" to an IAM user and then add the access and secret key of that user to your 'Profile'.  Alternatively you can generate the \"CredentialRole\" role later once your Cloudbreak VM is running by SSHing to the Cloudbreak VM and running the  cbd aws generate-role  command. This command creates a role with the name \"cbreak-deployer\" (equivalent to the \"CredentialRole\"). To customize the name of the role, add  export AWS_ROLE_NAME=my-cloudbreak-role-name  (where \"my-cloudbreak-role-name\" is your custom role name) as a new line to your Profile. If you choose this option, you must make sure that the \"CloudbreakRole\" or the IAM user have a permission not only to assume a role but also to create a role.     You can create these roles in the  IAM console , on the  Roles  page via the  Create Role  option. Detailed steps are provided below.   Create CloudbreakRole  Use these steps to create CloudbreakRole.   Use the following \"AssumeRole\" policy definition:   {\n  \"Version\": \"2012-10-17\",\n  \"Statement\": {\n    \"Sid\": \"Stmt1400068149000\",\n    \"Effect\": \"Allow\",\n    \"Action\": [\"sts:AssumeRole\"],\n    \"Resource\": \"*\"\n  }\n}  Steps    Navigate to the  IAM console     Roles  and click  Create Role .       In the \"Create Role\" wizard, select  AWS service  role type and then select any service.        When done, click  Next: Permissions  to navigate to the next page in the wizard.    Click  Create policy .     Click  Select  next to \"Create Your Own Policy\".        In the  Policy Name  field, enter \"AssumeRole\" and in the  Policy Document  paste the policy definition. You can either copy it from the section preceding these steps or download and copy it from  here .        When done, click  Create Policy .    Click  Refresh . Next, find the \"AsumeRole\" policy that you just created and select it by checking the box.       When done, click  Next: Review .    In the  Roles name  field, enter role name, for example \"CloudbreakRole\".        When done, click  Create role  to finish the role creation process.    Create CredentialRole  Use these steps to create CredentialRole.  Use the following \"cb-policy\" policy definition:   {\n  \"Version\": \"2012-10-17\",\n  \"Statement\": [\n    {\n      \"Effect\": \"Allow\",\n      \"Action\": [\n        \"cloudformation:CreateStack\",\n        \"cloudformation:DeleteStack\",\n        \"cloudformation:DescribeStackEvents\",\n        \"cloudformation:DescribeStackResource\",\n        \"cloudformation:DescribeStacks\"\n      ],\n      \"Resource\": [\n        \"*\"\n      ]\n    },\n    {\n      \"Effect\": \"Allow\",\n      \"Action\": [\n        \"ec2:AllocateAddress\",\n        \"ec2:AssociateAddress\",\n        \"ec2:AssociateRouteTable\",\n        \"ec2:AuthorizeSecurityGroupIngress\",\n        \"ec2:DescribeRegions\",\n        \"ec2:DescribeAvailabilityZones\",\n        \"ec2:CreateRoute\",\n        \"ec2:CreateRouteTable\",\n        \"ec2:CreateSecurityGroup\",\n        \"ec2:CreateSubnet\",\n        \"ec2:CreateTags\",\n        \"ec2:CreateVpc\",\n        \"ec2:ModifyVpcAttribute\",\n        \"ec2:DeleteSubnet\",\n        \"ec2:CreateInternetGateway\",\n        \"ec2:CreateKeyPair\",\n        \"ec2:DisassociateAddress\",\n        \"ec2:DisassociateRouteTable\",\n        \"ec2:ModifySubnetAttribute\",\n        \"ec2:ReleaseAddress\",\n        \"ec2:DescribeAddresses\",\n        \"ec2:DescribeImages\",\n        \"ec2:DescribeInstanceStatus\",\n        \"ec2:DescribeInstances\",\n        \"ec2:DescribeInternetGateways\",\n        \"ec2:DescribeKeyPairs\",\n        \"ec2:DescribeRouteTables\",\n        \"ec2:DescribeSecurityGroups\",\n        \"ec2:DescribeSubnets\",\n        \"ec2:DescribeVpcs\",\n        \"ec2:DescribeSpotInstanceRequests\",\n        \"ec2:DescribeVpcAttribute\",\n        \"ec2:ImportKeyPair\",\n        \"ec2:AttachInternetGateway\",\n        \"ec2:DeleteVpc\",\n        \"ec2:DeleteSecurityGroup\",\n        \"ec2:DeleteRouteTable\",\n        \"ec2:DeleteInternetGateway\",\n        \"ec2:DeleteRouteTable\",\n        \"ec2:DeleteRoute\",\n        \"ec2:DetachInternetGateway\"\n      ],\n      \"Resource\": [\n        \"*\"\n      ]\n    },\n    {\n      \"Effect\": \"Allow\",\n      \"Action\": [\n        \"iam:ListRolePolicies\",\n        \"iam:GetRolePolicy\",\n        \"iam:ListAttachedRolePolicies\",\n        \"iam:ListInstanceProfiles\",\n        \"iam:PutRolePolicy\",\n        \"iam:PassRole\",\n        \"iam:GetRole\"\n      ],\n      \"Resource\": [\n        \"*\"\n      ]\n    },\n    {\n      \"Effect\": \"Allow\",\n      \"Action\": [\n        \"autoscaling:CreateAutoScalingGroup\",\n        \"autoscaling:CreateLaunchConfiguration\",\n        \"autoscaling:DeleteAutoScalingGroup\",\n        \"autoscaling:DeleteLaunchConfiguration\",\n        \"autoscaling:DescribeAutoScalingGroups\",\n        \"autoscaling:DescribeLaunchConfigurations\",\n        \"autoscaling:DescribeScalingActivities\",\n        \"autoscaling:DetachInstances\",\n        \"autoscaling:ResumeProcesses\",\n        \"autoscaling:SuspendProcesses\",\n        \"autoscaling:UpdateAutoScalingGroup\"\n      ],\n      \"Resource\": [\n        \"*\"\n      ]\n    }\n  ]\n}  Steps    Navigate to the  IAM console     Roles  and click  Create Role .       In the \"Create Role\" wizard, select  Another AWS account  role type. Next, provide the following:   In the  Account ID  field, enter your AWS account ID.  Under  Options , check  Require external ID .  In the  External ID , enter \"provision-ambari\".        When done, click  Next: Permissions  to navigate to the next page in the wizard.    Click  Create policy .     Click  Select  next to \"Create Your Own Policy\".       In the  Policy Name  field, enter \"cb-policy\" and in the  Policy Document  paste the policy definition.  You can either copy it from the section preceding these steps or download and copy it from  here .        When done, click  Create Policy .    Click  Refresh . Next, find the \"cb-policy\" that you just created and select it by checking the box.       When done, click  Next: Review .    In the  Roles name  field, enter role name, for example \"CredentialRole\".        When done, click  Create role  to finish the role creation process.     Once you are done, you can proceed to  Launch the VM .    Related Links  Using Instance Profiles  (External)  Using an IAM Role to Grant Permissions to Applications  (External)", 
            "title": "Authentication"
        }, 
        {
            "location": "/aws-launch/index.html#launch-the-vm", 
            "text": "Now that you've met the prerequisites, you can launch the Cloudbreak deployer VM available as a Community AMI.  Steps    On AWS, navigate to the EC2 Console.      In the top right corner, select the region in which you want to launch Cloudbreak.         From the left pane, select  INSTANCES     Instances .      Click on  Launch Instance .    In  Step 1: Choose an Amazon Machine Image (AMI) , select  Community AMIs  from the left pane.        In the search box, enter the image name. The following Cloudbreak deployer images are available:     Region Name  Region  Community AMI      EU (Ireland)  eu-west-1  ami-6edb5817    EU (Frankfurt)  eu-central-1  ami-665ecb09    US East (N. Virginia)  us-east-1  ami-82324cf8    US West (N. California)  us-west-1  ami-a53532c5    US West (Oregon)  us-west-2  ami-d88725a0    South America (S\u00e3o Paulo)  sa-east-1  ami-a56627c9    Asia Pacific (Tokyo)  ap-northeast-1  ami-d40d84b2    Asia Pacific (Singapore)  ap-southeast-1  ami-02523e7e    Asia Pacific (Sydney)  ap-southeast-2  ami-afd726cd       Click  Select .     The steps listed below only mention required parameters. You may optionally review and adjust additional parameters.      In  Step2: Choose Instance Type , choose an instance type. The minimum instance type which is suitable for Cloudbreak is  m3.large . Minimum requirements are 8GB RAM, 10GB disk, 2 cores. When done, click  Next .         (Perform this step only if you are using role-based authorization) In  Step 3: Configure Instance Details     IAM , select the \"CloudbreakRole\" IAM role which you  created earlier .    In  Step 6: Configure Security Group , open the following ports:   22 (for access via SSH)    80 (for access via HTTP)     443 (for access via HTTPS).       When done, click  Review and Launch .    In  Step 7: Review Instance Launch , review the information carefully and then click  Launch .     When prompted select an existing key pair or create a new one. Next, acknowledge that you have access to the private key file and click  Launch Instance .         Click on the instance ID to navigate to the  Instances  view in your EC2 console.", 
            "title": "Launch the VM"
        }, 
        {
            "location": "/aws-launch/index.html#ssh-to-the-vm", 
            "text": "Now that your VM is ready, access it via SSH:    Use the private key from the key pair that you selected when launching the instance.   The SSH user is called \"cloudbreak\".  You can obtain the host IP from the EC2 console    Instances  view by selecting the instance, selecting the  Description  tab, and copying the value of the  Public DNS (IPv4)   or  IPv4 Public IP  parameter.   On Mac OS X, you can SSH to the VM by running the following from the Terminal app:  ssh -i \"your-private-key.pem\" cloudnreak@instance_IP  where \"your-private-key.pem\" points to the location of your private key and \"instance_IP\" is the public IP address of the VM.  On Windows, you can use  PuTTy .", 
            "title": "SSH to the VM"
        }, 
        {
            "location": "/aws-launch/index.html#launch-cloudbreak-deployer", 
            "text": "After accessing the VM via SSH, launch Cloudbreak deployer using the following steps.    Steps       Navigate to the cloudbreak-deployment directory:  cd /var/lib/cloudbreak-deployment/  This directory contains configuration files and the supporting binaries for Cloudbreak deployer.    Initialize your profile by creating a new file called  Profile  and adding the following content:  export UAA_DEFAULT_SECRET=MY-SECRET\nexport UAA_DEFAULT_USER_PW=MY-PASSWORD\nexport UAA_DEFAULT_USER_EMAIL=MY-EMAIL     For example:   export UAA_DEFAULT_SECRET=MySecret123\nexport UAA_DEFAULT_USER_PW=MySecurePassword123\nexport UAA_DEFAULT_USER_EMAIL=dbialek@hortonworks.com     You will need to provide the email and password when logging in to the Cloudbreak web UI and when using the Cloudbreak CLI. The secret will be used by Cloudbreak for authentication.       Start the Cloudbreak application by using the following command:  cbd start  This will start the Docker containers and initialize the application. The first time you start the Coudbreak app, this also downloads of all the necessary docker images.  Once the  cbd start  has finished, it returns the \"Uluwatu (Cloudbreak UI) url\" which you can later paste in your browser and log in to Cloudbreak web UI.     Check Cloudbreak deployer version and health:   cbd doctor    Check Cloudbreak Application logs:   cbd logs cloudbreak  You should see a message like this in the log:  Started CloudbreakApplication in 36.823 seconds.  Cloudbreak takes less than a minute to start. If you try to access the Cloudbreak UI before Cloudbreak started, you will get a \"Bad Gateway\" error or \"Cannot connect to Cloubdreak\" error.", 
            "title": "Launch Cloudbreak Deployer"
        }, 
        {
            "location": "/aws-launch/index.html#access-cloudbreak-ui", 
            "text": "Log in to the Cloudbreak UI using the following steps.  Steps    You can log into the Cloudbreak application at  https://IPv4_Public_IP /  or  https://Public_DNS . For example  https://34.212.141.253  or  https://ec2-34-212-141-253.us-west-2.compute.amazonaws.com .     Confirm the security exception to proceed to the Cloudbreak web UI.  The first time you access Cloudbreak UI, Cloudbreak will automatically generate a self-signed certificate, due to which your browser will warn you about an untrusted connection and will ask you to confirm a security exception.    The login page is displayed:        Log in to the Cloudbreak web UI using the credential that you configured in your  Profile  file when  launching Cloudbreak deployer :   The username is the  UAA_DEFAULT_USER_EMAIL        The password is the  UAA_DEFAULT_USER_PW       Upon a successful login, you are redirected to the dashboard:", 
            "title": "Access Cloudbreak UI"
        }, 
        {
            "location": "/aws-launch/index.html#create-cloudbreak-credential", 
            "text": "Before you can start creating clusters, you must first create a  Cloudbreak credential . Without this credential, you will not be able to create clusters via Cloudbreak.   As part of the  prerequisites , you had two options to allow Cloudbreak to authenticate with AWS and create resources on your behalf: key-based or role-based authentication. Depending on your choice, you must configure a key-based or role-based credential:    Create Key-Based Credential     Create Role-Based Credential", 
            "title": "Create Cloudbreak Credential"
        }, 
        {
            "location": "/aws-launch/index.html#create-key-based-credential", 
            "text": "To perform these steps, you must know your access and secret key. If needed, you or your AWS administrator can generate new access and secret keys from the  IAM Console     Users    select a user    Security credentials .   Steps    In the Cloudbreak web UI, select  Credentials  from the navigation pane.     Click  Create Credential .     Under  Cloud provider , select \"Amazon Web Services\":        Provide the following information:     Parameter  Description      Select Credential Type  Select  Key Based .    Name  Enter a name for your credential.    Description  (Optional) Enter a description.    Access Key  Paste your access key.    Secret Access Key  Paste your secret key.       Click  Create .    Your credential should now be displayed in the  Credentials  pane.  Congratulations! You've successfully launched Cloudbreak and create a Cloudbreak credential. Now it's time to  create a cluster .", 
            "title": "Create Key-Based Credential"
        }, 
        {
            "location": "/aws-launch/index.html#create-role-based-credential", 
            "text": "To perform these steps, you must know the  IAM Role ARN  corresponding to the \"CredentialRole\" (configured as a  prerequisite ).    Steps    In the Cloudbreak web UI, select  Credentials  from the navigation pane.     Click  Create Credential .     Under  Cloud provider , select \"Amazon Web Services\":        Provide the following information:     Parameter  Description      Select Credential Type  Select  Role Based  (default value).    Name  Enter a name for your credential.    Description  (Optional) Enter a description.    IAM Role ARN  Paste the IAM Role ARN corresponding to the \"CredentialRole\" that you created earlier. For example  arn:aws:iam::315627065446:role/CredentialRole  is a valid IAM Role ARN.       Click  Create .    Your credential should now be displayed in the  Credentials  pane.  Congratulations! You have successfully launched Cloudbreak and created a Cloudbreak credential. Now you can use Cloubdreak to  create clusters .      Next: Create a Cluster", 
            "title": "Create Role-Based Credential"
        }, 
        {
            "location": "/aws-create/index.html", 
            "text": "Creating a Cluster on AWS\n\n\nUse these steps to create a cluster.\n\n\nPrerequisites\n\n\nIf you would like to use \nOozie\n with \nAmbari 2.6.1 or newer\n, you must install the Ext JS library. For instructions, refer to \nRecipe to Install Ext JS for Oozie\n.\n\n\nSteps\n\n\n\n\n\n\nLog in to the Cloudbreak UI.\n\n\n\n\n\n\nClick \nCreate Cluster\n and the \nCreate Cluster\n wizard is displayed.\n\n    By default, \nBasic\n view is displayed. To view advanced options, click \nAdvanced\n. To learn about advanced options, refer to \nAdvanced Options\n.\n\n\n \n\n\n\n\n\n\nOn the \nGeneral Configuration\n page, specify the following general parameters for your cluster:\n\n\n\n\n\n\n\n\nParameter\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nSelect Credential\n\n\nChoose a previously created credential.\n\n\n\n\n\n\nCluster Name\n\n\nEnter a name for your cluster. The name must be between 5 and 40 characters, must start with a letter, and must only include lowercase letters, numbers, and hyphens.\n\n\n\n\n\n\nRegion\n\n\nSelect the AWS region in which you would like to launch your cluster. For information on available AWS regions, refer to \nAWS documentation\n.\n\n\n\n\n\n\nPlatform Version\n\n\nChoose the HDP version to use for this cluster.\n\n\n\n\n\n\nCluster Type\n\n\nChoose one of default cluster configurations, or, if you have defined your own cluster configuration via Ambari blueprint, you can choose it here. For more information on default and custom blueprints, refer to \nBlueprints\n.\n\n\n\n\n\n\n\n\n\n\n\n\nOn the \nHardware and Storage\n page, for each host group provide the following information to define your cluster nodes and attached storage:\n\n\n\n\n\n\n\n\nParameter\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nInstance Type\n\n\nSelect an instance type. For information about instance types on AWS refer to \nAWS documentation\n.\n\n\n\n\n\n\nInstance Count\n\n\nEnter the number of instances of a given type. Default is 1.\n\n\n\n\n\n\nAmbari Server\n\n\nYou must select one node for Ambari Server. The \"Group Size\" for that host group must be set to \"1\".\n\n\n\n\n\n\n\n\n\n\n\n\nOn the \nNetwork\n page, provide the following to specify the networking resources that will be used for your cluster:\n\n\n\n\n\n\n\n\nParameter\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nSelect Network\n\n\nSelect the virtual network in which you would like your cluster to be provisioned. You can select an existing network or create a new network.\n\n\n\n\n\n\nSelect Subnet\n\n\nSelect the subnet in which you would like your cluster to be provisioned. If you are using a new network, create a new subnet. If you are using an existing network, select an existing subnet.\n\n\n\n\n\n\nSubnet (CIDR)\n\n\nIf you selected to create a new subnet, you must define a valid \nCIDR\n for the subnet. Default is 10.0.0.0/16.\n\n\n\n\n\n\n\n\n\n\n\n\nDefine security groups for each host group. You can either create new security groups and define their rules or reuse existing security groups:\n\n\n\n\nExisting security groups are only available for an existing VPC. \n\n\n\n\n\n\n\n\n\n\nOption\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nNew Security Group\n\n\n(Default) Creates a new security group with the rules that you defined:\nA set of \ndefault rules\n is provided. You should review and adjust these default rules. If you do not make any modifications, default rules will be applied. \nYou may open ports by defining the CIDR, entering port range, selecting protocol and clicking \n+\n.\nYou may delete default or previously added rules using the delete icon.\nIf you don't want to use security group, remove the default rules.\n\n\n\n\n\n\nExisting Security Groups\n\n\nAllows you to select an existing security group that is already available in the selected provider region. This selection is disabled if no existing security groups are available in your chosen region.\n\n\n\n\n\n\n\n\n\n\nImportant\n\n\n\nBy default, ports 22, 443, and 9443 are set to 0.0.0.0/0 CIDR for inbound access on the Ambari node security group. We strongly recommend that you limit this CIDR, considering the following restrictions:\n\nPorts 22 and 9443 must be open to Cloudbreak's CIDR. You can set CB_DEFAULT_GATEWAY_CIDR in your Cloudbreak's Profile file in order to automatically open ports 22 and 9443 to your Cloudbrak IP. Refer to \nRestricting Inbound Access to Clusters\n.\n\n\nPort 22 must be open to your CIDR if you would like to access the master node via SSH.\n\n\nPort 443 must be open to your CIDR if you would like to access Cloudbreak web UI in a browser.\n\n\n\n\n\n\n\n\nImportant\n\n\n\nBy default, port 22 is set to 0.0.0.0/0 CIDR for inbound access on non-Ambari node security groups. We strongly recommend that you remove it.\n\n\n\n\n\n\n\n\nOn the \nSecurity\n page, provide the following parameters:\n\n\n\n\n\n\n\n\nParameter\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nCluster User\n\n\nYou can log in to the Ambari UI using this username. By default, this is set to \nadmin\n.\n\n\n\n\n\n\nPassword\n\n\nYou can log in to the Ambari UI using this password.\n\n\n\n\n\n\nConfirm Password\n\n\nConfirm the password.\n\n\n\n\n\n\nNew SSH public key\n\n\nCheck this option to specify a new public key and then enter the public key. You will use the matching private key to access your cluster nodes via SSH.\n\n\n\n\n\n\nExisting SSH public key\n\n\nSelect an existing public key. You will use the matching private key to access your cluster nodes via SSH. This is a default option as long as an existing SSH public key is available.\n\n\n\n\n\n\n\n\n\n\n\n\nClick on \nCreate Cluster\n to create a cluster.\n\n\n\n\n\n\nYou will be redirected to the Cloudbreak dashboard, and a new tile representing your cluster will appear at the top of the page.\n\n\n\n\n\n\nRelated Links\n\n\nBlueprints\n \n\n\nDefault Cluster Security Groups\n\n\nAmazon EC2 Instance Types\n (External) \n\n\nAWS Regions and Endpoints\n (External)   \n\n\nCIDR\n (External)   \n\n\nAdvanced Options\n\n\nClick on \nAdvanced\n to view and enter additional configuration options\n\n\nAvailability Zone\n\n\nChoose one of the availability zones within the selected region. \n\n\nChoose Image Catalog\n\n\nBy default, \nChoose Image Catalog\n is set to the default image catalog that is provided with Cloudbreak. If you would like to use a different image catalog, you must first create and register it. For complete instructions, refer to \nCustom Images\n.\n\n\nRelated Links\n   \n\n\nCustom Images\n  \n\n\nPrewarmed and Base Images\n\n\nCloudbreak supports the following types of images for launching clusters:\n\n\n\n\n\n\n\n\nImage Type\n\n\nDescription\n\n\nDefault Images Provided\n\n\nSupport for Custom Images\n\n\n\n\n\n\n\n\n\n\nBase Images\n\n\nBase images include default configuration and default tooling. These images include the operating system but do not include Ambari or HDP software.\n\n\nYes\n\n\nYes\n\n\n\n\n\n\nPrewarmed Images\n\n\nBy default, Cloudbreak launches clusters from prewarmed images. Prewarmed images include the operating system as well as Ambari and HDP. The HDP and Ambari version used by prewarmed images cannot be customized.\n\n\nYes\n\n\nNo\n\n\n\n\n\n\n\n\nBy default, Cloudbreak uses the included default \nprewarmed images\n, which include the operating system, as well as\nAmbari and HDP packages installed. You can optionally select the \nbase image\n option if you would like to:\n\n\n\n\nUse an Ambari and HDP versions different than what the prewarmed image includes and/or  \n\n\nChoose a previously created custom base image\n\n\n\n\nChoose Image\n  \n\n\nIf under \nChoose Image Catalog\n, you selected a custom image catalog, under \nChoose Image\n you can select an image from that catalog. For complete instructions, refer to \nCustom Images\n. \n\n\nIf you are trying to customize Ambari and HDP versions, you can ignore the \nChoose Image\n option; in this case default base image is used.\n\n\nAmbari Repository Specification\n\n\nIf you would like to use a custom Ambari version, provide the following information: \n\n\n\n\n Ambari 2.6.1\n\n\nIf you would like to use Ambari \n2.6.1\n, use the version provided by default in the Cloudbreak web UI, or newer.\n\n\n\n\n\n\n\n\n\nParameter\n\n\nDescription\n\n\nExample\n\n\n\n\n\n\n\n\n\n\nVersion\n\n\nAmbari version.\n\n\n2.6.1.0\n\n\n\n\n\n\nRepo Url\n\n\nURL to the Ambari version repo.\n\n\nhttp://public-repo-1.hortonworks.com/ambari/centos6/2.x/updates/2.6.1.0\n\n\n\n\n\n\nRepo Gpg Key Url\n\n\nURL to the repo GPG key. Each stable RPM package that is published by CentOS Project is signed with a GPG signature. By default, yum and the graphical update tools will verify these signatures and refuse to install any packages that are not signed, or have an incorrect signature.\n\n\nhttp://public-repo-1.hortonworks.com/ambari/centos6/RPM-GPG-KEY/RPM-GPG-KEY-Jenkins\n\n\n\n\n\n\n\n\nHDP Repository Specification\n\n\nIf you would like to use a custom HDP version, provide the following information: \n\n\n\n\n\n\n\n\nParameter\n\n\nDescription\n\n\nExample\n\n\n\n\n\n\n\n\n\n\nStack\n\n\nStack name.\n\n\nHDP\n\n\n\n\n\n\nVersion\n\n\nStack version.\n\n\n2.6\n\n\n\n\n\n\nOS\n\n\nOperating system.\n\n\ncentos7\n\n\n\n\n\n\nStack Repo Id\n\n\nIdentifier for the repo linked in \"Base Url\".\n\n\nHDP-2.6\n\n\n\n\n\n\nBase Url\n\n\nURL to the repo storing the desired stack version.\n\n\nhttp://public-repo-1.hortonworks.com/HDP/centos7/2.x/updates/2.6.1.0\n\n\n\n\n\n\nUtils Repo Id\n\n\nIdentifier for the repo linked in \"Utils Base Url\".\n\n\nHDP-UTILS-1.1.0.21\n\n\n\n\n\n\nUtils Base Url\n\n\nURL to the repo storing utilities for the desired stack version.\n\n\nhttp://public-repo-1.hortonworks.com/HDP-UTILS-1.1.0.21/repos/centos7\n\n\n\n\n\n\nEnable Ambari Server to download and install GPL Licensed LZO packages?\n\n\n(Optional, only available if using Ambari 2.6.1 or newer) Use this option to enable LZO compression in your HDP cluster. LZO is a lossless data compression library that favors speed over compression ratio. Ambari does not install nor enable LZO compression libraries by default, and must be explicitly configured to do so. For more information, refer to \nEnabling LZO\n.\n\n\n\n\n\n\n\n\n\n\nRelated Links\n    \n\n\nCustom Images\n      \n\n\nEnable Lifetime Management\n\n\nCheck this option if you would like your cluster to be automatically terminated after a specific amount of time (defined as \"Time to Live\" in minutes). \n\n\nTags\n\n\nYou can optionally add tags, which will help you find your cluster-related resources, such as VMs, in your cloud provider account. refer to \nResource Tagging\n.\n\n\nRelated Links\n    \n\n\nResource Tagging\n \n\n\nStorage\n\n\nYou can optionally specify the following storage options for your cluster:\n\n\n\n\n\n\n\n\nParameter\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nStorage Type\n\n\nSelect the volume type. The options are:\nMagnetic (default)\nGeneral Purpose (SSD)\nThroughput Optimized HDD\nFor more information about these options refer to \nAWS documentation\n.\n\n\n\n\n\n\nAttached Volumes Per Instance\n\n\nEnter the number of volumes attached per instance. Default is 1.\n\n\n\n\n\n\nVolume Size (GB)\n\n\nEnter the size in GBs for each volume. Default is 100.\n\n\n\n\n\n\n\n\nUse Spot Instances\n\n\nCheck this option to use EC2 spot instances as your cluster nodes. Next, enter your bid price. The price that is pre-loaded in the form is the current on-demand price for your chosen EC2 instance type.   \n\n\nNote that: \n\n\n\n\nWe recommend not using spot instances for any host group that includes Ambari server components.  \n\n\nIf you choose to use spot instances for a given host group when creating your cluster, any nodes that you add to that host group (during cluster creation or later) will be using spot instances. Any additional nodes will be requested at the same bid price that you entered when creating a cluster.  \n\n\nIf you decide not to use spot instances when creating your cluster, any nodes that you add to your host group (during cluster creation or later) will be using standard on-demand instances.     \n\n\nOnce someone outbids you, the spot instances are taken away, removing the nodes from the cluster. \n\n\nIf spot instances are not available right away, creating a cluster will take longer than usual. \n\n\n\n\nAfter creating a cluster, you can view your spot instance requests, including bid price, on the EC2 dashboard under \nINSTANCES\n \n \nSpot Requests\n. For more information about spot instances, refer to \nAWS documentation\n.  \n\n\nFile System\n\n\nHDP uses HDFS as the default filesystem and it supports accessing the Amazon S3 object store through the S3A connector. \n\n\nIf you would like to access S3 through the S3A connector, you must configure access to S3 trough an instance profile. For instructions, refer to \nConfiguring Access to S3\n. \n\n\nRecipes\n\n\nThis option allows you to select previously uploaded recipes (scripts that can be run pre or post cluster deployment) for each host group. For more information on recipes, refer to \nRecipes\n. \n\n\nRelated Links\n    \n\n\nRecipes\n\n\nAmbari Server Master Key\n\n\nThe Ambari Server Master Key is used to configure Ambari to encrypt database and Kerberos credentials that are retained by Ambari as part of the Ambari setup.  \n\n\nEnable Kerberos Security\n\n\nSelect this option to enable Kerberos for your cluster. For information about available Kerberos options, refer to \nEnabling Kerberos Security\n. \n\n\nRelated Links\n    \n\n\nKerberos\n  \n\n\nAmazon EC2 Instance Store\n (External)\n\n\nAmazon EC2 Spot Instances\n (External)   \n\n\n\n\nNext: Access Cluster", 
            "title": "Create a Cluster"
        }, 
        {
            "location": "/aws-create/index.html#creating-a-cluster-on-aws", 
            "text": "Use these steps to create a cluster.  Prerequisites  If you would like to use  Oozie  with  Ambari 2.6.1 or newer , you must install the Ext JS library. For instructions, refer to  Recipe to Install Ext JS for Oozie .  Steps    Log in to the Cloudbreak UI.    Click  Create Cluster  and the  Create Cluster  wizard is displayed. \n    By default,  Basic  view is displayed. To view advanced options, click  Advanced . To learn about advanced options, refer to  Advanced Options .       On the  General Configuration  page, specify the following general parameters for your cluster:     Parameter  Description      Select Credential  Choose a previously created credential.    Cluster Name  Enter a name for your cluster. The name must be between 5 and 40 characters, must start with a letter, and must only include lowercase letters, numbers, and hyphens.    Region  Select the AWS region in which you would like to launch your cluster. For information on available AWS regions, refer to  AWS documentation .    Platform Version  Choose the HDP version to use for this cluster.    Cluster Type  Choose one of default cluster configurations, or, if you have defined your own cluster configuration via Ambari blueprint, you can choose it here. For more information on default and custom blueprints, refer to  Blueprints .       On the  Hardware and Storage  page, for each host group provide the following information to define your cluster nodes and attached storage:     Parameter  Description      Instance Type  Select an instance type. For information about instance types on AWS refer to  AWS documentation .    Instance Count  Enter the number of instances of a given type. Default is 1.    Ambari Server  You must select one node for Ambari Server. The \"Group Size\" for that host group must be set to \"1\".       On the  Network  page, provide the following to specify the networking resources that will be used for your cluster:     Parameter  Description      Select Network  Select the virtual network in which you would like your cluster to be provisioned. You can select an existing network or create a new network.    Select Subnet  Select the subnet in which you would like your cluster to be provisioned. If you are using a new network, create a new subnet. If you are using an existing network, select an existing subnet.    Subnet (CIDR)  If you selected to create a new subnet, you must define a valid  CIDR  for the subnet. Default is 10.0.0.0/16.       Define security groups for each host group. You can either create new security groups and define their rules or reuse existing security groups:   Existing security groups are only available for an existing VPC.       Option  Description      New Security Group  (Default) Creates a new security group with the rules that you defined: A set of  default rules  is provided. You should review and adjust these default rules. If you do not make any modifications, default rules will be applied.  You may open ports by defining the CIDR, entering port range, selecting protocol and clicking  + . You may delete default or previously added rules using the delete icon. If you don't want to use security group, remove the default rules.    Existing Security Groups  Allows you to select an existing security group that is already available in the selected provider region. This selection is disabled if no existing security groups are available in your chosen region.      Important  \nBy default, ports 22, 443, and 9443 are set to 0.0.0.0/0 CIDR for inbound access on the Ambari node security group. We strongly recommend that you limit this CIDR, considering the following restrictions: Ports 22 and 9443 must be open to Cloudbreak's CIDR. You can set CB_DEFAULT_GATEWAY_CIDR in your Cloudbreak's Profile file in order to automatically open ports 22 and 9443 to your Cloudbrak IP. Refer to  Restricting Inbound Access to Clusters .  Port 22 must be open to your CIDR if you would like to access the master node via SSH.  Port 443 must be open to your CIDR if you would like to access Cloudbreak web UI in a browser.     Important  \nBy default, port 22 is set to 0.0.0.0/0 CIDR for inbound access on non-Ambari node security groups. We strongly recommend that you remove it.     On the  Security  page, provide the following parameters:     Parameter  Description      Cluster User  You can log in to the Ambari UI using this username. By default, this is set to  admin .    Password  You can log in to the Ambari UI using this password.    Confirm Password  Confirm the password.    New SSH public key  Check this option to specify a new public key and then enter the public key. You will use the matching private key to access your cluster nodes via SSH.    Existing SSH public key  Select an existing public key. You will use the matching private key to access your cluster nodes via SSH. This is a default option as long as an existing SSH public key is available.       Click on  Create Cluster  to create a cluster.    You will be redirected to the Cloudbreak dashboard, and a new tile representing your cluster will appear at the top of the page.    Related Links  Blueprints    Default Cluster Security Groups  Amazon EC2 Instance Types  (External)   AWS Regions and Endpoints  (External)     CIDR  (External)", 
            "title": "Creating a Cluster on AWS"
        }, 
        {
            "location": "/aws-create/index.html#advanced-options", 
            "text": "Click on  Advanced  to view and enter additional configuration options", 
            "title": "Advanced Options"
        }, 
        {
            "location": "/aws-create/index.html#availability-zone", 
            "text": "Choose one of the availability zones within the selected region.", 
            "title": "Availability Zone"
        }, 
        {
            "location": "/aws-create/index.html#choose-image-catalog", 
            "text": "By default,  Choose Image Catalog  is set to the default image catalog that is provided with Cloudbreak. If you would like to use a different image catalog, you must first create and register it. For complete instructions, refer to  Custom Images .  Related Links      Custom Images", 
            "title": "Choose Image Catalog"
        }, 
        {
            "location": "/aws-create/index.html#prewarmed-and-base-images", 
            "text": "Cloudbreak supports the following types of images for launching clusters:     Image Type  Description  Default Images Provided  Support for Custom Images      Base Images  Base images include default configuration and default tooling. These images include the operating system but do not include Ambari or HDP software.  Yes  Yes    Prewarmed Images  By default, Cloudbreak launches clusters from prewarmed images. Prewarmed images include the operating system as well as Ambari and HDP. The HDP and Ambari version used by prewarmed images cannot be customized.  Yes  No     By default, Cloudbreak uses the included default  prewarmed images , which include the operating system, as well as\nAmbari and HDP packages installed. You can optionally select the  base image  option if you would like to:   Use an Ambari and HDP versions different than what the prewarmed image includes and/or    Choose a previously created custom base image   Choose Image     If under  Choose Image Catalog , you selected a custom image catalog, under  Choose Image  you can select an image from that catalog. For complete instructions, refer to  Custom Images .   If you are trying to customize Ambari and HDP versions, you can ignore the  Choose Image  option; in this case default base image is used.  Ambari Repository Specification  If you would like to use a custom Ambari version, provide the following information:     Ambari 2.6.1  If you would like to use Ambari  2.6.1 , use the version provided by default in the Cloudbreak web UI, or newer.     Parameter  Description  Example      Version  Ambari version.  2.6.1.0    Repo Url  URL to the Ambari version repo.  http://public-repo-1.hortonworks.com/ambari/centos6/2.x/updates/2.6.1.0    Repo Gpg Key Url  URL to the repo GPG key. Each stable RPM package that is published by CentOS Project is signed with a GPG signature. By default, yum and the graphical update tools will verify these signatures and refuse to install any packages that are not signed, or have an incorrect signature.  http://public-repo-1.hortonworks.com/ambari/centos6/RPM-GPG-KEY/RPM-GPG-KEY-Jenkins     HDP Repository Specification  If you would like to use a custom HDP version, provide the following information:      Parameter  Description  Example      Stack  Stack name.  HDP    Version  Stack version.  2.6    OS  Operating system.  centos7    Stack Repo Id  Identifier for the repo linked in \"Base Url\".  HDP-2.6    Base Url  URL to the repo storing the desired stack version.  http://public-repo-1.hortonworks.com/HDP/centos7/2.x/updates/2.6.1.0    Utils Repo Id  Identifier for the repo linked in \"Utils Base Url\".  HDP-UTILS-1.1.0.21    Utils Base Url  URL to the repo storing utilities for the desired stack version.  http://public-repo-1.hortonworks.com/HDP-UTILS-1.1.0.21/repos/centos7    Enable Ambari Server to download and install GPL Licensed LZO packages?  (Optional, only available if using Ambari 2.6.1 or newer) Use this option to enable LZO compression in your HDP cluster. LZO is a lossless data compression library that favors speed over compression ratio. Ambari does not install nor enable LZO compression libraries by default, and must be explicitly configured to do so. For more information, refer to  Enabling LZO .      Related Links       Custom Images", 
            "title": "Prewarmed and Base Images"
        }, 
        {
            "location": "/aws-create/index.html#enable-lifetime-management", 
            "text": "Check this option if you would like your cluster to be automatically terminated after a specific amount of time (defined as \"Time to Live\" in minutes).", 
            "title": "Enable Lifetime Management"
        }, 
        {
            "location": "/aws-create/index.html#tags", 
            "text": "You can optionally add tags, which will help you find your cluster-related resources, such as VMs, in your cloud provider account. refer to  Resource Tagging .  Related Links       Resource Tagging", 
            "title": "Tags"
        }, 
        {
            "location": "/aws-create/index.html#storage", 
            "text": "You can optionally specify the following storage options for your cluster:     Parameter  Description      Storage Type  Select the volume type. The options are: Magnetic (default) General Purpose (SSD) Throughput Optimized HDD For more information about these options refer to  AWS documentation .    Attached Volumes Per Instance  Enter the number of volumes attached per instance. Default is 1.    Volume Size (GB)  Enter the size in GBs for each volume. Default is 100.", 
            "title": "Storage"
        }, 
        {
            "location": "/aws-create/index.html#use-spot-instances", 
            "text": "Check this option to use EC2 spot instances as your cluster nodes. Next, enter your bid price. The price that is pre-loaded in the form is the current on-demand price for your chosen EC2 instance type.     Note that:    We recommend not using spot instances for any host group that includes Ambari server components.    If you choose to use spot instances for a given host group when creating your cluster, any nodes that you add to that host group (during cluster creation or later) will be using spot instances. Any additional nodes will be requested at the same bid price that you entered when creating a cluster.    If you decide not to use spot instances when creating your cluster, any nodes that you add to your host group (during cluster creation or later) will be using standard on-demand instances.       Once someone outbids you, the spot instances are taken away, removing the nodes from the cluster.   If spot instances are not available right away, creating a cluster will take longer than usual.    After creating a cluster, you can view your spot instance requests, including bid price, on the EC2 dashboard under  INSTANCES     Spot Requests . For more information about spot instances, refer to  AWS documentation .", 
            "title": "Use Spot Instances"
        }, 
        {
            "location": "/aws-create/index.html#file-system", 
            "text": "HDP uses HDFS as the default filesystem and it supports accessing the Amazon S3 object store through the S3A connector.   If you would like to access S3 through the S3A connector, you must configure access to S3 trough an instance profile. For instructions, refer to  Configuring Access to S3 .", 
            "title": "File System"
        }, 
        {
            "location": "/aws-create/index.html#recipes", 
            "text": "This option allows you to select previously uploaded recipes (scripts that can be run pre or post cluster deployment) for each host group. For more information on recipes, refer to  Recipes .   Related Links       Recipes", 
            "title": "Recipes"
        }, 
        {
            "location": "/aws-create/index.html#ambari-server-master-key", 
            "text": "The Ambari Server Master Key is used to configure Ambari to encrypt database and Kerberos credentials that are retained by Ambari as part of the Ambari setup.", 
            "title": "Ambari Server Master Key"
        }, 
        {
            "location": "/aws-create/index.html#enable-kerberos-security", 
            "text": "Select this option to enable Kerberos for your cluster. For information about available Kerberos options, refer to  Enabling Kerberos Security .   Related Links       Kerberos     Amazon EC2 Instance Store  (External)  Amazon EC2 Spot Instances  (External)      Next: Access Cluster", 
            "title": "Enable Kerberos Security"
        }, 
        {
            "location": "/aws-clusters-access/index.html", 
            "text": "Accessing Your Cluster\n\n\nThe following section describes how to access the various services in the cluster.\n\n\nFinding Cluster Information in the UI\n\n\nOnce your cluster is up and running, click on the tile representing your cluster in the Cloudbreak UI to access information related the cluster and access cluster actions. \n\n\n \n\n\nThe information presented includes:\n\n\n\n\nCluster Summary\n  \n\n\nCluster Information\n  \n\n\nHardware\n  \n\n\nTags\n   \n\n\nRecipes\n  \n\n\nRepository Details\n  \n\n\nImage Details\n    \n\n\nNetwork\n   \n\n\nSecurity\n  \n\n\nAutoscaling\n    \n\n\nEvent History\n  \n\n\n\n\n\n  \nTips\n\n  \n\n  \n Access cluster actions such as resize and sync by clicking on \nACTIONS\n.\n\n  \n Access Ambari web UI by clicking on the link in the \nCLUSTER INFORMATION\n section.\n\n\n View public IP addresses for all cluster instances in the \nHARDWARE\n section. Click on the links to view the instances in the cloud console.\n\n\n The SSH user that you must use when accessing cluster VMs is \"cloudbreak\".\n \n\n\n\n\n\n\n\n\nCluster Summary\n\n\nThe summary bar includes the following information about your cluster:\n\n\n\n\n\n\n\n\nItem\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nCluster Name\n\n\nThe name that you selected for your cluster is displayed at the top of the page.\n\n\n\n\n\n\nTime Remaining\n\n\nIf you enabled lifetime management for your cluster, the clock next to the cluster name indicates the amount of time that your cluster will run before it gets terminated. Note that the time remaining counter does not stop when you stop the cluster.\n\n\n\n\n\n\nCloud Provider\n\n\nThe logo of the cloud provider on which the cluster is running.\n\n\n\n\n\n\nCredential\n\n\nThe name of the credential used to create the cluster.\n\n\n\n\n\n\nStatus\n\n\nCurrent status. When a cluster is healthy, the status is \nRunning\n.\n\n\n\n\n\n\nNodes\n\n\nThe current number of cluster nodes, including the master node.\n\n\n\n\n\n\nUptime\n\n\nThe amount of time (HH:MM) that the cluster has been in the running state since it was started. Each time you stop and restart the cluster, the running time is reset to 0.\n\n\n\n\n\n\nCreated\n\n\nThe date when the cluster was created. The date format is Mon DD, YYYY. For example: Oct 27, 2017.\n\n\n\n\n\n\n\n\nCluster Information\n\n\n\n\n\n\n\n\nItem\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nCluster User\n\n\nThe name of the cluster user that you created when creating the cluster.\n\n\n\n\n\n\nSSH Username\n\n\nThe SSH user which you must use when accessing cluster VMs via SSH. The SSH user is always \"cloudbreak\".\n\n\n\n\n\n\nAmbari URL\n\n\nLink to the Ambari web UI.\n\n\n\n\n\n\nRegion\n\n\nThe region in which the cluster is running in the cloud provider infrastructure.\n\n\n\n\n\n\nAvailability Zone\n\n\nThe availability zone within the region in which the cluster is running.\n\n\n\n\n\n\nBlueprint\n\n\nThe name of the blueprint selected under \"Cluster Type\" to create this cluster.\n\n\n\n\n\n\nCreated With\n\n\nThe version of Cloubdreak used to create this cluster.\n\n\n\n\n\n\nAmbari Version\n\n\nThe Ambari version which this cluster is currently running.\n\n\n\n\n\n\nHDP Version\n\n\nThe HDP version which this cluster is currently running.\n\n\n\n\n\n\n\n\nHardware\n\n\nThis section includes information about your cluster instances: instance names, instance IDs, instance types, their status, fully qualified domain names (FQDNs), and private and public IPs. \n\n\nTags\n\n\nThis section lists keys and values of the user-defined tags, in the same order as you added them.\n\n\nRecipes\n\n\nThis section includes recipe-related information. For each recipe, you can see the host group on which a recipe was executed, recipe name, and recipe type. \n\n\nRepository Details\n\n\nThis section includes Ambari and HDP repository information, as you provided it in the \"Base Images\" section when creating a cluster.\n\n\nImage Details\n\n\nThis section includes information about the base image that was used for the Cloudbreak instance. \n\n\nNetwork\n\n\nThis section includes information about the names of the network and subnet in which the cluster is running and the links to related cloud provider console.\n\n\nSecurity\n\n\nThis section is only available if you have enabled Kerberos security. It provides you with the details of your Kerberos configuration.  \n\n\nAutoscaling\n\n\nThis section includes configuration options related to autoscaling. Refer to \nAutoscaling\n.  \n\n\nEvent History\n\n\nThe Event History section shows you events logged for the cluster, with the most recent event at the top. For example, after your cluster has been created, the following messages will be written to the log:\n\n\n\nAmbari cluster built; Ambari ip:34.215.103.66\n10/26/2017, 9:41:58 AM\nBuilding Ambari cluster; Ambari ip:34.215.103.66\n10/26/2017, 9:30:20 AM\nStarting Ambari cluster services\n10/26/2017, 9:27:12 AM\nSetting up infrastructure metadata\n10/26/2017, 9:27:11 AM\nBootstrapping infrastructure cluster\n10/26/2017, 9:26:38 AM\nInfrastructure successfully provisioned\n10/26/2017, 9:26:37 AM\nBilling started, Infrastructure successfully provisioned\n10/26/2017, 9:26:37 AM\nInfrastructure metadata collection finished\n10/26/2017, 9:25:39 AM\nInfrastructure creation took 194 seconds\n10/26/2017, 9:25:37 AM\nCreating infrastructure\n10/26/2017, 9:22:22 AM\nSetting up HDP image\n10/26/2017, 9:22:21 AM\n\n\n\nAccessing Cluster via SSH\n\n\nIf you plan to access the cluster via the command line clients, SSH into the master node instance in the cluster. \n\n\n\n\nIn order to use SSH, you must generate an SSH key pair or use an existing SSH key pair.  \n\n\nYou can find the cluster instance public IP addresses on the cluster details page.  \n\n\nWhen accessing instances via SSH use the \ncloudbreak\n user. \n\n\n\n\nOn Mac OS, you can use the following syntax to SSH to the VM:\n\nssh -i \"privatekey.pem\" cloudbreak@publicIP\n\nFor example:\n\nssh -i \"dominika-kp.pem\" cloudbreak@p52.25.169.132\n\n\nOn Windows, you can SSH using an SSH client such as PuTTY.\n\n\nAccess Ambari\n\n\nYou can access Ambari web UI by clicking on the links provided in the \nCluster Information\n \n \nAmbari URL\n.\n\n\nSteps\n\n\n\n\n\n\nFrom the cluster dashboard, click on the tile representing your cluster to navigate to cluster details.\n\n\n\n\n\n\nFind the Ambari URL in the \nCluster Information\n section. This URL is available once the Ambari cluster creation process has completed.  \n\n\n\n\n\n\nClick on the \nAmbari URL\n link.\n\n\n\n\n\n\nThe first time you access the server, your browser will attempt to confirm that the SSL Certificate is valid. Since Cloudbreak automatically generates a self-signed certificate, your browser will warn you about an Untrusted Connection and ask you to confirm a Security Exception. Depending on your browser, perform the steps below to proceed.\n\n\n\n\n\n\n\n\nBrowser\n\n\nSteps\n\n\n\n\n\n\n\n\n\n\nFirefox\n\n\nClick \nAdvanced\n \n Click \nAdd Exception...\n \n Click \nConfirm Security Exception\n\n\n\n\n\n\nSafari\n\n\nClick \nContinue\n\n\n\n\n\n\nChrome\n\n\nClick \nAdvanced\n \n Click \nProceed...\n\n\n\n\n\n\n\n\n\n\n\n\nCloudbreak User Accounts\n\n\nThe following table describes what credentials to use to access Cloudbreak and Cloudbreak-managed clusters:\n\n\n\n\n\n\n\n\nComponent\n\n\nMethod\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nCloudbreak\n\n\nWeb UI, CLI\n\n\nAccess with the username and password provided when launching Cloudbreak on the cloud provider.\n\n\n\n\n\n\nCloudbreak\n\n\nSSH to VM\n\n\nAccess as the \"cloudbreak\" user with the SSH key provided when launching Cloudbreak on the cloud provider.\n\n\n\n\n\n\nCluster\n\n\nSSH to VMs\n\n\nAccess as the \"cloudbreak\" user with the SSH key provided during cluster creation.\n\n\n\n\n\n\nCluster\n\n\nAmbari UI\n\n\nAccess with the credentials provided in the \u201cCluster User\u201d parameter during cluster creation.\n\n\n\n\n\n\n\n\n\n\nNext: Manage and Monitor Clusters", 
            "title": "Access Cluster"
        }, 
        {
            "location": "/aws-clusters-access/index.html#accessing-your-cluster", 
            "text": "The following section describes how to access the various services in the cluster.", 
            "title": "Accessing Your Cluster"
        }, 
        {
            "location": "/aws-clusters-access/index.html#finding-cluster-information-in-the-ui", 
            "text": "Once your cluster is up and running, click on the tile representing your cluster in the Cloudbreak UI to access information related the cluster and access cluster actions.      The information presented includes:   Cluster Summary     Cluster Information     Hardware     Tags      Recipes     Repository Details     Image Details       Network      Security     Autoscaling       Event History      \n   Tips \n   \n    Access cluster actions such as resize and sync by clicking on  ACTIONS . \n    Access Ambari web UI by clicking on the link in the  CLUSTER INFORMATION  section.   View public IP addresses for all cluster instances in the  HARDWARE  section. Click on the links to view the instances in the cloud console.   The SSH user that you must use when accessing cluster VMs is \"cloudbreak\".", 
            "title": "Finding Cluster Information in the UI"
        }, 
        {
            "location": "/aws-clusters-access/index.html#cluster-summary", 
            "text": "The summary bar includes the following information about your cluster:     Item  Description      Cluster Name  The name that you selected for your cluster is displayed at the top of the page.    Time Remaining  If you enabled lifetime management for your cluster, the clock next to the cluster name indicates the amount of time that your cluster will run before it gets terminated. Note that the time remaining counter does not stop when you stop the cluster.    Cloud Provider  The logo of the cloud provider on which the cluster is running.    Credential  The name of the credential used to create the cluster.    Status  Current status. When a cluster is healthy, the status is  Running .    Nodes  The current number of cluster nodes, including the master node.    Uptime  The amount of time (HH:MM) that the cluster has been in the running state since it was started. Each time you stop and restart the cluster, the running time is reset to 0.    Created  The date when the cluster was created. The date format is Mon DD, YYYY. For example: Oct 27, 2017.", 
            "title": "Cluster Summary"
        }, 
        {
            "location": "/aws-clusters-access/index.html#cluster-information", 
            "text": "Item  Description      Cluster User  The name of the cluster user that you created when creating the cluster.    SSH Username  The SSH user which you must use when accessing cluster VMs via SSH. The SSH user is always \"cloudbreak\".    Ambari URL  Link to the Ambari web UI.    Region  The region in which the cluster is running in the cloud provider infrastructure.    Availability Zone  The availability zone within the region in which the cluster is running.    Blueprint  The name of the blueprint selected under \"Cluster Type\" to create this cluster.    Created With  The version of Cloubdreak used to create this cluster.    Ambari Version  The Ambari version which this cluster is currently running.    HDP Version  The HDP version which this cluster is currently running.", 
            "title": "Cluster Information"
        }, 
        {
            "location": "/aws-clusters-access/index.html#hardware", 
            "text": "This section includes information about your cluster instances: instance names, instance IDs, instance types, their status, fully qualified domain names (FQDNs), and private and public IPs.", 
            "title": "Hardware"
        }, 
        {
            "location": "/aws-clusters-access/index.html#tags", 
            "text": "This section lists keys and values of the user-defined tags, in the same order as you added them.", 
            "title": "Tags"
        }, 
        {
            "location": "/aws-clusters-access/index.html#recipes", 
            "text": "This section includes recipe-related information. For each recipe, you can see the host group on which a recipe was executed, recipe name, and recipe type.", 
            "title": "Recipes"
        }, 
        {
            "location": "/aws-clusters-access/index.html#repository-details", 
            "text": "This section includes Ambari and HDP repository information, as you provided it in the \"Base Images\" section when creating a cluster.", 
            "title": "Repository Details"
        }, 
        {
            "location": "/aws-clusters-access/index.html#image-details", 
            "text": "This section includes information about the base image that was used for the Cloudbreak instance.", 
            "title": "Image Details"
        }, 
        {
            "location": "/aws-clusters-access/index.html#network", 
            "text": "This section includes information about the names of the network and subnet in which the cluster is running and the links to related cloud provider console.", 
            "title": "Network"
        }, 
        {
            "location": "/aws-clusters-access/index.html#security", 
            "text": "This section is only available if you have enabled Kerberos security. It provides you with the details of your Kerberos configuration.", 
            "title": "Security"
        }, 
        {
            "location": "/aws-clusters-access/index.html#autoscaling", 
            "text": "This section includes configuration options related to autoscaling. Refer to  Autoscaling .", 
            "title": "Autoscaling"
        }, 
        {
            "location": "/aws-clusters-access/index.html#event-history", 
            "text": "The Event History section shows you events logged for the cluster, with the most recent event at the top. For example, after your cluster has been created, the following messages will be written to the log:  \nAmbari cluster built; Ambari ip:34.215.103.66\n10/26/2017, 9:41:58 AM\nBuilding Ambari cluster; Ambari ip:34.215.103.66\n10/26/2017, 9:30:20 AM\nStarting Ambari cluster services\n10/26/2017, 9:27:12 AM\nSetting up infrastructure metadata\n10/26/2017, 9:27:11 AM\nBootstrapping infrastructure cluster\n10/26/2017, 9:26:38 AM\nInfrastructure successfully provisioned\n10/26/2017, 9:26:37 AM\nBilling started, Infrastructure successfully provisioned\n10/26/2017, 9:26:37 AM\nInfrastructure metadata collection finished\n10/26/2017, 9:25:39 AM\nInfrastructure creation took 194 seconds\n10/26/2017, 9:25:37 AM\nCreating infrastructure\n10/26/2017, 9:22:22 AM\nSetting up HDP image\n10/26/2017, 9:22:21 AM", 
            "title": "Event History"
        }, 
        {
            "location": "/aws-clusters-access/index.html#accessing-cluster-via-ssh", 
            "text": "If you plan to access the cluster via the command line clients, SSH into the master node instance in the cluster.    In order to use SSH, you must generate an SSH key pair or use an existing SSH key pair.    You can find the cluster instance public IP addresses on the cluster details page.    When accessing instances via SSH use the  cloudbreak  user.    On Mac OS, you can use the following syntax to SSH to the VM: ssh -i \"privatekey.pem\" cloudbreak@publicIP \nFor example: ssh -i \"dominika-kp.pem\" cloudbreak@p52.25.169.132  On Windows, you can SSH using an SSH client such as PuTTY.", 
            "title": "Accessing Cluster via SSH"
        }, 
        {
            "location": "/aws-clusters-access/index.html#access-ambari", 
            "text": "You can access Ambari web UI by clicking on the links provided in the  Cluster Information     Ambari URL .  Steps    From the cluster dashboard, click on the tile representing your cluster to navigate to cluster details.    Find the Ambari URL in the  Cluster Information  section. This URL is available once the Ambari cluster creation process has completed.      Click on the  Ambari URL  link.    The first time you access the server, your browser will attempt to confirm that the SSL Certificate is valid. Since Cloudbreak automatically generates a self-signed certificate, your browser will warn you about an Untrusted Connection and ask you to confirm a Security Exception. Depending on your browser, perform the steps below to proceed.     Browser  Steps      Firefox  Click  Advanced    Click  Add Exception...    Click  Confirm Security Exception    Safari  Click  Continue    Chrome  Click  Advanced    Click  Proceed...", 
            "title": "Access Ambari"
        }, 
        {
            "location": "/aws-clusters-access/index.html#cloudbreak-user-accounts", 
            "text": "The following table describes what credentials to use to access Cloudbreak and Cloudbreak-managed clusters:     Component  Method  Description      Cloudbreak  Web UI, CLI  Access with the username and password provided when launching Cloudbreak on the cloud provider.    Cloudbreak  SSH to VM  Access as the \"cloudbreak\" user with the SSH key provided when launching Cloudbreak on the cloud provider.    Cluster  SSH to VMs  Access as the \"cloudbreak\" user with the SSH key provided during cluster creation.    Cluster  Ambari UI  Access with the credentials provided in the \u201cCluster User\u201d parameter during cluster creation.      Next: Manage and Monitor Clusters", 
            "title": "Cloudbreak User Accounts"
        }, 
        {
            "location": "/aws-clusters-manage/index.html", 
            "text": "Managing and Monitoring Clusters\n\n\nYou can manage monitor your clusters from the Cloudbreak UI. To do that, click on the tile representing the cluster that you want to access. The actions available for your cluster are listed in the top right corner: \n\n\n \n\n\n\n  \nTips\n\n  \n\n  \nTo add or remove nodes from your cluster click \nACTIONS>Resize\n.\n\n  \nTo synchronize your cluster with the cloud provider account click \nACTIONS>Sync\n.\n\n  \nTo temporarily stop your cluster click \nSTOP\n.\n\n  \nTo terminate your cluster click \nTERMINATE\n.\n\n\n\n\n\n\n\n\n\nResize a Cluster\n\n\nTo resize a cluster, follow these steps.\n\n\nSteps\n\n\n\n\n\n\nBrowse to the cluster details.\n\n\n\n\n\n\nClick \nActions\n and select \nResize\n. The cluster resize dialog is displayed.\n\n\n\n\n\n\nUsing the +/- controls, adjust the number of nodes for a chosen host group. \n\n\n\n\nYou can only modify one host group at a time. \n\nIt is not possible to resize the Ambari server host group.     \n\n\n\n\n\n\n\n\nClick \nYes\n to confirm the scale-up/scale-down.\n\n\nWhile nodes are being added or removed, cluster status changes to \"Update In Progress\". Once the operation has completed, cluster status changes back to \"Running\". \n\n\n\n\n\n\nSynchronize a Cluster\n\n\nUse the \nsync\n option if you:  \n\n\n\n\nMade changes on your cloud provider side (for example, deleted cluster VMs) and you would like to synchronize Cloudbreak with the cloud provider.  \n\n\nManually changed service status in Ambari (for example, restarted services).   \n\n\n\n\nTo synchronize your cluster with the cloud provider, follow these steps. \n\n\nSteps\n\n\n\n\n\n\nBrowse to the cluster details.\n\n\n\n\n\n\nClick \nActions\n and select \nSync\n.\n\n\n\n\n\n\nClick \nYes\n to confirm.\n\n\nYour cluster infrastructure is synchronized based on changes on the cloud provider. The updates are written to the \"Event History\". \n\n\n\n\n\n\nStop a Cluster\n\n\nCloudbreak supports stopping and restarting clusters. To stop and restart a cluster managed by Cloudbreak, use the options available from the Coudbreak UI. \n\n\nSteps\n\n\n\n\n\n\nBrowse to the cluster details.\n\n\n\n\n\n\nClick \nStop\n to stop a currently running cluster.  \n\n\n\n\n\n\nClick \nYes\n to confirm. \n\n\n\n\n\n\nYour cluster status changes to \"Stopping in progress\" and then to \"Stopped\". Once stopping the infrastructure has completed, you will see a \nStart\n option to restart your cluster. \n\n\n\n\n\n\nWhen a cluster is in the \"stopped\" state, you are not charged for the VMs, but you are charged for external storage.  \n\n\nRestart a Cluster\n\n\nIf your cluster is in the \"Stopped\" state, you can restart the cluster by follow these steps.\n\n\nSteps\n\n\n\n\n\n\nclick \nStart\n. This option is only available when the cluster has been stopped. \n\n\n\n\n\n\nClick \nYes\n to confirm.\n\n\nYour cluster status changes to \"Start in progress\" and then to \"Running\". \n\n\n\n\n\n\nTerminate a Cluster\n\n\nTo terminate a cluster managed by Cloudbreak, use the option available from the Coudbreak UI. \n\n\nSteps\n\n\n\n\n\n\nBrowse to the cluster details.\n\n\n\n\n\n\nClick \nTerminate\n. \n\n\n\n\n\n\nClick \nYes\n to confirm.\n\n\nAll cluster-related resources will be deleted, unless the resources (such as networks and subnets) existed prior to cluster creation or are used by other VMs in which case they will be preserved. \n\n\n\n\n\n\nForce Terminate a Cluster\n\n\nCluster deletion may fail if Cloudbreak is unable to delete one or more of the cloud resources that were part of your cluster infrastructure. In such as case, you can use the \nTerminate\n \n \nForce terminate\n option to remove the cluster entry from the Cloudbreak web UI, but you must also check your cloud provider account to see if there are any resources that must be deleted manually.\n\n\nSteps\n\n\n\n\n\n\nBrowse to the cluster details.\n\n\n\n\n\n\nClick \nTerminate\n. \n\n\n\n\n\n\nCheck  \nForce terminate\n.\n\n\n\n\n\n\nClick \nYes\n to confirm. \n\n\nWhen terminating a cluster with Kerberos enabled, you have an option to disable Kerberos prior to cluster termination. This option removes any cluster-related principals from the KDC.\n\n\n\n\n\n\nThis deletes the cluster tile from the UI.  \n\n\n\n\n\n\nLog in to your cloud provider account and \nmanually delete\n any resources that failed to be deleted.\n\n\n\n\n\n\nView Cluster History\n\n\nFrom the navigation menu in the Cloudbreak UI, you can access the History page that allows you to generate a report showing basic information related to the clusters that were running within the specified range of dates.\n\n\nTo generate a report, follow these steps.\n\n\nSteps\n\n\n\n\n\n\nFrom the Cloudbreak UI navigation menu, select \nHistory\n.\n\n\n\n\n\n\nOn the History page, select the range of dates and click \nShow History\n to generate a report for the selected period.\n\n\n \n\n\n\n\n\n\nHistory Report Content\n\n\nEach entry in the report represents one cluster instance group. For each entry, the report includes the following information:\n\n\n\n\nCreated\n - The date when your cluster was created (YYYY-MM-DD).\n\n\nProvider\n - The name of the cloud provider (AWS, Azure, Google, or OpenStack) on which the cluster instances are/were running.\n\n\nCluster Name\n - The name that you selected for the cluster.  \n\n\nInstance Group\n - The name of the host group.   \n\n\nInstance Count\n - The number of nodes in the host group. This number may be a decimal if a cluster has been resized.\n\n\nInstance Type\n - Provider-specific VM type of the cluster instances. \n\n\nRegion\n - The AWS region in which your cluster is/was running.\n\n\nAvailability Zone\n - The availability zone in which your cluster is/was running.      \n\n\nRunning Time (hours)\n - The sum of the running times for all the nodes in the instance group.\n\n\n\n\nThe \nAGGREGATE RUNNING TIME\n is the sum of the Running Times, adjusted for the selected time range.\n\n\nTo learn about how your cloud provider bills you for the VMs, refer to their documentation:\n\n\n\n\nAWS\n      \n\n\nAzure\n     \n\n\nGCP\n   \n\n\n\n\n\n\nNext: Access Data", 
            "title": "Manage and Monitor Clusters"
        }, 
        {
            "location": "/aws-clusters-manage/index.html#managing-and-monitoring-clusters", 
            "text": "You can manage monitor your clusters from the Cloudbreak UI. To do that, click on the tile representing the cluster that you want to access. The actions available for your cluster are listed in the top right corner:      \n   Tips \n   \n   To add or remove nodes from your cluster click  ACTIONS>Resize . \n   To synchronize your cluster with the cloud provider account click  ACTIONS>Sync . \n   To temporarily stop your cluster click  STOP . \n   To terminate your cluster click  TERMINATE .", 
            "title": "Managing and Monitoring Clusters"
        }, 
        {
            "location": "/aws-clusters-manage/index.html#resize-a-cluster", 
            "text": "To resize a cluster, follow these steps.  Steps    Browse to the cluster details.    Click  Actions  and select  Resize . The cluster resize dialog is displayed.    Using the +/- controls, adjust the number of nodes for a chosen host group.    You can only modify one host group at a time.  \nIt is not possible to resize the Ambari server host group.          Click  Yes  to confirm the scale-up/scale-down.  While nodes are being added or removed, cluster status changes to \"Update In Progress\". Once the operation has completed, cluster status changes back to \"Running\".", 
            "title": "Resize a Cluster"
        }, 
        {
            "location": "/aws-clusters-manage/index.html#synchronize-a-cluster", 
            "text": "Use the  sync  option if you:     Made changes on your cloud provider side (for example, deleted cluster VMs) and you would like to synchronize Cloudbreak with the cloud provider.    Manually changed service status in Ambari (for example, restarted services).      To synchronize your cluster with the cloud provider, follow these steps.   Steps    Browse to the cluster details.    Click  Actions  and select  Sync .    Click  Yes  to confirm.  Your cluster infrastructure is synchronized based on changes on the cloud provider. The updates are written to the \"Event History\".", 
            "title": "Synchronize a Cluster"
        }, 
        {
            "location": "/aws-clusters-manage/index.html#stop-a-cluster", 
            "text": "Cloudbreak supports stopping and restarting clusters. To stop and restart a cluster managed by Cloudbreak, use the options available from the Coudbreak UI.   Steps    Browse to the cluster details.    Click  Stop  to stop a currently running cluster.      Click  Yes  to confirm.     Your cluster status changes to \"Stopping in progress\" and then to \"Stopped\". Once stopping the infrastructure has completed, you will see a  Start  option to restart your cluster.     When a cluster is in the \"stopped\" state, you are not charged for the VMs, but you are charged for external storage.", 
            "title": "Stop a Cluster"
        }, 
        {
            "location": "/aws-clusters-manage/index.html#restart-a-cluster", 
            "text": "If your cluster is in the \"Stopped\" state, you can restart the cluster by follow these steps.  Steps    click  Start . This option is only available when the cluster has been stopped.     Click  Yes  to confirm.  Your cluster status changes to \"Start in progress\" and then to \"Running\".", 
            "title": "Restart a Cluster"
        }, 
        {
            "location": "/aws-clusters-manage/index.html#terminate-a-cluster", 
            "text": "To terminate a cluster managed by Cloudbreak, use the option available from the Coudbreak UI.   Steps    Browse to the cluster details.    Click  Terminate .     Click  Yes  to confirm.  All cluster-related resources will be deleted, unless the resources (such as networks and subnets) existed prior to cluster creation or are used by other VMs in which case they will be preserved.", 
            "title": "Terminate a Cluster"
        }, 
        {
            "location": "/aws-clusters-manage/index.html#force-terminate-a-cluster", 
            "text": "Cluster deletion may fail if Cloudbreak is unable to delete one or more of the cloud resources that were part of your cluster infrastructure. In such as case, you can use the  Terminate     Force terminate  option to remove the cluster entry from the Cloudbreak web UI, but you must also check your cloud provider account to see if there are any resources that must be deleted manually.  Steps    Browse to the cluster details.    Click  Terminate .     Check   Force terminate .    Click  Yes  to confirm.   When terminating a cluster with Kerberos enabled, you have an option to disable Kerberos prior to cluster termination. This option removes any cluster-related principals from the KDC.    This deletes the cluster tile from the UI.      Log in to your cloud provider account and  manually delete  any resources that failed to be deleted.", 
            "title": "Force Terminate a Cluster"
        }, 
        {
            "location": "/aws-clusters-manage/index.html#view-cluster-history", 
            "text": "From the navigation menu in the Cloudbreak UI, you can access the History page that allows you to generate a report showing basic information related to the clusters that were running within the specified range of dates.  To generate a report, follow these steps.  Steps    From the Cloudbreak UI navigation menu, select  History .    On the History page, select the range of dates and click  Show History  to generate a report for the selected period.", 
            "title": "View Cluster History"
        }, 
        {
            "location": "/aws-clusters-manage/index.html#history-report-content", 
            "text": "Each entry in the report represents one cluster instance group. For each entry, the report includes the following information:   Created  - The date when your cluster was created (YYYY-MM-DD).  Provider  - The name of the cloud provider (AWS, Azure, Google, or OpenStack) on which the cluster instances are/were running.  Cluster Name  - The name that you selected for the cluster.    Instance Group  - The name of the host group.     Instance Count  - The number of nodes in the host group. This number may be a decimal if a cluster has been resized.  Instance Type  - Provider-specific VM type of the cluster instances.   Region  - The AWS region in which your cluster is/was running.  Availability Zone  - The availability zone in which your cluster is/was running.        Running Time (hours)  - The sum of the running times for all the nodes in the instance group.   The  AGGREGATE RUNNING TIME  is the sum of the Running Times, adjusted for the selected time range.  To learn about how your cloud provider bills you for the VMs, refer to their documentation:   AWS         Azure        GCP        Next: Access Data", 
            "title": "History Report Content"
        }, 
        {
            "location": "/aws-data/index.html", 
            "text": "Access Data on S3\n\n\nAmazon S3 is not supported as a default file system, but access to data in Amazon S3 is possible via the s3a connector. \n\n\nPrerequisites\n\n\nTo use S3 storage, you must have one or more S3 buckets on your AWS account. For instructions on how to create a bucket on S3, refer to \nAWS documentation\n.\n\n\nRelated Links\n\n\nCreate a Bucket\n (External)    \n\n\nCreating an IAM Role for S3 Access\n\n\nIn order to configure access from your cluster to Amazon S3, you must have an existing IAM role which determines what actions can be performed on which S3 buckets. If you do not have an existing IAM role, use these steps to create one. \n\n\nSteps\n\n\n\n\n\n\nNavigate to the \nIAM console\n \n \nRoles\n and click \nCreate Role\n.\n\n\n \n\n\n\n\n\n\nIn the \"Create Role\" wizard, select \nAWS service\n role type and then select \nEC2\n service and \nEC2\n use case. \n\n\n \n\n\n\n\n\n\nWhen done, click \nNext: Permissions\n to navigate to the next page in the wizard.\n\n\n\n\n\n\nSelect an existing S3 access policy or click \nCreate policy\n to define a new policy. If you are just getting started, you can select a built-in policy called \"AmazonS3FullAccess\", which provides full access to S3 buckets that are part of your account:\n\n\n\n\n\n\n\n\nWhen done attaching the policy, click \nNext: Review\n.\n\n\n\n\n\n\nIn the \nRoles name\n field, enter a name for the role that you are creating:  \n\n\n \n\n\n\n\n\n\nClick \nCreate role\n to finish the role creation process.\n\n\n\n\n\n\nConfiguring Access to S3\n\n\nAmazon S3 is not supported as a default file system, but access to data in S3 from your cluster VMs can be automatically configured by attaching an \ninstance profile\n allowing access to S3. You can optionally create or attach an existing instance profile during cluster creation on the \nFile System\n page.\n\n\nTo configure access to S3 with an instance profile, follow these steps. \n\n\nSteps\n\n\n\n\nYou or your AWS admin must create an IAM role with an S3 access policy which can be used by cluster instances to access one or more S3 buckets. Refer to \nCreating an IAM Role for S3 Access\n.  \n\n\nOn the \nFile System\n page in the advanced cluster wizard view, select \nUse existing instance profile\n. \n\n\n\n\nSelect an existing IAM role created in step 1:\n\n\n \n\n\n\n\n\n\nDuring the cluster creation process, Cloudbreak assigns the IAM role and its associated permissions to the EC2 instances that are part of the cluster so that applications running on these instances can use the role to access S3.   \n\n\nTesting Access to S3\n\n\nTo tests access to S3, SSH to a cluster node and run a few hadoop fs shell commands against your existing S3 bucket.\n\n\nAmazon S3 access path syntax is:\n\n\ns3a://bucket/dir/file\n\n\n\nFor example, to access a file called \"mytestfile\" in a directory called \"mytestdir\", which is stored in a bucket called \"mytestbucket\", the URL is:\n\n\ns3a://mytestbucket/mytestdir/mytestfile\n\n\n\nThe following FileSystem shell commands demonstrate access to a bucket named \"mytestbucket\": \n\n\nhadoop fs -ls s3a://mytestbucket/\n\nhadoop fs -mkdir s3a://mytestbucket/testDir\n\nhadoop fs -put testFile s3a://mytestbucket/testFile\n\nhadoop fs -cat s3a://mytestbucket/testFile\ntest file content\n\n\n\nWorking with S3\n\n\nFor more information about configuring the S3 connector and working with data stored on S3, refer to \nCloud Data Access\n documentation.\n\n\nRelated Links\n\n\nCloud Data Access\n (Hortonworks)", 
            "title": "Access Data on S3"
        }, 
        {
            "location": "/aws-data/index.html#access-data-on-s3", 
            "text": "Amazon S3 is not supported as a default file system, but access to data in Amazon S3 is possible via the s3a connector.", 
            "title": "Access Data on S3"
        }, 
        {
            "location": "/aws-data/index.html#prerequisites", 
            "text": "To use S3 storage, you must have one or more S3 buckets on your AWS account. For instructions on how to create a bucket on S3, refer to  AWS documentation .  Related Links  Create a Bucket  (External)", 
            "title": "Prerequisites"
        }, 
        {
            "location": "/aws-data/index.html#creating-an-iam-role-for-s3-access", 
            "text": "In order to configure access from your cluster to Amazon S3, you must have an existing IAM role which determines what actions can be performed on which S3 buckets. If you do not have an existing IAM role, use these steps to create one.   Steps    Navigate to the  IAM console     Roles  and click  Create Role .       In the \"Create Role\" wizard, select  AWS service  role type and then select  EC2  service and  EC2  use case.        When done, click  Next: Permissions  to navigate to the next page in the wizard.    Select an existing S3 access policy or click  Create policy  to define a new policy. If you are just getting started, you can select a built-in policy called \"AmazonS3FullAccess\", which provides full access to S3 buckets that are part of your account:     When done attaching the policy, click  Next: Review .    In the  Roles name  field, enter a name for the role that you are creating:         Click  Create role  to finish the role creation process.", 
            "title": "Creating an IAM Role for S3 Access"
        }, 
        {
            "location": "/aws-data/index.html#configuring-access-to-s3", 
            "text": "Amazon S3 is not supported as a default file system, but access to data in S3 from your cluster VMs can be automatically configured by attaching an  instance profile  allowing access to S3. You can optionally create or attach an existing instance profile during cluster creation on the  File System  page.  To configure access to S3 with an instance profile, follow these steps.   Steps   You or your AWS admin must create an IAM role with an S3 access policy which can be used by cluster instances to access one or more S3 buckets. Refer to  Creating an IAM Role for S3 Access .    On the  File System  page in the advanced cluster wizard view, select  Use existing instance profile .    Select an existing IAM role created in step 1:       During the cluster creation process, Cloudbreak assigns the IAM role and its associated permissions to the EC2 instances that are part of the cluster so that applications running on these instances can use the role to access S3.", 
            "title": "Configuring Access to S3"
        }, 
        {
            "location": "/aws-data/index.html#testing-access-to-s3", 
            "text": "To tests access to S3, SSH to a cluster node and run a few hadoop fs shell commands against your existing S3 bucket.  Amazon S3 access path syntax is:  s3a://bucket/dir/file  For example, to access a file called \"mytestfile\" in a directory called \"mytestdir\", which is stored in a bucket called \"mytestbucket\", the URL is:  s3a://mytestbucket/mytestdir/mytestfile  The following FileSystem shell commands demonstrate access to a bucket named \"mytestbucket\":   hadoop fs -ls s3a://mytestbucket/\n\nhadoop fs -mkdir s3a://mytestbucket/testDir\n\nhadoop fs -put testFile s3a://mytestbucket/testFile\n\nhadoop fs -cat s3a://mytestbucket/testFile\ntest file content", 
            "title": "Testing Access to S3"
        }, 
        {
            "location": "/aws-data/index.html#working-with-s3", 
            "text": "For more information about configuring the S3 connector and working with data stored on S3, refer to  Cloud Data Access  documentation.  Related Links  Cloud Data Access  (Hortonworks)", 
            "title": "Working with S3"
        }, 
        {
            "location": "/azure-launch/index.html", 
            "text": "Launching Cloudbreak on Azure\n\n\nBefore launching Cloudbreak on Azure, review and meet the prerequisites. Next, launch Cloudbreak using one of the two available methods. Once Cloudbreak is running, log in to the Cloudbreak UI and create a Cloudbreak credential. \n\n\nMeet the Prerequisites\n\n\nBefore launching Cloudbreak on Azure, you must meet the following prerequisites.\n\n\nAzure Account\n\n\nIn order to launch Cloudbreak on the Azure, log in to your existing Microsoft Azure account. If you don't have an account, you can set it up at \nhttps://azure.microsoft.com\n.\n\n\nAzure Roles\n\n\nIn order to provision clusters on Azure, Cloudbreak must be able to assume a sufficient Azure role (\"Owner\" or \"Contributor\") via Cloudbreak credential: \n\n\n\n\n\n\nYour account must have the \"\nOwner\n\" role (or a role with equivalent permissions) in the subscription in order to \ncreate a Cloudbreak credential\n using the interactive credential method. \n\n\n\n\n\n\nYour account must have the \"\nContributor\n\" role (or a role with equivalent permissions) in the subscription in order to \ncreate a Cloudbreak credential\n using the app-based credential method. The role must be assigned to the app that you register in Cloudbreak.\n\n\n\n\n\n\nTo check the roles in your subscription, log in to your Azure account and navigate to \nSubscriptions\n. \n\n\nRelated Links\n\n\nBuilt-in Roles: Owner\n (External)\n\n\nBuilt-in Roles: Contributor\n (External)\n\n\nAzure Region\n\n\nDecide in which Azure region you would like to launch Cloudbreak. You can launch Cloudbreak and provision your clusters in all regions \nsupported by Microsoft Azure\n. \n\n\nClusters created via Cloudbreak can be in the same or different region as Cloudbreak; when you launch a cluster, you select the region in which to launch it.\n\n\nRelated Links\n\n\nAzure Regions\n (External) \n\n\nSSH Key Pair\n\n\nWhen launching Cloudbreak, you will be required to provide your public SSH key. If needed, you can generate a new SSH keypair:\n\n\n\n\nOn MacOS X and Linux using \nssh-keygen -t rsa -b 4096 -C \"your_email@example.com\"\n  \n\n\nOn Windows using \nPuTTygen\n\n\n\n\nLaunch Cloudbreak\n\n\nLaunch Cloudbreak deployer using the following steps.\n\n\nSteps\n\n\n\n\n\n\nLog in to your \nAzure Portal\n.\n\n\n\n\n\n\nClick here to get started with Cloudbreak installation using the Azure Resource Manager template:\n\n\n \n\n\n\n\n\n\nThe template for installing Cloudbreak will appear. On the \nBasics\n page, provide the following basic parameters:   \n\n\n\n\nAll parameters except \"SmartSense Id\" are required. \n\n\n\n\nBASICS\n\n\n\n\n\n\n\n\nParameter\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nSubscription\n\n\nSelect which existing subscription you want to use.\n\n\n\n\n\n\nResource group\n\n\nSelect an existing resource group or create a new one by selecting \nCreate new\n and entering a name for your new resource group. Cloudbreak resources will later be accessible in that chosen resource group.\n\n\n\n\n\n\nLocation\n\n\nSelect an Azure region in which you want to deploy Cloudbreak.\n\n\n\n\n\n\n\n\nSETTINGS\n\n\n\n\n\n\n\n\nParameter\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nBase Url\n\n\nThis is the URL to the page where the template is stored.\n\n\n\n\n\n\nLocation\n\n\nThis is an internal parameter. Do not change it.\n\n\n\n\n\n\nVm Size\n\n\nSelect virtual machine instance type to use for the Cloudbreak controller. The minimum instance type suitable for Cloudbreak is \nD2\n.\n\n\n\n\n\n\nAdmin Username\n\n\nCreate an admin login that you will use to log in to the Cloudbreak UI. Must be a valid email address. By default, admin@example.com is used but you should change it to your email address.\n\n\n\n\n\n\nAdmin User Password\n\n\nPassword for the admin login. Must be at least 8 characters containing letters, numbers, and symbols.\n\n\n\n\n\n\nUsername\n\n\nEnter an admin username for the virtual machine. You will use it to SSH to the VM.\n\n\n\n\n\n\nSmartSense\n\n\nSelect whether you want to use \nSmartSense telemetry\n. Default is \"false\" (not using SmartSense telemetry).\n\n\n\n\n\n\nSmartSense Id\n\n\n(Optional) Your SmartSense ID.\n\n\n\n\n\n\nRemote Location\n\n\nEnter a valid \nCIDR IP\n or use one of the default tags. Default value is \nInternet\n which allows access from all IP addresses. Examples: \n10.0.0.0/24 will allow access from 10.0.0.0 through 10.0.0.255\n'Internet' will allow access from all. This is not a secure option but you can use it it you are just getting started and are not planning to have the instance on for a longer period. \n(Advanced) 'VirtualNetwork' will allow access from the address space of the Virtual Network.\n (Advanced) 'AzureLoadBalancer' will allow access from the address space of the load balancer.\nFor more information, refer to the \nAzure documentation\n.\n\n\n\n\n\n\nSsh Key\n\n\nPaste your SSH public key.\nYou can use \npbcopy\n to quickly copy it. For example: \npbcopy \n /Users/homedir/.ssh/id_rsa.pub\n\n\n\n\n\n\nVnet New Or Existing\n\n\nBy default, Cloudbreak is launched in a new VNet called \ncbdeployerVnet\n and a new subnet called \ncbdeployerSubnet\n; if needed, you can customize the settings for the new VNet using available VNet and Subnet parameters.\n\n\n\n\n\n\nVnet Name\n\n\nProvide the name for a new Vnet. Default is \n`cbdeployerVnet\n.\n\n\n\n\n\n\nVnet Subnet Name\n\n\nProvide a name for a new subnet. Default is \ncbdeployerSubnet\n.\n\n\n\n\n\n\nVnet Address Prefix\n\n\nProvide a CIDR for the virtual network. Default is \n10.0.0.0/16\n.\n\n\n\n\n\n\nVnet Subnet Address Prefix\n\n\nProvide a CIDR for the subnet. Default is \n10.0.0.0/24\n.\n\n\n\n\n\n\nVnet RG Name\n\n\nThe name of the resource group in which the Vnet is located. If creating a new Vnet, enter the same resource group name as provided in the \nResource group\n field in the \nBASICS\n section.\n\n\n\n\n\n\n\n\n\n\n\n\nReview terms of use and check \"I agree to the terms and conditions stated above\". \n\n\n\n\n\n\nClick \nPurchase\n.\n\n\n\n\n\n\nYour deployment should be initiated.  \n\n\n\n\nIf you encounter errors, refer to \nTroubleshooting Azure\n.   \n\n\n\n\n\n\n\n\nProceed to the next step: \nExplore Newly Created Resources\n.\n\n\n\n\n\n\nRelated Links\n\n\nCIDR IP\n (External) \n\n\nFilter Network Traffic with Network Security Groups\n (External)  \n\n\nExplore Newly Created Resources\n\n\n\n\nThis step is optional. \n\n\n\n\nWhile the deployment is in progress, you can optionally navigate to the newly created resource group and see what Azure resources are being provisioned.\n\n\nSteps\n\n\n\n\n\n\nFrom the left pane, select \nResource groups\n.\n\n\n\n\n\n\nFind the the resource group that you just created and select it to view details.\n\n\n\n\n\n\nThe following resources should have been created in your resource group:\n\n\n\n\n\n\nIf you chose to use an existing virtual network, the virtual network will not be added to the resource group. \n\n\n\n\n\n\nVirtual network\n (VNet) securely connects Azure resources to each other.    \n\n\nNetwork security group\n (NSG) defines inbound and outbound security rules, which control network traffic flow.  \n\n\nVirtual machine\n runs Cloudbreak.   \n\n\nPublic IP address\n is assigned to your VM so that it can communicate with other Azure resources.  \n\n\nNetwork interface\n (NIC) attached to the VM provides the interconnection between the VM and the underlying software network.    \n\n\nBlob storage container\n is created to store Cloudbreak Deployer OS disk's data.  \n\n\n\n\n\n\n\n\nYou can click on each entry to view details of the resource. For example, click on \ncbdeployerVM\n to view details, including Cloudbreak IP address.\n\n\n\n\n\n\nOnce your deployment is ready, the status will change from \"Deploying\" to \"Success\".\n\n\n\n\n\n\nAccess Cloudbreak UI\n\n\nLog in to the Cloudbreak UI using the following steps.\n\n\nSteps\n\n\n\n\n\n\nWhen your deployment succeeds, you will receive a notification in the top-right corner. You can click on the link provided to navigate to the resource group created earlier.\n\n\n\n\nThis only works right after deployment. At other times, you can find your resource group by selecting \nResource Groups\n from the service menu and then finding your resource group by name.\n\n\n\n\n\n\n\n\nOnce you've navigated to your resource group, click on \nDeployments\n and then click on \nMicrosoft.Template\n:\n\n\n \n\n\n\n\n\n\nFrom \nOutputs\n, you can copy the link by clicking on the copy icon:\n\n\n   \n\n\n\n\n\n\nPaste the link in your browser's address bar.\n\n\nThe first time you access Cloudbreak UI, Cloudbreak will automatically generate a self-signed certificate, due to which your browser will warn you about an untrusted connection and will ask you to confirm a security exception. You can safely proceed to the website. \n\n\n\n\n\n\n\n\nBrowser\n\n\nSteps\n\n\n\n\n\n\n\n\n\n\nFirefox\n\n\nClick \nAdvanced\n \n Click \nAdd Exception...\n \n Click \nConfirm Security Exception\n\n\n\n\n\n\nSafari\n\n\nClick \nContinue\n\n\n\n\n\n\nChrome\n\n\nClick \nAdvanced\n \n Click \nProceed...\n\n\n\n\n\n\n\n\n\n\n\n\nNow you should be able to access Cloudbreak UI and log in with the \nAdmin email address\n and \nAdmin password\n that you created when launching Cloudbreak:\n\n\n\n\n\n\n\n\nUpon a successful login, you are redirected to the dashboard:\n\n\n  \n\n\n\n\n\n\nThe last task that you need to perform before you can use Cloudbreak is to \ncreate a cloudbreak credential\n.         \n\n\nCreate Cloudbreak Credential\n\n\nBefore you can start creating clusters, you must first create a \nCloudbreak credential\n. Without this credential, you will not be able to create clusters via Cloudbreak. Cloudbreak works by connecting your Azure account through this credential, and then uses it to create resources on your behalf.\n\n\nThere are two methods for creating a Cloudbreak credential:\n\n\n\n\n\n\n\n\nMethod\n\n\nDescription\n\n\nPrerequisite\n\n\nSteps\n\n\n\n\n\n\n\n\n\n\nInteractive\n\n\nThe advantage of using this method is that the app and service principal creation and role assignment are fully automated, so the only input that you need to provide is the Subscription ID and Directory ID. During the interactive credential creation, you are required to log in to your Azure account.\n\n\n(1) Your account must have the \"Owner\" role (or its equivalent) in the subscription. (2) You must be able log in to your Azure account.\n\n\nTo configure an interactive credential, refer to \nCreate an Interactive Credential\n.\n\n\n\n\n\n\nApp-based\n\n\nThe advantage of the app-based credential creation is that it allows you to create a credential without logging in to the Azure account, as long as you have been given all the information. In addition to providing your Subscription ID and  Directory ID, you must provide information for your previously created Azure AD application (its ID and key which allows access to it).\n\n\n(1) Your account must have the \"Contributor\" role (or equivalent) in the subscription. (2) You or your Azure administrator must perform prerequisite steps of registering an Azure application and assigning the  \"Contributor\" role to it. This step typically requires admin permissions so you may have to contact your Azure administrator.\n\n\nTo configure an app based credential, refer to \nCreate an App Based Credential\n.\n\n\n\n\n\n\n\n\nCreate an Interactive Credential\n\n\nFollow these steps to create an interactive Cloudbreak credential.\n\n\nSteps\n\n\n\n\n\n\nIn the Cloudbreak web UI, select \nCredentials\n from the navigation pane. \n\n\n\n\n\n\nClick \nCreate Credential\n. \n\n\n\n\n\n\nUnder \nCloud provider\n, select \"Microsoft Azure\". \n\n\n\n\n\n\nSelect \nInteractive Login\n:\n\n\n     \n\n\n\n\n\n\nProvide the following information:\n\n\n\n\n\n\n\n\nParameter\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nName\n\n\nEnter a name for your credential.\n\n\n\n\n\n\nDescription\n\n\n(Optional) Enter a description.\n\n\n\n\n\n\nSubscription Id\n\n\nCopy and paste the Subscription ID from your \nSubscriptions\n.\n\n\n\n\n\n\nTenant Id\n\n\nCopy and paste your Directory ID from your \nActive Directory\n \n \nProperties\n.\n\n\n\n\n\n\nAzure role type\n\n\nYou have the following options:\n\"Use existing Contributor role\" (default): If you select this option, Cloudbreak will use the \"\nContributor\n\" role to create resources. This requires no further input.\n\"Reuse existing custom role\": If you select this option and enter the name of an existing role, Cloudbreak will use this role to create resources.\n\"Let Cloudbreak create a custom role\": If you select this option and enter a name for the new role, the role will be created. When choosing role name, make sure that there is no existing role with the name chosen. For information on creating custom roles, refer to \nAzure\n documentation. \nIf using a custom role, make sure that it includes the necessary Action set for Cloudbreak to be able to manage clusters: \nMicrosoft.Compute/*\n, \nMicrosoft.Network/*\n, \nMicrosoft.Storage/*\n, \nMicrosoft.Resources/*\n.\n\n\n\n\n\n\n\n\nTo obtain the \nSubscription Id\n: \n\n\n   \n\n\nTo obtain the \nTenant ID\n (actually \nDirectory Id\n): \n\n\n    \n\n\n\n\n\n\nAfter providing the parameters, click \nInteractive Login\n.\n\n\n\n\n\n\nCopy the code provided in the UI:\n\n\n     \n\n\n\n\n\n\nClick \nAzure login\n and a new \nDevice login\n page will open in a new browser tab:\n\n\n  \n\n\n\n\n\n\nNext, paste the code in field on the  \nDevice login\n page and click \nContinue\n.\n\n\n\n\n\n\nConfirm your account by selecting it:\n\n\n\n\n\n\n\n\nA confirmation page will appear, confirming that you have signed in to the Microsoft Azure Cross-platform Command Line Interface application on your device. You may now close this window.\n\n\n\n\nIf you encounter errors, refer to \nTroubleshooting Azure\n. \n\n\n\n\nCongratulations! You've successfully launched and configured Cloudbreak. Now you can use Cloudbreak to \ncreate clusters\n.\n\n\n\n\n\n\nCreate an App Based Credential\n\n\nFollow these steps to create an app based Cloudbreak credential.\n\n\nPrerequisites\n\n\n\n\n\n\nOn Azure Portal, navigate to the \nActive Directory\n \n \nApp Registrations\n and register a new application. For more information, refer to \nCreate an Azure AD Application\n.\n\n\n\n\nAa an alternative to the steps listed below for creating an application registration, you use a utility called \nazure-cli-tools\n. The utility supports app creation and role assignment. It is available at \nhttps://github.com/sequenceiq/azure-cli-tools/blob/master/cli_tools\n.\n\n\n\n\n  \n\n\n\n\n\n\nNavigate to the \nSubscriptions\n, choose \nAccess control (IAM)\n. Click \nAdd\n and then assign the \"Contributor\" role to your newly created application by selecting \"Contributor\" under \nRole\n and your app name under \nSelect\n:\n\n\n\n\nThis step typically requires admin permissions so you may have to contact your Azure administrator.\n\n\n\n\n   \n\n\n\n\n\n\nSteps\n\n\n\n\n\n\nIn the Cloudbreak web UI, select \nCredentials\n from the navigation pane. \n\n\n\n\n\n\nClick \nCreate Credential\n. \n\n\n\n\n\n\nUnder \nCloud provider\n, select \"Microsoft Azure\". \n\n\n\n\n\n\nSelect \nApp based Login\n:\n\n\n \n\n\n\n\n\n\nOn the \nConfigure credential\n page, provide the following parameters:\n\n\n\n\n\n\n\n\nParameter\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nSelect Credential Type\n\n\nSelect \nApp based\n.\n\n\n\n\n\n\nName\n\n\nEnter a name for your credential.\n\n\n\n\n\n\nDescription\n\n\n(Optional) Enter a description.\n\n\n\n\n\n\nSubscription Id\n\n\nCopy and paste the Subscription ID from your \nSubscriptions\n.\n\n\n\n\n\n\nTenant Id\n\n\nCopy and paste your Directory ID from your \nActive Directory\n \n \nProperties\n.\n\n\n\n\n\n\nApp Id\n\n\nCopy and paste the Application ID from your \nAzure Active Directory\n \n \nApp Registrations\n \n your app registration's \nSettings\n \n \nProperties\n.\n\n\n\n\n\n\nPassword\n\n\nThis is your application key. You can generate it from your from your \nAzure Active Directory\n app registration's \nSettings\n \n \nKeys\n.\n\n\n\n\n\n\n\n\nTo obtain the \nSubscription Id\n from Subscriptions: \n\n\n   \n\n\nTo obtain the \nApp ID\n (actually \nApplication ID\n) and an application key from Azure Active Directory: \n\n\n  \n\n\n      \n\n\nTo obtain the \nTenant ID\n (actually \nDirectory Id\n) from Azure Active Directory: \n\n\n  \n\n\n\n\n\n\nClick \nCreate\n.\n\n\n\n\nIf you encounter errors, refer to \nTroubleshooting Azure\n.  \n\n\n\n\nCongratulations! You have successfully launched Cloudbreak and created a Cloudbreak credential. Now you can use Cloudbreak to \ncreate clusters\n.\n\n\n\n\n\n\nRelated Links\n\n\nCLI Tools\n (Hortonworks)  \n\n\nUse Portal to Create an Azure Active Directory Application\n (External)     \n\n\n\n\nNext: Create a Cluster", 
            "title": "Launch on Azure"
        }, 
        {
            "location": "/azure-launch/index.html#launching-cloudbreak-on-azure", 
            "text": "Before launching Cloudbreak on Azure, review and meet the prerequisites. Next, launch Cloudbreak using one of the two available methods. Once Cloudbreak is running, log in to the Cloudbreak UI and create a Cloudbreak credential.", 
            "title": "Launching Cloudbreak on Azure"
        }, 
        {
            "location": "/azure-launch/index.html#meet-the-prerequisites", 
            "text": "Before launching Cloudbreak on Azure, you must meet the following prerequisites.", 
            "title": "Meet the Prerequisites"
        }, 
        {
            "location": "/azure-launch/index.html#azure-account", 
            "text": "In order to launch Cloudbreak on the Azure, log in to your existing Microsoft Azure account. If you don't have an account, you can set it up at  https://azure.microsoft.com .", 
            "title": "Azure Account"
        }, 
        {
            "location": "/azure-launch/index.html#azure-roles", 
            "text": "In order to provision clusters on Azure, Cloudbreak must be able to assume a sufficient Azure role (\"Owner\" or \"Contributor\") via Cloudbreak credential:     Your account must have the \" Owner \" role (or a role with equivalent permissions) in the subscription in order to  create a Cloudbreak credential  using the interactive credential method.     Your account must have the \" Contributor \" role (or a role with equivalent permissions) in the subscription in order to  create a Cloudbreak credential  using the app-based credential method. The role must be assigned to the app that you register in Cloudbreak.    To check the roles in your subscription, log in to your Azure account and navigate to  Subscriptions .   Related Links  Built-in Roles: Owner  (External)  Built-in Roles: Contributor  (External)", 
            "title": "Azure Roles"
        }, 
        {
            "location": "/azure-launch/index.html#azure-region", 
            "text": "Decide in which Azure region you would like to launch Cloudbreak. You can launch Cloudbreak and provision your clusters in all regions  supported by Microsoft Azure .   Clusters created via Cloudbreak can be in the same or different region as Cloudbreak; when you launch a cluster, you select the region in which to launch it.  Related Links  Azure Regions  (External)", 
            "title": "Azure Region"
        }, 
        {
            "location": "/azure-launch/index.html#ssh-key-pair", 
            "text": "When launching Cloudbreak, you will be required to provide your public SSH key. If needed, you can generate a new SSH keypair:   On MacOS X and Linux using  ssh-keygen -t rsa -b 4096 -C \"your_email@example.com\"     On Windows using  PuTTygen", 
            "title": "SSH Key Pair"
        }, 
        {
            "location": "/azure-launch/index.html#launch-cloudbreak", 
            "text": "Launch Cloudbreak deployer using the following steps.  Steps    Log in to your  Azure Portal .    Click here to get started with Cloudbreak installation using the Azure Resource Manager template:       The template for installing Cloudbreak will appear. On the  Basics  page, provide the following basic parameters:      All parameters except \"SmartSense Id\" are required.    BASICS     Parameter  Description      Subscription  Select which existing subscription you want to use.    Resource group  Select an existing resource group or create a new one by selecting  Create new  and entering a name for your new resource group. Cloudbreak resources will later be accessible in that chosen resource group.    Location  Select an Azure region in which you want to deploy Cloudbreak.     SETTINGS     Parameter  Description      Base Url  This is the URL to the page where the template is stored.    Location  This is an internal parameter. Do not change it.    Vm Size  Select virtual machine instance type to use for the Cloudbreak controller. The minimum instance type suitable for Cloudbreak is  D2 .    Admin Username  Create an admin login that you will use to log in to the Cloudbreak UI. Must be a valid email address. By default, admin@example.com is used but you should change it to your email address.    Admin User Password  Password for the admin login. Must be at least 8 characters containing letters, numbers, and symbols.    Username  Enter an admin username for the virtual machine. You will use it to SSH to the VM.    SmartSense  Select whether you want to use  SmartSense telemetry . Default is \"false\" (not using SmartSense telemetry).    SmartSense Id  (Optional) Your SmartSense ID.    Remote Location  Enter a valid  CIDR IP  or use one of the default tags. Default value is  Internet  which allows access from all IP addresses. Examples:  10.0.0.0/24 will allow access from 10.0.0.0 through 10.0.0.255 'Internet' will allow access from all. This is not a secure option but you can use it it you are just getting started and are not planning to have the instance on for a longer period.  (Advanced) 'VirtualNetwork' will allow access from the address space of the Virtual Network.  (Advanced) 'AzureLoadBalancer' will allow access from the address space of the load balancer. For more information, refer to the  Azure documentation .    Ssh Key  Paste your SSH public key. You can use  pbcopy  to quickly copy it. For example:  pbcopy   /Users/homedir/.ssh/id_rsa.pub    Vnet New Or Existing  By default, Cloudbreak is launched in a new VNet called  cbdeployerVnet  and a new subnet called  cbdeployerSubnet ; if needed, you can customize the settings for the new VNet using available VNet and Subnet parameters.    Vnet Name  Provide the name for a new Vnet. Default is  `cbdeployerVnet .    Vnet Subnet Name  Provide a name for a new subnet. Default is  cbdeployerSubnet .    Vnet Address Prefix  Provide a CIDR for the virtual network. Default is  10.0.0.0/16 .    Vnet Subnet Address Prefix  Provide a CIDR for the subnet. Default is  10.0.0.0/24 .    Vnet RG Name  The name of the resource group in which the Vnet is located. If creating a new Vnet, enter the same resource group name as provided in the  Resource group  field in the  BASICS  section.       Review terms of use and check \"I agree to the terms and conditions stated above\".     Click  Purchase .    Your deployment should be initiated.     If you encounter errors, refer to  Troubleshooting Azure .        Proceed to the next step:  Explore Newly Created Resources .    Related Links  CIDR IP  (External)   Filter Network Traffic with Network Security Groups  (External)", 
            "title": "Launch Cloudbreak"
        }, 
        {
            "location": "/azure-launch/index.html#explore-newly-created-resources", 
            "text": "This step is optional.    While the deployment is in progress, you can optionally navigate to the newly created resource group and see what Azure resources are being provisioned.  Steps    From the left pane, select  Resource groups .    Find the the resource group that you just created and select it to view details.    The following resources should have been created in your resource group:    If you chose to use an existing virtual network, the virtual network will not be added to the resource group.     Virtual network  (VNet) securely connects Azure resources to each other.      Network security group  (NSG) defines inbound and outbound security rules, which control network traffic flow.    Virtual machine  runs Cloudbreak.     Public IP address  is assigned to your VM so that it can communicate with other Azure resources.    Network interface  (NIC) attached to the VM provides the interconnection between the VM and the underlying software network.      Blob storage container  is created to store Cloudbreak Deployer OS disk's data.       You can click on each entry to view details of the resource. For example, click on  cbdeployerVM  to view details, including Cloudbreak IP address.    Once your deployment is ready, the status will change from \"Deploying\" to \"Success\".", 
            "title": "Explore Newly Created Resources"
        }, 
        {
            "location": "/azure-launch/index.html#access-cloudbreak-ui", 
            "text": "Log in to the Cloudbreak UI using the following steps.  Steps    When your deployment succeeds, you will receive a notification in the top-right corner. You can click on the link provided to navigate to the resource group created earlier.   This only works right after deployment. At other times, you can find your resource group by selecting  Resource Groups  from the service menu and then finding your resource group by name.     Once you've navigated to your resource group, click on  Deployments  and then click on  Microsoft.Template :       From  Outputs , you can copy the link by clicking on the copy icon:         Paste the link in your browser's address bar.  The first time you access Cloudbreak UI, Cloudbreak will automatically generate a self-signed certificate, due to which your browser will warn you about an untrusted connection and will ask you to confirm a security exception. You can safely proceed to the website.      Browser  Steps      Firefox  Click  Advanced    Click  Add Exception...    Click  Confirm Security Exception    Safari  Click  Continue    Chrome  Click  Advanced    Click  Proceed...       Now you should be able to access Cloudbreak UI and log in with the  Admin email address  and  Admin password  that you created when launching Cloudbreak:     Upon a successful login, you are redirected to the dashboard:        The last task that you need to perform before you can use Cloudbreak is to  create a cloudbreak credential .", 
            "title": "Access Cloudbreak UI"
        }, 
        {
            "location": "/azure-launch/index.html#create-cloudbreak-credential", 
            "text": "Before you can start creating clusters, you must first create a  Cloudbreak credential . Without this credential, you will not be able to create clusters via Cloudbreak. Cloudbreak works by connecting your Azure account through this credential, and then uses it to create resources on your behalf.  There are two methods for creating a Cloudbreak credential:     Method  Description  Prerequisite  Steps      Interactive  The advantage of using this method is that the app and service principal creation and role assignment are fully automated, so the only input that you need to provide is the Subscription ID and Directory ID. During the interactive credential creation, you are required to log in to your Azure account.  (1) Your account must have the \"Owner\" role (or its equivalent) in the subscription. (2) You must be able log in to your Azure account.  To configure an interactive credential, refer to  Create an Interactive Credential .    App-based  The advantage of the app-based credential creation is that it allows you to create a credential without logging in to the Azure account, as long as you have been given all the information. In addition to providing your Subscription ID and  Directory ID, you must provide information for your previously created Azure AD application (its ID and key which allows access to it).  (1) Your account must have the \"Contributor\" role (or equivalent) in the subscription. (2) You or your Azure administrator must perform prerequisite steps of registering an Azure application and assigning the  \"Contributor\" role to it. This step typically requires admin permissions so you may have to contact your Azure administrator.  To configure an app based credential, refer to  Create an App Based Credential .", 
            "title": "Create Cloudbreak Credential"
        }, 
        {
            "location": "/azure-launch/index.html#create-an-interactive-credential", 
            "text": "Follow these steps to create an interactive Cloudbreak credential.  Steps    In the Cloudbreak web UI, select  Credentials  from the navigation pane.     Click  Create Credential .     Under  Cloud provider , select \"Microsoft Azure\".     Select  Interactive Login :           Provide the following information:     Parameter  Description      Name  Enter a name for your credential.    Description  (Optional) Enter a description.    Subscription Id  Copy and paste the Subscription ID from your  Subscriptions .    Tenant Id  Copy and paste your Directory ID from your  Active Directory     Properties .    Azure role type  You have the following options: \"Use existing Contributor role\" (default): If you select this option, Cloudbreak will use the \" Contributor \" role to create resources. This requires no further input. \"Reuse existing custom role\": If you select this option and enter the name of an existing role, Cloudbreak will use this role to create resources. \"Let Cloudbreak create a custom role\": If you select this option and enter a name for the new role, the role will be created. When choosing role name, make sure that there is no existing role with the name chosen. For information on creating custom roles, refer to  Azure  documentation.  If using a custom role, make sure that it includes the necessary Action set for Cloudbreak to be able to manage clusters:  Microsoft.Compute/* ,  Microsoft.Network/* ,  Microsoft.Storage/* ,  Microsoft.Resources/* .     To obtain the  Subscription Id :        To obtain the  Tenant ID  (actually  Directory Id ):           After providing the parameters, click  Interactive Login .    Copy the code provided in the UI:           Click  Azure login  and a new  Device login  page will open in a new browser tab:        Next, paste the code in field on the   Device login  page and click  Continue .    Confirm your account by selecting it:     A confirmation page will appear, confirming that you have signed in to the Microsoft Azure Cross-platform Command Line Interface application on your device. You may now close this window.   If you encounter errors, refer to  Troubleshooting Azure .    Congratulations! You've successfully launched and configured Cloudbreak. Now you can use Cloudbreak to  create clusters .", 
            "title": "Create an Interactive Credential"
        }, 
        {
            "location": "/azure-launch/index.html#create-an-app-based-credential", 
            "text": "Follow these steps to create an app based Cloudbreak credential.  Prerequisites    On Azure Portal, navigate to the  Active Directory     App Registrations  and register a new application. For more information, refer to  Create an Azure AD Application .   Aa an alternative to the steps listed below for creating an application registration, you use a utility called  azure-cli-tools . The utility supports app creation and role assignment. It is available at  https://github.com/sequenceiq/azure-cli-tools/blob/master/cli_tools .         Navigate to the  Subscriptions , choose  Access control (IAM) . Click  Add  and then assign the \"Contributor\" role to your newly created application by selecting \"Contributor\" under  Role  and your app name under  Select :   This step typically requires admin permissions so you may have to contact your Azure administrator.          Steps    In the Cloudbreak web UI, select  Credentials  from the navigation pane.     Click  Create Credential .     Under  Cloud provider , select \"Microsoft Azure\".     Select  App based Login :       On the  Configure credential  page, provide the following parameters:     Parameter  Description      Select Credential Type  Select  App based .    Name  Enter a name for your credential.    Description  (Optional) Enter a description.    Subscription Id  Copy and paste the Subscription ID from your  Subscriptions .    Tenant Id  Copy and paste your Directory ID from your  Active Directory     Properties .    App Id  Copy and paste the Application ID from your  Azure Active Directory     App Registrations    your app registration's  Settings     Properties .    Password  This is your application key. You can generate it from your from your  Azure Active Directory  app registration's  Settings     Keys .     To obtain the  Subscription Id  from Subscriptions:        To obtain the  App ID  (actually  Application ID ) and an application key from Azure Active Directory:               To obtain the  Tenant ID  (actually  Directory Id ) from Azure Active Directory:         Click  Create .   If you encounter errors, refer to  Troubleshooting Azure .     Congratulations! You have successfully launched Cloudbreak and created a Cloudbreak credential. Now you can use Cloudbreak to  create clusters .    Related Links  CLI Tools  (Hortonworks)    Use Portal to Create an Azure Active Directory Application  (External)        Next: Create a Cluster", 
            "title": "Create an App Based Credential"
        }, 
        {
            "location": "/azure-create/index.html", 
            "text": "Creating a Cluster on Azure\n\n\nUse these steps to create a cluster.\n\n\nPrerequisites\n\n\nIf you would like to use \nOozie\n with \nAmbari 2.6.1 or newer\n, you must install the Ext JS library. For instructions, refer to \nRecipe to Install Ext JS for Oozie\n.\n\n\nSteps\n\n\n\n\n\n\nLog in to the Cloudbreak UI.\n\n\n\n\n\n\nClick \nCreate Cluster\n and the \nCreate Cluster\n wizard is displayed.\n\n    By default, \nBasic\n view is displayed. To view advanced options, click \nAdvanced\n. To learn about advanced options, refer to \nAdvanced Options\n.\n\n\n \n\n\n\n\n\n\nOn the \nGeneral Configuration\n page, specify the following general parameters for your cluster:\n\n\n\n\n\n\n\n\nParameter\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nSelect Credential\n\n\nChoose a previously created credential.\n\n\n\n\n\n\nCluster Name\n\n\nEnter a name for your cluster. The name must be between 5 and 40 characters, must start with a letter, and must only include lowercase letters, numbers, and hyphens.\n\n\n\n\n\n\nRegion\n\n\nSelect the Azure region in which you would like to launch your cluster. For information on available Azure regions, refer to \nAzure documentation\n.\n\n\n\n\n\n\nPlatform Version\n\n\nChoose the HDP version to use for this cluster.\n\n\n\n\n\n\nCluster Type\n\n\nChoose one of default cluster configurations, or, if you have defined your own cluster configuration via Ambari blueprint, you can choose it here. For more information on default and custom blueprints, refer to \nBlueprints\n.\n\n\n\n\n\n\n\n\n\n\n\n\nOn the \nHardware and Storage\n page, for each host group provide the following information to define your cluster nodes and attached storage:\n\n\n\n\n\n\n\n\nParameter\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nInstance Type\n\n\nSelect an instance type. For information about instance types on Azure refer to \nAzure documentation\n.\n\n\n\n\n\n\nInstance Count\n\n\nEnter the number of instances of a given type. Default is 1.\n\n\n\n\n\n\nAmbari Server\n\n\nYou must select one node for Ambari Server. The \"Group Size\" for that host group must be set to \"1\".\n\n\n\n\n\n\n\n\n\n\n\n\nOn the \nNetwork\n page, provide the following to specify the networking resources that will be used for your cluster:\n\n\n\n\n\n\n\n\nParameter\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nSelect Network\n\n\nSelect the virtual network in which you would like your cluster to be provisioned. You can select an existing network or create a new network.\n\n\n\n\n\n\nSelect Subnet\n\n\nSelect the subnet in which you would like your cluster to be provisioned. If you are using a new network, create a new subnet. If you are using an existing network, select an existing subnet.\n\n\n\n\n\n\nSubnet (CIDR)\n\n\nIf you selected to create a new subnet, you must define a valid \nCIDR\n for the subnet. Default is 10.0.0.0/16.\n\n\n\n\n\n\n\n\n\n\n\n\nDefine security groups for each host group. You can either create new security groups and define their rules or reuse existing security groups:\n\n\n\n\n\n\n\n\nOption\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nNew Security Group\n\n\n(Default) Creates a new security group with the rules that you defined:\nA set of \ndefault rules\n is provided. You should review and adjust these default rules. If you do not make any modifications, default rules will be applied. \nYou may open ports by defining the CIDR, entering port range, selecting protocol and clicking \n+\n.\nYou may delete default or previously added rules using the delete icon.\nIf you don't want to use security group, remove the default rules.\n\n\n\n\n\n\nExisting Security Groups\n\n\nAllows you to select an existing security group that is already available in the selected provider region. This selection is disabled if no existing security groups are available in your chosen region.\n\n\n\n\n\n\n\n\n\n\nImportant\n\n\n\nBy default, ports 22, 443, and 9443 are set to 0.0.0.0/0 CIDR for inbound access on the Ambari node security group. We strongly recommend that you limit this CIDR, considering the following restrictions:\n\nPorts 22 and 9443 must be open to Cloudbreak's CIDR. You can set CB_DEFAULT_GATEWAY_CIDR in your Cloudbreak's Profile file in order to automatically open ports 22 and 9443 to your Cloudbrak IP. Refer to \nRestricting Inbound Access to Clusters\n.\n\n\nPort 22 must be open to your CIDR if you would like to access the master node via SSH.\n\n\nPort 443 must be open to your CIDR if you would like to access Cloudbreak web UI in a browser.\n\n\n\n\n\n\n\n\nImportant\n\n\n\nBy default, port 22 is set to 0.0.0.0/0 CIDR for inbound access on non-Ambari node security groups. We strongly recommend that you remove it.\n\n\n\n\n\n\n\n\nOn the \nSecurity\n page, provide the following parameters:\n\n\n\n\n\n\n\n\nParameter\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nCluster User\n\n\nYou can log in to the Ambari UI using this username. By default, this is set to \nadmin\n.\n\n\n\n\n\n\nPassword\n\n\nYou can log in to the Ambari UI using this password.\n\n\n\n\n\n\nConfirm Password\n\n\nConfirm the password.\n\n\n\n\n\n\nNew SSH public key\n\n\nCheck this option to specify a new public key and then enter the public key. You will use the matching private key to access your cluster nodes via SSH.\n\n\n\n\n\n\nExisting SSH public key\n\n\nSelect an existing public key. You will use the matching private key to access your cluster nodes via SSH. This is a default option as long as an existing SSH public key is available.\n\n\n\n\n\n\n\n\n\n\n\n\nClick on \nCreate Cluster\n to create a cluster.\n\n\n\n\n\n\nYou will be redirected to the Cloudbreak dashboard, and a new tile representing your cluster will appear at the top of the page.\n\n\n\n\n\n\nRelated Links\n\n\nBlueprints\n \n\n\nDefault Cluster Security Groups\n \n\n\nAzure Regions\n (External)   \n\n\nCIDR\n (External)\n\n\nGeneral Purpose Linux VM Sizes\n (External)  \n\n\nAdvanced Options\n\n\nClick on \nAdvanced\n to view and enter additional configuration options\n\n\nChoose Image Catalog\n\n\nBy default, \nChoose Image Catalog\n is set to the default image catalog that is provided with Cloudbreak. If you would like to use a different image catalog, you must first create and register it. For complete instructions, refer to \nCustom Images\n.\n\n\nRelated Links\n   \n\n\nCustom Images\n  \n\n\nPrewarmed and Base Images\n\n\nCloudbreak supports the following types of images for launching clusters:\n\n\n\n\n\n\n\n\nImage Type\n\n\nDescription\n\n\nDefault Images Provided\n\n\nSupport for Custom Images\n\n\n\n\n\n\n\n\n\n\nBase Images\n\n\nBase images include default configuration and default tooling. These images include the operating system but do not include Ambari or HDP software.\n\n\nYes\n\n\nYes\n\n\n\n\n\n\nPrewarmed Images\n\n\nBy default, Cloudbreak launches clusters from prewarmed images. Prewarmed images include the operating system as well as Ambari and HDP. The HDP and Ambari version used by prewarmed images cannot be customized.\n\n\nYes\n\n\nNo\n\n\n\n\n\n\n\n\nBy default, Cloudbreak uses the included default \nprewarmed images\n, which include the operating system, as well as\nAmbari and HDP packages installed. You can optionally select the \nbase image\n option if you would like to:\n\n\n\n\nUse an Ambari and HDP versions different than what the prewarmed image includes and/or  \n\n\nChoose a previously created custom base image\n\n\n\n\nChoose Image\n  \n\n\nIf under \nChoose Image Catalog\n, you selected a custom image catalog, under \nChoose Image\n you can select an image from that catalog. For complete instructions, refer to \nCustom Images\n. \n\n\nIf you are trying to customize Ambari and HDP versions, you can ignore the \nChoose Image\n option; in this case default base image is used.\n\n\nAmbari Repository Specification\n\n\nIf you would like to use a custom Ambari version, provide the following information: \n\n\n\n\n Ambari 2.6.1\n\n\nIf you would like to use Ambari \n2.6.1\n, use the version provided by default in the Cloudbreak web UI, or newer.\n\n\n\n\n\n\n\n\n\nParameter\n\n\nDescription\n\n\nExample\n\n\n\n\n\n\n\n\n\n\nVersion\n\n\nAmbari version.\n\n\n2.6.1.0\n\n\n\n\n\n\nRepo Url\n\n\nURL to the Ambari version repo.\n\n\nhttp://public-repo-1.hortonworks.com/ambari/centos6/2.x/updates/2.6.1.0\n\n\n\n\n\n\nRepo Gpg Key Url\n\n\nURL to the repo GPG key. Each stable RPM package that is published by CentOS Project is signed with a GPG signature. By default, yum and the graphical update tools will verify these signatures and refuse to install any packages that are not signed, or have an incorrect signature.\n\n\nhttp://public-repo-1.hortonworks.com/ambari/centos6/RPM-GPG-KEY/RPM-GPG-KEY-Jenkins\n\n\n\n\n\n\n\n\nHDP Repository Specification\n\n\nIf you would like to use a custom HDP version, provide the following information: \n\n\n\n\n\n\n\n\nParameter\n\n\nDescription\n\n\nExample\n\n\n\n\n\n\n\n\n\n\nStack\n\n\nStack name.\n\n\nHDP\n\n\n\n\n\n\nVersion\n\n\nStack version.\n\n\n2.6\n\n\n\n\n\n\nOS\n\n\nOperating system.\n\n\ncentos7\n\n\n\n\n\n\nStack Repo Id\n\n\nIdentifier for the repo linked in \"Base Url\".\n\n\nHDP-2.6\n\n\n\n\n\n\nBase Url\n\n\nURL to the repo storing the desired stack version.\n\n\nhttp://public-repo-1.hortonworks.com/HDP/centos7/2.x/updates/2.6.1.0\n\n\n\n\n\n\nUtils Repo Id\n\n\nIdentifier for the repo linked in \"Utils Base Url\".\n\n\nHDP-UTILS-1.1.0.21\n\n\n\n\n\n\nUtils Base Url\n\n\nURL to the repo storing utilities for the desired stack version.\n\n\nhttp://public-repo-1.hortonworks.com/HDP-UTILS-1.1.0.21/repos/centos7\n\n\n\n\n\n\nEnable Ambari Server to download and install GPL Licensed LZO packages?\n\n\n(Optional, only available if using Ambari 2.6.1 or newer) Use this option to enable LZO compression in your HDP cluster. LZO is a lossless data compression library that favors speed over compression ratio. Ambari does not install nor enable LZO compression libraries by default, and must be explicitly configured to do so. For more information, refer to \nEnabling LZO\n.\n\n\n\n\n\n\n\n\n\n\nRelated Links\n    \n\n\nCustom Images\n      \n\n\nEnable Lifetime Management\n\n\nCheck this option if you would like your cluster to be automatically terminated after a specific amount of time (defined as \"Time to Live\" in minutes). \n\n\nTags\n\n\nYou can optionally add tags, which will help you find your cluster-related resources, such as VMs, in your cloud provider account. refer to \nResource Tagging\n.\n\n\nRelated Links\n    \n\n\nResource Tagging\n \n\n\nStorage\n\n\nYou can optionally specify the following storage options for your cluster:\n\n\n\n\n\n\n\n\nParameter\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nStorage Type\n\n\nSelect the volume type. The options are:\nLocally-redundant storage\nGeo-redundant storage\nPremium locally-redundant storage\n For more information about these options refer to \nAzure documentation\n.\n\n\n\n\n\n\nAttached Volumes Per Instance\n\n\nEnter the number of volumes attached per instance. Default is 1.\n\n\n\n\n\n\nVolume Size (GB)\n\n\nEnter the size in GBs for each volume. Default is 100.\n\n\n\n\n\n\n\n\nAvailability Sets\n\n\nTo support fault tolerance for VMs, Azure uses the concept of \navailability sets\n. This allows two or more VMs to be mapped to multiple fault domains, each of which defines a group of virtual machines that share a common power source and a network switch. When adding VMs to an availability set, Azure automatically assigns each VM a fault domain. The SLA includes guarantees that during OS Patching in Azure or during maintenance operations, at least one VM belonging to a given fault domain will be available.\n\n\nIn Cloudbreak, an availability set is automatically configured during cluster creation for each non-Ambari host group with \"Instance Count\" that is set to 2 or larger. The assignment of fault domains is automated by Azure, so there is no option for this in Cloudbreak UI. \n\n\nCloudbreak allows you to configure the availability set on the advanced \nHardware and Storage\n page of the create cluster wizard by providing the following options for each host group:  \n\n\n\n\n\n\n\n\nParameter\n\n\nDescription\n\n\nDefault\n\n\n\n\n\n\n\n\n\n\nAvailability Set Name\n\n\nChoose a name for the availability set that will be created for the selected host group\n\n\nThe following convention is used: \"clustername-hostgroupname-as\"\n\n\n\n\n\n\nFault Domain Count\n\n\nThe number of fault domains.\n\n\n2 or 3, depending on the setting supported by Azure\n\n\n\n\n\n\nUpdate Domain Count\n\n\nThis number of update domains. This can be set to a number in range of 2-20.\n\n\n20\n\n\n\n\n\n\n\n\nAfter the deployment is finished, you can check the layout of the VMs inside an availability set on Azure Portal. You will find the \"Availability set\" resources corresponding to the host groups inside the deployment's resource group.\n\n\nRecipes\n\n\nThis option allows you to select previously uploaded recipes (scripts that can be run pre or post cluster deployment) for each host group. For more information on recipes, refer to \nRecipes\n. \n\n\nRelated Links\n    \n\n\nRecipes\n \n\n\nDon't Create Public IP\n\n\nThis option is available if you are creating a cluster in an existing network and subnet. Select this option if you don't want to use public IPs for the network. \n\n\nDon't Create New Firewall Rules\n\n\nThis option is available if you are creating a cluster in an existing network and subnet. Select this option if you don't want to create new firewall rules for the network. \n\n\nAmbari Server Master Key\n\n\nThe Ambari Server Master Key is used to configure Ambari to encrypt database and Kerberos credentials that are retained by Ambari as part of the Ambari setup.  \n\n\nEnable Azure Disk Encryption\n\n\nCheck this option if you would like to have your virtual machine disks encrypted using the Azure Disk Encryption capability provided by Azure. For more information, refer to \nAzure documentation\n.  \n\n\nEnable Kerberos Security\n\n\nSelect this option to enable Kerberos for your cluster. For information about available Kerberos options, refer to \nEnabling Kerberos Security\n. \n\n\nRelated Links\n    \n\n\nKerberos\n  \n\n\nIntroduction to Microsoft Azure Storage\n (External)  \n\n\n\n\nNext: Access Cluster", 
            "title": "Create a Cluster"
        }, 
        {
            "location": "/azure-create/index.html#creating-a-cluster-on-azure", 
            "text": "Use these steps to create a cluster.  Prerequisites  If you would like to use  Oozie  with  Ambari 2.6.1 or newer , you must install the Ext JS library. For instructions, refer to  Recipe to Install Ext JS for Oozie .  Steps    Log in to the Cloudbreak UI.    Click  Create Cluster  and the  Create Cluster  wizard is displayed. \n    By default,  Basic  view is displayed. To view advanced options, click  Advanced . To learn about advanced options, refer to  Advanced Options .       On the  General Configuration  page, specify the following general parameters for your cluster:     Parameter  Description      Select Credential  Choose a previously created credential.    Cluster Name  Enter a name for your cluster. The name must be between 5 and 40 characters, must start with a letter, and must only include lowercase letters, numbers, and hyphens.    Region  Select the Azure region in which you would like to launch your cluster. For information on available Azure regions, refer to  Azure documentation .    Platform Version  Choose the HDP version to use for this cluster.    Cluster Type  Choose one of default cluster configurations, or, if you have defined your own cluster configuration via Ambari blueprint, you can choose it here. For more information on default and custom blueprints, refer to  Blueprints .       On the  Hardware and Storage  page, for each host group provide the following information to define your cluster nodes and attached storage:     Parameter  Description      Instance Type  Select an instance type. For information about instance types on Azure refer to  Azure documentation .    Instance Count  Enter the number of instances of a given type. Default is 1.    Ambari Server  You must select one node for Ambari Server. The \"Group Size\" for that host group must be set to \"1\".       On the  Network  page, provide the following to specify the networking resources that will be used for your cluster:     Parameter  Description      Select Network  Select the virtual network in which you would like your cluster to be provisioned. You can select an existing network or create a new network.    Select Subnet  Select the subnet in which you would like your cluster to be provisioned. If you are using a new network, create a new subnet. If you are using an existing network, select an existing subnet.    Subnet (CIDR)  If you selected to create a new subnet, you must define a valid  CIDR  for the subnet. Default is 10.0.0.0/16.       Define security groups for each host group. You can either create new security groups and define their rules or reuse existing security groups:     Option  Description      New Security Group  (Default) Creates a new security group with the rules that you defined: A set of  default rules  is provided. You should review and adjust these default rules. If you do not make any modifications, default rules will be applied.  You may open ports by defining the CIDR, entering port range, selecting protocol and clicking  + . You may delete default or previously added rules using the delete icon. If you don't want to use security group, remove the default rules.    Existing Security Groups  Allows you to select an existing security group that is already available in the selected provider region. This selection is disabled if no existing security groups are available in your chosen region.      Important  \nBy default, ports 22, 443, and 9443 are set to 0.0.0.0/0 CIDR for inbound access on the Ambari node security group. We strongly recommend that you limit this CIDR, considering the following restrictions: Ports 22 and 9443 must be open to Cloudbreak's CIDR. You can set CB_DEFAULT_GATEWAY_CIDR in your Cloudbreak's Profile file in order to automatically open ports 22 and 9443 to your Cloudbrak IP. Refer to  Restricting Inbound Access to Clusters .  Port 22 must be open to your CIDR if you would like to access the master node via SSH.  Port 443 must be open to your CIDR if you would like to access Cloudbreak web UI in a browser.     Important  \nBy default, port 22 is set to 0.0.0.0/0 CIDR for inbound access on non-Ambari node security groups. We strongly recommend that you remove it.     On the  Security  page, provide the following parameters:     Parameter  Description      Cluster User  You can log in to the Ambari UI using this username. By default, this is set to  admin .    Password  You can log in to the Ambari UI using this password.    Confirm Password  Confirm the password.    New SSH public key  Check this option to specify a new public key and then enter the public key. You will use the matching private key to access your cluster nodes via SSH.    Existing SSH public key  Select an existing public key. You will use the matching private key to access your cluster nodes via SSH. This is a default option as long as an existing SSH public key is available.       Click on  Create Cluster  to create a cluster.    You will be redirected to the Cloudbreak dashboard, and a new tile representing your cluster will appear at the top of the page.    Related Links  Blueprints    Default Cluster Security Groups    Azure Regions  (External)     CIDR  (External)  General Purpose Linux VM Sizes  (External)", 
            "title": "Creating a Cluster on Azure"
        }, 
        {
            "location": "/azure-create/index.html#advanced-options", 
            "text": "Click on  Advanced  to view and enter additional configuration options", 
            "title": "Advanced Options"
        }, 
        {
            "location": "/azure-create/index.html#choose-image-catalog", 
            "text": "By default,  Choose Image Catalog  is set to the default image catalog that is provided with Cloudbreak. If you would like to use a different image catalog, you must first create and register it. For complete instructions, refer to  Custom Images .  Related Links      Custom Images", 
            "title": "Choose Image Catalog"
        }, 
        {
            "location": "/azure-create/index.html#prewarmed-and-base-images", 
            "text": "Cloudbreak supports the following types of images for launching clusters:     Image Type  Description  Default Images Provided  Support for Custom Images      Base Images  Base images include default configuration and default tooling. These images include the operating system but do not include Ambari or HDP software.  Yes  Yes    Prewarmed Images  By default, Cloudbreak launches clusters from prewarmed images. Prewarmed images include the operating system as well as Ambari and HDP. The HDP and Ambari version used by prewarmed images cannot be customized.  Yes  No     By default, Cloudbreak uses the included default  prewarmed images , which include the operating system, as well as\nAmbari and HDP packages installed. You can optionally select the  base image  option if you would like to:   Use an Ambari and HDP versions different than what the prewarmed image includes and/or    Choose a previously created custom base image   Choose Image     If under  Choose Image Catalog , you selected a custom image catalog, under  Choose Image  you can select an image from that catalog. For complete instructions, refer to  Custom Images .   If you are trying to customize Ambari and HDP versions, you can ignore the  Choose Image  option; in this case default base image is used.  Ambari Repository Specification  If you would like to use a custom Ambari version, provide the following information:     Ambari 2.6.1  If you would like to use Ambari  2.6.1 , use the version provided by default in the Cloudbreak web UI, or newer.     Parameter  Description  Example      Version  Ambari version.  2.6.1.0    Repo Url  URL to the Ambari version repo.  http://public-repo-1.hortonworks.com/ambari/centos6/2.x/updates/2.6.1.0    Repo Gpg Key Url  URL to the repo GPG key. Each stable RPM package that is published by CentOS Project is signed with a GPG signature. By default, yum and the graphical update tools will verify these signatures and refuse to install any packages that are not signed, or have an incorrect signature.  http://public-repo-1.hortonworks.com/ambari/centos6/RPM-GPG-KEY/RPM-GPG-KEY-Jenkins     HDP Repository Specification  If you would like to use a custom HDP version, provide the following information:      Parameter  Description  Example      Stack  Stack name.  HDP    Version  Stack version.  2.6    OS  Operating system.  centos7    Stack Repo Id  Identifier for the repo linked in \"Base Url\".  HDP-2.6    Base Url  URL to the repo storing the desired stack version.  http://public-repo-1.hortonworks.com/HDP/centos7/2.x/updates/2.6.1.0    Utils Repo Id  Identifier for the repo linked in \"Utils Base Url\".  HDP-UTILS-1.1.0.21    Utils Base Url  URL to the repo storing utilities for the desired stack version.  http://public-repo-1.hortonworks.com/HDP-UTILS-1.1.0.21/repos/centos7    Enable Ambari Server to download and install GPL Licensed LZO packages?  (Optional, only available if using Ambari 2.6.1 or newer) Use this option to enable LZO compression in your HDP cluster. LZO is a lossless data compression library that favors speed over compression ratio. Ambari does not install nor enable LZO compression libraries by default, and must be explicitly configured to do so. For more information, refer to  Enabling LZO .      Related Links       Custom Images", 
            "title": "Prewarmed and Base Images"
        }, 
        {
            "location": "/azure-create/index.html#enable-lifetime-management", 
            "text": "Check this option if you would like your cluster to be automatically terminated after a specific amount of time (defined as \"Time to Live\" in minutes).", 
            "title": "Enable Lifetime Management"
        }, 
        {
            "location": "/azure-create/index.html#tags", 
            "text": "You can optionally add tags, which will help you find your cluster-related resources, such as VMs, in your cloud provider account. refer to  Resource Tagging .  Related Links       Resource Tagging", 
            "title": "Tags"
        }, 
        {
            "location": "/azure-create/index.html#storage", 
            "text": "You can optionally specify the following storage options for your cluster:     Parameter  Description      Storage Type  Select the volume type. The options are: Locally-redundant storage Geo-redundant storage Premium locally-redundant storage  For more information about these options refer to  Azure documentation .    Attached Volumes Per Instance  Enter the number of volumes attached per instance. Default is 1.    Volume Size (GB)  Enter the size in GBs for each volume. Default is 100.", 
            "title": "Storage"
        }, 
        {
            "location": "/azure-create/index.html#availability-sets", 
            "text": "To support fault tolerance for VMs, Azure uses the concept of  availability sets . This allows two or more VMs to be mapped to multiple fault domains, each of which defines a group of virtual machines that share a common power source and a network switch. When adding VMs to an availability set, Azure automatically assigns each VM a fault domain. The SLA includes guarantees that during OS Patching in Azure or during maintenance operations, at least one VM belonging to a given fault domain will be available.  In Cloudbreak, an availability set is automatically configured during cluster creation for each non-Ambari host group with \"Instance Count\" that is set to 2 or larger. The assignment of fault domains is automated by Azure, so there is no option for this in Cloudbreak UI.   Cloudbreak allows you to configure the availability set on the advanced  Hardware and Storage  page of the create cluster wizard by providing the following options for each host group:       Parameter  Description  Default      Availability Set Name  Choose a name for the availability set that will be created for the selected host group  The following convention is used: \"clustername-hostgroupname-as\"    Fault Domain Count  The number of fault domains.  2 or 3, depending on the setting supported by Azure    Update Domain Count  This number of update domains. This can be set to a number in range of 2-20.  20     After the deployment is finished, you can check the layout of the VMs inside an availability set on Azure Portal. You will find the \"Availability set\" resources corresponding to the host groups inside the deployment's resource group.", 
            "title": "Availability Sets"
        }, 
        {
            "location": "/azure-create/index.html#recipes", 
            "text": "This option allows you to select previously uploaded recipes (scripts that can be run pre or post cluster deployment) for each host group. For more information on recipes, refer to  Recipes .   Related Links       Recipes", 
            "title": "Recipes"
        }, 
        {
            "location": "/azure-create/index.html#dont-create-public-ip", 
            "text": "This option is available if you are creating a cluster in an existing network and subnet. Select this option if you don't want to use public IPs for the network.", 
            "title": "Don't Create Public IP"
        }, 
        {
            "location": "/azure-create/index.html#dont-create-new-firewall-rules", 
            "text": "This option is available if you are creating a cluster in an existing network and subnet. Select this option if you don't want to create new firewall rules for the network.", 
            "title": "Don't Create New Firewall Rules"
        }, 
        {
            "location": "/azure-create/index.html#ambari-server-master-key", 
            "text": "The Ambari Server Master Key is used to configure Ambari to encrypt database and Kerberos credentials that are retained by Ambari as part of the Ambari setup.", 
            "title": "Ambari Server Master Key"
        }, 
        {
            "location": "/azure-create/index.html#enable-azure-disk-encryption", 
            "text": "Check this option if you would like to have your virtual machine disks encrypted using the Azure Disk Encryption capability provided by Azure. For more information, refer to  Azure documentation .", 
            "title": "Enable Azure Disk Encryption"
        }, 
        {
            "location": "/azure-create/index.html#enable-kerberos-security", 
            "text": "Select this option to enable Kerberos for your cluster. For information about available Kerberos options, refer to  Enabling Kerberos Security .   Related Links       Kerberos     Introduction to Microsoft Azure Storage  (External)     Next: Access Cluster", 
            "title": "Enable Kerberos Security"
        }, 
        {
            "location": "/azure-clusters-access/index.html", 
            "text": "Accessing Your Cluster\n\n\nThe following section describes how to access the various services in the cluster.\n\n\nFinding Cluster Information in the UI\n\n\nOnce your cluster is up and running, click on the tile representing your cluster in the Cloudbreak UI to access information related the cluster and access cluster actions. \n\n\n \n\n\nThe information presented includes:\n\n\n\n\nCluster Summary\n  \n\n\nCluster Information\n  \n\n\nHardware\n  \n\n\nTags\n   \n\n\nRecipes\n  \n\n\nRepository Details\n  \n\n\nImage Details\n    \n\n\nNetwork\n   \n\n\nSecurity\n  \n\n\nAutoscaling\n    \n\n\nEvent History\n  \n\n\n\n\n\n  \nTips\n\n  \n\n  \n Access cluster actions such as resize and sync by clicking on \nACTIONS\n.\n\n  \n Access Ambari web UI by clicking on the link in the \nCLUSTER INFORMATION\n section.\n\n\n View public IP addresses for all cluster instances in the \nHARDWARE\n section. Click on the links to view the instances in the cloud console.\n\n\n The SSH user that you must use when accessing cluster VMs is \"cloudbreak\".\n \n\n\n\n\n\n\n\n\nCluster Summary\n\n\nThe summary bar includes the following information about your cluster:\n\n\n\n\n\n\n\n\nItem\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nCluster Name\n\n\nThe name that you selected for your cluster is displayed at the top of the page.\n\n\n\n\n\n\nTime Remaining\n\n\nIf you enabled lifetime management for your cluster, the clock next to the cluster name indicates the amount of time that your cluster will run before it gets terminated. Note that the time remaining counter does not stop when you stop the cluster.\n\n\n\n\n\n\nCloud Provider\n\n\nThe logo of the cloud provider on which the cluster is running.\n\n\n\n\n\n\nCredential\n\n\nThe name of the credential used to create the cluster.\n\n\n\n\n\n\nStatus\n\n\nCurrent status. When a cluster is healthy, the status is \nRunning\n.\n\n\n\n\n\n\nNodes\n\n\nThe current number of cluster nodes, including the master node.\n\n\n\n\n\n\nUptime\n\n\nThe amount of time (HH:MM) that the cluster has been in the running state since it was started. Each time you stop and restart the cluster, the running time is reset to 0.\n\n\n\n\n\n\nCreated\n\n\nThe date when the cluster was created. The date format is Mon DD, YYYY. For example: Oct 27, 2017.\n\n\n\n\n\n\n\n\nCluster Information\n\n\n\n\n\n\n\n\nItem\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nCluster User\n\n\nThe name of the cluster user that you created when creating the cluster.\n\n\n\n\n\n\nSSH Username\n\n\nThe SSH user which you must use when accessing cluster VMs via SSH. The SSH user is always \"cloudbreak\".\n\n\n\n\n\n\nAmbari URL\n\n\nLink to the Ambari web UI.\n\n\n\n\n\n\nRegion\n\n\nThe region in which the cluster is running in the cloud provider infrastructure.\n\n\n\n\n\n\nAvailability Zone\n\n\nThe availability zone within the region in which the cluster is running.\n\n\n\n\n\n\nBlueprint\n\n\nThe name of the blueprint selected under \"Cluster Type\" to create this cluster.\n\n\n\n\n\n\nCreated With\n\n\nThe version of Cloubdreak used to create this cluster.\n\n\n\n\n\n\nAmbari Version\n\n\nThe Ambari version which this cluster is currently running.\n\n\n\n\n\n\nHDP Version\n\n\nThe HDP version which this cluster is currently running.\n\n\n\n\n\n\n\n\nHardware\n\n\nThis section includes information about your cluster instances: instance names, instance IDs, instance types, their status, fully qualified domain names (FQDNs), and private and public IPs. \n\n\nTags\n\n\nThis section lists keys and values of the user-defined tags, in the same order as you added them.\n\n\nRecipes\n\n\nThis section includes recipe-related information. For each recipe, you can see the host group on which a recipe was executed, recipe name, and recipe type. \n\n\nRepository Details\n\n\nThis section includes Ambari and HDP repository information, as you provided it in the \"Base Images\" section when creating a cluster.\n\n\nImage Details\n\n\nThis section includes information about the base image that was used for the Cloudbreak instance. \n\n\nNetwork\n\n\nThis section includes information about the names of the network and subnet in which the cluster is running and the links to related cloud provider console.\n\n\nSecurity\n\n\nThis section is only available if you have enabled Kerberos security. It provides you with the details of your Kerberos configuration.  \n\n\nAutoscaling\n\n\nThis section includes configuration options related to autoscaling. Refer to \nAutoscaling\n.  \n\n\nEvent History\n\n\nThe Event History section shows you events logged for the cluster, with the most recent event at the top. For example, after your cluster has been created, the following messages will be written to the log:\n\n\n\nAmbari cluster built; Ambari ip:34.215.103.66\n10/26/2017, 9:41:58 AM\nBuilding Ambari cluster; Ambari ip:34.215.103.66\n10/26/2017, 9:30:20 AM\nStarting Ambari cluster services\n10/26/2017, 9:27:12 AM\nSetting up infrastructure metadata\n10/26/2017, 9:27:11 AM\nBootstrapping infrastructure cluster\n10/26/2017, 9:26:38 AM\nInfrastructure successfully provisioned\n10/26/2017, 9:26:37 AM\nBilling started, Infrastructure successfully provisioned\n10/26/2017, 9:26:37 AM\nInfrastructure metadata collection finished\n10/26/2017, 9:25:39 AM\nInfrastructure creation took 194 seconds\n10/26/2017, 9:25:37 AM\nCreating infrastructure\n10/26/2017, 9:22:22 AM\nSetting up HDP image\n10/26/2017, 9:22:21 AM\n\n\n\nAccessing Cluster via SSH\n\n\nIf you plan to access the cluster via the command line clients, SSH into the master node instance in the cluster. \n\n\n\n\nIn order to use SSH, you must generate an SSH key pair or use an existing SSH key pair.  \n\n\nYou can find the cluster instance public IP addresses on the cluster details page.  \n\n\nWhen accessing instances via SSH use the \ncloudbreak\n user. \n\n\n\n\nOn Mac OS, you can use the following syntax to SSH to the VM:\n\nssh -i \"privatekey.pem\" cloudbreak@publicIP\n\nFor example:\n\nssh -i \"dominika-kp.pem\" cloudbreak@p52.25.169.132\n\n\nOn Windows, you can SSH using an SSH client such as PuTTY.\n\n\nAccess Ambari\n\n\nYou can access Ambari web UI by clicking on the links provided in the \nCluster Information\n \n \nAmbari URL\n.\n\n\nSteps\n\n\n\n\n\n\nFrom the cluster dashboard, click on the tile representing your cluster to navigate to cluster details.\n\n\n\n\n\n\nFind the Ambari URL in the \nCluster Information\n section. This URL is available once the Ambari cluster creation process has completed.  \n\n\n\n\n\n\nClick on the \nAmbari URL\n link.\n\n\n\n\n\n\nThe first time you access the server, your browser will attempt to confirm that the SSL Certificate is valid. Since Cloudbreak automatically generates a self-signed certificate, your browser will warn you about an Untrusted Connection and ask you to confirm a Security Exception. Depending on your browser, perform the steps below to proceed.\n\n\n\n\n\n\n\n\nBrowser\n\n\nSteps\n\n\n\n\n\n\n\n\n\n\nFirefox\n\n\nClick \nAdvanced\n \n Click \nAdd Exception...\n \n Click \nConfirm Security Exception\n\n\n\n\n\n\nSafari\n\n\nClick \nContinue\n\n\n\n\n\n\nChrome\n\n\nClick \nAdvanced\n \n Click \nProceed...\n\n\n\n\n\n\n\n\n\n\n\n\nCloudbreak User Accounts\n\n\nThe following table describes what credentials to use to access Cloudbreak and Cloudbreak-managed clusters:\n\n\n\n\n\n\n\n\nComponent\n\n\nMethod\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nCloudbreak\n\n\nWeb UI, CLI\n\n\nAccess with the username and password provided when launching Cloudbreak on the cloud provider.\n\n\n\n\n\n\nCloudbreak\n\n\nSSH to VM\n\n\nAccess as the \"cloudbreak\" user with the SSH key provided when launching Cloudbreak on the cloud provider.\n\n\n\n\n\n\nCluster\n\n\nSSH to VMs\n\n\nAccess as the \"cloudbreak\" user with the SSH key provided during cluster creation.\n\n\n\n\n\n\nCluster\n\n\nAmbari UI\n\n\nAccess with the credentials provided in the \u201cCluster User\u201d parameter during cluster creation.\n\n\n\n\n\n\n\n\n\n\nNext: Manage and Monitor Clusters", 
            "title": "Access Cluster"
        }, 
        {
            "location": "/azure-clusters-access/index.html#accessing-your-cluster", 
            "text": "The following section describes how to access the various services in the cluster.", 
            "title": "Accessing Your Cluster"
        }, 
        {
            "location": "/azure-clusters-access/index.html#finding-cluster-information-in-the-ui", 
            "text": "Once your cluster is up and running, click on the tile representing your cluster in the Cloudbreak UI to access information related the cluster and access cluster actions.      The information presented includes:   Cluster Summary     Cluster Information     Hardware     Tags      Recipes     Repository Details     Image Details       Network      Security     Autoscaling       Event History      \n   Tips \n   \n    Access cluster actions such as resize and sync by clicking on  ACTIONS . \n    Access Ambari web UI by clicking on the link in the  CLUSTER INFORMATION  section.   View public IP addresses for all cluster instances in the  HARDWARE  section. Click on the links to view the instances in the cloud console.   The SSH user that you must use when accessing cluster VMs is \"cloudbreak\".", 
            "title": "Finding Cluster Information in the UI"
        }, 
        {
            "location": "/azure-clusters-access/index.html#cluster-summary", 
            "text": "The summary bar includes the following information about your cluster:     Item  Description      Cluster Name  The name that you selected for your cluster is displayed at the top of the page.    Time Remaining  If you enabled lifetime management for your cluster, the clock next to the cluster name indicates the amount of time that your cluster will run before it gets terminated. Note that the time remaining counter does not stop when you stop the cluster.    Cloud Provider  The logo of the cloud provider on which the cluster is running.    Credential  The name of the credential used to create the cluster.    Status  Current status. When a cluster is healthy, the status is  Running .    Nodes  The current number of cluster nodes, including the master node.    Uptime  The amount of time (HH:MM) that the cluster has been in the running state since it was started. Each time you stop and restart the cluster, the running time is reset to 0.    Created  The date when the cluster was created. The date format is Mon DD, YYYY. For example: Oct 27, 2017.", 
            "title": "Cluster Summary"
        }, 
        {
            "location": "/azure-clusters-access/index.html#cluster-information", 
            "text": "Item  Description      Cluster User  The name of the cluster user that you created when creating the cluster.    SSH Username  The SSH user which you must use when accessing cluster VMs via SSH. The SSH user is always \"cloudbreak\".    Ambari URL  Link to the Ambari web UI.    Region  The region in which the cluster is running in the cloud provider infrastructure.    Availability Zone  The availability zone within the region in which the cluster is running.    Blueprint  The name of the blueprint selected under \"Cluster Type\" to create this cluster.    Created With  The version of Cloubdreak used to create this cluster.    Ambari Version  The Ambari version which this cluster is currently running.    HDP Version  The HDP version which this cluster is currently running.", 
            "title": "Cluster Information"
        }, 
        {
            "location": "/azure-clusters-access/index.html#hardware", 
            "text": "This section includes information about your cluster instances: instance names, instance IDs, instance types, their status, fully qualified domain names (FQDNs), and private and public IPs.", 
            "title": "Hardware"
        }, 
        {
            "location": "/azure-clusters-access/index.html#tags", 
            "text": "This section lists keys and values of the user-defined tags, in the same order as you added them.", 
            "title": "Tags"
        }, 
        {
            "location": "/azure-clusters-access/index.html#recipes", 
            "text": "This section includes recipe-related information. For each recipe, you can see the host group on which a recipe was executed, recipe name, and recipe type.", 
            "title": "Recipes"
        }, 
        {
            "location": "/azure-clusters-access/index.html#repository-details", 
            "text": "This section includes Ambari and HDP repository information, as you provided it in the \"Base Images\" section when creating a cluster.", 
            "title": "Repository Details"
        }, 
        {
            "location": "/azure-clusters-access/index.html#image-details", 
            "text": "This section includes information about the base image that was used for the Cloudbreak instance.", 
            "title": "Image Details"
        }, 
        {
            "location": "/azure-clusters-access/index.html#network", 
            "text": "This section includes information about the names of the network and subnet in which the cluster is running and the links to related cloud provider console.", 
            "title": "Network"
        }, 
        {
            "location": "/azure-clusters-access/index.html#security", 
            "text": "This section is only available if you have enabled Kerberos security. It provides you with the details of your Kerberos configuration.", 
            "title": "Security"
        }, 
        {
            "location": "/azure-clusters-access/index.html#autoscaling", 
            "text": "This section includes configuration options related to autoscaling. Refer to  Autoscaling .", 
            "title": "Autoscaling"
        }, 
        {
            "location": "/azure-clusters-access/index.html#event-history", 
            "text": "The Event History section shows you events logged for the cluster, with the most recent event at the top. For example, after your cluster has been created, the following messages will be written to the log:  \nAmbari cluster built; Ambari ip:34.215.103.66\n10/26/2017, 9:41:58 AM\nBuilding Ambari cluster; Ambari ip:34.215.103.66\n10/26/2017, 9:30:20 AM\nStarting Ambari cluster services\n10/26/2017, 9:27:12 AM\nSetting up infrastructure metadata\n10/26/2017, 9:27:11 AM\nBootstrapping infrastructure cluster\n10/26/2017, 9:26:38 AM\nInfrastructure successfully provisioned\n10/26/2017, 9:26:37 AM\nBilling started, Infrastructure successfully provisioned\n10/26/2017, 9:26:37 AM\nInfrastructure metadata collection finished\n10/26/2017, 9:25:39 AM\nInfrastructure creation took 194 seconds\n10/26/2017, 9:25:37 AM\nCreating infrastructure\n10/26/2017, 9:22:22 AM\nSetting up HDP image\n10/26/2017, 9:22:21 AM", 
            "title": "Event History"
        }, 
        {
            "location": "/azure-clusters-access/index.html#accessing-cluster-via-ssh", 
            "text": "If you plan to access the cluster via the command line clients, SSH into the master node instance in the cluster.    In order to use SSH, you must generate an SSH key pair or use an existing SSH key pair.    You can find the cluster instance public IP addresses on the cluster details page.    When accessing instances via SSH use the  cloudbreak  user.    On Mac OS, you can use the following syntax to SSH to the VM: ssh -i \"privatekey.pem\" cloudbreak@publicIP \nFor example: ssh -i \"dominika-kp.pem\" cloudbreak@p52.25.169.132  On Windows, you can SSH using an SSH client such as PuTTY.", 
            "title": "Accessing Cluster via SSH"
        }, 
        {
            "location": "/azure-clusters-access/index.html#access-ambari", 
            "text": "You can access Ambari web UI by clicking on the links provided in the  Cluster Information     Ambari URL .  Steps    From the cluster dashboard, click on the tile representing your cluster to navigate to cluster details.    Find the Ambari URL in the  Cluster Information  section. This URL is available once the Ambari cluster creation process has completed.      Click on the  Ambari URL  link.    The first time you access the server, your browser will attempt to confirm that the SSL Certificate is valid. Since Cloudbreak automatically generates a self-signed certificate, your browser will warn you about an Untrusted Connection and ask you to confirm a Security Exception. Depending on your browser, perform the steps below to proceed.     Browser  Steps      Firefox  Click  Advanced    Click  Add Exception...    Click  Confirm Security Exception    Safari  Click  Continue    Chrome  Click  Advanced    Click  Proceed...", 
            "title": "Access Ambari"
        }, 
        {
            "location": "/azure-clusters-access/index.html#cloudbreak-user-accounts", 
            "text": "The following table describes what credentials to use to access Cloudbreak and Cloudbreak-managed clusters:     Component  Method  Description      Cloudbreak  Web UI, CLI  Access with the username and password provided when launching Cloudbreak on the cloud provider.    Cloudbreak  SSH to VM  Access as the \"cloudbreak\" user with the SSH key provided when launching Cloudbreak on the cloud provider.    Cluster  SSH to VMs  Access as the \"cloudbreak\" user with the SSH key provided during cluster creation.    Cluster  Ambari UI  Access with the credentials provided in the \u201cCluster User\u201d parameter during cluster creation.      Next: Manage and Monitor Clusters", 
            "title": "Cloudbreak User Accounts"
        }, 
        {
            "location": "/azure-clusters-manage/index.html", 
            "text": "Managing and Monitoring Clusters\n\n\nYou can manage monitor your clusters from the Cloudbreak UI. To do that, click on the tile representing the cluster that you want to access. The actions available for your cluster are listed in the top right corner: \n\n\n \n\n\n\n  \nTips\n\n  \n\n  \nTo add or remove nodes from your cluster click \nACTIONS>Resize\n.\n\n  \nTo synchronize your cluster with the cloud provider account click \nACTIONS>Sync\n.\n\n  \nTo temporarily stop your cluster click \nSTOP\n.\n\n  \nTo terminate your cluster click \nTERMINATE\n.\n\n\n\n\n\n\n\n\n\nResize a Cluster\n\n\nTo resize a cluster, follow these steps.\n\n\nSteps\n\n\n\n\n\n\nBrowse to the cluster details.\n\n\n\n\n\n\nClick \nActions\n and select \nResize\n. The cluster resize dialog is displayed.\n\n\n\n\n\n\nUsing the +/- controls, adjust the number of nodes for a chosen host group. \n\n\n\n\nYou can only modify one host group at a time. \n\nIt is not possible to resize the Ambari server host group.     \n\n\n\n\n\n\n\n\nClick \nYes\n to confirm the scale-up/scale-down.\n\n\nWhile nodes are being added or removed, cluster status changes to \"Update In Progress\". Once the operation has completed, cluster status changes back to \"Running\". \n\n\n\n\n\n\nSynchronize a Cluster\n\n\nUse the \nsync\n option if you:  \n\n\n\n\nMade changes on your cloud provider side (for example, deleted cluster VMs) and you would like to synchronize Cloudbreak with the cloud provider.  \n\n\nManually changed service status in Ambari (for example, restarted services).   \n\n\n\n\nTo synchronize your cluster with the cloud provider, follow these steps. \n\n\nSteps\n\n\n\n\n\n\nBrowse to the cluster details.\n\n\n\n\n\n\nClick \nActions\n and select \nSync\n.\n\n\n\n\n\n\nClick \nYes\n to confirm.\n\n\nYour cluster infrastructure is synchronized based on changes on the cloud provider. The updates are written to the \"Event History\". \n\n\n\n\n\n\nStop a Cluster\n\n\nCloudbreak supports stopping and restarting clusters. To stop and restart a cluster managed by Cloudbreak, use the options available from the Coudbreak UI. \n\n\nSteps\n\n\n\n\n\n\nBrowse to the cluster details.\n\n\n\n\n\n\nClick \nStop\n to stop a currently running cluster.  \n\n\n\n\n\n\nClick \nYes\n to confirm. \n\n\n\n\n\n\nYour cluster status changes to \"Stopping in progress\" and then to \"Stopped\". Once stopping the infrastructure has completed, you will see a \nStart\n option to restart your cluster. \n\n\n\n\n\n\nWhen a cluster is in the \"stopped\" state, you are not charged for the VMs, but you are charged for external storage.  \n\n\nRestart a Cluster\n\n\nIf your cluster is in the \"Stopped\" state, you can restart the cluster by follow these steps.\n\n\nSteps\n\n\n\n\n\n\nclick \nStart\n. This option is only available when the cluster has been stopped. \n\n\n\n\n\n\nClick \nYes\n to confirm.\n\n\nYour cluster status changes to \"Start in progress\" and then to \"Running\". \n\n\n\n\n\n\nTerminate a Cluster\n\n\nTo terminate a cluster managed by Cloudbreak, use the option available from the Coudbreak UI. \n\n\nSteps\n\n\n\n\n\n\nBrowse to the cluster details.\n\n\n\n\n\n\nClick \nTerminate\n. \n\n\n\n\n\n\nClick \nYes\n to confirm.\n\n\nAll cluster-related resources will be deleted, unless the resources (such as networks and subnets) existed prior to cluster creation or are used by other VMs in which case they will be preserved. \n\n\n\n\n\n\nForce Terminate a Cluster\n\n\nCluster deletion may fail if Cloudbreak is unable to delete one or more of the cloud resources that were part of your cluster infrastructure. In such as case, you can use the \nTerminate\n \n \nForce terminate\n option to remove the cluster entry from the Cloudbreak web UI, but you must also check your cloud provider account to see if there are any resources that must be deleted manually.\n\n\nSteps\n\n\n\n\n\n\nBrowse to the cluster details.\n\n\n\n\n\n\nClick \nTerminate\n. \n\n\n\n\n\n\nCheck  \nForce terminate\n.\n\n\n\n\n\n\nClick \nYes\n to confirm. \n\n\nWhen terminating a cluster with Kerberos enabled, you have an option to disable Kerberos prior to cluster termination. This option removes any cluster-related principals from the KDC.\n\n\n\n\n\n\nThis deletes the cluster tile from the UI.  \n\n\n\n\n\n\nLog in to your cloud provider account and \nmanually delete\n any resources that failed to be deleted.\n\n\n\n\n\n\nView Cluster History\n\n\nFrom the navigation menu in the Cloudbreak UI, you can access the History page that allows you to generate a report showing basic information related to the clusters that were running within the specified range of dates.\n\n\nTo generate a report, follow these steps.\n\n\nSteps\n\n\n\n\n\n\nFrom the Cloudbreak UI navigation menu, select \nHistory\n.\n\n\n\n\n\n\nOn the History page, select the range of dates and click \nShow History\n to generate a report for the selected period.\n\n\n \n\n\n\n\n\n\nHistory Report Content\n\n\nEach entry in the report represents one cluster instance group. For each entry, the report includes the following information:\n\n\n\n\nCreated\n - The date when your cluster was created (YYYY-MM-DD).\n\n\nProvider\n - The name of the cloud provider (AWS, Azure, Google, or OpenStack) on which the cluster instances are/were running.\n\n\nCluster Name\n - The name that you selected for the cluster.  \n\n\nInstance Group\n - The name of the host group.   \n\n\nInstance Count\n - The number of nodes in the host group. This number may be a decimal if a cluster has been resized.\n\n\nInstance Type\n - Provider-specific VM type of the cluster instances. \n\n\nRegion\n - The AWS region in which your cluster is/was running.\n\n\nAvailability Zone\n - The availability zone in which your cluster is/was running.      \n\n\nRunning Time (hours)\n - The sum of the running times for all the nodes in the instance group.\n\n\n\n\nThe \nAGGREGATE RUNNING TIME\n is the sum of the Running Times, adjusted for the selected time range.\n\n\nTo learn about how your cloud provider bills you for the VMs, refer to their documentation:\n\n\n\n\nAWS\n      \n\n\nAzure\n     \n\n\nGCP\n   \n\n\n\n\n\n\nNext: Access Data", 
            "title": "Manage and Monitor Clusters"
        }, 
        {
            "location": "/azure-clusters-manage/index.html#managing-and-monitoring-clusters", 
            "text": "You can manage monitor your clusters from the Cloudbreak UI. To do that, click on the tile representing the cluster that you want to access. The actions available for your cluster are listed in the top right corner:      \n   Tips \n   \n   To add or remove nodes from your cluster click  ACTIONS>Resize . \n   To synchronize your cluster with the cloud provider account click  ACTIONS>Sync . \n   To temporarily stop your cluster click  STOP . \n   To terminate your cluster click  TERMINATE .", 
            "title": "Managing and Monitoring Clusters"
        }, 
        {
            "location": "/azure-clusters-manage/index.html#resize-a-cluster", 
            "text": "To resize a cluster, follow these steps.  Steps    Browse to the cluster details.    Click  Actions  and select  Resize . The cluster resize dialog is displayed.    Using the +/- controls, adjust the number of nodes for a chosen host group.    You can only modify one host group at a time.  \nIt is not possible to resize the Ambari server host group.          Click  Yes  to confirm the scale-up/scale-down.  While nodes are being added or removed, cluster status changes to \"Update In Progress\". Once the operation has completed, cluster status changes back to \"Running\".", 
            "title": "Resize a Cluster"
        }, 
        {
            "location": "/azure-clusters-manage/index.html#synchronize-a-cluster", 
            "text": "Use the  sync  option if you:     Made changes on your cloud provider side (for example, deleted cluster VMs) and you would like to synchronize Cloudbreak with the cloud provider.    Manually changed service status in Ambari (for example, restarted services).      To synchronize your cluster with the cloud provider, follow these steps.   Steps    Browse to the cluster details.    Click  Actions  and select  Sync .    Click  Yes  to confirm.  Your cluster infrastructure is synchronized based on changes on the cloud provider. The updates are written to the \"Event History\".", 
            "title": "Synchronize a Cluster"
        }, 
        {
            "location": "/azure-clusters-manage/index.html#stop-a-cluster", 
            "text": "Cloudbreak supports stopping and restarting clusters. To stop and restart a cluster managed by Cloudbreak, use the options available from the Coudbreak UI.   Steps    Browse to the cluster details.    Click  Stop  to stop a currently running cluster.      Click  Yes  to confirm.     Your cluster status changes to \"Stopping in progress\" and then to \"Stopped\". Once stopping the infrastructure has completed, you will see a  Start  option to restart your cluster.     When a cluster is in the \"stopped\" state, you are not charged for the VMs, but you are charged for external storage.", 
            "title": "Stop a Cluster"
        }, 
        {
            "location": "/azure-clusters-manage/index.html#restart-a-cluster", 
            "text": "If your cluster is in the \"Stopped\" state, you can restart the cluster by follow these steps.  Steps    click  Start . This option is only available when the cluster has been stopped.     Click  Yes  to confirm.  Your cluster status changes to \"Start in progress\" and then to \"Running\".", 
            "title": "Restart a Cluster"
        }, 
        {
            "location": "/azure-clusters-manage/index.html#terminate-a-cluster", 
            "text": "To terminate a cluster managed by Cloudbreak, use the option available from the Coudbreak UI.   Steps    Browse to the cluster details.    Click  Terminate .     Click  Yes  to confirm.  All cluster-related resources will be deleted, unless the resources (such as networks and subnets) existed prior to cluster creation or are used by other VMs in which case they will be preserved.", 
            "title": "Terminate a Cluster"
        }, 
        {
            "location": "/azure-clusters-manage/index.html#force-terminate-a-cluster", 
            "text": "Cluster deletion may fail if Cloudbreak is unable to delete one or more of the cloud resources that were part of your cluster infrastructure. In such as case, you can use the  Terminate     Force terminate  option to remove the cluster entry from the Cloudbreak web UI, but you must also check your cloud provider account to see if there are any resources that must be deleted manually.  Steps    Browse to the cluster details.    Click  Terminate .     Check   Force terminate .    Click  Yes  to confirm.   When terminating a cluster with Kerberos enabled, you have an option to disable Kerberos prior to cluster termination. This option removes any cluster-related principals from the KDC.    This deletes the cluster tile from the UI.      Log in to your cloud provider account and  manually delete  any resources that failed to be deleted.", 
            "title": "Force Terminate a Cluster"
        }, 
        {
            "location": "/azure-clusters-manage/index.html#view-cluster-history", 
            "text": "From the navigation menu in the Cloudbreak UI, you can access the History page that allows you to generate a report showing basic information related to the clusters that were running within the specified range of dates.  To generate a report, follow these steps.  Steps    From the Cloudbreak UI navigation menu, select  History .    On the History page, select the range of dates and click  Show History  to generate a report for the selected period.", 
            "title": "View Cluster History"
        }, 
        {
            "location": "/azure-clusters-manage/index.html#history-report-content", 
            "text": "Each entry in the report represents one cluster instance group. For each entry, the report includes the following information:   Created  - The date when your cluster was created (YYYY-MM-DD).  Provider  - The name of the cloud provider (AWS, Azure, Google, or OpenStack) on which the cluster instances are/were running.  Cluster Name  - The name that you selected for the cluster.    Instance Group  - The name of the host group.     Instance Count  - The number of nodes in the host group. This number may be a decimal if a cluster has been resized.  Instance Type  - Provider-specific VM type of the cluster instances.   Region  - The AWS region in which your cluster is/was running.  Availability Zone  - The availability zone in which your cluster is/was running.        Running Time (hours)  - The sum of the running times for all the nodes in the instance group.   The  AGGREGATE RUNNING TIME  is the sum of the Running Times, adjusted for the selected time range.  To learn about how your cloud provider bills you for the VMs, refer to their documentation:   AWS         Azure        GCP        Next: Access Data", 
            "title": "History Report Content"
        }, 
        {
            "location": "/azure-data/index.html", 
            "text": "Accessing Data on Azure\n\n\nHortonworks Data Platform (HDP) supports reading and writing both block blobs and page blobs\nfrom/to \nWindows Azure Storage Blob (WASB)\n object store, as well as reading and writing files stored in an\n\nAzure Data Lake Storage (ADLS)\n account. This allows you to:\n\n\n\n\nPersist data using cloud storage services beyond the lifetime of your HDP clusters.  \n\n\nLoad data in Hadoop ecosystem applications directly from Azure storage services, without first importing or uploading data from external resources to HDFS.  \n\n\nUse other applications (not necessarily in your Hadoop ecosystem) to manipulate the data stored in Azure storage services beyond the lifetime of your HDP clusters.  \n\n\nShare data between multiple HDP clusters fast and easily by pointing to the same Azure data sets. \n\n\nMove or copy data between different Azure storage services or between Azure storage services and HDFS to facilitate different scenarios for big data analytics workloads.  \n\n\nBack up unlimited archive data at any scale from HDP cluster to fully managed, durable, and highly available Azure storage services.   \n\n\n\n\nAccessing Data in ADLS\n\n\nAzure Data Lake Store (ADLS)\n is an enterprise-wide hyper-scale repository for big data analytic workloads.\n\n\nPrerequisites\n\n\nIf you want to use ADLS to store your data, you must enable Azure subscription for Data Lake Store, and then create an Azure Data Lake Store \nstorage account\n.\n\n\nConfiguring Access to ADLS\n\n\nADLS is not supported as a default file system, but access to data in ADLS via the adl connector. To configure access to ADLS from a cluster managed via Cloudbreak use the steps described in \nHow to Configure Authentication with ADLS\n.\n\n\nTesting Access to ADLS\n\n\nTo tests access to ADLS, SSH to a cluster node and run a few hadoop fs shell commands against your existing ADLS account.\n\n\nADLS access path syntax is:\n\n\nadl://\naccount_name\n.azuredatalakestore.net/\ndir/file\n\n\n\nFor example, the following Hadoop FileSystem shell commands demonstrate access to a storage account named \"myaccount\":\n\n\nhadoop fs -mkdir adl://myaccount.azuredatalakestore.net/testdir\n\n\n\nhadoop fs -put testfile adl://myaccount.azuredatalakestore.net/testdir/testfile\n\n\n\nTo use DistCp against ADLS, use the following syntax:\n\nhadoop distcp\n    [-D hadoop.security.credential.provider.path=localjceks://file/home/user/adls.jceks]\n    hdfs://\nnamenode_hostname\n:9001/user/foo/007020615\n    adl://\nmyaccount\n.azuredatalakestore.net/testDir/\n\n\nWorking with ADLS\n\n\nFor more information about configuring the ADLS connector and working with data stored in ADLS, refer to \nCloud Data Access\n documentation.\n\n\nRelated Links\n \n\n\nCloud Data Access\n (Hortonworks) \n\nHow to Configure Authentication with ADLS\n (Hortonworks)\n\n\nAzure Data Lake Store\n (External)   \n\n\nCreate a Storage Account\n (External)   \n\n\nGet started with Azure Data Lake Store\n (External)  \n\n\nAccessing Data in WASB\n\n\nWindows Azure Storage Blob (WASB) is an object store service available on Azure.\n\n\nPrerequisites\n\n\nIf you want to use Windows Azure Storage Blob to store your data, you must enable Azure subscription for Blob Storage, and then create a \nstorage account\n.  \n\n\nConfiguring Access to WASB\n\n\nIn order to access data stored in your Azure blob storage account, you must configure your storage account access key in \ncore-site.xml\n. The configuration property that you must use is \nfs.azure.account.key.\naccount name\n.blob.core.windows.net\n and the value is the access key. \n\n\nFor example the following property should be used for a storage account called \"testaccount\": \n\n\nproperty\n\n  \nname\nfs.azure.account.key.testaccount.blob.core.windows.net\n/name\n\n  \nvalue\nTESTACCOUNT-ACCESS-KEY\n/value\n\n\n/property\n\n\n\n\n\nYou can obtain your access key from the Access keys in your storage account settings.\n\n\nTesting Access to WASB\n\n\nTo tests access to WASB, SSH to a cluster node and run a few hadoop fs shell commands against your existing WASB account.\n\n\nWASB access path syntax is:\n\n\nwasb://\ncontainer_name\n@\nstorage_account_name\n.blob.core.windows.net/\ndir/file\n\n\n\nFor example, to access a file called \"testfile\" located in a directory called \"testdir\", stored in the container called \"testcontainer\" on the account called \"hortonworks\", the URL is:\n\n\nwasb://testcontainer@hortonworks.blob.core.windows.net/testdir/testfile\n\n\n\nYou can also use \"wasbs\" prefix to utilize SSL-encrypted HTTPS access:\n\n\nwasbs://\n@\n.blob.core.windows.net/dir/file\n\n\n\nThe following Hadoop FileSystem shell commands demonstrate access to a storage account named \"myaccount\" and a container named \"mycontainer\":\n\n\nhadoop fs -ls wasb://mycontainer@myaccount.blob.core.windows.net/\n\nhadoop fs -mkdir wasb://mycontainer@myaccount.blob.core.windows.net/testDir\n\nhadoop fs -put testFile wasb://mycontainer@myaccount.blob.core.windows.net/testDir/testFile\n\nhadoop fs -cat wasb://mycontainer@myaccount.blob.core.windows.net/testDir/testFile\ntest file content\n\n\n\nWorking with WASB\n\n\nFor more information about configuring the WASB connector and working with data stored in WASB, refer to \nCloud Data Access\n documentation.\n\n\nRelated Links\n \n\n\nCloud Data Access\n (Hortonworks) \n\n\nCreate a Storage Account\n (External)", 
            "title": "Access Data on Azure"
        }, 
        {
            "location": "/azure-data/index.html#accessing-data-on-azure", 
            "text": "Hortonworks Data Platform (HDP) supports reading and writing both block blobs and page blobs\nfrom/to  Windows Azure Storage Blob (WASB)  object store, as well as reading and writing files stored in an Azure Data Lake Storage (ADLS)  account. This allows you to:   Persist data using cloud storage services beyond the lifetime of your HDP clusters.    Load data in Hadoop ecosystem applications directly from Azure storage services, without first importing or uploading data from external resources to HDFS.    Use other applications (not necessarily in your Hadoop ecosystem) to manipulate the data stored in Azure storage services beyond the lifetime of your HDP clusters.    Share data between multiple HDP clusters fast and easily by pointing to the same Azure data sets.   Move or copy data between different Azure storage services or between Azure storage services and HDFS to facilitate different scenarios for big data analytics workloads.    Back up unlimited archive data at any scale from HDP cluster to fully managed, durable, and highly available Azure storage services.", 
            "title": "Accessing Data on Azure"
        }, 
        {
            "location": "/azure-data/index.html#accessing-data-in-adls", 
            "text": "Azure Data Lake Store (ADLS)  is an enterprise-wide hyper-scale repository for big data analytic workloads.", 
            "title": "Accessing Data in ADLS"
        }, 
        {
            "location": "/azure-data/index.html#prerequisites", 
            "text": "If you want to use ADLS to store your data, you must enable Azure subscription for Data Lake Store, and then create an Azure Data Lake Store  storage account .", 
            "title": "Prerequisites"
        }, 
        {
            "location": "/azure-data/index.html#configuring-access-to-adls", 
            "text": "ADLS is not supported as a default file system, but access to data in ADLS via the adl connector. To configure access to ADLS from a cluster managed via Cloudbreak use the steps described in  How to Configure Authentication with ADLS .", 
            "title": "Configuring Access to ADLS"
        }, 
        {
            "location": "/azure-data/index.html#testing-access-to-adls", 
            "text": "To tests access to ADLS, SSH to a cluster node and run a few hadoop fs shell commands against your existing ADLS account.  ADLS access path syntax is:  adl:// account_name .azuredatalakestore.net/ dir/file  For example, the following Hadoop FileSystem shell commands demonstrate access to a storage account named \"myaccount\":  hadoop fs -mkdir adl://myaccount.azuredatalakestore.net/testdir  hadoop fs -put testfile adl://myaccount.azuredatalakestore.net/testdir/testfile  To use DistCp against ADLS, use the following syntax: hadoop distcp\n    [-D hadoop.security.credential.provider.path=localjceks://file/home/user/adls.jceks]\n    hdfs:// namenode_hostname :9001/user/foo/007020615\n    adl:// myaccount .azuredatalakestore.net/testDir/", 
            "title": "Testing Access to ADLS"
        }, 
        {
            "location": "/azure-data/index.html#working-with-adls", 
            "text": "For more information about configuring the ADLS connector and working with data stored in ADLS, refer to  Cloud Data Access  documentation.  Related Links    Cloud Data Access  (Hortonworks)  How to Configure Authentication with ADLS  (Hortonworks)  Azure Data Lake Store  (External)     Create a Storage Account  (External)     Get started with Azure Data Lake Store  (External)", 
            "title": "Working with ADLS"
        }, 
        {
            "location": "/azure-data/index.html#accessing-data-in-wasb", 
            "text": "Windows Azure Storage Blob (WASB) is an object store service available on Azure.", 
            "title": "Accessing Data in WASB"
        }, 
        {
            "location": "/azure-data/index.html#prerequisites_1", 
            "text": "If you want to use Windows Azure Storage Blob to store your data, you must enable Azure subscription for Blob Storage, and then create a  storage account .", 
            "title": "Prerequisites"
        }, 
        {
            "location": "/azure-data/index.html#configuring-access-to-wasb", 
            "text": "In order to access data stored in your Azure blob storage account, you must configure your storage account access key in  core-site.xml . The configuration property that you must use is  fs.azure.account.key. account name .blob.core.windows.net  and the value is the access key.   For example the following property should be used for a storage account called \"testaccount\":   property \n   name fs.azure.account.key.testaccount.blob.core.windows.net /name \n   value TESTACCOUNT-ACCESS-KEY /value  /property   You can obtain your access key from the Access keys in your storage account settings.", 
            "title": "Configuring Access to WASB"
        }, 
        {
            "location": "/azure-data/index.html#testing-access-to-wasb", 
            "text": "To tests access to WASB, SSH to a cluster node and run a few hadoop fs shell commands against your existing WASB account.  WASB access path syntax is:  wasb:// container_name @ storage_account_name .blob.core.windows.net/ dir/file  For example, to access a file called \"testfile\" located in a directory called \"testdir\", stored in the container called \"testcontainer\" on the account called \"hortonworks\", the URL is:  wasb://testcontainer@hortonworks.blob.core.windows.net/testdir/testfile  You can also use \"wasbs\" prefix to utilize SSL-encrypted HTTPS access:  wasbs:// @ .blob.core.windows.net/dir/file  The following Hadoop FileSystem shell commands demonstrate access to a storage account named \"myaccount\" and a container named \"mycontainer\":  hadoop fs -ls wasb://mycontainer@myaccount.blob.core.windows.net/\n\nhadoop fs -mkdir wasb://mycontainer@myaccount.blob.core.windows.net/testDir\n\nhadoop fs -put testFile wasb://mycontainer@myaccount.blob.core.windows.net/testDir/testFile\n\nhadoop fs -cat wasb://mycontainer@myaccount.blob.core.windows.net/testDir/testFile\ntest file content", 
            "title": "Testing Access to WASB"
        }, 
        {
            "location": "/azure-data/index.html#working-with-wasb", 
            "text": "For more information about configuring the WASB connector and working with data stored in WASB, refer to  Cloud Data Access  documentation.  Related Links    Cloud Data Access  (Hortonworks)   Create a Storage Account  (External)", 
            "title": "Working with WASB"
        }, 
        {
            "location": "/gcp-launch/index.html", 
            "text": "Launching Cloudbreak on GCP\n\n\nBefore launching Cloudbreak on Google Cloud, review and meet the prerequisites. Next, import Cloudbreak image, launch a VM, SSH to the VM, and start Cloudbreak. Once Cloudbreak is running, log in to the Cloudbreak UI and create a Cloudbreak credential. \n\n\nMeet the Prerequisites\n\n\nBefore launching Cloudbreak on GCP, you must meet the following prerequisites.\n\n\nGCP Account\n\n\nIn order to launch Cloudbreak on GCP, you must log in to your GCP account. If you don't have an account, you can create one at \nhttps://console.cloud.google.com\n.\n\n\nOnce you log in to your GCP account, you must either create a project or use an existing project. \n\n\nService Account\n\n\nIn order to launch clusters on GCP via Cloudbreak, you must have a Service Account that Cloudbreak can use to create resources. In addition, you must also have a P12 key associated with the account. If you need to create these, refer to \nGCP documentation\n on how to create a service account and generate a P12 key. \n\n\nOnce you have the service account that you want to use for Cloudbreak, make sure that your service account fulfills one of the following APIs are enabled for your service account:\n\n\n\n\nCompute Image User   \n\n\nCompute Instance Admin (v1)  \n\n\nCompute Network Admin  \n\n\nCompute Security Admin  \n\n\n\n\nA user with an \"Owner\" role can assign roles or access rules to service accounts from \nIAM \n Admin\n \n \nIAM\n. For example:\n\n\n \n\n\nRelated Links\n\n\nService Account Credentials\n (External)  \n\n\nSSH Key Pair\n\n\nGenerate a new SSH key pair\n or use an existing SSH key pair. You will be required to provide it when launching the VM.  \n\n\nVPC Network\n\n\nWhen launching Cloudbreak, you will be required to select an existing network in which Cloudbreak can be placed. The following ports must be open on the security group: 22 (for access via SSH), 80 (for access via HTTP), and 443 (for access via HTTPS). You may use the \ndefault\n network as long as the aforementioned ports are open. \n\n\nYou can manage networks under \nNetworking\n \n \nVPC Networks\n. To edit ports, click on the network name and then click on \nAdd firewall rules\n.\n\n\nRegion and Zone\n\n\nDecide in which region and zone you would like to launch Cloudbreak. You can launch Cloudbreak and provision your clusters in all regions \nsupported by GCP\n.  \n\n\nClusters created via Cloudbreak can be in the same or different region as Cloudbreak; when you launch a cluster, you select the region in which to launch it. \n\n\nRelated Links\n\n\nRegions and Zones\n (External)  \n\n\nLaunch the VM\n\n\nSteps\n\n\n\n\n\n\nLog in to Google Cloud Platform.\n\n\n\n\n\n\nOpen the \nGoogle Cloud Shell\n by clicking on the  \n icon in the top-right corner:\n\n\n \n\n\n\n\n\n\nImport the Cloudbreak deployer image by executing the following command: \n\n\ngcloud compute images create cloudbreak-deployer-220-2017-12-19 --source-uri gs://sequenceiqimage/cloudbreak-deployer-220-2017-12-19.tar.gz\n\n\n\n\n\n\nIn the GCP UI, from the \nProducts and services\n menu, select \nCompute Engine\n \n \nImages\n.\n\n\n\n\n\n\nIn the search bar, type the name of the Cloudbreak deployer image that you imported earlier.\n\n\n\n\n\n\nSelect the image and then select \nCreate Instance\n:  \n\n\n  \n\n\n\n\n\n\nYou will be redirected to \nVM instances\n \n \nCreate an instance\n form. Provide the following parameters for your VM:\n\n\n  \n\n\n\n\n\n\n\n\nParameter\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nName\n\n\nEnter a name for the VM.\n\n\n\n\n\n\nZone\n\n\nSelect the zone in which to launch the VM.\n\n\n\n\n\n\nMachine type\n\n\nThe minimum instance type suitable for Cloudbreak is \nn1-standard-2\n. The minimum requirements are 4GB RAM, 10GB disk, 2 cores.\n\n\n\n\n\n\nBoot disk\n\n\nVerify that the Cloudbreak deployer disk which you imported earlier is pre-selected.\n\n\n\n\n\n\n\n\n\n\n\n\nClick on \nManagement, disks, networking, SSH keys\n to view the options.\n\n\n\n\n\n\nUnder \nNetworking\n \n \nNetwork interfaces\n, select the network in which you want to launch Cloudbreak. \n\n\n\n\n\n\nUnder \nSSH Keys\n, check \nBlock project-wise SSH keys\n and paste your public SSH key.\n\n\n\n\n\n\nClick \nCreate\n. \n\n\n\n\n\n\nSSH to the VM\n\n\nNow that your VM is ready, access it via SSH: \n\n\n\n\nUse a private key matching the public key that you added to your  project.\n\n\nThe SSH user is called \"cloudbreak\".\n\n\nYou can obtain the VM's IP address from \nCompute Engine\n \n \nVM Instances\n, the \nExternal IP\n column.\n\n\n\n\nOn Mac OS X, you can SSH to the VM by running the following from the Terminal app: \nssh -i \"your-private-key.pem\" cloudbreak@instance_IP\n where \"your-private-key.pem\" points to the location of your private key and \"instance_IP\" is the public IP address of the VM.\n\n\nOn Windows, you can use \nPuTTy\n.\n\n\nLaunch Cloudbreak Deployer\n\n\nAfter accessing the VM via SSH, launch Cloudbreak deployer using the following steps.\n\n\nSteps\n \n\n\n\n\n\n\nNavigate to the cloudbreak-deployment directory:\n\n\ncd /var/lib/cloudbreak-deployment/\n\n\nThis directory contains configuration files and the supporting binaries for Cloudbreak deployer.\n\n\n\n\n\n\nInitialize your profile by creating a new file called \nProfile\n and adding the following content:\n\n\nexport UAA_DEFAULT_SECRET=MY-SECRET\nexport UAA_DEFAULT_USER_PW=MY-PASSWORD\nexport UAA_DEFAULT_USER_EMAIL=MY-EMAIL\n  \n\n\nFor example: \n\n\nexport UAA_DEFAULT_SECRET=MySecret123\nexport UAA_DEFAULT_USER_PW=MySecurePassword123\nexport UAA_DEFAULT_USER_EMAIL=dbialek@hortonworks.com\n \n\n\n\n\nYou will need to provide the email and password when logging in to the Cloudbreak web UI and when using the Cloudbreak CLI. The secret will be used by Cloudbreak for authentication.  \n\n\n\n\n\n\n\n\nStart the Cloudbreak application by using the following command:\n\n\ncbd start\n\n\nThis will start the Docker containers and initialize the application. The first time you start the Coudbreak app, this also downloads of all the necessary docker images.\n\n\nOnce the \ncbd start\n has finished, it returns the \"Uluwatu (Cloudbreak UI) url\" which you can later paste in your browser and log in to Cloudbreak web UI.\n\n\n\n\n\n\nCheck Cloudbreak deployer version and health: \n\n\ncbd doctor\n\n\n\n\n\n\nNext, check Cloudbreak Application logs: \n\n\ncbd logs cloudbreak\n\n\nYou should see a message like this in the log: \nStarted CloudbreakApplication in 36.823 seconds.\n Cloudbreak takes less than a minute to start. If you try to access the Cloudbreak UI before Cloudbreak started, you will get a \"Bad Gateway\" error or \"Cannot connect to Cloubdreak\" error.\n\n\n\n\n\n\nAccess Cloudbreak UI\n\n\nLog in to the Cloudbreak UI using the following steps.\n\n\nSteps\n\n\n\n\n\n\nYou can log into the Cloudbreak application at \nhttps://IP_Address\n. For example \nhttps://34.212.141.253\n. You can obtain the VM's IP address from \nCompute Engine\n \n \nVM Instances\n, the \nExternal IP\n column.\n\n\n\n\n\n\nConfirm the security exception to proceed to the Cloudbreak web UI.\n\n\nThe first time you access Cloudbreak UI, Cloudbreak will automatically generate a self-signed certificate, due to which your browser will warn you about an untrusted connection and will ask you to confirm a security exception.\n\n\n\n\n\n\nThe login page is displayed:\n\n\n  \n\n\n\n\n\n\nLog in to the Cloudbreak web UI using the credential that you configured in your \nProfile\n file when \nlaunching Cloudbreak deployer\n:\n\n\n\n\nThe username is the \nUAA_DEFAULT_USER_EMAIL\n     \n\n\nThe password is the \nUAA_DEFAULT_USER_PW\n \n\n\n\n\n\n\n\n\nUpon a successful login, you are redirected to the dashboard:\n\n\n  \n\n\n\n\n\n\nCreate Cloudbreak Credential\n\n\nCloudbreak works by connecting your GCP account through this credential, and then uses it to create resources on your behalf. Before you can start provisioning cluster using Cloudbreak, you must create a Cloudbreak credential.\n\n\nSteps\n\n\n\n\n\n\nIn the Cloudbreak web UI, select \nCredentials\n from the navigation pane. \n\n\n\n\n\n\nClick \nCreate Credential\n. \n\n\n\n\n\n\nUnder \nCloud provider\n, select \"Google Cloud Platform\":\n\n\n  \n\n\n\n\n\n\nProvide the following information:\n\n\n\n\n\n\n\n\nParameter\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nName\n\n\nEnter a name for your credential.\n\n\n\n\n\n\nDescription\n\n\n(Optional) Enter a description.\n\n\n\n\n\n\nProject Id\n\n\nEnter the project ID. You can obtain it from your GCP account by clicking on the name of your project at the top of the page and copying the \nID\n.\n\n\n\n\n\n\nService Account Email Address\n\n\n\"Service account ID\" value for your service account created in prerequisites. You can find it on GCP at \nIAM \n Admin\n \n \nService accounts\n.\n\n\n\n\n\n\nService Account Private (p12) Key\n\n\nUpload the P12 key that you created in the prerequisites when creating a service account.\n\n\n\n\n\n\n\n\n\n\n\n\nClick \nCreate\n.\n\n\n\n\n\n\nYour credential should now be displayed in the \nCredentials\n pane.\n\n\nCongratulations! You have successfully launched Cloudbreak and created a Cloudbreak credential. Now you can use Cloudbreak to \ncreate clusters\n. \n\n\n\n\n\n\n\n\nNext: Create a Cluster", 
            "title": "Launch on GCP"
        }, 
        {
            "location": "/gcp-launch/index.html#launching-cloudbreak-on-gcp", 
            "text": "Before launching Cloudbreak on Google Cloud, review and meet the prerequisites. Next, import Cloudbreak image, launch a VM, SSH to the VM, and start Cloudbreak. Once Cloudbreak is running, log in to the Cloudbreak UI and create a Cloudbreak credential.", 
            "title": "Launching Cloudbreak on GCP"
        }, 
        {
            "location": "/gcp-launch/index.html#meet-the-prerequisites", 
            "text": "Before launching Cloudbreak on GCP, you must meet the following prerequisites.", 
            "title": "Meet the Prerequisites"
        }, 
        {
            "location": "/gcp-launch/index.html#gcp-account", 
            "text": "In order to launch Cloudbreak on GCP, you must log in to your GCP account. If you don't have an account, you can create one at  https://console.cloud.google.com .  Once you log in to your GCP account, you must either create a project or use an existing project.", 
            "title": "GCP Account"
        }, 
        {
            "location": "/gcp-launch/index.html#service-account", 
            "text": "In order to launch clusters on GCP via Cloudbreak, you must have a Service Account that Cloudbreak can use to create resources. In addition, you must also have a P12 key associated with the account. If you need to create these, refer to  GCP documentation  on how to create a service account and generate a P12 key.   Once you have the service account that you want to use for Cloudbreak, make sure that your service account fulfills one of the following APIs are enabled for your service account:   Compute Image User     Compute Instance Admin (v1)    Compute Network Admin    Compute Security Admin     A user with an \"Owner\" role can assign roles or access rules to service accounts from  IAM   Admin     IAM . For example:     Related Links  Service Account Credentials  (External)", 
            "title": "Service Account"
        }, 
        {
            "location": "/gcp-launch/index.html#ssh-key-pair", 
            "text": "Generate a new SSH key pair  or use an existing SSH key pair. You will be required to provide it when launching the VM.", 
            "title": "SSH Key Pair"
        }, 
        {
            "location": "/gcp-launch/index.html#vpc-network", 
            "text": "When launching Cloudbreak, you will be required to select an existing network in which Cloudbreak can be placed. The following ports must be open on the security group: 22 (for access via SSH), 80 (for access via HTTP), and 443 (for access via HTTPS). You may use the  default  network as long as the aforementioned ports are open.   You can manage networks under  Networking     VPC Networks . To edit ports, click on the network name and then click on  Add firewall rules .", 
            "title": "VPC Network"
        }, 
        {
            "location": "/gcp-launch/index.html#region-and-zone", 
            "text": "Decide in which region and zone you would like to launch Cloudbreak. You can launch Cloudbreak and provision your clusters in all regions  supported by GCP .    Clusters created via Cloudbreak can be in the same or different region as Cloudbreak; when you launch a cluster, you select the region in which to launch it.   Related Links  Regions and Zones  (External)", 
            "title": "Region and Zone"
        }, 
        {
            "location": "/gcp-launch/index.html#launch-the-vm", 
            "text": "Steps    Log in to Google Cloud Platform.    Open the  Google Cloud Shell  by clicking on the    icon in the top-right corner:       Import the Cloudbreak deployer image by executing the following command:   gcloud compute images create cloudbreak-deployer-220-2017-12-19 --source-uri gs://sequenceiqimage/cloudbreak-deployer-220-2017-12-19.tar.gz    In the GCP UI, from the  Products and services  menu, select  Compute Engine     Images .    In the search bar, type the name of the Cloudbreak deployer image that you imported earlier.    Select the image and then select  Create Instance :          You will be redirected to  VM instances     Create an instance  form. Provide the following parameters for your VM:         Parameter  Description      Name  Enter a name for the VM.    Zone  Select the zone in which to launch the VM.    Machine type  The minimum instance type suitable for Cloudbreak is  n1-standard-2 . The minimum requirements are 4GB RAM, 10GB disk, 2 cores.    Boot disk  Verify that the Cloudbreak deployer disk which you imported earlier is pre-selected.       Click on  Management, disks, networking, SSH keys  to view the options.    Under  Networking     Network interfaces , select the network in which you want to launch Cloudbreak.     Under  SSH Keys , check  Block project-wise SSH keys  and paste your public SSH key.    Click  Create .", 
            "title": "Launch the VM"
        }, 
        {
            "location": "/gcp-launch/index.html#ssh-to-the-vm", 
            "text": "Now that your VM is ready, access it via SSH:    Use a private key matching the public key that you added to your  project.  The SSH user is called \"cloudbreak\".  You can obtain the VM's IP address from  Compute Engine     VM Instances , the  External IP  column.   On Mac OS X, you can SSH to the VM by running the following from the Terminal app:  ssh -i \"your-private-key.pem\" cloudbreak@instance_IP  where \"your-private-key.pem\" points to the location of your private key and \"instance_IP\" is the public IP address of the VM.  On Windows, you can use  PuTTy .", 
            "title": "SSH to the VM"
        }, 
        {
            "location": "/gcp-launch/index.html#launch-cloudbreak-deployer", 
            "text": "After accessing the VM via SSH, launch Cloudbreak deployer using the following steps.  Steps      Navigate to the cloudbreak-deployment directory:  cd /var/lib/cloudbreak-deployment/  This directory contains configuration files and the supporting binaries for Cloudbreak deployer.    Initialize your profile by creating a new file called  Profile  and adding the following content:  export UAA_DEFAULT_SECRET=MY-SECRET\nexport UAA_DEFAULT_USER_PW=MY-PASSWORD\nexport UAA_DEFAULT_USER_EMAIL=MY-EMAIL     For example:   export UAA_DEFAULT_SECRET=MySecret123\nexport UAA_DEFAULT_USER_PW=MySecurePassword123\nexport UAA_DEFAULT_USER_EMAIL=dbialek@hortonworks.com     You will need to provide the email and password when logging in to the Cloudbreak web UI and when using the Cloudbreak CLI. The secret will be used by Cloudbreak for authentication.       Start the Cloudbreak application by using the following command:  cbd start  This will start the Docker containers and initialize the application. The first time you start the Coudbreak app, this also downloads of all the necessary docker images.  Once the  cbd start  has finished, it returns the \"Uluwatu (Cloudbreak UI) url\" which you can later paste in your browser and log in to Cloudbreak web UI.    Check Cloudbreak deployer version and health:   cbd doctor    Next, check Cloudbreak Application logs:   cbd logs cloudbreak  You should see a message like this in the log:  Started CloudbreakApplication in 36.823 seconds.  Cloudbreak takes less than a minute to start. If you try to access the Cloudbreak UI before Cloudbreak started, you will get a \"Bad Gateway\" error or \"Cannot connect to Cloubdreak\" error.", 
            "title": "Launch Cloudbreak Deployer"
        }, 
        {
            "location": "/gcp-launch/index.html#access-cloudbreak-ui", 
            "text": "Log in to the Cloudbreak UI using the following steps.  Steps    You can log into the Cloudbreak application at  https://IP_Address . For example  https://34.212.141.253 . You can obtain the VM's IP address from  Compute Engine     VM Instances , the  External IP  column.    Confirm the security exception to proceed to the Cloudbreak web UI.  The first time you access Cloudbreak UI, Cloudbreak will automatically generate a self-signed certificate, due to which your browser will warn you about an untrusted connection and will ask you to confirm a security exception.    The login page is displayed:        Log in to the Cloudbreak web UI using the credential that you configured in your  Profile  file when  launching Cloudbreak deployer :   The username is the  UAA_DEFAULT_USER_EMAIL        The password is the  UAA_DEFAULT_USER_PW       Upon a successful login, you are redirected to the dashboard:", 
            "title": "Access Cloudbreak UI"
        }, 
        {
            "location": "/gcp-launch/index.html#create-cloudbreak-credential", 
            "text": "Cloudbreak works by connecting your GCP account through this credential, and then uses it to create resources on your behalf. Before you can start provisioning cluster using Cloudbreak, you must create a Cloudbreak credential.  Steps    In the Cloudbreak web UI, select  Credentials  from the navigation pane.     Click  Create Credential .     Under  Cloud provider , select \"Google Cloud Platform\":        Provide the following information:     Parameter  Description      Name  Enter a name for your credential.    Description  (Optional) Enter a description.    Project Id  Enter the project ID. You can obtain it from your GCP account by clicking on the name of your project at the top of the page and copying the  ID .    Service Account Email Address  \"Service account ID\" value for your service account created in prerequisites. You can find it on GCP at  IAM   Admin     Service accounts .    Service Account Private (p12) Key  Upload the P12 key that you created in the prerequisites when creating a service account.       Click  Create .    Your credential should now be displayed in the  Credentials  pane.  Congratulations! You have successfully launched Cloudbreak and created a Cloudbreak credential. Now you can use Cloudbreak to  create clusters .      Next: Create a Cluster", 
            "title": "Create Cloudbreak Credential"
        }, 
        {
            "location": "/gcp-create/index.html", 
            "text": "Creating a Cluster on GCP\n\n\nUse these steps to create a cluster.\n\n\nPrerequisites\n\n\nIf you would like to use \nOozie\n with \nAmbari 2.6.1 or newer\n, you must install the Ext JS library. For instructions, refer to \nRecipe to Install Ext JS for Oozie\n.\n\n\nSteps\n\n\n\n\n\n\nLog in to the Cloudbreak UI.\n\n\n\n\n\n\nClick \nCreate Cluster\n and the \nCreate Cluster\n wizard is displayed.\n\n    By default, \nBasic\n view is displayed. To view advanced options, click \nAdvanced\n. To learn about advanced options, refer to \nAdvanced Options\n.\n\n\n \n\n\n\n\n\n\nOn the \nGeneral Configuration\n page, specify the following general parameters for your cluster:\n\n\n\n\n\n\n\n\nParameter\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nSelect Credential\n\n\nChoose a previously created credential.\n\n\n\n\n\n\nCluster Name\n\n\nEnter a name for your cluster. The name must be between 5 and 40 characters, must start with a letter, and must only include lowercase letters, numbers, and hyphens.\n\n\n\n\n\n\nRegion\n\n\nSelect the GCP region in which you would like to launch your cluster. For information on available GCP regions, refer to \nGCP documentation\n.\n\n\n\n\n\n\nPlatform Version\n\n\nChoose the HDP version to use for this cluster.\n\n\n\n\n\n\nCluster Type\n\n\nChoose one of default cluster configurations, or, if you have defined your own cluster configuration via Ambari blueprint, you can choose it here. For more information on default and custom blueprints, refer to \nBlueprints\n.\n\n\n\n\n\n\n\n\n\n\n\n\nOn the \nHardware and Storage\n page, for each host group provide the following information to define your cluster nodes and attached storage:\n\n\n\n\n\n\n\n\nParameter\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nInstance Type\n\n\nSelect a VM instance type. For information about instance types on GCP refer to \nGCP documentation\n.\n\n\n\n\n\n\nInstance Count\n\n\nEnter the number of instances of a given type. Default is 1.\n\n\n\n\n\n\nAmbari Server\n\n\nYou must select one node for Ambari Server. The \"Group Size\" for that host group must be set to \"1\".\n\n\n\n\n\n\n\n\n\n\n\n\nOn the \nNetwork\n page, provide the following to specify the networking resources that will be used for your cluster:\n\n\n\n\n\n\n\n\nParameter\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nSelect Network\n\n\nSelect the virtual network in which you would like your cluster to be provisioned. You can select an existing network or create a new network.\n\n\n\n\n\n\nSelect Subnet\n\n\nSelect the subnet in which you would like your cluster to be provisioned. If you are using a new network, create a new subnet. If you are using an existing network, select an existing subnet.\n\n\n\n\n\n\nSubnet (CIDR)\n\n\nIf you selected to create a new subnet, you must define a valid \nCIDR\n for the subnet. Default is 10.0.0.0/16.\n\n\n\n\n\n\n\n\n\n\n\n\nDefine security groups for each host group. You can either create new security groups and define their rules or reuse existing security groups:\n\n\n\n\nExisting security groups are only available for an existing network. \n\n\n\n\n\n\n\n\n\n\nOption\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nNew Security Group\n\n\n(Default) Creates a new security group with the rules that you defined:\nA set of \ndefault rules\n is provided. You should review and adjust these default rules. If you do not make any modifications, default rules will be applied. \nYou may open ports by defining the CIDR, entering port range, selecting protocol and clicking \n+\n.\nYou may delete default or previously added rules using the delete icon.\nIf you don't want to use security group, remove the default rules.\n\n\n\n\n\n\nExisting Security Groups\n\n\nAllows you to select an existing security group that is already available in the selected provider region. This selection is disabled if no existing security groups are available in your chosen region.\n\n\n\n\n\n\n\n\n\n\nImportant\n\n\n\nBy default, ports 22, 443, and 9443 are set to 0.0.0.0/0 CIDR for inbound access on the Ambari node security group. We strongly recommend that you limit this CIDR, considering the following restrictions:\n\nPorts 22 and 9443 must be open to Cloudbreak's CIDR. You can set CB_DEFAULT_GATEWAY_CIDR in your Cloudbreak's Profile file in order to automatically open ports 22 and 9443 to your Cloudbrak IP. Refer to \nRestricting Inbound Access to Clusters\n.\n\n\nPort 22 must be open to your CIDR if you would like to access the master node via SSH.\n\n\nPort 443 must be open to your CIDR if you would like to access Cloudbreak web UI in a browser.\n\n\n\n\n\n\n\n\nImportant\n\n\n\nBy default, port 22 is set to 0.0.0.0/0 CIDR for inbound access on non-Ambari node security groups. We strongly recommend that you remove it.\n\n\n\n\n\n\n\n\nOn the \nSecurity\n page, provide the following parameters:\n\n\n\n\n\n\n\n\nParameter\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nCluster User\n\n\nYou can log in to the Ambari UI using this username. By default, this is set to \nadmin\n.\n\n\n\n\n\n\nPassword\n\n\nYou can log in to the Ambari UI using this password.\n\n\n\n\n\n\nConfirm Password\n\n\nConfirm the password.\n\n\n\n\n\n\nNew SSH public key\n\n\nCheck this option to specify a new public key and then enter the public key. You will use the matching private key to access your cluster nodes via SSH.\n\n\n\n\n\n\nExisting SSH public key\n\n\nSelect an existing public key. You will use the matching private key to access your cluster nodes via SSH. This is a default option as long as an existing SSH public key is available.\n\n\n\n\n\n\n\n\n\n\n\n\nClick on \nCreate Cluster\n to create a cluster.\n\n\n\n\n\n\nYou will be redirected to the Cloudbreak dashboard, and a new tile representing your cluster will appear at the top of the page.\n\n\n\n\n\n\nRelated Links\n\n\nBlueprints\n \n\n\nDefault Cluster Security Groups\n \n\n\nCIDR\n (External) \n\n\nCloud Locations\n (External)\n\n\nMachine Types\n (External)     \n\n\nAdvanced Options\n\n\nClick on \nAdvanced\n to view and enter additional configuration options\n\n\nAvailability Zone\n\n\nChoose one of the availability zones within the selected region. \n\n\nChoose Image Catalog\n\n\nBy default, \nChoose Image Catalog\n is set to the default image catalog that is provided with Cloudbreak. If you would like to use a different image catalog, you must first create and register it. For complete instructions, refer to \nCustom Images\n.\n\n\nRelated Links\n   \n\n\nCustom Images\n  \n\n\nPrewarmed and Base Images\n\n\nCloudbreak supports the following types of images for launching clusters:\n\n\n\n\n\n\n\n\nImage Type\n\n\nDescription\n\n\nDefault Images Provided\n\n\nSupport for Custom Images\n\n\n\n\n\n\n\n\n\n\nBase Images\n\n\nBase images include default configuration and default tooling. These images include the operating system but do not include Ambari or HDP software.\n\n\nYes\n\n\nYes\n\n\n\n\n\n\nPrewarmed Images\n\n\nBy default, Cloudbreak launches clusters from prewarmed images. Prewarmed images include the operating system as well as Ambari and HDP. The HDP and Ambari version used by prewarmed images cannot be customized.\n\n\nYes\n\n\nNo\n\n\n\n\n\n\n\n\nBy default, Cloudbreak uses the included default \nprewarmed images\n, which include the operating system, as well as\nAmbari and HDP packages installed. You can optionally select the \nbase image\n option if you would like to:\n\n\n\n\nUse an Ambari and HDP versions different than what the prewarmed image includes and/or  \n\n\nChoose a previously created custom base image\n\n\n\n\nChoose Image\n  \n\n\nIf under \nChoose Image Catalog\n, you selected a custom image catalog, under \nChoose Image\n you can select an image from that catalog. For complete instructions, refer to \nCustom Images\n. \n\n\nIf you are trying to customize Ambari and HDP versions, you can ignore the \nChoose Image\n option; in this case default base image is used.\n\n\nAmbari Repository Specification\n\n\nIf you would like to use a custom Ambari version, provide the following information: \n\n\n\n\n Ambari 2.6.1\n\n\nIf you would like to use Ambari \n2.6.1\n, use the version provided by default in the Cloudbreak web UI, or newer.\n\n\n\n\n\n\n\n\n\nParameter\n\n\nDescription\n\n\nExample\n\n\n\n\n\n\n\n\n\n\nVersion\n\n\nAmbari version.\n\n\n2.6.1.0\n\n\n\n\n\n\nRepo Url\n\n\nURL to the Ambari version repo.\n\n\nhttp://public-repo-1.hortonworks.com/ambari/centos6/2.x/updates/2.6.1.0\n\n\n\n\n\n\nRepo Gpg Key Url\n\n\nURL to the repo GPG key. Each stable RPM package that is published by CentOS Project is signed with a GPG signature. By default, yum and the graphical update tools will verify these signatures and refuse to install any packages that are not signed, or have an incorrect signature.\n\n\nhttp://public-repo-1.hortonworks.com/ambari/centos6/RPM-GPG-KEY/RPM-GPG-KEY-Jenkins\n\n\n\n\n\n\n\n\nHDP Repository Specification\n\n\nIf you would like to use a custom HDP version, provide the following information: \n\n\n\n\n\n\n\n\nParameter\n\n\nDescription\n\n\nExample\n\n\n\n\n\n\n\n\n\n\nStack\n\n\nStack name.\n\n\nHDP\n\n\n\n\n\n\nVersion\n\n\nStack version.\n\n\n2.6\n\n\n\n\n\n\nOS\n\n\nOperating system.\n\n\ncentos7\n\n\n\n\n\n\nStack Repo Id\n\n\nIdentifier for the repo linked in \"Base Url\".\n\n\nHDP-2.6\n\n\n\n\n\n\nBase Url\n\n\nURL to the repo storing the desired stack version.\n\n\nhttp://public-repo-1.hortonworks.com/HDP/centos7/2.x/updates/2.6.1.0\n\n\n\n\n\n\nUtils Repo Id\n\n\nIdentifier for the repo linked in \"Utils Base Url\".\n\n\nHDP-UTILS-1.1.0.21\n\n\n\n\n\n\nUtils Base Url\n\n\nURL to the repo storing utilities for the desired stack version.\n\n\nhttp://public-repo-1.hortonworks.com/HDP-UTILS-1.1.0.21/repos/centos7\n\n\n\n\n\n\nEnable Ambari Server to download and install GPL Licensed LZO packages?\n\n\n(Optional, only available if using Ambari 2.6.1 or newer) Use this option to enable LZO compression in your HDP cluster. LZO is a lossless data compression library that favors speed over compression ratio. Ambari does not install nor enable LZO compression libraries by default, and must be explicitly configured to do so. For more information, refer to \nEnabling LZO\n.\n\n\n\n\n\n\n\n\n\n\nRelated Links\n    \n\n\nCustom Images\n      \n\n\nEnable Lifetime Management\n\n\nCheck this option if you would like your cluster to be automatically terminated after a specific amount of time (defined as \"Time to Live\" in minutes). \n\n\nTags\n\n\nYou can optionally add tags, which will help you find your cluster-related resources, such as VMs, in your cloud provider account. refer to \nResource Tagging\n.\n\n\nRelated Links\n    \n\n\nResource Tagging\n \n\n\nStorage\n\n\nYou can optionally specify the following storage options for your cluster:\n\n\n\n\n\n\n\n\nParameter\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nStorage Type\n\n\nSelect the volume type. The options are:\nStandard persistent disks (HDD)\nSolid-state persistent disks (SSD)\n For more information about these options refer to \nGCP documentation\n.\n\n\n\n\n\n\nAttached Volumes Per Instance\n\n\nEnter the number of volumes attached per instance. Default is 1.\n\n\n\n\n\n\nVolume Size (GB)\n\n\nEnter the size in GBs for each volume. Default is 100.\n\n\n\n\n\n\n\n\nUse Preemptible Instances\n\n\nCheck this option to use Google Cloud preemptive VM instances as your cluster nodes. To learn more, refer to \nGoogle Cloud documentation\n.    \n\n\nNote that: \n\n\n\n\nWe recommend not using preemptible instances for any host group that includes Ambari server components.  \n\n\nIf you choose to use preemptible instances for a given host group when creating your cluster, any nodes that you add to that host group (during cluster creation or later) will be using preemptible instances.   \n\n\nIf you decide not to use preemptible instances when creating your cluster, any nodes that you add to your host group (during cluster creation or later) will be using standard on-demand instances.     \n\n\nOnce someone outbids you, the preemptible instances are taken away, removing the nodes from the cluster. \n\n\nIf the preemptible instances are not available right away, creating a cluster will take longer than usual. \n\n\n\n\nRecipes\n\n\nThis option allows you to select previously uploaded recipes (scripts that can be run pre or post cluster deployment) for each host group. For more information on recipes, refer to \nRecipes\n. \n\n\nRelated Links\n    \n\n\nRecipes\n \n\n\nAmbari Server Master Key\n\n\nThe Ambari Server Master Key is used to configure Ambari to encrypt database and Kerberos credentials that are retained by Ambari as part of the Ambari setup.  \n\n\nEnable Kerberos Security\n\n\nSelect this option to enable Kerberos for your cluster. For information about available Kerberos options, refer to \nEnabling Kerberos Security\n. \n\n\nRelated Links\n    \n\n\nKerberos\n  \n\n\nPreemptible VM Instances\n (External) \n\n\nStorage Options\n (External)  \n\n\n\n\nNext: Access Cluster", 
            "title": "Create a Cluster"
        }, 
        {
            "location": "/gcp-create/index.html#creating-a-cluster-on-gcp", 
            "text": "Use these steps to create a cluster.  Prerequisites  If you would like to use  Oozie  with  Ambari 2.6.1 or newer , you must install the Ext JS library. For instructions, refer to  Recipe to Install Ext JS for Oozie .  Steps    Log in to the Cloudbreak UI.    Click  Create Cluster  and the  Create Cluster  wizard is displayed. \n    By default,  Basic  view is displayed. To view advanced options, click  Advanced . To learn about advanced options, refer to  Advanced Options .       On the  General Configuration  page, specify the following general parameters for your cluster:     Parameter  Description      Select Credential  Choose a previously created credential.    Cluster Name  Enter a name for your cluster. The name must be between 5 and 40 characters, must start with a letter, and must only include lowercase letters, numbers, and hyphens.    Region  Select the GCP region in which you would like to launch your cluster. For information on available GCP regions, refer to  GCP documentation .    Platform Version  Choose the HDP version to use for this cluster.    Cluster Type  Choose one of default cluster configurations, or, if you have defined your own cluster configuration via Ambari blueprint, you can choose it here. For more information on default and custom blueprints, refer to  Blueprints .       On the  Hardware and Storage  page, for each host group provide the following information to define your cluster nodes and attached storage:     Parameter  Description      Instance Type  Select a VM instance type. For information about instance types on GCP refer to  GCP documentation .    Instance Count  Enter the number of instances of a given type. Default is 1.    Ambari Server  You must select one node for Ambari Server. The \"Group Size\" for that host group must be set to \"1\".       On the  Network  page, provide the following to specify the networking resources that will be used for your cluster:     Parameter  Description      Select Network  Select the virtual network in which you would like your cluster to be provisioned. You can select an existing network or create a new network.    Select Subnet  Select the subnet in which you would like your cluster to be provisioned. If you are using a new network, create a new subnet. If you are using an existing network, select an existing subnet.    Subnet (CIDR)  If you selected to create a new subnet, you must define a valid  CIDR  for the subnet. Default is 10.0.0.0/16.       Define security groups for each host group. You can either create new security groups and define their rules or reuse existing security groups:   Existing security groups are only available for an existing network.       Option  Description      New Security Group  (Default) Creates a new security group with the rules that you defined: A set of  default rules  is provided. You should review and adjust these default rules. If you do not make any modifications, default rules will be applied.  You may open ports by defining the CIDR, entering port range, selecting protocol and clicking  + . You may delete default or previously added rules using the delete icon. If you don't want to use security group, remove the default rules.    Existing Security Groups  Allows you to select an existing security group that is already available in the selected provider region. This selection is disabled if no existing security groups are available in your chosen region.      Important  \nBy default, ports 22, 443, and 9443 are set to 0.0.0.0/0 CIDR for inbound access on the Ambari node security group. We strongly recommend that you limit this CIDR, considering the following restrictions: Ports 22 and 9443 must be open to Cloudbreak's CIDR. You can set CB_DEFAULT_GATEWAY_CIDR in your Cloudbreak's Profile file in order to automatically open ports 22 and 9443 to your Cloudbrak IP. Refer to  Restricting Inbound Access to Clusters .  Port 22 must be open to your CIDR if you would like to access the master node via SSH.  Port 443 must be open to your CIDR if you would like to access Cloudbreak web UI in a browser.     Important  \nBy default, port 22 is set to 0.0.0.0/0 CIDR for inbound access on non-Ambari node security groups. We strongly recommend that you remove it.     On the  Security  page, provide the following parameters:     Parameter  Description      Cluster User  You can log in to the Ambari UI using this username. By default, this is set to  admin .    Password  You can log in to the Ambari UI using this password.    Confirm Password  Confirm the password.    New SSH public key  Check this option to specify a new public key and then enter the public key. You will use the matching private key to access your cluster nodes via SSH.    Existing SSH public key  Select an existing public key. You will use the matching private key to access your cluster nodes via SSH. This is a default option as long as an existing SSH public key is available.       Click on  Create Cluster  to create a cluster.    You will be redirected to the Cloudbreak dashboard, and a new tile representing your cluster will appear at the top of the page.    Related Links  Blueprints    Default Cluster Security Groups    CIDR  (External)   Cloud Locations  (External)  Machine Types  (External)", 
            "title": "Creating a Cluster on GCP"
        }, 
        {
            "location": "/gcp-create/index.html#advanced-options", 
            "text": "Click on  Advanced  to view and enter additional configuration options", 
            "title": "Advanced Options"
        }, 
        {
            "location": "/gcp-create/index.html#availability-zone", 
            "text": "Choose one of the availability zones within the selected region.", 
            "title": "Availability Zone"
        }, 
        {
            "location": "/gcp-create/index.html#choose-image-catalog", 
            "text": "By default,  Choose Image Catalog  is set to the default image catalog that is provided with Cloudbreak. If you would like to use a different image catalog, you must first create and register it. For complete instructions, refer to  Custom Images .  Related Links      Custom Images", 
            "title": "Choose Image Catalog"
        }, 
        {
            "location": "/gcp-create/index.html#prewarmed-and-base-images", 
            "text": "Cloudbreak supports the following types of images for launching clusters:     Image Type  Description  Default Images Provided  Support for Custom Images      Base Images  Base images include default configuration and default tooling. These images include the operating system but do not include Ambari or HDP software.  Yes  Yes    Prewarmed Images  By default, Cloudbreak launches clusters from prewarmed images. Prewarmed images include the operating system as well as Ambari and HDP. The HDP and Ambari version used by prewarmed images cannot be customized.  Yes  No     By default, Cloudbreak uses the included default  prewarmed images , which include the operating system, as well as\nAmbari and HDP packages installed. You can optionally select the  base image  option if you would like to:   Use an Ambari and HDP versions different than what the prewarmed image includes and/or    Choose a previously created custom base image   Choose Image     If under  Choose Image Catalog , you selected a custom image catalog, under  Choose Image  you can select an image from that catalog. For complete instructions, refer to  Custom Images .   If you are trying to customize Ambari and HDP versions, you can ignore the  Choose Image  option; in this case default base image is used.  Ambari Repository Specification  If you would like to use a custom Ambari version, provide the following information:     Ambari 2.6.1  If you would like to use Ambari  2.6.1 , use the version provided by default in the Cloudbreak web UI, or newer.     Parameter  Description  Example      Version  Ambari version.  2.6.1.0    Repo Url  URL to the Ambari version repo.  http://public-repo-1.hortonworks.com/ambari/centos6/2.x/updates/2.6.1.0    Repo Gpg Key Url  URL to the repo GPG key. Each stable RPM package that is published by CentOS Project is signed with a GPG signature. By default, yum and the graphical update tools will verify these signatures and refuse to install any packages that are not signed, or have an incorrect signature.  http://public-repo-1.hortonworks.com/ambari/centos6/RPM-GPG-KEY/RPM-GPG-KEY-Jenkins     HDP Repository Specification  If you would like to use a custom HDP version, provide the following information:      Parameter  Description  Example      Stack  Stack name.  HDP    Version  Stack version.  2.6    OS  Operating system.  centos7    Stack Repo Id  Identifier for the repo linked in \"Base Url\".  HDP-2.6    Base Url  URL to the repo storing the desired stack version.  http://public-repo-1.hortonworks.com/HDP/centos7/2.x/updates/2.6.1.0    Utils Repo Id  Identifier for the repo linked in \"Utils Base Url\".  HDP-UTILS-1.1.0.21    Utils Base Url  URL to the repo storing utilities for the desired stack version.  http://public-repo-1.hortonworks.com/HDP-UTILS-1.1.0.21/repos/centos7    Enable Ambari Server to download and install GPL Licensed LZO packages?  (Optional, only available if using Ambari 2.6.1 or newer) Use this option to enable LZO compression in your HDP cluster. LZO is a lossless data compression library that favors speed over compression ratio. Ambari does not install nor enable LZO compression libraries by default, and must be explicitly configured to do so. For more information, refer to  Enabling LZO .      Related Links       Custom Images", 
            "title": "Prewarmed and Base Images"
        }, 
        {
            "location": "/gcp-create/index.html#enable-lifetime-management", 
            "text": "Check this option if you would like your cluster to be automatically terminated after a specific amount of time (defined as \"Time to Live\" in minutes).", 
            "title": "Enable Lifetime Management"
        }, 
        {
            "location": "/gcp-create/index.html#tags", 
            "text": "You can optionally add tags, which will help you find your cluster-related resources, such as VMs, in your cloud provider account. refer to  Resource Tagging .  Related Links       Resource Tagging", 
            "title": "Tags"
        }, 
        {
            "location": "/gcp-create/index.html#storage", 
            "text": "You can optionally specify the following storage options for your cluster:     Parameter  Description      Storage Type  Select the volume type. The options are: Standard persistent disks (HDD) Solid-state persistent disks (SSD)  For more information about these options refer to  GCP documentation .    Attached Volumes Per Instance  Enter the number of volumes attached per instance. Default is 1.    Volume Size (GB)  Enter the size in GBs for each volume. Default is 100.", 
            "title": "Storage"
        }, 
        {
            "location": "/gcp-create/index.html#use-preemptible-instances", 
            "text": "Check this option to use Google Cloud preemptive VM instances as your cluster nodes. To learn more, refer to  Google Cloud documentation .      Note that:    We recommend not using preemptible instances for any host group that includes Ambari server components.    If you choose to use preemptible instances for a given host group when creating your cluster, any nodes that you add to that host group (during cluster creation or later) will be using preemptible instances.     If you decide not to use preemptible instances when creating your cluster, any nodes that you add to your host group (during cluster creation or later) will be using standard on-demand instances.       Once someone outbids you, the preemptible instances are taken away, removing the nodes from the cluster.   If the preemptible instances are not available right away, creating a cluster will take longer than usual.", 
            "title": "Use Preemptible Instances"
        }, 
        {
            "location": "/gcp-create/index.html#recipes", 
            "text": "This option allows you to select previously uploaded recipes (scripts that can be run pre or post cluster deployment) for each host group. For more information on recipes, refer to  Recipes .   Related Links       Recipes", 
            "title": "Recipes"
        }, 
        {
            "location": "/gcp-create/index.html#ambari-server-master-key", 
            "text": "The Ambari Server Master Key is used to configure Ambari to encrypt database and Kerberos credentials that are retained by Ambari as part of the Ambari setup.", 
            "title": "Ambari Server Master Key"
        }, 
        {
            "location": "/gcp-create/index.html#enable-kerberos-security", 
            "text": "Select this option to enable Kerberos for your cluster. For information about available Kerberos options, refer to  Enabling Kerberos Security .   Related Links       Kerberos     Preemptible VM Instances  (External)   Storage Options  (External)     Next: Access Cluster", 
            "title": "Enable Kerberos Security"
        }, 
        {
            "location": "/gcp-clusters-access/index.html", 
            "text": "Accessing Your Cluster\n\n\nThe following section describes how to access the various services in the cluster.\n\n\nFinding Cluster Information in the UI\n\n\nOnce your cluster is up and running, click on the tile representing your cluster in the Cloudbreak UI to access information related the cluster and access cluster actions. \n\n\n \n\n\nThe information presented includes:\n\n\n\n\nCluster Summary\n  \n\n\nCluster Information\n  \n\n\nHardware\n  \n\n\nTags\n   \n\n\nRecipes\n  \n\n\nRepository Details\n  \n\n\nImage Details\n    \n\n\nNetwork\n   \n\n\nSecurity\n  \n\n\nAutoscaling\n    \n\n\nEvent History\n  \n\n\n\n\n\n  \nTips\n\n  \n\n  \n Access cluster actions such as resize and sync by clicking on \nACTIONS\n.\n\n  \n Access Ambari web UI by clicking on the link in the \nCLUSTER INFORMATION\n section.\n\n\n View public IP addresses for all cluster instances in the \nHARDWARE\n section. Click on the links to view the instances in the cloud console.\n\n\n The SSH user that you must use when accessing cluster VMs is \"cloudbreak\".\n \n\n\n\n\n\n\n\n\nCluster Summary\n\n\nThe summary bar includes the following information about your cluster:\n\n\n\n\n\n\n\n\nItem\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nCluster Name\n\n\nThe name that you selected for your cluster is displayed at the top of the page.\n\n\n\n\n\n\nTime Remaining\n\n\nIf you enabled lifetime management for your cluster, the clock next to the cluster name indicates the amount of time that your cluster will run before it gets terminated. Note that the time remaining counter does not stop when you stop the cluster.\n\n\n\n\n\n\nCloud Provider\n\n\nThe logo of the cloud provider on which the cluster is running.\n\n\n\n\n\n\nCredential\n\n\nThe name of the credential used to create the cluster.\n\n\n\n\n\n\nStatus\n\n\nCurrent status. When a cluster is healthy, the status is \nRunning\n.\n\n\n\n\n\n\nNodes\n\n\nThe current number of cluster nodes, including the master node.\n\n\n\n\n\n\nUptime\n\n\nThe amount of time (HH:MM) that the cluster has been in the running state since it was started. Each time you stop and restart the cluster, the running time is reset to 0.\n\n\n\n\n\n\nCreated\n\n\nThe date when the cluster was created. The date format is Mon DD, YYYY. For example: Oct 27, 2017.\n\n\n\n\n\n\n\n\nCluster Information\n\n\n\n\n\n\n\n\nItem\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nCluster User\n\n\nThe name of the cluster user that you created when creating the cluster.\n\n\n\n\n\n\nSSH Username\n\n\nThe SSH user which you must use when accessing cluster VMs via SSH. The SSH user is always \"cloudbreak\".\n\n\n\n\n\n\nAmbari URL\n\n\nLink to the Ambari web UI.\n\n\n\n\n\n\nRegion\n\n\nThe region in which the cluster is running in the cloud provider infrastructure.\n\n\n\n\n\n\nAvailability Zone\n\n\nThe availability zone within the region in which the cluster is running.\n\n\n\n\n\n\nBlueprint\n\n\nThe name of the blueprint selected under \"Cluster Type\" to create this cluster.\n\n\n\n\n\n\nCreated With\n\n\nThe version of Cloubdreak used to create this cluster.\n\n\n\n\n\n\nAmbari Version\n\n\nThe Ambari version which this cluster is currently running.\n\n\n\n\n\n\nHDP Version\n\n\nThe HDP version which this cluster is currently running.\n\n\n\n\n\n\n\n\nHardware\n\n\nThis section includes information about your cluster instances: instance names, instance IDs, instance types, their status, fully qualified domain names (FQDNs), and private and public IPs. \n\n\nTags\n\n\nThis section lists keys and values of the user-defined tags, in the same order as you added them.\n\n\nRecipes\n\n\nThis section includes recipe-related information. For each recipe, you can see the host group on which a recipe was executed, recipe name, and recipe type. \n\n\nRepository Details\n\n\nThis section includes Ambari and HDP repository information, as you provided it in the \"Base Images\" section when creating a cluster.\n\n\nImage Details\n\n\nThis section includes information about the base image that was used for the Cloudbreak instance. \n\n\nNetwork\n\n\nThis section includes information about the names of the network and subnet in which the cluster is running and the links to related cloud provider console.\n\n\nSecurity\n\n\nThis section is only available if you have enabled Kerberos security. It provides you with the details of your Kerberos configuration.  \n\n\nAutoscaling\n\n\nThis section includes configuration options related to autoscaling. Refer to \nAutoscaling\n.  \n\n\nEvent History\n\n\nThe Event History section shows you events logged for the cluster, with the most recent event at the top. For example, after your cluster has been created, the following messages will be written to the log:\n\n\n\nAmbari cluster built; Ambari ip:34.215.103.66\n10/26/2017, 9:41:58 AM\nBuilding Ambari cluster; Ambari ip:34.215.103.66\n10/26/2017, 9:30:20 AM\nStarting Ambari cluster services\n10/26/2017, 9:27:12 AM\nSetting up infrastructure metadata\n10/26/2017, 9:27:11 AM\nBootstrapping infrastructure cluster\n10/26/2017, 9:26:38 AM\nInfrastructure successfully provisioned\n10/26/2017, 9:26:37 AM\nBilling started, Infrastructure successfully provisioned\n10/26/2017, 9:26:37 AM\nInfrastructure metadata collection finished\n10/26/2017, 9:25:39 AM\nInfrastructure creation took 194 seconds\n10/26/2017, 9:25:37 AM\nCreating infrastructure\n10/26/2017, 9:22:22 AM\nSetting up HDP image\n10/26/2017, 9:22:21 AM\n\n\n\nAccessing Cluster via SSH\n\n\nIf you plan to access the cluster via the command line clients, SSH into the master node instance in the cluster. \n\n\n\n\nIn order to use SSH, you must generate an SSH key pair or use an existing SSH key pair.  \n\n\nYou can find the cluster instance public IP addresses on the cluster details page.  \n\n\nWhen accessing instances via SSH use the \ncloudbreak\n user. \n\n\n\n\nOn Mac OS, you can use the following syntax to SSH to the VM:\n\nssh -i \"privatekey.pem\" cloudbreak@publicIP\n\nFor example:\n\nssh -i \"dominika-kp.pem\" cloudbreak@p52.25.169.132\n\n\nOn Windows, you can SSH using an SSH client such as PuTTY.\n\n\nAccess Ambari\n\n\nYou can access Ambari web UI by clicking on the links provided in the \nCluster Information\n \n \nAmbari URL\n.\n\n\nSteps\n\n\n\n\n\n\nFrom the cluster dashboard, click on the tile representing your cluster to navigate to cluster details.\n\n\n\n\n\n\nFind the Ambari URL in the \nCluster Information\n section. This URL is available once the Ambari cluster creation process has completed.  \n\n\n\n\n\n\nClick on the \nAmbari URL\n link.\n\n\n\n\n\n\nThe first time you access the server, your browser will attempt to confirm that the SSL Certificate is valid. Since Cloudbreak automatically generates a self-signed certificate, your browser will warn you about an Untrusted Connection and ask you to confirm a Security Exception. Depending on your browser, perform the steps below to proceed.\n\n\n\n\n\n\n\n\nBrowser\n\n\nSteps\n\n\n\n\n\n\n\n\n\n\nFirefox\n\n\nClick \nAdvanced\n \n Click \nAdd Exception...\n \n Click \nConfirm Security Exception\n\n\n\n\n\n\nSafari\n\n\nClick \nContinue\n\n\n\n\n\n\nChrome\n\n\nClick \nAdvanced\n \n Click \nProceed...\n\n\n\n\n\n\n\n\n\n\n\n\nCloudbreak User Accounts\n\n\nThe following table describes what credentials to use to access Cloudbreak and Cloudbreak-managed clusters:\n\n\n\n\n\n\n\n\nComponent\n\n\nMethod\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nCloudbreak\n\n\nWeb UI, CLI\n\n\nAccess with the username and password provided when launching Cloudbreak on the cloud provider.\n\n\n\n\n\n\nCloudbreak\n\n\nSSH to VM\n\n\nAccess as the \"cloudbreak\" user with the SSH key provided when launching Cloudbreak on the cloud provider.\n\n\n\n\n\n\nCluster\n\n\nSSH to VMs\n\n\nAccess as the \"cloudbreak\" user with the SSH key provided during cluster creation.\n\n\n\n\n\n\nCluster\n\n\nAmbari UI\n\n\nAccess with the credentials provided in the \u201cCluster User\u201d parameter during cluster creation.\n\n\n\n\n\n\n\n\n\n\nNext: Manage and Monitor Clusters", 
            "title": "Access Cluster"
        }, 
        {
            "location": "/gcp-clusters-access/index.html#accessing-your-cluster", 
            "text": "The following section describes how to access the various services in the cluster.", 
            "title": "Accessing Your Cluster"
        }, 
        {
            "location": "/gcp-clusters-access/index.html#finding-cluster-information-in-the-ui", 
            "text": "Once your cluster is up and running, click on the tile representing your cluster in the Cloudbreak UI to access information related the cluster and access cluster actions.      The information presented includes:   Cluster Summary     Cluster Information     Hardware     Tags      Recipes     Repository Details     Image Details       Network      Security     Autoscaling       Event History      \n   Tips \n   \n    Access cluster actions such as resize and sync by clicking on  ACTIONS . \n    Access Ambari web UI by clicking on the link in the  CLUSTER INFORMATION  section.   View public IP addresses for all cluster instances in the  HARDWARE  section. Click on the links to view the instances in the cloud console.   The SSH user that you must use when accessing cluster VMs is \"cloudbreak\".", 
            "title": "Finding Cluster Information in the UI"
        }, 
        {
            "location": "/gcp-clusters-access/index.html#cluster-summary", 
            "text": "The summary bar includes the following information about your cluster:     Item  Description      Cluster Name  The name that you selected for your cluster is displayed at the top of the page.    Time Remaining  If you enabled lifetime management for your cluster, the clock next to the cluster name indicates the amount of time that your cluster will run before it gets terminated. Note that the time remaining counter does not stop when you stop the cluster.    Cloud Provider  The logo of the cloud provider on which the cluster is running.    Credential  The name of the credential used to create the cluster.    Status  Current status. When a cluster is healthy, the status is  Running .    Nodes  The current number of cluster nodes, including the master node.    Uptime  The amount of time (HH:MM) that the cluster has been in the running state since it was started. Each time you stop and restart the cluster, the running time is reset to 0.    Created  The date when the cluster was created. The date format is Mon DD, YYYY. For example: Oct 27, 2017.", 
            "title": "Cluster Summary"
        }, 
        {
            "location": "/gcp-clusters-access/index.html#cluster-information", 
            "text": "Item  Description      Cluster User  The name of the cluster user that you created when creating the cluster.    SSH Username  The SSH user which you must use when accessing cluster VMs via SSH. The SSH user is always \"cloudbreak\".    Ambari URL  Link to the Ambari web UI.    Region  The region in which the cluster is running in the cloud provider infrastructure.    Availability Zone  The availability zone within the region in which the cluster is running.    Blueprint  The name of the blueprint selected under \"Cluster Type\" to create this cluster.    Created With  The version of Cloubdreak used to create this cluster.    Ambari Version  The Ambari version which this cluster is currently running.    HDP Version  The HDP version which this cluster is currently running.", 
            "title": "Cluster Information"
        }, 
        {
            "location": "/gcp-clusters-access/index.html#hardware", 
            "text": "This section includes information about your cluster instances: instance names, instance IDs, instance types, their status, fully qualified domain names (FQDNs), and private and public IPs.", 
            "title": "Hardware"
        }, 
        {
            "location": "/gcp-clusters-access/index.html#tags", 
            "text": "This section lists keys and values of the user-defined tags, in the same order as you added them.", 
            "title": "Tags"
        }, 
        {
            "location": "/gcp-clusters-access/index.html#recipes", 
            "text": "This section includes recipe-related information. For each recipe, you can see the host group on which a recipe was executed, recipe name, and recipe type.", 
            "title": "Recipes"
        }, 
        {
            "location": "/gcp-clusters-access/index.html#repository-details", 
            "text": "This section includes Ambari and HDP repository information, as you provided it in the \"Base Images\" section when creating a cluster.", 
            "title": "Repository Details"
        }, 
        {
            "location": "/gcp-clusters-access/index.html#image-details", 
            "text": "This section includes information about the base image that was used for the Cloudbreak instance.", 
            "title": "Image Details"
        }, 
        {
            "location": "/gcp-clusters-access/index.html#network", 
            "text": "This section includes information about the names of the network and subnet in which the cluster is running and the links to related cloud provider console.", 
            "title": "Network"
        }, 
        {
            "location": "/gcp-clusters-access/index.html#security", 
            "text": "This section is only available if you have enabled Kerberos security. It provides you with the details of your Kerberos configuration.", 
            "title": "Security"
        }, 
        {
            "location": "/gcp-clusters-access/index.html#autoscaling", 
            "text": "This section includes configuration options related to autoscaling. Refer to  Autoscaling .", 
            "title": "Autoscaling"
        }, 
        {
            "location": "/gcp-clusters-access/index.html#event-history", 
            "text": "The Event History section shows you events logged for the cluster, with the most recent event at the top. For example, after your cluster has been created, the following messages will be written to the log:  \nAmbari cluster built; Ambari ip:34.215.103.66\n10/26/2017, 9:41:58 AM\nBuilding Ambari cluster; Ambari ip:34.215.103.66\n10/26/2017, 9:30:20 AM\nStarting Ambari cluster services\n10/26/2017, 9:27:12 AM\nSetting up infrastructure metadata\n10/26/2017, 9:27:11 AM\nBootstrapping infrastructure cluster\n10/26/2017, 9:26:38 AM\nInfrastructure successfully provisioned\n10/26/2017, 9:26:37 AM\nBilling started, Infrastructure successfully provisioned\n10/26/2017, 9:26:37 AM\nInfrastructure metadata collection finished\n10/26/2017, 9:25:39 AM\nInfrastructure creation took 194 seconds\n10/26/2017, 9:25:37 AM\nCreating infrastructure\n10/26/2017, 9:22:22 AM\nSetting up HDP image\n10/26/2017, 9:22:21 AM", 
            "title": "Event History"
        }, 
        {
            "location": "/gcp-clusters-access/index.html#accessing-cluster-via-ssh", 
            "text": "If you plan to access the cluster via the command line clients, SSH into the master node instance in the cluster.    In order to use SSH, you must generate an SSH key pair or use an existing SSH key pair.    You can find the cluster instance public IP addresses on the cluster details page.    When accessing instances via SSH use the  cloudbreak  user.    On Mac OS, you can use the following syntax to SSH to the VM: ssh -i \"privatekey.pem\" cloudbreak@publicIP \nFor example: ssh -i \"dominika-kp.pem\" cloudbreak@p52.25.169.132  On Windows, you can SSH using an SSH client such as PuTTY.", 
            "title": "Accessing Cluster via SSH"
        }, 
        {
            "location": "/gcp-clusters-access/index.html#access-ambari", 
            "text": "You can access Ambari web UI by clicking on the links provided in the  Cluster Information     Ambari URL .  Steps    From the cluster dashboard, click on the tile representing your cluster to navigate to cluster details.    Find the Ambari URL in the  Cluster Information  section. This URL is available once the Ambari cluster creation process has completed.      Click on the  Ambari URL  link.    The first time you access the server, your browser will attempt to confirm that the SSL Certificate is valid. Since Cloudbreak automatically generates a self-signed certificate, your browser will warn you about an Untrusted Connection and ask you to confirm a Security Exception. Depending on your browser, perform the steps below to proceed.     Browser  Steps      Firefox  Click  Advanced    Click  Add Exception...    Click  Confirm Security Exception    Safari  Click  Continue    Chrome  Click  Advanced    Click  Proceed...", 
            "title": "Access Ambari"
        }, 
        {
            "location": "/gcp-clusters-access/index.html#cloudbreak-user-accounts", 
            "text": "The following table describes what credentials to use to access Cloudbreak and Cloudbreak-managed clusters:     Component  Method  Description      Cloudbreak  Web UI, CLI  Access with the username and password provided when launching Cloudbreak on the cloud provider.    Cloudbreak  SSH to VM  Access as the \"cloudbreak\" user with the SSH key provided when launching Cloudbreak on the cloud provider.    Cluster  SSH to VMs  Access as the \"cloudbreak\" user with the SSH key provided during cluster creation.    Cluster  Ambari UI  Access with the credentials provided in the \u201cCluster User\u201d parameter during cluster creation.      Next: Manage and Monitor Clusters", 
            "title": "Cloudbreak User Accounts"
        }, 
        {
            "location": "/gcp-clusters-manage/index.html", 
            "text": "Managing and Monitoring Clusters\n\n\nYou can manage monitor your clusters from the Cloudbreak UI. To do that, click on the tile representing the cluster that you want to access. The actions available for your cluster are listed in the top right corner: \n\n\n \n\n\n\n  \nTips\n\n  \n\n  \nTo add or remove nodes from your cluster click \nACTIONS>Resize\n.\n\n  \nTo synchronize your cluster with the cloud provider account click \nACTIONS>Sync\n.\n\n  \nTo temporarily stop your cluster click \nSTOP\n.\n\n  \nTo terminate your cluster click \nTERMINATE\n.\n\n\n\n\n\n\n\n\n\nResize a Cluster\n\n\nTo resize a cluster, follow these steps.\n\n\nSteps\n\n\n\n\n\n\nBrowse to the cluster details.\n\n\n\n\n\n\nClick \nActions\n and select \nResize\n. The cluster resize dialog is displayed.\n\n\n\n\n\n\nUsing the +/- controls, adjust the number of nodes for a chosen host group. \n\n\n\n\nYou can only modify one host group at a time. \n\nIt is not possible to resize the Ambari server host group.     \n\n\n\n\n\n\n\n\nClick \nYes\n to confirm the scale-up/scale-down.\n\n\nWhile nodes are being added or removed, cluster status changes to \"Update In Progress\". Once the operation has completed, cluster status changes back to \"Running\". \n\n\n\n\n\n\nSynchronize a Cluster\n\n\nUse the \nsync\n option if you:  \n\n\n\n\nMade changes on your cloud provider side (for example, deleted cluster VMs) and you would like to synchronize Cloudbreak with the cloud provider.  \n\n\nManually changed service status in Ambari (for example, restarted services).   \n\n\n\n\nTo synchronize your cluster with the cloud provider, follow these steps. \n\n\nSteps\n\n\n\n\n\n\nBrowse to the cluster details.\n\n\n\n\n\n\nClick \nActions\n and select \nSync\n.\n\n\n\n\n\n\nClick \nYes\n to confirm.\n\n\nYour cluster infrastructure is synchronized based on changes on the cloud provider. The updates are written to the \"Event History\". \n\n\n\n\n\n\nStop a Cluster\n\n\nCloudbreak supports stopping and restarting clusters. To stop and restart a cluster managed by Cloudbreak, use the options available from the Coudbreak UI. \n\n\nSteps\n\n\n\n\n\n\nBrowse to the cluster details.\n\n\n\n\n\n\nClick \nStop\n to stop a currently running cluster.  \n\n\n\n\n\n\nClick \nYes\n to confirm. \n\n\n\n\n\n\nYour cluster status changes to \"Stopping in progress\" and then to \"Stopped\". Once stopping the infrastructure has completed, you will see a \nStart\n option to restart your cluster. \n\n\n\n\n\n\nWhen a cluster is in the \"stopped\" state, you are not charged for the VMs, but you are charged for external storage.  \n\n\nRestart a Cluster\n\n\nIf your cluster is in the \"Stopped\" state, you can restart the cluster by follow these steps.\n\n\nSteps\n\n\n\n\n\n\nclick \nStart\n. This option is only available when the cluster has been stopped. \n\n\n\n\n\n\nClick \nYes\n to confirm.\n\n\nYour cluster status changes to \"Start in progress\" and then to \"Running\". \n\n\n\n\n\n\nTerminate a Cluster\n\n\nTo terminate a cluster managed by Cloudbreak, use the option available from the Coudbreak UI. \n\n\nSteps\n\n\n\n\n\n\nBrowse to the cluster details.\n\n\n\n\n\n\nClick \nTerminate\n. \n\n\n\n\n\n\nClick \nYes\n to confirm.\n\n\nAll cluster-related resources will be deleted, unless the resources (such as networks and subnets) existed prior to cluster creation or are used by other VMs in which case they will be preserved. \n\n\n\n\n\n\nForce Terminate a Cluster\n\n\nCluster deletion may fail if Cloudbreak is unable to delete one or more of the cloud resources that were part of your cluster infrastructure. In such as case, you can use the \nTerminate\n \n \nForce terminate\n option to remove the cluster entry from the Cloudbreak web UI, but you must also check your cloud provider account to see if there are any resources that must be deleted manually.\n\n\nSteps\n\n\n\n\n\n\nBrowse to the cluster details.\n\n\n\n\n\n\nClick \nTerminate\n. \n\n\n\n\n\n\nCheck  \nForce terminate\n.\n\n\n\n\n\n\nClick \nYes\n to confirm. \n\n\nWhen terminating a cluster with Kerberos enabled, you have an option to disable Kerberos prior to cluster termination. This option removes any cluster-related principals from the KDC.\n\n\n\n\n\n\nThis deletes the cluster tile from the UI.  \n\n\n\n\n\n\nLog in to your cloud provider account and \nmanually delete\n any resources that failed to be deleted.\n\n\n\n\n\n\nView Cluster History\n\n\nFrom the navigation menu in the Cloudbreak UI, you can access the History page that allows you to generate a report showing basic information related to the clusters that were running within the specified range of dates.\n\n\nTo generate a report, follow these steps.\n\n\nSteps\n\n\n\n\n\n\nFrom the Cloudbreak UI navigation menu, select \nHistory\n.\n\n\n\n\n\n\nOn the History page, select the range of dates and click \nShow History\n to generate a report for the selected period.\n\n\n \n\n\n\n\n\n\nHistory Report Content\n\n\nEach entry in the report represents one cluster instance group. For each entry, the report includes the following information:\n\n\n\n\nCreated\n - The date when your cluster was created (YYYY-MM-DD).\n\n\nProvider\n - The name of the cloud provider (AWS, Azure, Google, or OpenStack) on which the cluster instances are/were running.\n\n\nCluster Name\n - The name that you selected for the cluster.  \n\n\nInstance Group\n - The name of the host group.   \n\n\nInstance Count\n - The number of nodes in the host group. This number may be a decimal if a cluster has been resized.\n\n\nInstance Type\n - Provider-specific VM type of the cluster instances. \n\n\nRegion\n - The AWS region in which your cluster is/was running.\n\n\nAvailability Zone\n - The availability zone in which your cluster is/was running.      \n\n\nRunning Time (hours)\n - The sum of the running times for all the nodes in the instance group.\n\n\n\n\nThe \nAGGREGATE RUNNING TIME\n is the sum of the Running Times, adjusted for the selected time range.\n\n\nTo learn about how your cloud provider bills you for the VMs, refer to their documentation:\n\n\n\n\nAWS\n      \n\n\nAzure\n     \n\n\nGCP", 
            "title": "Manage and Monitor Clusters"
        }, 
        {
            "location": "/gcp-clusters-manage/index.html#managing-and-monitoring-clusters", 
            "text": "You can manage monitor your clusters from the Cloudbreak UI. To do that, click on the tile representing the cluster that you want to access. The actions available for your cluster are listed in the top right corner:      \n   Tips \n   \n   To add or remove nodes from your cluster click  ACTIONS>Resize . \n   To synchronize your cluster with the cloud provider account click  ACTIONS>Sync . \n   To temporarily stop your cluster click  STOP . \n   To terminate your cluster click  TERMINATE .", 
            "title": "Managing and Monitoring Clusters"
        }, 
        {
            "location": "/gcp-clusters-manage/index.html#resize-a-cluster", 
            "text": "To resize a cluster, follow these steps.  Steps    Browse to the cluster details.    Click  Actions  and select  Resize . The cluster resize dialog is displayed.    Using the +/- controls, adjust the number of nodes for a chosen host group.    You can only modify one host group at a time.  \nIt is not possible to resize the Ambari server host group.          Click  Yes  to confirm the scale-up/scale-down.  While nodes are being added or removed, cluster status changes to \"Update In Progress\". Once the operation has completed, cluster status changes back to \"Running\".", 
            "title": "Resize a Cluster"
        }, 
        {
            "location": "/gcp-clusters-manage/index.html#synchronize-a-cluster", 
            "text": "Use the  sync  option if you:     Made changes on your cloud provider side (for example, deleted cluster VMs) and you would like to synchronize Cloudbreak with the cloud provider.    Manually changed service status in Ambari (for example, restarted services).      To synchronize your cluster with the cloud provider, follow these steps.   Steps    Browse to the cluster details.    Click  Actions  and select  Sync .    Click  Yes  to confirm.  Your cluster infrastructure is synchronized based on changes on the cloud provider. The updates are written to the \"Event History\".", 
            "title": "Synchronize a Cluster"
        }, 
        {
            "location": "/gcp-clusters-manage/index.html#stop-a-cluster", 
            "text": "Cloudbreak supports stopping and restarting clusters. To stop and restart a cluster managed by Cloudbreak, use the options available from the Coudbreak UI.   Steps    Browse to the cluster details.    Click  Stop  to stop a currently running cluster.      Click  Yes  to confirm.     Your cluster status changes to \"Stopping in progress\" and then to \"Stopped\". Once stopping the infrastructure has completed, you will see a  Start  option to restart your cluster.     When a cluster is in the \"stopped\" state, you are not charged for the VMs, but you are charged for external storage.", 
            "title": "Stop a Cluster"
        }, 
        {
            "location": "/gcp-clusters-manage/index.html#restart-a-cluster", 
            "text": "If your cluster is in the \"Stopped\" state, you can restart the cluster by follow these steps.  Steps    click  Start . This option is only available when the cluster has been stopped.     Click  Yes  to confirm.  Your cluster status changes to \"Start in progress\" and then to \"Running\".", 
            "title": "Restart a Cluster"
        }, 
        {
            "location": "/gcp-clusters-manage/index.html#terminate-a-cluster", 
            "text": "To terminate a cluster managed by Cloudbreak, use the option available from the Coudbreak UI.   Steps    Browse to the cluster details.    Click  Terminate .     Click  Yes  to confirm.  All cluster-related resources will be deleted, unless the resources (such as networks and subnets) existed prior to cluster creation or are used by other VMs in which case they will be preserved.", 
            "title": "Terminate a Cluster"
        }, 
        {
            "location": "/gcp-clusters-manage/index.html#force-terminate-a-cluster", 
            "text": "Cluster deletion may fail if Cloudbreak is unable to delete one or more of the cloud resources that were part of your cluster infrastructure. In such as case, you can use the  Terminate     Force terminate  option to remove the cluster entry from the Cloudbreak web UI, but you must also check your cloud provider account to see if there are any resources that must be deleted manually.  Steps    Browse to the cluster details.    Click  Terminate .     Check   Force terminate .    Click  Yes  to confirm.   When terminating a cluster with Kerberos enabled, you have an option to disable Kerberos prior to cluster termination. This option removes any cluster-related principals from the KDC.    This deletes the cluster tile from the UI.      Log in to your cloud provider account and  manually delete  any resources that failed to be deleted.", 
            "title": "Force Terminate a Cluster"
        }, 
        {
            "location": "/gcp-clusters-manage/index.html#view-cluster-history", 
            "text": "From the navigation menu in the Cloudbreak UI, you can access the History page that allows you to generate a report showing basic information related to the clusters that were running within the specified range of dates.  To generate a report, follow these steps.  Steps    From the Cloudbreak UI navigation menu, select  History .    On the History page, select the range of dates and click  Show History  to generate a report for the selected period.", 
            "title": "View Cluster History"
        }, 
        {
            "location": "/gcp-clusters-manage/index.html#history-report-content", 
            "text": "Each entry in the report represents one cluster instance group. For each entry, the report includes the following information:   Created  - The date when your cluster was created (YYYY-MM-DD).  Provider  - The name of the cloud provider (AWS, Azure, Google, or OpenStack) on which the cluster instances are/were running.  Cluster Name  - The name that you selected for the cluster.    Instance Group  - The name of the host group.     Instance Count  - The number of nodes in the host group. This number may be a decimal if a cluster has been resized.  Instance Type  - Provider-specific VM type of the cluster instances.   Region  - The AWS region in which your cluster is/was running.  Availability Zone  - The availability zone in which your cluster is/was running.        Running Time (hours)  - The sum of the running times for all the nodes in the instance group.   The  AGGREGATE RUNNING TIME  is the sum of the Running Times, adjusted for the selected time range.  To learn about how your cloud provider bills you for the VMs, refer to their documentation:   AWS         Azure        GCP", 
            "title": "History Report Content"
        }, 
        {
            "location": "/os-launch/index.html", 
            "text": "Launching Cloudbreak on OpenStack\n\n\nBefore launching Cloudbreak on OpenStack, review and meet the prerequisites. Next, import Cloudbreak image, launch a VM, SSH to the VM, and start Cloudbreak. Once Cloudbreak is running, log in to the Cloudbreak UI and create a Cloudbreak credential.\n\n\nMeet Minimum System Requirements\n\n\nBefore launching Cloudbreak on your OpenStack, make sure that your OpenStack deployment fulfills the following requirements.\n\n\nSupported Linux Distributions\n\n\nThe following versions of the \nRed Hat Distribution of OpenStack\n (RDO) are supported:\n\n\n\n\nJuno\n\n\nKilo\n\n\nLiberty\n\n\nMitaka\n\n\n\n\nStandard Modules\n\n\nCloudbreak requires that the following standard modules are installed and configured on OpenStack:\n\n\n\n\nKeystone V2 or Keystone V3\n\n\nNeutron (Self-service and provider networking)\n\n\nNova (KVM or Xen hypervisor)\n\n\nGlance\n\n\nCinder (Optional)\n\n\nHeat (Optional but highly recommended, since provisioning through native API calls will be deprecated in the future)\n\n\n\n\nRelated Links\n\n\nRed Hat Distribution of OpenStack\n (External)\n\n\nMeet the Prerequisites\n\n\nBefore launching Cloudbreak on OpenStack, you must meet the following prerequisites.\n\n\nSSH Key Pair\n\n\nGenerate a new SSH key pair\n or use an existing SSH key pair to your OpenStack account. You will be required to select it when launching the VM.\n\n\nSecurity Group\n\n\nIn order to launch Cloudbreak, you must have an existing security group with the following ports open: 22 (for access via SSH), 80 (for access via HTTP), and 443 (for access via HTTPS).\n\n\nFor information about OpenStack security groups, refer to the \nOpenStack Administrator Guide\n.\n\n\nRelated Links\n\n\nOpenStack Administrator Guide\n (External)\n\n\nImport Images to OpenStack\n\n\nAn OpenStack administrator must perform these steps to add the Cloudbreak deployer and HDP images to your OpenStack deployment.\n\n\nImport Cloudbreak Deployer Image\n\n\nImport Cloudbreak deployer image using the following steps.\n\n\nSteps\n\n\n\n\n\n\nDownload the latest Cloudbreak deployer image to your local machine:\n\n\ncurl -O https://public-repo-1.hortonworks.com/HDP/cloudbreak/cloudbreak-deployer-220-2017-12-19.img\n\n\n\n\n\n\nSet the following environment variables for the OpenStack image import:\n\n\nexport CBD_LATEST_IMAGE=cloudbreak-deployer-220-2017-12-19.img\nexport OS_IMAGE_NAME=cloudbreak-deployer-220-2017-12-19.img\nexport OS_USERNAME=your_os_user_name\nexport OS_AUTH_URL=your_authentication_url\nexport OS_TENANT_NAME=your_os_tenant_name\n\n\n\n\n\n\nImport the new image into your OpenStack:\n\n\nglance image-create --name \"$OS_IMAGE_NAME\" --file \"$CBD_LATEST_IMAGE\" --disk-format qcow2 --container-format bare --progress\n\n\n\n\n\n\nAfter performing the import, you should be able to see the Cloudbreak deployer image among your other OpenStack images.\n\n\nImport HDP Image\n\n\nImport HDP image using the following steps.\n\n\nSteps\n\n\n\n\n\n\nDownload the latest HDP image to your local machine:\n\n\ncurl -O https://public-repo-1.hortonworks.com/HDP/cloudbreak/hdc-hdp--1710161226.img\n\n\n\n\n\n\nSet the following environment variables for the OpenStack image import:\n\n\nexport CB_LATEST_IMAGE=hdc-hdp--1710161226.img\nexport CB_LATEST_IMAGE_NAME=hdc-hdp--1710161226.img\nexport OS_USERNAME=your_os_user_name\nexport OS_AUTH_URL=your_authentication_url\nexport OS_TENANT_NAME=your_os_tenant_name\n\n\n\n\n\n\nImport the new image into your OpenStack:\n\n\nglance image-create --name \"$CB_LATEST_IMAGE_NAME\" --file \"$CB_LATEST_IMAGE\" --disk-format qcow2 --container-format bare --progress\n\n\n\n\n\n\nAfter performing the import, you should be able to see the Cloudbreak image among your OpenStack images.\n\n\nLaunch the VM\n\n\nIn your OpenStack, launch and instance providing the following parameters:\n\n\n\n\nSelect a VM flavor which meets the following minimum requirements: 4GB RAM, 10GB disk, 2 cores.\n\n\nSelect the Cloudbreak deployer image that you imported earlier and launch an instance using that image.\n\n\nSelect your SSH key pair.\n\n\nSelect the security group which has the following ports open: 22 (SSH) and 443 (HTTPS).\n\n\nSelect your preconfigured network.\n\n\n\n\nSSH to the VM\n\n\nNow that your VM is ready, access it via SSH:\n\n\n\n\nUse a private key matching the public key that you added to your OpenStack project.\n\n\nThe SSH user is called \"cloudbreak\".\n\n\nYou can obtain the VM's IP address from the details of your instance.\n\n\n\n\nOn Mac OS X, you can SSH to the VM by running the following from the Terminal app: \nssh -i \"your-private-key.pem\" cloudbreak@instance_IP\n where \"your-private-key.pem\" points to the location of your private key and \"instance_IP\" is the public IP address of the VM.\n\n\nOn Windows, you can use \nPuTTy\n.\n\n\nInitialize Your Profile\n\n\nAfter accessing the VM via SSH, you must initialize your Profile.\n\n\nSteps\n\n\n\n\n\n\nNavigate to the cloudbreak-deployment directory:\n\n\ncd /var/lib/cloudbreak-deployment/\n\n\nThis directory contains configuration files and the supporting binaries for Cloudbreak deployer.\n\n\n\n\n\n\nInitialize your profile by creating a new file called \nProfile\n and adding the following content:\n\n\nexport UAA_DEFAULT_SECRET=MY-SECRET\nexport UAA_DEFAULT_USER_PW=MY-PASSWORD\nexport UAA_DEFAULT_USER_EMAIL=MY-EMAIL\nexport PUBLIC_IP=VM-PUBLIC-IP\n\n\nFor example:\n\n\nexport UAA_DEFAULT_SECRET=MySecret123\nexport UAA_DEFAULT_USER_PW=MySecurePassword123\nexport UAA_DEFAULT_USER_EMAIL=dbialek@hortonworks.com\nexport PUBLIC_IP=34.212.141.253\n\n\n\n\nYou will need to provide the email and password when logging in to the Cloudbreak web UI and when using the Cloudbreak CLI. The secret will be used by Cloudbreak for authentication.\n\n\n\n\n\n\n\n\nPerform Optional Configurations\n\n\n\n\nThese configurations are optional.\n\n\n\n\nConfiguring a Self-Signed Certificate\n\n\nIf your OpenStack is secured with a self-signed certificate, you need to import that certificate into Cloudbreak, or else Cloudbreak won't be able to communicate with your OpenStack.\n\n\nTo import the certificate, place the certificate file in the \n/certs/trusted/\n directory, follow these steps.\n\n\nSteps\n\n\n\n\nNavigate to the \ncerts\n directory (automatically generated).\n\n\nCreate the \ntrusted\n directory.\n\n\nCopy the certificate to the \ntrusted\n directory.\n\n\n\n\nCloudbreak will automatically pick up the certificate and import it into its truststore upon start.\n\n\nConfiguring Availability Zone and Region\n\n\nBy default, Cloudbreak uses \nRegionOne\n region with \nnova\n availability zone, but you can customize Cloudbreak deployment and enable multiple regions and availability zones by creating an \nopenstack-zone.json\n file in the \netc\n directory of Cloudbreak deployment (that is\n/var/lib/cloudbreak-deployment/etc/openstack-zone.json\n). If the etc directory does not exist in the Cloudbreak deployment directory, then create it.\n\n\nThe following is an example of \nopenstack-zone.json\n containing two regions and four availability zones:\n\n\n{\n  \"items\": [\n    {\n      \"name\": \"MyRegionOne\",\n      \"zones\": [ \"az1\", \"az2\", \"az3\"]\n    },\n    {\n      \"name\": \"MyRegionTwo\",\n      \"zones\": [ \"myaz\"]\n    }\n  ]\n}\n\n\n\n\n\n\nIf you are performing this after you have started cbd, perform \ncbd restart\n.\n\n\n\n\nLaunch Cloudbreak Deployer\n\n\nLaunch Cloudbreak deployer using the following steps.\n\n\nSteps\n\n\n\n\n\n\nStart the Cloudbreak application by using the following command:\n\n\ncbd start\n\n\nThis will start the Docker containers and initialize the application. The first time you start the Coudbreak app, the process will take longer than usual due to the download of all the necessary docker images.\n\n\nThis will start the Docker containers and initialize the application. The first time you start the Coudbreak app, this also downloads of all the necessary docker images.\n\n\nOnce the \ncbd start\n has finished, it returns the \"Uluwatu (Cloudbreak UI) url\" which you can later paste in your browser and log in to Cloudbreak web UI.\n\n\n\n\n\n\nCheck Cloudbreak deployer version and health:\n\n\ncbd doctor\n\n\n\n\n\n\nNext, check Cloudbreak Application logs:\n\n\ncbd logs cloudbreak\n\n\nYou should see a message like this in the log: \nStarted CloudbreakApplication in 36.823 seconds.\n Cloudbreak takes less than a minute to start. If you try to access the Cloudbreak UI before Cloudbreak started, you will get a \"Bad Gateway\" error or \"Cannot connect to Cloubdreak\" error.\n\n\n\n\n\n\nAccess Cloudbreak UI\n\n\nLog in to the Cloudbreak UI using the following steps.\n\n\nSteps\n\n\n\n\n\n\nYou can log into the Cloudbreak application at \nhttps://IP_Address\n where \"IP_Address\" if the public IP of your OpenStack VM. For example \nhttps://34.212.141.253\n.\n\n\n\n\n\n\nConfirm the security exception to proceed to the Cloudbreak web UI.\n\n\nThe first time you access Cloudbreak UI, Cloudbreak will automatically generate a self-signed certificate, due to which your browser will warn you about an untrusted connection and will ask you to confirm a security exception.\n\n\n\n\n\n\nThe login page is displayed:\n\n\n\n\n\n\n\n\nLog in to the Cloudbreak web UI using the credential that you configured in your \nProfile\n file when \nlaunching Cloudbreak deployer\n:\n\n\n\n\nThe username is the \nUAA_DEFAULT_USER_EMAIL\n\n\nThe password is the \nUAA_DEFAULT_USER_PW\n\n\n\n\n\n\n\n\nUpon a successful login, you are redirected to the dashboard:\n\n\n\n\n\n\n\n\nCreate Cloudbreak Credential\n\n\nCloudbreak works by connecting your OpenStack account through this credential, and then uses it to create resources on your behalf. Before you can start provisioning cluster using Cloudbreak, you must create a Cloudbreak credential.\n\n\nSteps\n\n\n\n\n\n\nIn the Cloudbreak web UI, select \nCredentials\n from the navigation pane.\n\n\n\n\n\n\nClick \nCreate Credential\n.\n\n\n\n\n\n\nUnder \nCloud provider\n, select \"Google Cloud Platform\".\n\n\n\n\n\n\n\n\nSelect the keystone version.\n\n\n\n\n\n\nProvide the  following information:\n\n\nFor Keystone v2:\n\n\n\n\n\n\n\n\nParameter\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nName\n\n\nEnter a name for your credential.\n\n\n\n\n\n\nDescription\n\n\n(Optional) Enter a description.\n\n\n\n\n\n\nUser\n\n\nEnter your OpenStack user name.\n\n\n\n\n\n\nPassword\n\n\nEnter your OpenStack password.\n\n\n\n\n\n\nTenant Name\n\n\nEnter the OpenStack tenant name.\n\n\n\n\n\n\nEndpoint\n\n\nEnter the OpenStack endpoint.\n\n\n\n\n\n\nAPI Facing\n\n\n(Optional) Select \npublic\n, \nprivate\n, or \ninternal\n.\n\n\n\n\n\n\n\n\nFor Keystone v3:\n\n\n\n\n\n\n\n\nParameter\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nKeystone scope\n\n\nSelect the scope: default, domain, or project.\n\n\n\n\n\n\nName\n\n\nEnter a name for your credential.\n\n\n\n\n\n\nDescription\n\n\n(Optional) Enter a description.\n\n\n\n\n\n\nUser\n\n\nEnter your OpenStack user name.\n\n\n\n\n\n\nPassword\n\n\nEnter your OpenStack password.\n\n\n\n\n\n\nUser Domain\n\n\nEnter your OpenStack user domain.\n\n\n\n\n\n\nEndpoint\n\n\nEnter the OpenStack endpoint.\n\n\n\n\n\n\nAPI Facing\n\n\n(Optional) Select \npublic\n, \nprivate\n, or \ninternal\n.\n\n\n\n\n\n\n\n\n\n\n\n\nClick \nCreate\n.\n\n\n\n\n\n\nYour credential should now be displayed in the \nCredentials\n pane.\n\n\nCongratulations! You have successfully launched Cloudbreak and created a Cloudbreak credential. Now you can use Cloudbreak to \ncreate clusters\n.\n\n\n\n\n\n\n\n\nNext: Create a Cluster", 
            "title": "Launch on Open Stack"
        }, 
        {
            "location": "/os-launch/index.html#launching-cloudbreak-on-openstack", 
            "text": "Before launching Cloudbreak on OpenStack, review and meet the prerequisites. Next, import Cloudbreak image, launch a VM, SSH to the VM, and start Cloudbreak. Once Cloudbreak is running, log in to the Cloudbreak UI and create a Cloudbreak credential.", 
            "title": "Launching Cloudbreak on OpenStack"
        }, 
        {
            "location": "/os-launch/index.html#meet-minimum-system-requirements", 
            "text": "Before launching Cloudbreak on your OpenStack, make sure that your OpenStack deployment fulfills the following requirements.", 
            "title": "Meet Minimum System Requirements"
        }, 
        {
            "location": "/os-launch/index.html#supported-linux-distributions", 
            "text": "The following versions of the  Red Hat Distribution of OpenStack  (RDO) are supported:   Juno  Kilo  Liberty  Mitaka", 
            "title": "Supported Linux Distributions"
        }, 
        {
            "location": "/os-launch/index.html#standard-modules", 
            "text": "Cloudbreak requires that the following standard modules are installed and configured on OpenStack:   Keystone V2 or Keystone V3  Neutron (Self-service and provider networking)  Nova (KVM or Xen hypervisor)  Glance  Cinder (Optional)  Heat (Optional but highly recommended, since provisioning through native API calls will be deprecated in the future)   Related Links  Red Hat Distribution of OpenStack  (External)", 
            "title": "Standard Modules"
        }, 
        {
            "location": "/os-launch/index.html#meet-the-prerequisites", 
            "text": "Before launching Cloudbreak on OpenStack, you must meet the following prerequisites.", 
            "title": "Meet the Prerequisites"
        }, 
        {
            "location": "/os-launch/index.html#ssh-key-pair", 
            "text": "Generate a new SSH key pair  or use an existing SSH key pair to your OpenStack account. You will be required to select it when launching the VM.", 
            "title": "SSH Key Pair"
        }, 
        {
            "location": "/os-launch/index.html#security-group", 
            "text": "In order to launch Cloudbreak, you must have an existing security group with the following ports open: 22 (for access via SSH), 80 (for access via HTTP), and 443 (for access via HTTPS).  For information about OpenStack security groups, refer to the  OpenStack Administrator Guide .  Related Links  OpenStack Administrator Guide  (External)", 
            "title": "Security Group"
        }, 
        {
            "location": "/os-launch/index.html#import-images-to-openstack", 
            "text": "An OpenStack administrator must perform these steps to add the Cloudbreak deployer and HDP images to your OpenStack deployment.", 
            "title": "Import Images to OpenStack"
        }, 
        {
            "location": "/os-launch/index.html#import-cloudbreak-deployer-image", 
            "text": "Import Cloudbreak deployer image using the following steps.  Steps    Download the latest Cloudbreak deployer image to your local machine:  curl -O https://public-repo-1.hortonworks.com/HDP/cloudbreak/cloudbreak-deployer-220-2017-12-19.img    Set the following environment variables for the OpenStack image import:  export CBD_LATEST_IMAGE=cloudbreak-deployer-220-2017-12-19.img\nexport OS_IMAGE_NAME=cloudbreak-deployer-220-2017-12-19.img\nexport OS_USERNAME=your_os_user_name\nexport OS_AUTH_URL=your_authentication_url\nexport OS_TENANT_NAME=your_os_tenant_name    Import the new image into your OpenStack:  glance image-create --name \"$OS_IMAGE_NAME\" --file \"$CBD_LATEST_IMAGE\" --disk-format qcow2 --container-format bare --progress    After performing the import, you should be able to see the Cloudbreak deployer image among your other OpenStack images.", 
            "title": "Import Cloudbreak Deployer Image"
        }, 
        {
            "location": "/os-launch/index.html#import-hdp-image", 
            "text": "Import HDP image using the following steps.  Steps    Download the latest HDP image to your local machine:  curl -O https://public-repo-1.hortonworks.com/HDP/cloudbreak/hdc-hdp--1710161226.img    Set the following environment variables for the OpenStack image import:  export CB_LATEST_IMAGE=hdc-hdp--1710161226.img\nexport CB_LATEST_IMAGE_NAME=hdc-hdp--1710161226.img\nexport OS_USERNAME=your_os_user_name\nexport OS_AUTH_URL=your_authentication_url\nexport OS_TENANT_NAME=your_os_tenant_name    Import the new image into your OpenStack:  glance image-create --name \"$CB_LATEST_IMAGE_NAME\" --file \"$CB_LATEST_IMAGE\" --disk-format qcow2 --container-format bare --progress    After performing the import, you should be able to see the Cloudbreak image among your OpenStack images.", 
            "title": "Import HDP Image"
        }, 
        {
            "location": "/os-launch/index.html#launch-the-vm", 
            "text": "In your OpenStack, launch and instance providing the following parameters:   Select a VM flavor which meets the following minimum requirements: 4GB RAM, 10GB disk, 2 cores.  Select the Cloudbreak deployer image that you imported earlier and launch an instance using that image.  Select your SSH key pair.  Select the security group which has the following ports open: 22 (SSH) and 443 (HTTPS).  Select your preconfigured network.", 
            "title": "Launch the VM"
        }, 
        {
            "location": "/os-launch/index.html#ssh-to-the-vm", 
            "text": "Now that your VM is ready, access it via SSH:   Use a private key matching the public key that you added to your OpenStack project.  The SSH user is called \"cloudbreak\".  You can obtain the VM's IP address from the details of your instance.   On Mac OS X, you can SSH to the VM by running the following from the Terminal app:  ssh -i \"your-private-key.pem\" cloudbreak@instance_IP  where \"your-private-key.pem\" points to the location of your private key and \"instance_IP\" is the public IP address of the VM.  On Windows, you can use  PuTTy .", 
            "title": "SSH to the VM"
        }, 
        {
            "location": "/os-launch/index.html#initialize-your-profile", 
            "text": "After accessing the VM via SSH, you must initialize your Profile.  Steps    Navigate to the cloudbreak-deployment directory:  cd /var/lib/cloudbreak-deployment/  This directory contains configuration files and the supporting binaries for Cloudbreak deployer.    Initialize your profile by creating a new file called  Profile  and adding the following content:  export UAA_DEFAULT_SECRET=MY-SECRET\nexport UAA_DEFAULT_USER_PW=MY-PASSWORD\nexport UAA_DEFAULT_USER_EMAIL=MY-EMAIL\nexport PUBLIC_IP=VM-PUBLIC-IP  For example:  export UAA_DEFAULT_SECRET=MySecret123\nexport UAA_DEFAULT_USER_PW=MySecurePassword123\nexport UAA_DEFAULT_USER_EMAIL=dbialek@hortonworks.com\nexport PUBLIC_IP=34.212.141.253   You will need to provide the email and password when logging in to the Cloudbreak web UI and when using the Cloudbreak CLI. The secret will be used by Cloudbreak for authentication.", 
            "title": "Initialize Your Profile"
        }, 
        {
            "location": "/os-launch/index.html#perform-optional-configurations", 
            "text": "These configurations are optional.", 
            "title": "Perform Optional Configurations"
        }, 
        {
            "location": "/os-launch/index.html#configuring-a-self-signed-certificate", 
            "text": "If your OpenStack is secured with a self-signed certificate, you need to import that certificate into Cloudbreak, or else Cloudbreak won't be able to communicate with your OpenStack.  To import the certificate, place the certificate file in the  /certs/trusted/  directory, follow these steps.  Steps   Navigate to the  certs  directory (automatically generated).  Create the  trusted  directory.  Copy the certificate to the  trusted  directory.   Cloudbreak will automatically pick up the certificate and import it into its truststore upon start.", 
            "title": "Configuring a Self-Signed Certificate"
        }, 
        {
            "location": "/os-launch/index.html#configuring-availability-zone-and-region", 
            "text": "By default, Cloudbreak uses  RegionOne  region with  nova  availability zone, but you can customize Cloudbreak deployment and enable multiple regions and availability zones by creating an  openstack-zone.json  file in the  etc  directory of Cloudbreak deployment (that is /var/lib/cloudbreak-deployment/etc/openstack-zone.json ). If the etc directory does not exist in the Cloudbreak deployment directory, then create it.  The following is an example of  openstack-zone.json  containing two regions and four availability zones:  {\n  \"items\": [\n    {\n      \"name\": \"MyRegionOne\",\n      \"zones\": [ \"az1\", \"az2\", \"az3\"]\n    },\n    {\n      \"name\": \"MyRegionTwo\",\n      \"zones\": [ \"myaz\"]\n    }\n  ]\n}   If you are performing this after you have started cbd, perform  cbd restart .", 
            "title": "Configuring Availability Zone and Region"
        }, 
        {
            "location": "/os-launch/index.html#launch-cloudbreak-deployer", 
            "text": "Launch Cloudbreak deployer using the following steps.  Steps    Start the Cloudbreak application by using the following command:  cbd start  This will start the Docker containers and initialize the application. The first time you start the Coudbreak app, the process will take longer than usual due to the download of all the necessary docker images.  This will start the Docker containers and initialize the application. The first time you start the Coudbreak app, this also downloads of all the necessary docker images.  Once the  cbd start  has finished, it returns the \"Uluwatu (Cloudbreak UI) url\" which you can later paste in your browser and log in to Cloudbreak web UI.    Check Cloudbreak deployer version and health:  cbd doctor    Next, check Cloudbreak Application logs:  cbd logs cloudbreak  You should see a message like this in the log:  Started CloudbreakApplication in 36.823 seconds.  Cloudbreak takes less than a minute to start. If you try to access the Cloudbreak UI before Cloudbreak started, you will get a \"Bad Gateway\" error or \"Cannot connect to Cloubdreak\" error.", 
            "title": "Launch Cloudbreak Deployer"
        }, 
        {
            "location": "/os-launch/index.html#access-cloudbreak-ui", 
            "text": "Log in to the Cloudbreak UI using the following steps.  Steps    You can log into the Cloudbreak application at  https://IP_Address  where \"IP_Address\" if the public IP of your OpenStack VM. For example  https://34.212.141.253 .    Confirm the security exception to proceed to the Cloudbreak web UI.  The first time you access Cloudbreak UI, Cloudbreak will automatically generate a self-signed certificate, due to which your browser will warn you about an untrusted connection and will ask you to confirm a security exception.    The login page is displayed:     Log in to the Cloudbreak web UI using the credential that you configured in your  Profile  file when  launching Cloudbreak deployer :   The username is the  UAA_DEFAULT_USER_EMAIL  The password is the  UAA_DEFAULT_USER_PW     Upon a successful login, you are redirected to the dashboard:", 
            "title": "Access Cloudbreak UI"
        }, 
        {
            "location": "/os-launch/index.html#create-cloudbreak-credential", 
            "text": "Cloudbreak works by connecting your OpenStack account through this credential, and then uses it to create resources on your behalf. Before you can start provisioning cluster using Cloudbreak, you must create a Cloudbreak credential.  Steps    In the Cloudbreak web UI, select  Credentials  from the navigation pane.    Click  Create Credential .    Under  Cloud provider , select \"Google Cloud Platform\".     Select the keystone version.    Provide the  following information:  For Keystone v2:     Parameter  Description      Name  Enter a name for your credential.    Description  (Optional) Enter a description.    User  Enter your OpenStack user name.    Password  Enter your OpenStack password.    Tenant Name  Enter the OpenStack tenant name.    Endpoint  Enter the OpenStack endpoint.    API Facing  (Optional) Select  public ,  private , or  internal .     For Keystone v3:     Parameter  Description      Keystone scope  Select the scope: default, domain, or project.    Name  Enter a name for your credential.    Description  (Optional) Enter a description.    User  Enter your OpenStack user name.    Password  Enter your OpenStack password.    User Domain  Enter your OpenStack user domain.    Endpoint  Enter the OpenStack endpoint.    API Facing  (Optional) Select  public ,  private , or  internal .       Click  Create .    Your credential should now be displayed in the  Credentials  pane.  Congratulations! You have successfully launched Cloudbreak and created a Cloudbreak credential. Now you can use Cloudbreak to  create clusters .     Next: Create a Cluster", 
            "title": "Create Cloudbreak Credential"
        }, 
        {
            "location": "/os-create/index.html", 
            "text": "Creating a Cluster on OpenStack\n\n\nUse these steps to create a cluster.\n\n\nPrerequisites\n\n\nIf you would like to use \nOozie\n with \nAmbari 2.6.1 or newer\n, you must install the Ext JS library. For instructions, refer to \nRecipe to Install Ext JS for Oozie\n.\n\n\nSteps\n\n\n\n\n\n\nLog in to the Cloudbreak UI.\n\n\n\n\n\n\nClick \nCreate Cluster\n and the \nCreate Cluster\n wizard is displayed.\n\n    By default, \nBasic\n view is displayed. To view advanced options, click \nAdvanced\n. To learn about advanced options, refer to \nAdvanced Options\n.\n\n\n \n\n\n\n\n\n\nOn the \nGeneral Configuration\n page, specify the following general parameters for your cluster:\n\n\n\n\n\n\n\n\nParameter\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nSelect Credential\n\n\nChoose a previously created credential.\n\n\n\n\n\n\nCluster Name\n\n\nEnter a name for your cluster. The name must be between 5 and 40 characters, must start with a letter, and must only include lowercase letters, numbers, and hyphens.\n\n\n\n\n\n\nRegion\n\n\nSelect the region in which you would like to launch your cluster.\n\n\n\n\n\n\nPlatform Version\n\n\nChoose the HDP version to use for this cluster.\n\n\n\n\n\n\nCluster Type\n\n\nChoose one of default cluster configurations, or, if you have defined your own cluster configuration via Ambari blueprint, you can choose it here. For more information on default and custom blueprints, refer to \nBlueprints\n.\n\n\n\n\n\n\n\n\n\n\n\n\nOn the \nHardware and Storage\n page, for each host group provide the following information to define your cluster nodes and attached storage:\n\n\n\n\n\n\n\n\nParameter\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nInstance Type\n\n\nSelect an instance type.\n\n\n\n\n\n\nInstance Count\n\n\nEnter the number of instances of a given type. Default is 1.\n\n\n\n\n\n\nAmbari Server\n\n\nYou must select one node for Ambari Server. The \"Group Size\" for that host group must be set to \"1\".\n\n\n\n\n\n\n\n\n\n\n\n\nOn the \nNetwork\n page, provide the following to specify the networking resources that will be used for your cluster:\n\n\n\n\n\n\n\n\nParameter\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nSelect Network\n\n\nSelect the virtual network in which you would like your cluster to be provisioned. You can select an existing network or create a new network.\n\n\n\n\n\n\nSelect Subnet\n\n\nSelect the subnet in which you would like your cluster to be provisioned. If you are using a new network, create a new subnet. If you are using an existing network, select an existing subnet.\n\n\n\n\n\n\nSubnet (CIDR)\n\n\nIf you selected to create a new subnet, you must define a valid \nCIDR\n for the subnet. Default is 10.0.0.0/16.\n\n\n\n\n\n\n\n\n\n\n\n\nDefine security groups for each host group. You can either create new security groups and define their rules or reuse existing security groups:\n\n\n\n\n\n\n\n\nOption\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nNew Security Group\n\n\n(Default) Creates a new security group with the rules that you defined:\nA set of \ndefault rules\n is provided. You should review and adjust these default rules. If you do not make any modifications, default rules will be applied. \nYou may open ports by defining the CIDR, entering port range, selecting protocol and clicking \n+\n.\nYou may delete default or previously added rules using the delete icon.\nIf you don't want to use security group, remove the default rules.\n\n\n\n\n\n\nExisting Security Groups\n\n\nAllows you to select an existing security group that is already available in the selected provider region. This selection is disabled if no existing security groups are available in your chosen region.\n\n\n\n\n\n\n\n\n\n\nImportant\n\n\n\nBy default, ports 22, 443, and 9443 are set to 0.0.0.0/0 CIDR for inbound access on the Ambari node security group. We strongly recommend that you limit this CIDR, considering the following restrictions:\n\nPorts 22 and 9443 must be open to Cloudbreak's CIDR. You can set CB_DEFAULT_GATEWAY_CIDR in your Cloudbreak's Profile file in order to automatically open ports 22 and 9443 to your Cloudbrak IP. Refer to \nRestricting Inbound Access to Clusters\n.\n\n\nPort 22 must be open to your CIDR if you would like to access the master node via SSH.\n\n\nPort 443 must be open to your CIDR if you would like to access Cloudbreak web UI in a browser.\n\n\n\n\n\n\n\n\nImportant\n\n\n\nBy default, port 22 is set to 0.0.0.0/0 CIDR for inbound access on non-Ambari node security groups. We strongly recommend that you remove it.\n\n\n\n\n\n\n\n\nOn the \nSecurity\n page, provide the following parameters:\n\n\n\n\n\n\n\n\nParameter\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nCluster User\n\n\nYou can log in to the Ambari UI using this username. By default, this is set to \nadmin\n.\n\n\n\n\n\n\nPassword\n\n\nYou can log in to the Ambari UI using this password.\n\n\n\n\n\n\nConfirm Password\n\n\nConfirm the password.\n\n\n\n\n\n\nNew SSH public key\n\n\nCheck this option to specify a new public key and then enter the public key. You will use the matching private key to access your cluster nodes via SSH.\n\n\n\n\n\n\nExisting SSH public key\n\n\nSelect an existing public key. You will use the matching private key to access your cluster nodes via SSH. This is a default option as long as an existing SSH public key is available.\n\n\n\n\n\n\n\n\n\n\n\n\nClick on \nCreate Cluster\n to create a cluster.\n\n\n\n\n\n\nYou will be redirected to the Cloudbreak dashboard, and a new tile representing your cluster will appear at the top of the page.\n\n\n\n\n\n\nRelated Links\n\n\nBlueprints\n \n\n\nDefault Cluster Security Groups\n\n\nCIDR\n (External)    \n\n\nAdvanced Options\n\n\nClick on \nAdvanced\n to view and enter additional configuration options\n\n\nAvailability Zone\n\n\nChoose one of the availability zones within the selected region. \n\n\nChoose Image Catalog\n\n\nBy default, \nChoose Image Catalog\n is set to the default image catalog that is provided with Cloudbreak. If you would like to use a different image catalog, you must first create and register it. For complete instructions, refer to \nCustom Images\n.\n\n\nRelated Links\n   \n\n\nCustom Images\n  \n\n\nPrewarmed and Base Images\n\n\nCloudbreak supports the following types of images for launching clusters:\n\n\n\n\n\n\n\n\nImage Type\n\n\nDescription\n\n\nDefault Images Provided\n\n\nSupport for Custom Images\n\n\n\n\n\n\n\n\n\n\nBase Images\n\n\nBase images include default configuration and default tooling. These images include the operating system but do not include Ambari or HDP software.\n\n\nYes\n\n\nYes\n\n\n\n\n\n\nPrewarmed Images\n\n\nBy default, Cloudbreak launches clusters from prewarmed images. Prewarmed images include the operating system as well as Ambari and HDP. The HDP and Ambari version used by prewarmed images cannot be customized.\n\n\nYes\n\n\nNo\n\n\n\n\n\n\n\n\nBy default, Cloudbreak uses the included default \nprewarmed images\n, which include the operating system, as well as\nAmbari and HDP packages installed. You can optionally select the \nbase image\n option if you would like to:\n\n\n\n\nUse an Ambari and HDP versions different than what the prewarmed image includes and/or  \n\n\nChoose a previously created custom base image\n\n\n\n\nChoose Image\n  \n\n\nIf under \nChoose Image Catalog\n, you selected a custom image catalog, under \nChoose Image\n you can select an image from that catalog. For complete instructions, refer to \nCustom Images\n. \n\n\nIf you are trying to customize Ambari and HDP versions, you can ignore the \nChoose Image\n option; in this case default base image is used.\n\n\nAmbari Repository Specification\n\n\nIf you would like to use a custom Ambari version, provide the following information: \n\n\n\n\n Ambari 2.6.1\n\n\nIf you would like to use Ambari \n2.6.1\n, use the version provided by default in the Cloudbreak web UI, or newer.\n\n\n\n\n\n\n\n\n\nParameter\n\n\nDescription\n\n\nExample\n\n\n\n\n\n\n\n\n\n\nVersion\n\n\nAmbari version.\n\n\n2.6.1.0\n\n\n\n\n\n\nRepo Url\n\n\nURL to the Ambari version repo.\n\n\nhttp://public-repo-1.hortonworks.com/ambari/centos6/2.x/updates/2.6.1.0\n\n\n\n\n\n\nRepo Gpg Key Url\n\n\nURL to the repo GPG key. Each stable RPM package that is published by CentOS Project is signed with a GPG signature. By default, yum and the graphical update tools will verify these signatures and refuse to install any packages that are not signed, or have an incorrect signature.\n\n\nhttp://public-repo-1.hortonworks.com/ambari/centos6/RPM-GPG-KEY/RPM-GPG-KEY-Jenkins\n\n\n\n\n\n\n\n\nHDP Repository Specification\n\n\nIf you would like to use a custom HDP version, provide the following information: \n\n\n\n\n\n\n\n\nParameter\n\n\nDescription\n\n\nExample\n\n\n\n\n\n\n\n\n\n\nStack\n\n\nStack name.\n\n\nHDP\n\n\n\n\n\n\nVersion\n\n\nStack version.\n\n\n2.6\n\n\n\n\n\n\nOS\n\n\nOperating system.\n\n\ncentos7\n\n\n\n\n\n\nStack Repo Id\n\n\nIdentifier for the repo linked in \"Base Url\".\n\n\nHDP-2.6\n\n\n\n\n\n\nBase Url\n\n\nURL to the repo storing the desired stack version.\n\n\nhttp://public-repo-1.hortonworks.com/HDP/centos7/2.x/updates/2.6.1.0\n\n\n\n\n\n\nUtils Repo Id\n\n\nIdentifier for the repo linked in \"Utils Base Url\".\n\n\nHDP-UTILS-1.1.0.21\n\n\n\n\n\n\nUtils Base Url\n\n\nURL to the repo storing utilities for the desired stack version.\n\n\nhttp://public-repo-1.hortonworks.com/HDP-UTILS-1.1.0.21/repos/centos7\n\n\n\n\n\n\nEnable Ambari Server to download and install GPL Licensed LZO packages?\n\n\n(Optional, only available if using Ambari 2.6.1 or newer) Use this option to enable LZO compression in your HDP cluster. LZO is a lossless data compression library that favors speed over compression ratio. Ambari does not install nor enable LZO compression libraries by default, and must be explicitly configured to do so. For more information, refer to \nEnabling LZO\n.\n\n\n\n\n\n\n\n\n\n\nRelated Links\n    \n\n\nCustom Images\n      \n\n\nEnable Lifetime Management\n\n\nCheck this option if you would like your cluster to be automatically terminated after a specific amount of time (defined as \"Time to Live\" in minutes). \n\n\nTags\n\n\nYou can optionally add tags, which will help you find your cluster-related resources, such as VMs, in your cloud provider account. refer to \nResource Tagging\n.\n\n\nRelated Links\n    \n\n\nResource Tagging\n \n\n\nStorage\n\n\nYou can optionally specify the following storage options for your cluster:\n\n\n\n\n\n\n\n\nParameter\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nStorage Type\n\n\nSelect the volume type. The options are:\nMagnetic\nEphemeral\nGeneral Purpose (SSD)\nThroughput Optimized HDD\nFor more information about these options refer to \nAWS documentation\n.\n\n\n\n\n\n\nAttached Volumes Per Instance\n\n\nEnter the number of volumes attached per instance. Default is 1.\n\n\n\n\n\n\nVolume Size (GB)\n\n\nEnter the size in GBs for each volume. Default is 100.\n\n\n\n\n\n\n\n\nRecipes\n\n\nThis option allows you to select previously uploaded recipes (scripts that can be run pre or post cluster deployment) for each host group. For more information on recipes, refer to \nRecipes\n. \n\n\nRelated Links\n    \n\n\nRecipes\n\n\nAmbari Server Master Key\n\n\nThe Ambari Server Master Key is used to configure Ambari to encrypt database and Kerberos credentials that are retained by Ambari as part of the Ambari setup.  \n\n\nEnable Kerberos Security\n\n\nSelect this option to enable Kerberos for your cluster. For information about available Kerberos options, refer to \nEnabling Kerberos Security\n. \n\n\nRelated Links\n    \n\n\nKerberos\n  \n\n\n\n\nNext: Access Cluster", 
            "title": "Create a Cluster"
        }, 
        {
            "location": "/os-create/index.html#creating-a-cluster-on-openstack", 
            "text": "Use these steps to create a cluster.  Prerequisites  If you would like to use  Oozie  with  Ambari 2.6.1 or newer , you must install the Ext JS library. For instructions, refer to  Recipe to Install Ext JS for Oozie .  Steps    Log in to the Cloudbreak UI.    Click  Create Cluster  and the  Create Cluster  wizard is displayed. \n    By default,  Basic  view is displayed. To view advanced options, click  Advanced . To learn about advanced options, refer to  Advanced Options .       On the  General Configuration  page, specify the following general parameters for your cluster:     Parameter  Description      Select Credential  Choose a previously created credential.    Cluster Name  Enter a name for your cluster. The name must be between 5 and 40 characters, must start with a letter, and must only include lowercase letters, numbers, and hyphens.    Region  Select the region in which you would like to launch your cluster.    Platform Version  Choose the HDP version to use for this cluster.    Cluster Type  Choose one of default cluster configurations, or, if you have defined your own cluster configuration via Ambari blueprint, you can choose it here. For more information on default and custom blueprints, refer to  Blueprints .       On the  Hardware and Storage  page, for each host group provide the following information to define your cluster nodes and attached storage:     Parameter  Description      Instance Type  Select an instance type.    Instance Count  Enter the number of instances of a given type. Default is 1.    Ambari Server  You must select one node for Ambari Server. The \"Group Size\" for that host group must be set to \"1\".       On the  Network  page, provide the following to specify the networking resources that will be used for your cluster:     Parameter  Description      Select Network  Select the virtual network in which you would like your cluster to be provisioned. You can select an existing network or create a new network.    Select Subnet  Select the subnet in which you would like your cluster to be provisioned. If you are using a new network, create a new subnet. If you are using an existing network, select an existing subnet.    Subnet (CIDR)  If you selected to create a new subnet, you must define a valid  CIDR  for the subnet. Default is 10.0.0.0/16.       Define security groups for each host group. You can either create new security groups and define their rules or reuse existing security groups:     Option  Description      New Security Group  (Default) Creates a new security group with the rules that you defined: A set of  default rules  is provided. You should review and adjust these default rules. If you do not make any modifications, default rules will be applied.  You may open ports by defining the CIDR, entering port range, selecting protocol and clicking  + . You may delete default or previously added rules using the delete icon. If you don't want to use security group, remove the default rules.    Existing Security Groups  Allows you to select an existing security group that is already available in the selected provider region. This selection is disabled if no existing security groups are available in your chosen region.      Important  \nBy default, ports 22, 443, and 9443 are set to 0.0.0.0/0 CIDR for inbound access on the Ambari node security group. We strongly recommend that you limit this CIDR, considering the following restrictions: Ports 22 and 9443 must be open to Cloudbreak's CIDR. You can set CB_DEFAULT_GATEWAY_CIDR in your Cloudbreak's Profile file in order to automatically open ports 22 and 9443 to your Cloudbrak IP. Refer to  Restricting Inbound Access to Clusters .  Port 22 must be open to your CIDR if you would like to access the master node via SSH.  Port 443 must be open to your CIDR if you would like to access Cloudbreak web UI in a browser.     Important  \nBy default, port 22 is set to 0.0.0.0/0 CIDR for inbound access on non-Ambari node security groups. We strongly recommend that you remove it.     On the  Security  page, provide the following parameters:     Parameter  Description      Cluster User  You can log in to the Ambari UI using this username. By default, this is set to  admin .    Password  You can log in to the Ambari UI using this password.    Confirm Password  Confirm the password.    New SSH public key  Check this option to specify a new public key and then enter the public key. You will use the matching private key to access your cluster nodes via SSH.    Existing SSH public key  Select an existing public key. You will use the matching private key to access your cluster nodes via SSH. This is a default option as long as an existing SSH public key is available.       Click on  Create Cluster  to create a cluster.    You will be redirected to the Cloudbreak dashboard, and a new tile representing your cluster will appear at the top of the page.    Related Links  Blueprints    Default Cluster Security Groups  CIDR  (External)", 
            "title": "Creating a Cluster on OpenStack"
        }, 
        {
            "location": "/os-create/index.html#advanced-options", 
            "text": "Click on  Advanced  to view and enter additional configuration options", 
            "title": "Advanced Options"
        }, 
        {
            "location": "/os-create/index.html#availability-zone", 
            "text": "Choose one of the availability zones within the selected region.", 
            "title": "Availability Zone"
        }, 
        {
            "location": "/os-create/index.html#choose-image-catalog", 
            "text": "By default,  Choose Image Catalog  is set to the default image catalog that is provided with Cloudbreak. If you would like to use a different image catalog, you must first create and register it. For complete instructions, refer to  Custom Images .  Related Links      Custom Images", 
            "title": "Choose Image Catalog"
        }, 
        {
            "location": "/os-create/index.html#prewarmed-and-base-images", 
            "text": "Cloudbreak supports the following types of images for launching clusters:     Image Type  Description  Default Images Provided  Support for Custom Images      Base Images  Base images include default configuration and default tooling. These images include the operating system but do not include Ambari or HDP software.  Yes  Yes    Prewarmed Images  By default, Cloudbreak launches clusters from prewarmed images. Prewarmed images include the operating system as well as Ambari and HDP. The HDP and Ambari version used by prewarmed images cannot be customized.  Yes  No     By default, Cloudbreak uses the included default  prewarmed images , which include the operating system, as well as\nAmbari and HDP packages installed. You can optionally select the  base image  option if you would like to:   Use an Ambari and HDP versions different than what the prewarmed image includes and/or    Choose a previously created custom base image   Choose Image     If under  Choose Image Catalog , you selected a custom image catalog, under  Choose Image  you can select an image from that catalog. For complete instructions, refer to  Custom Images .   If you are trying to customize Ambari and HDP versions, you can ignore the  Choose Image  option; in this case default base image is used.  Ambari Repository Specification  If you would like to use a custom Ambari version, provide the following information:     Ambari 2.6.1  If you would like to use Ambari  2.6.1 , use the version provided by default in the Cloudbreak web UI, or newer.     Parameter  Description  Example      Version  Ambari version.  2.6.1.0    Repo Url  URL to the Ambari version repo.  http://public-repo-1.hortonworks.com/ambari/centos6/2.x/updates/2.6.1.0    Repo Gpg Key Url  URL to the repo GPG key. Each stable RPM package that is published by CentOS Project is signed with a GPG signature. By default, yum and the graphical update tools will verify these signatures and refuse to install any packages that are not signed, or have an incorrect signature.  http://public-repo-1.hortonworks.com/ambari/centos6/RPM-GPG-KEY/RPM-GPG-KEY-Jenkins     HDP Repository Specification  If you would like to use a custom HDP version, provide the following information:      Parameter  Description  Example      Stack  Stack name.  HDP    Version  Stack version.  2.6    OS  Operating system.  centos7    Stack Repo Id  Identifier for the repo linked in \"Base Url\".  HDP-2.6    Base Url  URL to the repo storing the desired stack version.  http://public-repo-1.hortonworks.com/HDP/centos7/2.x/updates/2.6.1.0    Utils Repo Id  Identifier for the repo linked in \"Utils Base Url\".  HDP-UTILS-1.1.0.21    Utils Base Url  URL to the repo storing utilities for the desired stack version.  http://public-repo-1.hortonworks.com/HDP-UTILS-1.1.0.21/repos/centos7    Enable Ambari Server to download and install GPL Licensed LZO packages?  (Optional, only available if using Ambari 2.6.1 or newer) Use this option to enable LZO compression in your HDP cluster. LZO is a lossless data compression library that favors speed over compression ratio. Ambari does not install nor enable LZO compression libraries by default, and must be explicitly configured to do so. For more information, refer to  Enabling LZO .      Related Links       Custom Images", 
            "title": "Prewarmed and Base Images"
        }, 
        {
            "location": "/os-create/index.html#enable-lifetime-management", 
            "text": "Check this option if you would like your cluster to be automatically terminated after a specific amount of time (defined as \"Time to Live\" in minutes).", 
            "title": "Enable Lifetime Management"
        }, 
        {
            "location": "/os-create/index.html#tags", 
            "text": "You can optionally add tags, which will help you find your cluster-related resources, such as VMs, in your cloud provider account. refer to  Resource Tagging .  Related Links       Resource Tagging", 
            "title": "Tags"
        }, 
        {
            "location": "/os-create/index.html#storage", 
            "text": "You can optionally specify the following storage options for your cluster:     Parameter  Description      Storage Type  Select the volume type. The options are: Magnetic Ephemeral General Purpose (SSD) Throughput Optimized HDD For more information about these options refer to  AWS documentation .    Attached Volumes Per Instance  Enter the number of volumes attached per instance. Default is 1.    Volume Size (GB)  Enter the size in GBs for each volume. Default is 100.", 
            "title": "Storage"
        }, 
        {
            "location": "/os-create/index.html#recipes", 
            "text": "This option allows you to select previously uploaded recipes (scripts that can be run pre or post cluster deployment) for each host group. For more information on recipes, refer to  Recipes .   Related Links       Recipes", 
            "title": "Recipes"
        }, 
        {
            "location": "/os-create/index.html#ambari-server-master-key", 
            "text": "The Ambari Server Master Key is used to configure Ambari to encrypt database and Kerberos credentials that are retained by Ambari as part of the Ambari setup.", 
            "title": "Ambari Server Master Key"
        }, 
        {
            "location": "/os-create/index.html#enable-kerberos-security", 
            "text": "Select this option to enable Kerberos for your cluster. For information about available Kerberos options, refer to  Enabling Kerberos Security .   Related Links       Kerberos      Next: Access Cluster", 
            "title": "Enable Kerberos Security"
        }, 
        {
            "location": "/os-clusters-access/index.html", 
            "text": "Accessing Your Cluster\n\n\nThe following section describes how to access the various services in the cluster.\n\n\nFinding Cluster Information in the UI\n\n\nOnce your cluster is up and running, click on the tile representing your cluster in the Cloudbreak UI to access information related the cluster and access cluster actions. \n\n\n \n\n\nThe information presented includes:\n\n\n\n\nCluster Summary\n  \n\n\nCluster Information\n  \n\n\nHardware\n  \n\n\nTags\n   \n\n\nRecipes\n  \n\n\nRepository Details\n  \n\n\nImage Details\n    \n\n\nNetwork\n   \n\n\nSecurity\n  \n\n\nAutoscaling\n    \n\n\nEvent History\n  \n\n\n\n\n\n  \nTips\n\n  \n\n  \n Access cluster actions such as resize and sync by clicking on \nACTIONS\n.\n\n  \n Access Ambari web UI by clicking on the link in the \nCLUSTER INFORMATION\n section.\n\n\n View public IP addresses for all cluster instances in the \nHARDWARE\n section. Click on the links to view the instances in the cloud console.\n\n\n The SSH user that you must use when accessing cluster VMs is \"cloudbreak\".\n \n\n\n\n\n\n\n\n\nCluster Summary\n\n\nThe summary bar includes the following information about your cluster:\n\n\n\n\n\n\n\n\nItem\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nCluster Name\n\n\nThe name that you selected for your cluster is displayed at the top of the page.\n\n\n\n\n\n\nTime Remaining\n\n\nIf you enabled lifetime management for your cluster, the clock next to the cluster name indicates the amount of time that your cluster will run before it gets terminated. Note that the time remaining counter does not stop when you stop the cluster.\n\n\n\n\n\n\nCloud Provider\n\n\nThe logo of the cloud provider on which the cluster is running.\n\n\n\n\n\n\nCredential\n\n\nThe name of the credential used to create the cluster.\n\n\n\n\n\n\nStatus\n\n\nCurrent status. When a cluster is healthy, the status is \nRunning\n.\n\n\n\n\n\n\nNodes\n\n\nThe current number of cluster nodes, including the master node.\n\n\n\n\n\n\nUptime\n\n\nThe amount of time (HH:MM) that the cluster has been in the running state since it was started. Each time you stop and restart the cluster, the running time is reset to 0.\n\n\n\n\n\n\nCreated\n\n\nThe date when the cluster was created. The date format is Mon DD, YYYY. For example: Oct 27, 2017.\n\n\n\n\n\n\n\n\nCluster Information\n\n\n\n\n\n\n\n\nItem\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nCluster User\n\n\nThe name of the cluster user that you created when creating the cluster.\n\n\n\n\n\n\nSSH Username\n\n\nThe SSH user which you must use when accessing cluster VMs via SSH. The SSH user is always \"cloudbreak\".\n\n\n\n\n\n\nAmbari URL\n\n\nLink to the Ambari web UI.\n\n\n\n\n\n\nRegion\n\n\nThe region in which the cluster is running in the cloud provider infrastructure.\n\n\n\n\n\n\nAvailability Zone\n\n\nThe availability zone within the region in which the cluster is running.\n\n\n\n\n\n\nBlueprint\n\n\nThe name of the blueprint selected under \"Cluster Type\" to create this cluster.\n\n\n\n\n\n\nCreated With\n\n\nThe version of Cloubdreak used to create this cluster.\n\n\n\n\n\n\nAmbari Version\n\n\nThe Ambari version which this cluster is currently running.\n\n\n\n\n\n\nHDP Version\n\n\nThe HDP version which this cluster is currently running.\n\n\n\n\n\n\n\n\nHardware\n\n\nThis section includes information about your cluster instances: instance names, instance IDs, instance types, their status, fully qualified domain names (FQDNs), and private and public IPs. \n\n\nTags\n\n\nThis section lists keys and values of the user-defined tags, in the same order as you added them.\n\n\nRecipes\n\n\nThis section includes recipe-related information. For each recipe, you can see the host group on which a recipe was executed, recipe name, and recipe type. \n\n\nRepository Details\n\n\nThis section includes Ambari and HDP repository information, as you provided it in the \"Base Images\" section when creating a cluster.\n\n\nImage Details\n\n\nThis section includes information about the base image that was used for the Cloudbreak instance. \n\n\nNetwork\n\n\nThis section includes information about the names of the network and subnet in which the cluster is running and the links to related cloud provider console.\n\n\nSecurity\n\n\nThis section is only available if you have enabled Kerberos security. It provides you with the details of your Kerberos configuration.  \n\n\nAutoscaling\n\n\nThis section includes configuration options related to autoscaling. Refer to \nAutoscaling\n.  \n\n\nEvent History\n\n\nThe Event History section shows you events logged for the cluster, with the most recent event at the top. For example, after your cluster has been created, the following messages will be written to the log:\n\n\n\nAmbari cluster built; Ambari ip:34.215.103.66\n10/26/2017, 9:41:58 AM\nBuilding Ambari cluster; Ambari ip:34.215.103.66\n10/26/2017, 9:30:20 AM\nStarting Ambari cluster services\n10/26/2017, 9:27:12 AM\nSetting up infrastructure metadata\n10/26/2017, 9:27:11 AM\nBootstrapping infrastructure cluster\n10/26/2017, 9:26:38 AM\nInfrastructure successfully provisioned\n10/26/2017, 9:26:37 AM\nBilling started, Infrastructure successfully provisioned\n10/26/2017, 9:26:37 AM\nInfrastructure metadata collection finished\n10/26/2017, 9:25:39 AM\nInfrastructure creation took 194 seconds\n10/26/2017, 9:25:37 AM\nCreating infrastructure\n10/26/2017, 9:22:22 AM\nSetting up HDP image\n10/26/2017, 9:22:21 AM\n\n\n\nAccessing Cluster via SSH\n\n\nIf you plan to access the cluster via the command line clients, SSH into the master node instance in the cluster. \n\n\n\n\nIn order to use SSH, you must generate an SSH key pair or use an existing SSH key pair.  \n\n\nYou can find the cluster instance public IP addresses on the cluster details page.  \n\n\nWhen accessing instances via SSH use the \ncloudbreak\n user. \n\n\n\n\nOn Mac OS, you can use the following syntax to SSH to the VM:\n\nssh -i \"privatekey.pem\" cloudbreak@publicIP\n\nFor example:\n\nssh -i \"dominika-kp.pem\" cloudbreak@p52.25.169.132\n\n\nOn Windows, you can SSH using an SSH client such as PuTTY.\n\n\nAccess Ambari\n\n\nYou can access Ambari web UI by clicking on the links provided in the \nCluster Information\n \n \nAmbari URL\n.\n\n\nSteps\n\n\n\n\n\n\nFrom the cluster dashboard, click on the tile representing your cluster to navigate to cluster details.\n\n\n\n\n\n\nFind the Ambari URL in the \nCluster Information\n section. This URL is available once the Ambari cluster creation process has completed.  \n\n\n\n\n\n\nClick on the \nAmbari URL\n link.\n\n\n\n\n\n\nThe first time you access the server, your browser will attempt to confirm that the SSL Certificate is valid. Since Cloudbreak automatically generates a self-signed certificate, your browser will warn you about an Untrusted Connection and ask you to confirm a Security Exception. Depending on your browser, perform the steps below to proceed.\n\n\n\n\n\n\n\n\nBrowser\n\n\nSteps\n\n\n\n\n\n\n\n\n\n\nFirefox\n\n\nClick \nAdvanced\n \n Click \nAdd Exception...\n \n Click \nConfirm Security Exception\n\n\n\n\n\n\nSafari\n\n\nClick \nContinue\n\n\n\n\n\n\nChrome\n\n\nClick \nAdvanced\n \n Click \nProceed...\n\n\n\n\n\n\n\n\n\n\n\n\nCloudbreak User Accounts\n\n\nThe following table describes what credentials to use to access Cloudbreak and Cloudbreak-managed clusters:\n\n\n\n\n\n\n\n\nComponent\n\n\nMethod\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nCloudbreak\n\n\nWeb UI, CLI\n\n\nAccess with the username and password provided when launching Cloudbreak on the cloud provider.\n\n\n\n\n\n\nCloudbreak\n\n\nSSH to VM\n\n\nAccess as the \"cloudbreak\" user with the SSH key provided when launching Cloudbreak on the cloud provider.\n\n\n\n\n\n\nCluster\n\n\nSSH to VMs\n\n\nAccess as the \"cloudbreak\" user with the SSH key provided during cluster creation.\n\n\n\n\n\n\nCluster\n\n\nAmbari UI\n\n\nAccess with the credentials provided in the \u201cCluster User\u201d parameter during cluster creation.\n\n\n\n\n\n\n\n\n\n\nNext: Manage and Monitor Clusters", 
            "title": "Access Cluster"
        }, 
        {
            "location": "/os-clusters-access/index.html#accessing-your-cluster", 
            "text": "The following section describes how to access the various services in the cluster.", 
            "title": "Accessing Your Cluster"
        }, 
        {
            "location": "/os-clusters-access/index.html#finding-cluster-information-in-the-ui", 
            "text": "Once your cluster is up and running, click on the tile representing your cluster in the Cloudbreak UI to access information related the cluster and access cluster actions.      The information presented includes:   Cluster Summary     Cluster Information     Hardware     Tags      Recipes     Repository Details     Image Details       Network      Security     Autoscaling       Event History      \n   Tips \n   \n    Access cluster actions such as resize and sync by clicking on  ACTIONS . \n    Access Ambari web UI by clicking on the link in the  CLUSTER INFORMATION  section.   View public IP addresses for all cluster instances in the  HARDWARE  section. Click on the links to view the instances in the cloud console.   The SSH user that you must use when accessing cluster VMs is \"cloudbreak\".", 
            "title": "Finding Cluster Information in the UI"
        }, 
        {
            "location": "/os-clusters-access/index.html#cluster-summary", 
            "text": "The summary bar includes the following information about your cluster:     Item  Description      Cluster Name  The name that you selected for your cluster is displayed at the top of the page.    Time Remaining  If you enabled lifetime management for your cluster, the clock next to the cluster name indicates the amount of time that your cluster will run before it gets terminated. Note that the time remaining counter does not stop when you stop the cluster.    Cloud Provider  The logo of the cloud provider on which the cluster is running.    Credential  The name of the credential used to create the cluster.    Status  Current status. When a cluster is healthy, the status is  Running .    Nodes  The current number of cluster nodes, including the master node.    Uptime  The amount of time (HH:MM) that the cluster has been in the running state since it was started. Each time you stop and restart the cluster, the running time is reset to 0.    Created  The date when the cluster was created. The date format is Mon DD, YYYY. For example: Oct 27, 2017.", 
            "title": "Cluster Summary"
        }, 
        {
            "location": "/os-clusters-access/index.html#cluster-information", 
            "text": "Item  Description      Cluster User  The name of the cluster user that you created when creating the cluster.    SSH Username  The SSH user which you must use when accessing cluster VMs via SSH. The SSH user is always \"cloudbreak\".    Ambari URL  Link to the Ambari web UI.    Region  The region in which the cluster is running in the cloud provider infrastructure.    Availability Zone  The availability zone within the region in which the cluster is running.    Blueprint  The name of the blueprint selected under \"Cluster Type\" to create this cluster.    Created With  The version of Cloubdreak used to create this cluster.    Ambari Version  The Ambari version which this cluster is currently running.    HDP Version  The HDP version which this cluster is currently running.", 
            "title": "Cluster Information"
        }, 
        {
            "location": "/os-clusters-access/index.html#hardware", 
            "text": "This section includes information about your cluster instances: instance names, instance IDs, instance types, their status, fully qualified domain names (FQDNs), and private and public IPs.", 
            "title": "Hardware"
        }, 
        {
            "location": "/os-clusters-access/index.html#tags", 
            "text": "This section lists keys and values of the user-defined tags, in the same order as you added them.", 
            "title": "Tags"
        }, 
        {
            "location": "/os-clusters-access/index.html#recipes", 
            "text": "This section includes recipe-related information. For each recipe, you can see the host group on which a recipe was executed, recipe name, and recipe type.", 
            "title": "Recipes"
        }, 
        {
            "location": "/os-clusters-access/index.html#repository-details", 
            "text": "This section includes Ambari and HDP repository information, as you provided it in the \"Base Images\" section when creating a cluster.", 
            "title": "Repository Details"
        }, 
        {
            "location": "/os-clusters-access/index.html#image-details", 
            "text": "This section includes information about the base image that was used for the Cloudbreak instance.", 
            "title": "Image Details"
        }, 
        {
            "location": "/os-clusters-access/index.html#network", 
            "text": "This section includes information about the names of the network and subnet in which the cluster is running and the links to related cloud provider console.", 
            "title": "Network"
        }, 
        {
            "location": "/os-clusters-access/index.html#security", 
            "text": "This section is only available if you have enabled Kerberos security. It provides you with the details of your Kerberos configuration.", 
            "title": "Security"
        }, 
        {
            "location": "/os-clusters-access/index.html#autoscaling", 
            "text": "This section includes configuration options related to autoscaling. Refer to  Autoscaling .", 
            "title": "Autoscaling"
        }, 
        {
            "location": "/os-clusters-access/index.html#event-history", 
            "text": "The Event History section shows you events logged for the cluster, with the most recent event at the top. For example, after your cluster has been created, the following messages will be written to the log:  \nAmbari cluster built; Ambari ip:34.215.103.66\n10/26/2017, 9:41:58 AM\nBuilding Ambari cluster; Ambari ip:34.215.103.66\n10/26/2017, 9:30:20 AM\nStarting Ambari cluster services\n10/26/2017, 9:27:12 AM\nSetting up infrastructure metadata\n10/26/2017, 9:27:11 AM\nBootstrapping infrastructure cluster\n10/26/2017, 9:26:38 AM\nInfrastructure successfully provisioned\n10/26/2017, 9:26:37 AM\nBilling started, Infrastructure successfully provisioned\n10/26/2017, 9:26:37 AM\nInfrastructure metadata collection finished\n10/26/2017, 9:25:39 AM\nInfrastructure creation took 194 seconds\n10/26/2017, 9:25:37 AM\nCreating infrastructure\n10/26/2017, 9:22:22 AM\nSetting up HDP image\n10/26/2017, 9:22:21 AM", 
            "title": "Event History"
        }, 
        {
            "location": "/os-clusters-access/index.html#accessing-cluster-via-ssh", 
            "text": "If you plan to access the cluster via the command line clients, SSH into the master node instance in the cluster.    In order to use SSH, you must generate an SSH key pair or use an existing SSH key pair.    You can find the cluster instance public IP addresses on the cluster details page.    When accessing instances via SSH use the  cloudbreak  user.    On Mac OS, you can use the following syntax to SSH to the VM: ssh -i \"privatekey.pem\" cloudbreak@publicIP \nFor example: ssh -i \"dominika-kp.pem\" cloudbreak@p52.25.169.132  On Windows, you can SSH using an SSH client such as PuTTY.", 
            "title": "Accessing Cluster via SSH"
        }, 
        {
            "location": "/os-clusters-access/index.html#access-ambari", 
            "text": "You can access Ambari web UI by clicking on the links provided in the  Cluster Information     Ambari URL .  Steps    From the cluster dashboard, click on the tile representing your cluster to navigate to cluster details.    Find the Ambari URL in the  Cluster Information  section. This URL is available once the Ambari cluster creation process has completed.      Click on the  Ambari URL  link.    The first time you access the server, your browser will attempt to confirm that the SSL Certificate is valid. Since Cloudbreak automatically generates a self-signed certificate, your browser will warn you about an Untrusted Connection and ask you to confirm a Security Exception. Depending on your browser, perform the steps below to proceed.     Browser  Steps      Firefox  Click  Advanced    Click  Add Exception...    Click  Confirm Security Exception    Safari  Click  Continue    Chrome  Click  Advanced    Click  Proceed...", 
            "title": "Access Ambari"
        }, 
        {
            "location": "/os-clusters-access/index.html#cloudbreak-user-accounts", 
            "text": "The following table describes what credentials to use to access Cloudbreak and Cloudbreak-managed clusters:     Component  Method  Description      Cloudbreak  Web UI, CLI  Access with the username and password provided when launching Cloudbreak on the cloud provider.    Cloudbreak  SSH to VM  Access as the \"cloudbreak\" user with the SSH key provided when launching Cloudbreak on the cloud provider.    Cluster  SSH to VMs  Access as the \"cloudbreak\" user with the SSH key provided during cluster creation.    Cluster  Ambari UI  Access with the credentials provided in the \u201cCluster User\u201d parameter during cluster creation.      Next: Manage and Monitor Clusters", 
            "title": "Cloudbreak User Accounts"
        }, 
        {
            "location": "/os-clusters-manage/index.html", 
            "text": "Managing and Monitoring Clusters\n\n\nYou can manage monitor your clusters from the Cloudbreak UI. To do that, click on the tile representing the cluster that you want to access. The actions available for your cluster are listed in the top right corner: \n\n\n \n\n\n\n  \nTips\n\n  \n\n  \nTo add or remove nodes from your cluster click \nACTIONS>Resize\n.\n\n  \nTo synchronize your cluster with the cloud provider account click \nACTIONS>Sync\n.\n\n  \nTo temporarily stop your cluster click \nSTOP\n.\n\n  \nTo terminate your cluster click \nTERMINATE\n.\n\n\n\n\n\n\n\n\n\nResize a Cluster\n\n\nTo resize a cluster, follow these steps.\n\n\nSteps\n\n\n\n\n\n\nBrowse to the cluster details.\n\n\n\n\n\n\nClick \nActions\n and select \nResize\n. The cluster resize dialog is displayed.\n\n\n\n\n\n\nUsing the +/- controls, adjust the number of nodes for a chosen host group. \n\n\n\n\nYou can only modify one host group at a time. \n\nIt is not possible to resize the Ambari server host group.     \n\n\n\n\n\n\n\n\nClick \nYes\n to confirm the scale-up/scale-down.\n\n\nWhile nodes are being added or removed, cluster status changes to \"Update In Progress\". Once the operation has completed, cluster status changes back to \"Running\". \n\n\n\n\n\n\nSynchronize a Cluster\n\n\nUse the \nsync\n option if you:  \n\n\n\n\nMade changes on your cloud provider side (for example, deleted cluster VMs) and you would like to synchronize Cloudbreak with the cloud provider.  \n\n\nManually changed service status in Ambari (for example, restarted services).   \n\n\n\n\nTo synchronize your cluster with the cloud provider, follow these steps. \n\n\nSteps\n\n\n\n\n\n\nBrowse to the cluster details.\n\n\n\n\n\n\nClick \nActions\n and select \nSync\n.\n\n\n\n\n\n\nClick \nYes\n to confirm.\n\n\nYour cluster infrastructure is synchronized based on changes on the cloud provider. The updates are written to the \"Event History\". \n\n\n\n\n\n\nStop a Cluster\n\n\nCloudbreak supports stopping and restarting clusters. To stop and restart a cluster managed by Cloudbreak, use the options available from the Coudbreak UI. \n\n\nSteps\n\n\n\n\n\n\nBrowse to the cluster details.\n\n\n\n\n\n\nClick \nStop\n to stop a currently running cluster.  \n\n\n\n\n\n\nClick \nYes\n to confirm. \n\n\n\n\n\n\nYour cluster status changes to \"Stopping in progress\" and then to \"Stopped\". Once stopping the infrastructure has completed, you will see a \nStart\n option to restart your cluster. \n\n\n\n\n\n\nWhen a cluster is in the \"stopped\" state, you are not charged for the VMs, but you are charged for external storage.  \n\n\nRestart a Cluster\n\n\nIf your cluster is in the \"Stopped\" state, you can restart the cluster by follow these steps.\n\n\nSteps\n\n\n\n\n\n\nclick \nStart\n. This option is only available when the cluster has been stopped. \n\n\n\n\n\n\nClick \nYes\n to confirm.\n\n\nYour cluster status changes to \"Start in progress\" and then to \"Running\". \n\n\n\n\n\n\nTerminate a Cluster\n\n\nTo terminate a cluster managed by Cloudbreak, use the option available from the Coudbreak UI. \n\n\nSteps\n\n\n\n\n\n\nBrowse to the cluster details.\n\n\n\n\n\n\nClick \nTerminate\n. \n\n\n\n\n\n\nClick \nYes\n to confirm.\n\n\nAll cluster-related resources will be deleted, unless the resources (such as networks and subnets) existed prior to cluster creation or are used by other VMs in which case they will be preserved. \n\n\n\n\n\n\nForce Terminate a Cluster\n\n\nCluster deletion may fail if Cloudbreak is unable to delete one or more of the cloud resources that were part of your cluster infrastructure. In such as case, you can use the \nTerminate\n \n \nForce terminate\n option to remove the cluster entry from the Cloudbreak web UI, but you must also check your cloud provider account to see if there are any resources that must be deleted manually.\n\n\nSteps\n\n\n\n\n\n\nBrowse to the cluster details.\n\n\n\n\n\n\nClick \nTerminate\n. \n\n\n\n\n\n\nCheck  \nForce terminate\n.\n\n\n\n\n\n\nClick \nYes\n to confirm. \n\n\nWhen terminating a cluster with Kerberos enabled, you have an option to disable Kerberos prior to cluster termination. This option removes any cluster-related principals from the KDC.\n\n\n\n\n\n\nThis deletes the cluster tile from the UI.  \n\n\n\n\n\n\nLog in to your cloud provider account and \nmanually delete\n any resources that failed to be deleted.\n\n\n\n\n\n\nView Cluster History\n\n\nFrom the navigation menu in the Cloudbreak UI, you can access the History page that allows you to generate a report showing basic information related to the clusters that were running within the specified range of dates.\n\n\nTo generate a report, follow these steps.\n\n\nSteps\n\n\n\n\n\n\nFrom the Cloudbreak UI navigation menu, select \nHistory\n.\n\n\n\n\n\n\nOn the History page, select the range of dates and click \nShow History\n to generate a report for the selected period.\n\n\n \n\n\n\n\n\n\nHistory Report Content\n\n\nEach entry in the report represents one cluster instance group. For each entry, the report includes the following information:\n\n\n\n\nCreated\n - The date when your cluster was created (YYYY-MM-DD).\n\n\nProvider\n - The name of the cloud provider (AWS, Azure, Google, or OpenStack) on which the cluster instances are/were running.\n\n\nCluster Name\n - The name that you selected for the cluster.  \n\n\nInstance Group\n - The name of the host group.   \n\n\nInstance Count\n - The number of nodes in the host group. This number may be a decimal if a cluster has been resized.\n\n\nInstance Type\n - Provider-specific VM type of the cluster instances. \n\n\nRegion\n - The AWS region in which your cluster is/was running.\n\n\nAvailability Zone\n - The availability zone in which your cluster is/was running.      \n\n\nRunning Time (hours)\n - The sum of the running times for all the nodes in the instance group.\n\n\n\n\nThe \nAGGREGATE RUNNING TIME\n is the sum of the Running Times, adjusted for the selected time range.\n\n\nTo learn about how your cloud provider bills you for the VMs, refer to their documentation:\n\n\n\n\nAWS\n      \n\n\nAzure\n     \n\n\nGCP", 
            "title": "Manage and Monitor Clusters"
        }, 
        {
            "location": "/os-clusters-manage/index.html#managing-and-monitoring-clusters", 
            "text": "You can manage monitor your clusters from the Cloudbreak UI. To do that, click on the tile representing the cluster that you want to access. The actions available for your cluster are listed in the top right corner:      \n   Tips \n   \n   To add or remove nodes from your cluster click  ACTIONS>Resize . \n   To synchronize your cluster with the cloud provider account click  ACTIONS>Sync . \n   To temporarily stop your cluster click  STOP . \n   To terminate your cluster click  TERMINATE .", 
            "title": "Managing and Monitoring Clusters"
        }, 
        {
            "location": "/os-clusters-manage/index.html#resize-a-cluster", 
            "text": "To resize a cluster, follow these steps.  Steps    Browse to the cluster details.    Click  Actions  and select  Resize . The cluster resize dialog is displayed.    Using the +/- controls, adjust the number of nodes for a chosen host group.    You can only modify one host group at a time.  \nIt is not possible to resize the Ambari server host group.          Click  Yes  to confirm the scale-up/scale-down.  While nodes are being added or removed, cluster status changes to \"Update In Progress\". Once the operation has completed, cluster status changes back to \"Running\".", 
            "title": "Resize a Cluster"
        }, 
        {
            "location": "/os-clusters-manage/index.html#synchronize-a-cluster", 
            "text": "Use the  sync  option if you:     Made changes on your cloud provider side (for example, deleted cluster VMs) and you would like to synchronize Cloudbreak with the cloud provider.    Manually changed service status in Ambari (for example, restarted services).      To synchronize your cluster with the cloud provider, follow these steps.   Steps    Browse to the cluster details.    Click  Actions  and select  Sync .    Click  Yes  to confirm.  Your cluster infrastructure is synchronized based on changes on the cloud provider. The updates are written to the \"Event History\".", 
            "title": "Synchronize a Cluster"
        }, 
        {
            "location": "/os-clusters-manage/index.html#stop-a-cluster", 
            "text": "Cloudbreak supports stopping and restarting clusters. To stop and restart a cluster managed by Cloudbreak, use the options available from the Coudbreak UI.   Steps    Browse to the cluster details.    Click  Stop  to stop a currently running cluster.      Click  Yes  to confirm.     Your cluster status changes to \"Stopping in progress\" and then to \"Stopped\". Once stopping the infrastructure has completed, you will see a  Start  option to restart your cluster.     When a cluster is in the \"stopped\" state, you are not charged for the VMs, but you are charged for external storage.", 
            "title": "Stop a Cluster"
        }, 
        {
            "location": "/os-clusters-manage/index.html#restart-a-cluster", 
            "text": "If your cluster is in the \"Stopped\" state, you can restart the cluster by follow these steps.  Steps    click  Start . This option is only available when the cluster has been stopped.     Click  Yes  to confirm.  Your cluster status changes to \"Start in progress\" and then to \"Running\".", 
            "title": "Restart a Cluster"
        }, 
        {
            "location": "/os-clusters-manage/index.html#terminate-a-cluster", 
            "text": "To terminate a cluster managed by Cloudbreak, use the option available from the Coudbreak UI.   Steps    Browse to the cluster details.    Click  Terminate .     Click  Yes  to confirm.  All cluster-related resources will be deleted, unless the resources (such as networks and subnets) existed prior to cluster creation or are used by other VMs in which case they will be preserved.", 
            "title": "Terminate a Cluster"
        }, 
        {
            "location": "/os-clusters-manage/index.html#force-terminate-a-cluster", 
            "text": "Cluster deletion may fail if Cloudbreak is unable to delete one or more of the cloud resources that were part of your cluster infrastructure. In such as case, you can use the  Terminate     Force terminate  option to remove the cluster entry from the Cloudbreak web UI, but you must also check your cloud provider account to see if there are any resources that must be deleted manually.  Steps    Browse to the cluster details.    Click  Terminate .     Check   Force terminate .    Click  Yes  to confirm.   When terminating a cluster with Kerberos enabled, you have an option to disable Kerberos prior to cluster termination. This option removes any cluster-related principals from the KDC.    This deletes the cluster tile from the UI.      Log in to your cloud provider account and  manually delete  any resources that failed to be deleted.", 
            "title": "Force Terminate a Cluster"
        }, 
        {
            "location": "/os-clusters-manage/index.html#view-cluster-history", 
            "text": "From the navigation menu in the Cloudbreak UI, you can access the History page that allows you to generate a report showing basic information related to the clusters that were running within the specified range of dates.  To generate a report, follow these steps.  Steps    From the Cloudbreak UI navigation menu, select  History .    On the History page, select the range of dates and click  Show History  to generate a report for the selected period.", 
            "title": "View Cluster History"
        }, 
        {
            "location": "/os-clusters-manage/index.html#history-report-content", 
            "text": "Each entry in the report represents one cluster instance group. For each entry, the report includes the following information:   Created  - The date when your cluster was created (YYYY-MM-DD).  Provider  - The name of the cloud provider (AWS, Azure, Google, or OpenStack) on which the cluster instances are/were running.  Cluster Name  - The name that you selected for the cluster.    Instance Group  - The name of the host group.     Instance Count  - The number of nodes in the host group. This number may be a decimal if a cluster has been resized.  Instance Type  - Provider-specific VM type of the cluster instances.   Region  - The AWS region in which your cluster is/was running.  Availability Zone  - The availability zone in which your cluster is/was running.        Running Time (hours)  - The sum of the running times for all the nodes in the instance group.   The  AGGREGATE RUNNING TIME  is the sum of the Running Times, adjusted for the selected time range.  To learn about how your cloud provider bills you for the VMs, refer to their documentation:   AWS         Azure        GCP", 
            "title": "History Report Content"
        }, 
        {
            "location": "/blueprints/index.html", 
            "text": "Blueprints\n\n\nAmbari blueprints\n are your declarative definition of your HDP cluster, defining the host groups and which components to install on which host group. Ambari uses them as a base for your clusters. \n\n\nYou have two options concerning using blueprints with Cloudbreak:\n\n\n\n\nUse one of the pre-defined blueprints.    \n\n\nAdd your custom blueprint by uploading a JSON file or pasting the JSON text. \n\n\n\n\nWe recommend that you review the default blueprints to check if they meet your requirements. You can do this by selecting \nBlueprints\n from the navigation pane in the Cloudbreak web UI or by reading the documentation below.\n\n\nUsing Default Blueprints\n\n\nTo use one of the default blueprints, simply select them when creating a cluster. The option is available on the \nGeneral Configuration\n page. First select the \nStack Version\n and then select your chosen blueprint under \nCluster Type\n. \n\n\nDefault Blueprints\n\n\nCloudbreak includes the following default HDP cluster blueprints:\n\n\nPlatform Version: \nHDP 2.6\n\n\n\n\n\n\n\n\nCluster Type\n\n\nMain Services\n\n\nDescription\n\n\nList of All Services Included\n\n\n\n\n\n\n\n\n\n\nData Science\n\n\n Spark 1.6,\nZeppelin 0.7.0\n\n\nUseful for data science with Spark 1.6 and Zeppelin.\n\n\nHDFS, YARN, MapReduce2, Tez, Hive 1.2.1, Pig, Sqoop, ZooKeeper, Ambari Metrics, Spark 1.6, Zeppelin 0.7.0\n\n\n\n\n\n\nData Science\n\n\n Spark 2.1,\nZeppelin 0.7.0\n\n\nUseful for data science with Spark 2.1 and Zeppelin.\n\n\nHDFS, YARN, MapReduce2, Tez, Hive 1.2.1, Pig, Sqoop, ZooKeeper, Ambari Metrics, Spark 2.1, Zeppelin 0.7.0\n\n\n\n\n\n\nEDW - Analytics\n\n\n Hive 2 LLAP\n,\nZeppelin 0.7.0\n\n\nUseful for EDW analytics using Hive LLAP.\n\n\nHDFS, YARN, MapReduce2, Tez, Hive 2 LLAP, Druid, Pig, ZooKeeper, Ambari Metrics, Spark 2.1\n\n\n\n\n\n\nEDW - ETL\n\n\n Hive 1.2.1,\nSpark 1.6\n\n\nUseful for ETL data processing with Hive and Spark 1.6.\n\n\nHDFS, YARN, MapReduce2, Tez, Hive 1.2.1, Pig, Sqoop, ZooKeeper, Ambari Metrics, Spark 1.6, Druid 0.9.2\n\n\n\n\n\n\nEDW - ETL\n\n\n Hive 1.2.1,\n Spark 2.1\n\n\nUseful for ETL data processing with Hive and Spark 2.1.\n\n\nHDFS, YARN, MapReduce2, Tez, Hive 1.2.1, Pig, ZooKeeper, Ambari Metrics, Spark 2.1\n\n\n\n\n\n\nBI\n\n\n Druid 0.9.2\n\n\nTechnical preview of Druid.\n\n\nHDFS, YARN, MapReduce2, Tez, Druid, Sqoop, ZooKeeper, Ambari Metrics\n\n\n\n\n\n\n\n\nThe following configuration classification applies:\n\n\n\n Stable configurations are the best choice if you want to avoid issues and other problems with launching and using clusters.\n\n\n If you want to use a Technical Preview version of a component in a release of HDP, use these configurations.\n\n\n These are the most cutting edge of the configurations, including Technical Preview components in a Technical Preview HDP release.\n\n\n\n\nUsing Custom Blueprints\n\n\nThis option allows you to create and save your custom blueprints. \n\n\n\n    \nSupported Ambari and HDP Versions\n\n    \n\nCloudbreak supports the following Ambari and HDP versions:\nAmbari \n2.5.x\nHDP \n2.6.x\n and HDP \n2.5.x\nAmbari 2.6.x is not supported.\n\n\n\n\n\n\nCreating a Blueprint\n\n\nAmbari blueprints are specified in the JSON format. \n\n\nA blueprint can be exported from a running Ambari cluster and can be reused in Cloudbreak after slight modifications. When a blueprint is exported, it includes  some hardcoded configurations such as domain names, memory configurations, and so on, that are not applicable to the Cloudbreak cluster. There is no automatic way to modify an exported blueprint and make it instantly usable in Cloudbreak, the modifications have to be done manually. \n\n\nIn general, the blueprint should include the following elements:\n\n\n\"Blueprints\": {\n\u2002\u2002\u2002\u2002\"blueprint_name\": \"hdp-small-default\",\n\u2002\u2002\u2002\u2002\"stack_name\": \"HDP\",\n\u2002\u2002\u2002\u2002\"stack_version\": \"2.6\"\n\u2002\u2002},\n\u2002\u2002\"settings\": [],\n\u2002\u2002\"configurations\": [],\n\u2002\u2002\"host_groups\": [\n\u2002\u2002{\n      \"name\": \"master\",\n      \"configurations\": [],\n      \"components\": []\n    },\n    {\n      \"name\": \"worker\",\n      \"configurations\": [],\n      \"components\": [ ]\n    },\n    {\n      \"name\": \"compute\",\n      \"configurations\": [],\n      \"components\": []\n    }\n\u2002\u2002 ]\n\u2002\u2002}\n\n\n\nFor correct blueprint layout and other information about Ambari blueprints, refer to the \nAmbari cwiki\n page. \n\n\nCloudbreak requires you to define an additional element in the blueprint called \"blueprint_name\".\u2002This should be a unique name within Cloudbreak list of blueprints.\u2002That is not included in the Ambari export. For example:\n\n\n\"Blueprints\": {\n\u2002\u2002\u2002\u2002\"blueprint_name\": \"hdp-small-default\",\n\u2002\u2002\u2002\u2002\"stack_name\": \"HDP\",\n\u2002\u2002\u2002\u2002\"stack_version\": \"2.6\"\n\u2002\u2002},\n\u2002\u2002\"settings\": [],\n\u2002\u2002\"configurations\": [],\n\u2002\u2002\"host_groups\": [\n\u2002\u2002...\n\n\n\n\nAfter you provide the blueprint to Cloudbreak, the host groups in the JSON will be mapped to a set of instances when starting the cluster, and the specified services and components will be installed on the corresponding nodes. It is not necessary to define a complete configuration in the blueprint. If a configuration is missing, Ambari will use a default value. \n\n\nHere are a few \nblueprint examples\n. You can also refer to the default blueprints provided in the Cloudbreak UI.\n\n\nRelated Links\n\n\nBlueprint Examples\n (Hortonworks)   \n\n\nAmbari cwiki\n (External)  \n\n\nUpload a Blueprint\n\n\nOnce you have your blueprint ready, perform these steps.\n\n\nSteps\n\n\n\n\nIn the Cloudbreak UI, select \nBlueprints\n from the navigation pane. \n\n\n\n\nTo add your own blueprint, click \nCreate Blueprint\n and enter the following parameters:\n\n\n\n\n\n\n\n\nParameter\n\n\nValue\n\n\n\n\n\n\n\n\n\n\nName\n\n\nEnter a name for your blueprint.\n\n\n\n\n\n\nDescription\n\n\n(Optional) Enter a description for your blueprint.\n\n\n\n\n\n\nBlueprint Source\n\n\nSelect one of: \nText\n: Paste blueprint in JSON format.\n \nFile\n: Upload a file that contains the blueprint.\n \nURL\n: Specify the URL for your blueprint.\n\n\n\n\n\n\n\n\n \n\n\n\n\n\n\nTo use the uploaded blueprints, select it when creating a cluster. The option is available on the \nGeneral Configuration\n page. First select the \nPlatform Version\n and then select your chosen blueprint under \nCluster Type\n. \n\n\n \n\n\n\n\n\n\nView Blueprint Details\n\n\nOnce a blueprint has been registered in Cloudbreak, you can access its details in the Cloudbreak UI.\n\n\nSteps\n\n\n\n\n\n\nIn the Cloudbreak UI, select \nBlueprints\n from the navigation pane. \n\n\n\n\n\n\nClick on an entry to navigate to details. \n\n\nYou can view blueprint details using the \nList View\n and \nRaw View\n:\n\n\n \n\n\n\n\n\n\nDelete Blueprint\n\n\nTo delete a default or custom blueprint, perform these steps.\n\n\nSteps\n\n\n\n\n\n\nIn the Cloudbreak UI, select \nBlueprints\n from the navigation pane. \n\n\n\n\n\n\nClick on an entry to navigate to details. \n\n\n\n\n\n\nClick \nDelete\n. \n\n\n\n\n\n\nConfirm delete.", 
            "title": "Blueprints"
        }, 
        {
            "location": "/blueprints/index.html#blueprints", 
            "text": "Ambari blueprints  are your declarative definition of your HDP cluster, defining the host groups and which components to install on which host group. Ambari uses them as a base for your clusters.   You have two options concerning using blueprints with Cloudbreak:   Use one of the pre-defined blueprints.      Add your custom blueprint by uploading a JSON file or pasting the JSON text.    We recommend that you review the default blueprints to check if they meet your requirements. You can do this by selecting  Blueprints  from the navigation pane in the Cloudbreak web UI or by reading the documentation below.", 
            "title": "Blueprints"
        }, 
        {
            "location": "/blueprints/index.html#using-default-blueprints", 
            "text": "To use one of the default blueprints, simply select them when creating a cluster. The option is available on the  General Configuration  page. First select the  Stack Version  and then select your chosen blueprint under  Cluster Type .", 
            "title": "Using Default Blueprints"
        }, 
        {
            "location": "/blueprints/index.html#default-blueprints", 
            "text": "Cloudbreak includes the following default HDP cluster blueprints:  Platform Version:  HDP 2.6     Cluster Type  Main Services  Description  List of All Services Included      Data Science   Spark 1.6, Zeppelin 0.7.0  Useful for data science with Spark 1.6 and Zeppelin.  HDFS, YARN, MapReduce2, Tez, Hive 1.2.1, Pig, Sqoop, ZooKeeper, Ambari Metrics, Spark 1.6, Zeppelin 0.7.0    Data Science   Spark 2.1, Zeppelin 0.7.0  Useful for data science with Spark 2.1 and Zeppelin.  HDFS, YARN, MapReduce2, Tez, Hive 1.2.1, Pig, Sqoop, ZooKeeper, Ambari Metrics, Spark 2.1, Zeppelin 0.7.0    EDW - Analytics   Hive 2 LLAP , Zeppelin 0.7.0  Useful for EDW analytics using Hive LLAP.  HDFS, YARN, MapReduce2, Tez, Hive 2 LLAP, Druid, Pig, ZooKeeper, Ambari Metrics, Spark 2.1    EDW - ETL   Hive 1.2.1, Spark 1.6  Useful for ETL data processing with Hive and Spark 1.6.  HDFS, YARN, MapReduce2, Tez, Hive 1.2.1, Pig, Sqoop, ZooKeeper, Ambari Metrics, Spark 1.6, Druid 0.9.2    EDW - ETL   Hive 1.2.1,  Spark 2.1  Useful for ETL data processing with Hive and Spark 2.1.  HDFS, YARN, MapReduce2, Tez, Hive 1.2.1, Pig, ZooKeeper, Ambari Metrics, Spark 2.1    BI   Druid 0.9.2  Technical preview of Druid.  HDFS, YARN, MapReduce2, Tez, Druid, Sqoop, ZooKeeper, Ambari Metrics     The following configuration classification applies:   Stable configurations are the best choice if you want to avoid issues and other problems with launching and using clusters.   If you want to use a Technical Preview version of a component in a release of HDP, use these configurations.   These are the most cutting edge of the configurations, including Technical Preview components in a Technical Preview HDP release.", 
            "title": "Default Blueprints"
        }, 
        {
            "location": "/blueprints/index.html#using-custom-blueprints", 
            "text": "This option allows you to create and save your custom blueprints.   \n     Supported Ambari and HDP Versions \n     \nCloudbreak supports the following Ambari and HDP versions: Ambari  2.5.x HDP  2.6.x  and HDP  2.5.x Ambari 2.6.x is not supported.", 
            "title": "Using Custom Blueprints"
        }, 
        {
            "location": "/blueprints/index.html#creating-a-blueprint", 
            "text": "Ambari blueprints are specified in the JSON format.   A blueprint can be exported from a running Ambari cluster and can be reused in Cloudbreak after slight modifications. When a blueprint is exported, it includes  some hardcoded configurations such as domain names, memory configurations, and so on, that are not applicable to the Cloudbreak cluster. There is no automatic way to modify an exported blueprint and make it instantly usable in Cloudbreak, the modifications have to be done manually.   In general, the blueprint should include the following elements:  \"Blueprints\": {\n\u2002\u2002\u2002\u2002\"blueprint_name\": \"hdp-small-default\",\n\u2002\u2002\u2002\u2002\"stack_name\": \"HDP\",\n\u2002\u2002\u2002\u2002\"stack_version\": \"2.6\"\n\u2002\u2002},\n\u2002\u2002\"settings\": [],\n\u2002\u2002\"configurations\": [],\n\u2002\u2002\"host_groups\": [\n\u2002\u2002{\n      \"name\": \"master\",\n      \"configurations\": [],\n      \"components\": []\n    },\n    {\n      \"name\": \"worker\",\n      \"configurations\": [],\n      \"components\": [ ]\n    },\n    {\n      \"name\": \"compute\",\n      \"configurations\": [],\n      \"components\": []\n    }\n\u2002\u2002 ]\n\u2002\u2002}  For correct blueprint layout and other information about Ambari blueprints, refer to the  Ambari cwiki  page.   Cloudbreak requires you to define an additional element in the blueprint called \"blueprint_name\".\u2002This should be a unique name within Cloudbreak list of blueprints.\u2002That is not included in the Ambari export. For example:  \"Blueprints\": {\n\u2002\u2002\u2002\u2002\"blueprint_name\": \"hdp-small-default\",\n\u2002\u2002\u2002\u2002\"stack_name\": \"HDP\",\n\u2002\u2002\u2002\u2002\"stack_version\": \"2.6\"\n\u2002\u2002},\n\u2002\u2002\"settings\": [],\n\u2002\u2002\"configurations\": [],\n\u2002\u2002\"host_groups\": [\n\u2002\u2002...  After you provide the blueprint to Cloudbreak, the host groups in the JSON will be mapped to a set of instances when starting the cluster, and the specified services and components will be installed on the corresponding nodes. It is not necessary to define a complete configuration in the blueprint. If a configuration is missing, Ambari will use a default value.   Here are a few  blueprint examples . You can also refer to the default blueprints provided in the Cloudbreak UI.  Related Links  Blueprint Examples  (Hortonworks)     Ambari cwiki  (External)", 
            "title": "Creating a Blueprint"
        }, 
        {
            "location": "/blueprints/index.html#upload-a-blueprint", 
            "text": "Once you have your blueprint ready, perform these steps.  Steps   In the Cloudbreak UI, select  Blueprints  from the navigation pane.    To add your own blueprint, click  Create Blueprint  and enter the following parameters:     Parameter  Value      Name  Enter a name for your blueprint.    Description  (Optional) Enter a description for your blueprint.    Blueprint Source  Select one of:  Text : Paste blueprint in JSON format.   File : Upload a file that contains the blueprint.   URL : Specify the URL for your blueprint.          To use the uploaded blueprints, select it when creating a cluster. The option is available on the  General Configuration  page. First select the  Platform Version  and then select your chosen blueprint under  Cluster Type .", 
            "title": "Upload a Blueprint"
        }, 
        {
            "location": "/blueprints/index.html#view-blueprint-details", 
            "text": "Once a blueprint has been registered in Cloudbreak, you can access its details in the Cloudbreak UI.  Steps    In the Cloudbreak UI, select  Blueprints  from the navigation pane.     Click on an entry to navigate to details.   You can view blueprint details using the  List View  and  Raw View :", 
            "title": "View Blueprint Details"
        }, 
        {
            "location": "/blueprints/index.html#delete-blueprint", 
            "text": "To delete a default or custom blueprint, perform these steps.  Steps    In the Cloudbreak UI, select  Blueprints  from the navigation pane.     Click on an entry to navigate to details.     Click  Delete .     Confirm delete.", 
            "title": "Delete Blueprint"
        }, 
        {
            "location": "/recipes/index.html", 
            "text": "Recipes\n\n\nAlthough Cloudbreak lets you provision HDP clusters in the cloud based on custom Ambari blueprints, Cloudbreak provisioning options don't consider all possible use cases. For that reason, we introduced recipes. \n\n\nA recipe is a script that runs on all nodes of a selected node group before or after the Ambari cluster installation. You can use recipes for tasks such as installing additional software or performing advanced cluster configuration. For example, you can use a recipe to put a JAR file on the Hadoop classpath.\n\n\nWhen creating a cluster, you can optionally upload one or more \"recipes\" (custom scripts) and they will be executed on a specific host group at a specified time. Available recipe execution times are:  \n\n\n\n\nBefore Ambari server start    \n\n\nAfter Ambari server start    \n\n\nAfter cluster installation    \n\n\nBefore cluster termination   \n\n\n\n\nWriting Recipes\n\n\nWhen using recipes, consider the following:\n\n\n\n\nThe scripts will be executed on the node types you specify (such as \"master\", \"worker\", \"compute\"). If you want to run a a script on all nodes, define the recipe one per node type.  \n\n\nThe script will execute on all of the nodes of that type as root.  \n\n\nIn order to be executed, your script must be in a network location which is accessible from the cloud controller and cluster instances VPC.  \n\n\nMake sure to follow Linux best practices when creating your scripts. For example, don't forget to script \"Yes\" auto-answers where needed.  \n\n\nDo not execute yum update \u2013y since it may update other components on the instances (such as salt), which can create unintended or unstable behavior.   \n\n\nThe scripts will be executed as root. The recipe output is written to \n/var/log/recipes\n on each node on which it was executed.\n\n\n\n\nSample Recipe for Yum Proxy Setting\n\n\n#!/bin/bash\ncat \n /etc/yum.conf \nENDOF\nproxy=http://10.0.0.133:3128\nENDOF\n\n\n\n\nRecipe to Install Ext JS for Oozie\n\n\nExt JS is GPL licensed software and is no longer included in builds of HDP 2.6. Because of this, the Oozie WAR file is not built to include the Ext JS-based user interface unless Ext JS is manually installed on the Oozie server. If you add Oozie using Ambari 2.6.1.0 to an HDP 2.6.4 or greater stack, no Oozie UI will be available by default. Therefore, if you plan to use Oozie with Ambari 2.6.1.0 to an HDP 2.6.4 or greater, you must ensure that ExtJS is installed prior to installing Oozie (that is, prior to cluster install). \n\n\nYou can install Ext JS by adding the following PRE-AMBARI-START recipe:\n\n\nexport EXT_JS_VERSION=2.2-1\n     export OS_NAME=centos6\n     wget http://public-repo-1.hortonworks.com/HDP-UTILS-GPL-1.1.0.22/repos/$OS_NAME/extjs/extjs-$EXT_JS_VERSION.noarch.rpm\n     rpm -ivh extjs-$EXT_JS_VERSION.noarch.rpm\n\n\n\nMake the following changes to the script:\n\n\n\n\nChange the EXT_JS_VERSION to the specific ExtJS version that you want to use.  \n\n\nChange the OS_NAME to the name of the operating system. Supported values are: centos6, centos7, centos7-ppc.\n\n\n\n\nThe general steps are:\n\n\n\n\nBe sure to review and agree to the Ext JS license prior to using this recipe.  \n\n\nCreate a PRE-AMBARI-START recipe. For instructions on how to create a recipe, refer to \nAdd Recipes\n.   \n\n\nWhen creating a cluster, choose this recipe to be executed on all host groups of the cluster.  \n\n\n\n\nAdd Recipes\n\n\nTo add a recipe, perform these steps.\n\n\nSteps\n\n\n\n\n\n\nPlace your script in a network location accessible from Cloudbreak and cluster instances virtual network. \n\n\n\n\n\n\nSelect \nBlueprints\n from the navigation pane. \n\n\n\n\n\n\nClick on \nCreate Blueprint\n. \n\n\n\n\n\n\nProvide the following:\n\n\n\n\n\n\n\n\nParameter\n\n\nValue\n\n\n\n\n\n\n\n\n\n\nName\n\n\nEnter a name for your recipe.\n\n\n\n\n\n\nDescription\n\n\n(Optional) Enter a description for your recipe.\n\n\n\n\n\n\nExecution Type\n\n\nSelect one of the following options: \npre-ambari-start\n: The script will be executed prior to Ambari server start.\npost-ambari-start\n: The script will be executed after Ambari server start but prior to cluster installation.\npost-cluster-install\n: The script will be executed after cluster deployment.\npre-termination\n: The script will be executed before cluster termination.\n\n\n\n\n\n\nScript\n\n\nSelect one of: \nScript\n: Paste the script.\n \nFile\n: Point to a file on your machine that contains the recipe.\n \nURL\n: Specify the URL for your recipe.\n\n\n\n\n\n\n\n\n \n\n\n\n\n\n\nWhen creating a cluster, you can select previously added recipes in the \nRecipes\n section. \n\n\n \n\n\n\n\n\n\nDelete Recipes\n\n\nYou can delete previously added items by selecting and item and using the \ndelete\n option.", 
            "title": "Recipes"
        }, 
        {
            "location": "/recipes/index.html#recipes", 
            "text": "Although Cloudbreak lets you provision HDP clusters in the cloud based on custom Ambari blueprints, Cloudbreak provisioning options don't consider all possible use cases. For that reason, we introduced recipes.   A recipe is a script that runs on all nodes of a selected node group before or after the Ambari cluster installation. You can use recipes for tasks such as installing additional software or performing advanced cluster configuration. For example, you can use a recipe to put a JAR file on the Hadoop classpath.  When creating a cluster, you can optionally upload one or more \"recipes\" (custom scripts) and they will be executed on a specific host group at a specified time. Available recipe execution times are:     Before Ambari server start      After Ambari server start      After cluster installation      Before cluster termination", 
            "title": "Recipes"
        }, 
        {
            "location": "/recipes/index.html#writing-recipes", 
            "text": "When using recipes, consider the following:   The scripts will be executed on the node types you specify (such as \"master\", \"worker\", \"compute\"). If you want to run a a script on all nodes, define the recipe one per node type.    The script will execute on all of the nodes of that type as root.    In order to be executed, your script must be in a network location which is accessible from the cloud controller and cluster instances VPC.    Make sure to follow Linux best practices when creating your scripts. For example, don't forget to script \"Yes\" auto-answers where needed.    Do not execute yum update \u2013y since it may update other components on the instances (such as salt), which can create unintended or unstable behavior.     The scripts will be executed as root. The recipe output is written to  /var/log/recipes  on each node on which it was executed.", 
            "title": "Writing Recipes"
        }, 
        {
            "location": "/recipes/index.html#sample-recipe-for-yum-proxy-setting", 
            "text": "#!/bin/bash\ncat   /etc/yum.conf  ENDOF\nproxy=http://10.0.0.133:3128\nENDOF", 
            "title": "Sample Recipe for Yum Proxy Setting"
        }, 
        {
            "location": "/recipes/index.html#recipe-to-install-ext-js-for-oozie", 
            "text": "Ext JS is GPL licensed software and is no longer included in builds of HDP 2.6. Because of this, the Oozie WAR file is not built to include the Ext JS-based user interface unless Ext JS is manually installed on the Oozie server. If you add Oozie using Ambari 2.6.1.0 to an HDP 2.6.4 or greater stack, no Oozie UI will be available by default. Therefore, if you plan to use Oozie with Ambari 2.6.1.0 to an HDP 2.6.4 or greater, you must ensure that ExtJS is installed prior to installing Oozie (that is, prior to cluster install).   You can install Ext JS by adding the following PRE-AMBARI-START recipe:  export EXT_JS_VERSION=2.2-1\n     export OS_NAME=centos6\n     wget http://public-repo-1.hortonworks.com/HDP-UTILS-GPL-1.1.0.22/repos/$OS_NAME/extjs/extjs-$EXT_JS_VERSION.noarch.rpm\n     rpm -ivh extjs-$EXT_JS_VERSION.noarch.rpm  Make the following changes to the script:   Change the EXT_JS_VERSION to the specific ExtJS version that you want to use.    Change the OS_NAME to the name of the operating system. Supported values are: centos6, centos7, centos7-ppc.   The general steps are:   Be sure to review and agree to the Ext JS license prior to using this recipe.    Create a PRE-AMBARI-START recipe. For instructions on how to create a recipe, refer to  Add Recipes .     When creating a cluster, choose this recipe to be executed on all host groups of the cluster.", 
            "title": "Recipe to Install Ext JS for Oozie"
        }, 
        {
            "location": "/recipes/index.html#add-recipes", 
            "text": "To add a recipe, perform these steps.  Steps    Place your script in a network location accessible from Cloudbreak and cluster instances virtual network.     Select  Blueprints  from the navigation pane.     Click on  Create Blueprint .     Provide the following:     Parameter  Value      Name  Enter a name for your recipe.    Description  (Optional) Enter a description for your recipe.    Execution Type  Select one of the following options:  pre-ambari-start : The script will be executed prior to Ambari server start. post-ambari-start : The script will be executed after Ambari server start but prior to cluster installation. post-cluster-install : The script will be executed after cluster deployment. pre-termination : The script will be executed before cluster termination.    Script  Select one of:  Script : Paste the script.   File : Point to a file on your machine that contains the recipe.   URL : Specify the URL for your recipe.          When creating a cluster, you can select previously added recipes in the  Recipes  section.", 
            "title": "Add Recipes"
        }, 
        {
            "location": "/recipes/index.html#delete-recipes", 
            "text": "You can delete previously added items by selecting and item and using the  delete  option.", 
            "title": "Delete Recipes"
        }, 
        {
            "location": "/tags/index.html", 
            "text": "Resource Tagging\n\n\nWhen you manually create resources in the cloud, you have an option to add custom tags that help you track these resources. Likewise, when creating clusters, you can instruct Cloudbreak to tag the cloud resources that it creates on your behalf. The tags added during cluster creation are displayed in your cloud account on the resources that Cloudbreak provisioned for your clusters. \n\n\nYou can use tags to categorize your cloud resources by purpose, owner, and so on. Tags come in especially handy when you are using a corporate AWS account and you want to quickly identify which resources belong to your cluster(s). In fact, your corporate cloud account admin may require you to tag all the resources that you create, in particular resources, such as VMs, which incur charges.\n\n\nAdd Tags When Creating a Cluster\n\n\nYou can tag the cloud resources used for a cluster by providing custom tag names and values when creating a cluster via UI or CLI. In the Cloudbreak UI, this option is available in the create cluster wizard, in the advanced \nGeneral Configuration\n \n \nTags\n section:\n\n\n \n\n\nIt is not possible to add tags via Cloudbreak after your cluster has been created.  \n\n\nTo learn more about tags and their restrictions, refer to the cloud provider documentation. \n\n\nRelated Links\n\n\nTags on AWS\n  \n\n\nTags on Azure\n\n\nLabels on GCP\n\n\nTags on OpenStack", 
            "title": "Resource Tagging"
        }, 
        {
            "location": "/tags/index.html#resource-tagging", 
            "text": "When you manually create resources in the cloud, you have an option to add custom tags that help you track these resources. Likewise, when creating clusters, you can instruct Cloudbreak to tag the cloud resources that it creates on your behalf. The tags added during cluster creation are displayed in your cloud account on the resources that Cloudbreak provisioned for your clusters.   You can use tags to categorize your cloud resources by purpose, owner, and so on. Tags come in especially handy when you are using a corporate AWS account and you want to quickly identify which resources belong to your cluster(s). In fact, your corporate cloud account admin may require you to tag all the resources that you create, in particular resources, such as VMs, which incur charges.", 
            "title": "Resource Tagging"
        }, 
        {
            "location": "/tags/index.html#add-tags-when-creating-a-cluster", 
            "text": "You can tag the cloud resources used for a cluster by providing custom tag names and values when creating a cluster via UI or CLI. In the Cloudbreak UI, this option is available in the create cluster wizard, in the advanced  General Configuration     Tags  section:     It is not possible to add tags via Cloudbreak after your cluster has been created.    To learn more about tags and their restrictions, refer to the cloud provider documentation.   Related Links  Tags on AWS     Tags on Azure  Labels on GCP  Tags on OpenStack", 
            "title": "Add Tags When Creating a Cluster"
        }, 
        {
            "location": "/images/index.html", 
            "text": "Custom Images\n\n\nDefault images are available for each supported cloud provider and region. The following table lists the default base images available: \n\n\n\n\n\n\n\n\nCloud Provider\n\n\nDefault Image\n\n\n\n\n\n\n\n\n\n\nAWS\n\n\nAmazon Linux 2017\n\n\n\n\n\n\nAzure\n\n\nCentOS 7\n\n\n\n\n\n\nGCP\n\n\nCentOS 7\n\n\n\n\n\n\nOpenStack\n\n\nCentOS 7\n\n\n\n\n\n\n\n\nSince these default images may not fit the requirements of some users (for example when user requirements include custom OS hardening, custom libraries, custom tooling, and so on) Cloudbeak allows you to use your own \ncustom base images\n.\n\n\nIn order to use your own custom base images you must:\n\n\n\n\nBuild your custom images  \n\n\nPrepare the custom image catalog JSON file and save it in a location accessible to the Cloudbreak VM  \n\n\nRegister your custom image catalog with Cloudbreak    \n\n\nSelect a custom image when creating a cluster  \n\n\n\n\n\n    \nImportant\n\n    \n\nOnly \nbase images\n can be created and registered as custom images. Do not create or register \nprewarmed images\n as custom images. \n\n\n\n\n\n\nBuild Custom Images\n\n\nRefer to \nCustom Images for Cloudbreak\n for information on how to build custom images. \n\n\nThis repository includes instructions and scripts to help you build custom images. Once you have the images, refer to the documentation below for information on how to create an image catalog and register it with Cloudbreak.\n\n\nPrepare the Image Catalog\n\n\nOnce you've built the custom images, prepare your custom image catalog JSON file. Once your image catalog JSON file is ready, save it in a location accessible via HTTP/HTTPS. \n\n\nStructure of the Image Catalog JSON File\n\n\nThe image catalog JSON file includes the following two high-level sections: \n\n\n\n\nimages\n: Contains information about the created images. The burned images are stored in the \nbase-images\n section.  \n\n\nversions\n: Contains the \ncloudbreak\n entry, which includes mapping between Cloudbreak versions and the image identifiers of burned images available for these Cloudbreak versions. \n\n\n\n\nImages Section\n  \n\n\nThe burned images are stored in the \nbase-images\n sub-section of \nimages\n. The \nbase-images\n section stores one or more image \"records\". Every image \"record\" must contain the date, description, images, os, os_type, and uuid fields.\n\n\n\n\n\n\n\n\nParameter\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\ndate\n\n\nDate for your image catalog entry.\n\n\n\n\n\n\ndescription\n\n\nDescription for your image catalog entry.\n\n\n\n\n\n\nimages\n\n\nThe image sets by cloud provider. An image set must store the virtual machine image IDs by the related region of the provider (AWS, Azure) or contain one default image for all regions (GCP, OpenStack). The virtual machine image IDs come from the result of the image burning process and must be an existing identifier of a virtual machine image on the related provider side. For the providers which use global rather than per-region images, the region should be replaced with \ndefault\n.\n\n\n\n\n\n\nos\n\n\nThe operating system used in the image.\n\n\n\n\n\n\nos_type\n\n\nThe type of operating system which will be used to determine the default Ambari and HDP repositories to use. Set \nos_type\n to \"redhat6\" for amazonlinux or centos6 images. Set \nos_type\n to \"redhat7\" for centos7 or rhel7 images.\n\n\n\n\n\n\nuuid\n\n\nThe \nuuid\n field must be a unique identifier within the file. You can generate it or select it manually. The utility \nuuidgen\n available from your command line is a convenient way to generate a unique ID.\n\n\n\n\n\n\n\n\nVersions Section\n  \n\n\nThe \nversions\n section includes a single \"cloudbreak\" entry, which maps the uuids to a specific Cloudbreak version:\n\n\n\n\n\n\n\n\nParameter\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nimages\n\n\nImage \nuuid\n, same as the one that you specified in the \nbase-images\n section.\n\n\n\n\n\n\nversions\n\n\nThe Cloudbreak version(s) for which you would like to use the images.\n\n\n\n\n\n\n\n\nExample Image Catalog JSON File\n\n\nHere is an example image catalog JSON file that includes two sets of custom base images: \n\n\n\n\nA custom base image for AWS:\n\n\nThat is using Amazon Linux operating system \n\n\nThat will use the Redhat 6 repos as default Ambari and HDP repositories during cluster create     \n\n\nHas a unique ID of \"44b140a4-bd0b-457d-b174-e988bee3ca47\"\n\n\nIs available for Cloudbreak 2.4.0    \n\n\n\n\n\n\nA custom base image for Azure, Google, and OpenStack: \n\n\nThat is using CentOS 7 operating system \n\n\nThat will use the Redhat 7 repos as default Ambari and HDP repositories during cluster create   \n\n\nHas a unique ID of \"f6e778fc-7f17-4535-9021-515351df3692\"\n\n\nIs available to Cloudbreak 2.4.0      \n\n\n\n\n\n\n\n\nYou can also download it from \nhere\n.\n\n\n\n{\n  \"images\": {\n    \"base-images\": [\n      {\n        \"date\": \"2017-10-13\",\n        \"description\": \"Cloudbreak official base image\",\n        \"images\": {\n          \"aws\": {\n            \"ap-northeast-1\": \"ami-78e9311e\",\n            \"ap-northeast-2\": \"ami-84b613ea\",\n            \"ap-southeast-1\": \"ami-75226716\",\n            \"ap-southeast-2\": \"ami-92ce23f0\",\n            \"eu-central-1\": \"ami-d95be5b6\",\n            \"eu-west-1\": \"ami-46429e3f\",\n            \"sa-east-1\": \"ami-86d5abea\",\n            \"us-east-1\": \"ami-51a2742b\",\n            \"us-west-1\": \"ami-21ccfe41\",\n            \"us-west-2\": \"ami-2a1cdc52\"\n          }\n        },\n        \"os\": \"amazonlinux\",\n        \"os_type\": \"redhat6\",\n        \"uuid\": \"44b140a4-bd0b-457d-b174-e988bee3ca47\"\n      },\n      {\n        \"date\": \"2017-10-13\",\n        \"description\": \"Cloudbreak official base image\",\n        \"images\": {\n          \"azure\": {\n            \"Australia East\": \"https://hwxaustraliaeast.blob.core.windows.net/images/hdc-hdp--1710161226.vhd\",\n            \"Australia South East\": \"https://hwxaustralisoutheast.blob.core.windows.net/images/hdc-hdp--1710161226.vhd\",\n            \"Brazil South\": \"https://sequenceiqbrazilsouth2.blob.core.windows.net/images/hdc-hdp--1710161226.vhd\",\n            \"Canada Central\": \"https://sequenceiqcanadacentral.blob.core.windows.net/images/hdc-hdp--1710161226.vhd\",\n            \"Canada East\": \"https://sequenceiqcanadaeast.blob.core.windows.net/images/hdc-hdp--1710161226.vhd\",\n            \"Central India\": \"https://hwxcentralindia.blob.core.windows.net/images/hdc-hdp--1710161226.vhd\",\n            \"Central US\": \"https://sequenceiqcentralus2.blob.core.windows.net/images/hdc-hdp--1710161226.vhd\",\n            \"East Asia\": \"https://sequenceiqeastasia2.blob.core.windows.net/images/hdc-hdp--1710161226.vhd\",\n            \"East US\": \"https://sequenceiqeastus12.blob.core.windows.net/images/hdc-hdp--1710161226.vhd\",\n            \"East US 2\": \"https://sequenceiqeastus22.blob.core.windows.net/images/hdc-hdp--1710161226.vhd\",\n            \"Japan East\": \"https://sequenceiqjapaneast2.blob.core.windows.net/images/hdc-hdp--1710161226.vhd\",\n            \"Japan West\": \"https://sequenceiqjapanwest2.blob.core.windows.net/images/hdc-hdp--1710161226.vhd\",\n            \"Korea Central\": \"https://hwxkoreacentral.blob.core.windows.net/images/hdc-hdp--1710161226.vhd\",\n            \"Korea South\": \"https://hwxkoreasouth.blob.core.windows.net/images/hdc-hdp--1710161226.vhd\",\n            \"North Central US\": \"https://sequenceiqorthcentralus2.blob.core.windows.net/images/hdc-hdp--1710161226.vhd\",\n            \"North Europe\": \"https://sequenceiqnortheurope2.blob.core.windows.net/images/hdc-hdp--1710161226.vhd\",\n            \"South Central US\": \"https://sequenceiqouthcentralus2.blob.core.windows.net/images/hdc-hdp--1710161226.vhd\",\n            \"South India\": \"https://hwxsouthindia.blob.core.windows.net/images/hdc-hdp--1710161226.vhd\",\n            \"Southeast Asia\": \"https://sequenceiqsoutheastasia2.blob.core.windows.net/images/hdc-hdp--1710161226.vhd\",\n            \"UK South\": \"https://hwxsouthuk.blob.core.windows.net/images/hdc-hdp--1710161226.vhd\",\n            \"UK West\": \"https://hwxwestuk.blob.core.windows.net/images/hdc-hdp--1710161226.vhd\",\n            \"West Central US\": \"https://hwxwestcentralus.blob.core.windows.net/images/hdc-hdp--1710161226.vhd\",\n            \"West Europe\": \"https://sequenceiqwesteurope2.blob.core.windows.net/images/hdc-hdp--1710161226.vhd\",\n            \"West India\": \"https://hwxwestindia.blob.core.windows.net/images/hdc-hdp--1710161226.vhd\",\n            \"West US\": \"https://sequenceiqwestus2.blob.core.windows.net/images/hdc-hdp--1710161226.vhd\",\n            \"West US 2\": \"https://hwxwestus2.blob.core.windows.net/images/hdc-hdp--1710161226.vhd\"\n          },\n          \"gcp\": {\n            \"default\": \"sequenceiqimage/hdc-hdp--1710161226.tar.gz\"\n          },\n          \"openstack\": {\n            \"default\": \"hdc-hdp--1710161226\"\n          }\n        },\n        \"os\": \"centos7\",\n        \"os_type\": \"redhat7\",\n        \"uuid\": \"f6e778fc-7f17-4535-9021-515351df3691\"\n      }\n    ]\n},\n  \"versions\": {\n    \"cloudbreak\": [\n      {\n        \"images\": [\n          \"44b140a4-bd0b-457d-b174-e988bee3ca47\",\n          \"f6e778fc-7f17-4535-9021-515351df3692\"\n        ],\n        \"versions\": [\n          \"2.4.0\"\n        ]\n      }\n    ]\n  }\n}\n\n\n\n\nRegister Image Catalog\n\n\nNow that you have created your image catalog JSON file, register it with your Cloudbreak instance. \n\n\nRegister Image Catalog in the UI\n\n\nUse these steps to register your custom image catalog in the Cloudbreak UI. \n\n\nSteps\n\n\n\n\nIn the Cloudbreak UI, select \nSettings\n \n \nImage Catalogs\n from the navigation menu.  \n\n\n\n\nClick \nCreate Image Catalog\n.\n    \n\n\n\n\n\n\nEnter name for your image catalog and the URL to the location where it is stored. \n\n\n\n\nClick \nCreate\n. \n\n\n\n\nAfter performing these steps, the image catalog will be available and automatically selected as the default entry in the image catalog drop-down list in the create cluster wizard.\n\n\nRegister Image Catalog in the CLI\n\n\nTo register your custom image catalog using the CLI, use the \ncb imagecatalog create\n command. Refer to \nCLI documentation\n. \n\n\nSelect a Custom Image When Creating a Cluster\n\n\nOnce you have registered your image catalog, you can use your custom image(s) when creating a cluster. \n\n\nSelect a Custom Image in Cloudbreak UI\n\n\nPerform these steps in the advanced \nGeneral Configuration\n section of the create wizard wizard.\n\n\nSteps\n  \n\n\n\n\nUnder \nChoose Image Catalog\n, select your custom image catalog.  \n\n\nUnder \nBase Images\n \n \nChoose Image\n, select the provider-specific image that you would like to use. \n\n    The \"os\" that you specified in the image catalog will be displayed in the selection and the content of the \"description\" will be displayed in green.    \n\n\n\n\nYou can leave the default entries for the Ambari and HDP repositories, or you can customize to point to specific versions of Ambari and HDP that you want to use for the cluster.  \n\n\n\n\n\n\n\n\nSelect a Custom Image in CLI\n\n\nTo use the custom image when creating a cluster via CLI, perform these steps.  \n\n\nSteps\n  \n\n\n\n\n\n\nObtain the image ID. For example: \n\n\ncb imagecatalog images aws --imagecatalog custom-catalog\n[\n  {\n\"Date\": \"2017-10-13\",\n\"Description\": \"Cloudbreak official base image\",\n\"Version\": \"2.5.1.0\",\n\"ImageID\": \"44b140a4-bd0b-457d-b174-e988bee3ca47\"\n  },\n  {\n\"Date\": \"2017-11-16\",\n\"Description\": \"Official Cloudbreak image\",\n\"Version\": \"2.5.1.0\",\n\"ImageID\": \"3c7598a4-ebd6-4a02-5638-882f5c7f7add\"\n  }\n]\n\n\n\n\n\n\nWhen preparing a CLI JSON template for your cluster, set the \"ImageCatalog\" parameter to the image catalog that you would like to use, and set the \"ImageId\" parameter to the uuid of the image from that catalog that you would like to use. For example: \n\n\n...\n  \"name\": \"aszegedi-cli-ci\",\n  \"network\": {\n\"subnetCIDR\": \"10.0.0.0/16\"\n  },\n  \"orchestrator\": {\n\"type\": \"SALT\"\n  },\n  \"parameters\": {\n\"instanceProfileStrategy\": \"CREATE\"\n  },\n  \"region\": \"eu-west-1\",\n  \"stackAuthentication\": {\n\"publicKeyId\": \"seq-master\"\n  },\n  \"userDefinedTags\": {\n\"owner\": \"aszegedi\"\n  },\n  \"imageCatalog\": \"custom-catalog\",\n  \"imageId\": \"3c7598a4-ebd6-4a02-5638-882f5c7f7add\"\n}\n\n\n\n\n\n\nRelated Links\n\n\nCLI Reference", 
            "title": "Custom Images"
        }, 
        {
            "location": "/images/index.html#custom-images", 
            "text": "Default images are available for each supported cloud provider and region. The following table lists the default base images available:      Cloud Provider  Default Image      AWS  Amazon Linux 2017    Azure  CentOS 7    GCP  CentOS 7    OpenStack  CentOS 7     Since these default images may not fit the requirements of some users (for example when user requirements include custom OS hardening, custom libraries, custom tooling, and so on) Cloudbeak allows you to use your own  custom base images .  In order to use your own custom base images you must:   Build your custom images    Prepare the custom image catalog JSON file and save it in a location accessible to the Cloudbreak VM    Register your custom image catalog with Cloudbreak      Select a custom image when creating a cluster     \n     Important \n     \nOnly  base images  can be created and registered as custom images. Do not create or register  prewarmed images  as custom images.", 
            "title": "Custom Images"
        }, 
        {
            "location": "/images/index.html#build-custom-images", 
            "text": "Refer to  Custom Images for Cloudbreak  for information on how to build custom images.   This repository includes instructions and scripts to help you build custom images. Once you have the images, refer to the documentation below for information on how to create an image catalog and register it with Cloudbreak.", 
            "title": "Build Custom Images"
        }, 
        {
            "location": "/images/index.html#prepare-the-image-catalog", 
            "text": "Once you've built the custom images, prepare your custom image catalog JSON file. Once your image catalog JSON file is ready, save it in a location accessible via HTTP/HTTPS.", 
            "title": "Prepare the Image Catalog"
        }, 
        {
            "location": "/images/index.html#structure-of-the-image-catalog-json-file", 
            "text": "The image catalog JSON file includes the following two high-level sections:    images : Contains information about the created images. The burned images are stored in the  base-images  section.    versions : Contains the  cloudbreak  entry, which includes mapping between Cloudbreak versions and the image identifiers of burned images available for these Cloudbreak versions.    Images Section     The burned images are stored in the  base-images  sub-section of  images . The  base-images  section stores one or more image \"records\". Every image \"record\" must contain the date, description, images, os, os_type, and uuid fields.     Parameter  Description      date  Date for your image catalog entry.    description  Description for your image catalog entry.    images  The image sets by cloud provider. An image set must store the virtual machine image IDs by the related region of the provider (AWS, Azure) or contain one default image for all regions (GCP, OpenStack). The virtual machine image IDs come from the result of the image burning process and must be an existing identifier of a virtual machine image on the related provider side. For the providers which use global rather than per-region images, the region should be replaced with  default .    os  The operating system used in the image.    os_type  The type of operating system which will be used to determine the default Ambari and HDP repositories to use. Set  os_type  to \"redhat6\" for amazonlinux or centos6 images. Set  os_type  to \"redhat7\" for centos7 or rhel7 images.    uuid  The  uuid  field must be a unique identifier within the file. You can generate it or select it manually. The utility  uuidgen  available from your command line is a convenient way to generate a unique ID.     Versions Section     The  versions  section includes a single \"cloudbreak\" entry, which maps the uuids to a specific Cloudbreak version:     Parameter  Description      images  Image  uuid , same as the one that you specified in the  base-images  section.    versions  The Cloudbreak version(s) for which you would like to use the images.", 
            "title": "Structure of the Image Catalog JSON File"
        }, 
        {
            "location": "/images/index.html#example-image-catalog-json-file", 
            "text": "Here is an example image catalog JSON file that includes two sets of custom base images:    A custom base image for AWS:  That is using Amazon Linux operating system   That will use the Redhat 6 repos as default Ambari and HDP repositories during cluster create       Has a unique ID of \"44b140a4-bd0b-457d-b174-e988bee3ca47\"  Is available for Cloudbreak 2.4.0        A custom base image for Azure, Google, and OpenStack:   That is using CentOS 7 operating system   That will use the Redhat 7 repos as default Ambari and HDP repositories during cluster create     Has a unique ID of \"f6e778fc-7f17-4535-9021-515351df3692\"  Is available to Cloudbreak 2.4.0           You can also download it from  here .  \n{\n  \"images\": {\n    \"base-images\": [\n      {\n        \"date\": \"2017-10-13\",\n        \"description\": \"Cloudbreak official base image\",\n        \"images\": {\n          \"aws\": {\n            \"ap-northeast-1\": \"ami-78e9311e\",\n            \"ap-northeast-2\": \"ami-84b613ea\",\n            \"ap-southeast-1\": \"ami-75226716\",\n            \"ap-southeast-2\": \"ami-92ce23f0\",\n            \"eu-central-1\": \"ami-d95be5b6\",\n            \"eu-west-1\": \"ami-46429e3f\",\n            \"sa-east-1\": \"ami-86d5abea\",\n            \"us-east-1\": \"ami-51a2742b\",\n            \"us-west-1\": \"ami-21ccfe41\",\n            \"us-west-2\": \"ami-2a1cdc52\"\n          }\n        },\n        \"os\": \"amazonlinux\",\n        \"os_type\": \"redhat6\",\n        \"uuid\": \"44b140a4-bd0b-457d-b174-e988bee3ca47\"\n      },\n      {\n        \"date\": \"2017-10-13\",\n        \"description\": \"Cloudbreak official base image\",\n        \"images\": {\n          \"azure\": {\n            \"Australia East\": \"https://hwxaustraliaeast.blob.core.windows.net/images/hdc-hdp--1710161226.vhd\",\n            \"Australia South East\": \"https://hwxaustralisoutheast.blob.core.windows.net/images/hdc-hdp--1710161226.vhd\",\n            \"Brazil South\": \"https://sequenceiqbrazilsouth2.blob.core.windows.net/images/hdc-hdp--1710161226.vhd\",\n            \"Canada Central\": \"https://sequenceiqcanadacentral.blob.core.windows.net/images/hdc-hdp--1710161226.vhd\",\n            \"Canada East\": \"https://sequenceiqcanadaeast.blob.core.windows.net/images/hdc-hdp--1710161226.vhd\",\n            \"Central India\": \"https://hwxcentralindia.blob.core.windows.net/images/hdc-hdp--1710161226.vhd\",\n            \"Central US\": \"https://sequenceiqcentralus2.blob.core.windows.net/images/hdc-hdp--1710161226.vhd\",\n            \"East Asia\": \"https://sequenceiqeastasia2.blob.core.windows.net/images/hdc-hdp--1710161226.vhd\",\n            \"East US\": \"https://sequenceiqeastus12.blob.core.windows.net/images/hdc-hdp--1710161226.vhd\",\n            \"East US 2\": \"https://sequenceiqeastus22.blob.core.windows.net/images/hdc-hdp--1710161226.vhd\",\n            \"Japan East\": \"https://sequenceiqjapaneast2.blob.core.windows.net/images/hdc-hdp--1710161226.vhd\",\n            \"Japan West\": \"https://sequenceiqjapanwest2.blob.core.windows.net/images/hdc-hdp--1710161226.vhd\",\n            \"Korea Central\": \"https://hwxkoreacentral.blob.core.windows.net/images/hdc-hdp--1710161226.vhd\",\n            \"Korea South\": \"https://hwxkoreasouth.blob.core.windows.net/images/hdc-hdp--1710161226.vhd\",\n            \"North Central US\": \"https://sequenceiqorthcentralus2.blob.core.windows.net/images/hdc-hdp--1710161226.vhd\",\n            \"North Europe\": \"https://sequenceiqnortheurope2.blob.core.windows.net/images/hdc-hdp--1710161226.vhd\",\n            \"South Central US\": \"https://sequenceiqouthcentralus2.blob.core.windows.net/images/hdc-hdp--1710161226.vhd\",\n            \"South India\": \"https://hwxsouthindia.blob.core.windows.net/images/hdc-hdp--1710161226.vhd\",\n            \"Southeast Asia\": \"https://sequenceiqsoutheastasia2.blob.core.windows.net/images/hdc-hdp--1710161226.vhd\",\n            \"UK South\": \"https://hwxsouthuk.blob.core.windows.net/images/hdc-hdp--1710161226.vhd\",\n            \"UK West\": \"https://hwxwestuk.blob.core.windows.net/images/hdc-hdp--1710161226.vhd\",\n            \"West Central US\": \"https://hwxwestcentralus.blob.core.windows.net/images/hdc-hdp--1710161226.vhd\",\n            \"West Europe\": \"https://sequenceiqwesteurope2.blob.core.windows.net/images/hdc-hdp--1710161226.vhd\",\n            \"West India\": \"https://hwxwestindia.blob.core.windows.net/images/hdc-hdp--1710161226.vhd\",\n            \"West US\": \"https://sequenceiqwestus2.blob.core.windows.net/images/hdc-hdp--1710161226.vhd\",\n            \"West US 2\": \"https://hwxwestus2.blob.core.windows.net/images/hdc-hdp--1710161226.vhd\"\n          },\n          \"gcp\": {\n            \"default\": \"sequenceiqimage/hdc-hdp--1710161226.tar.gz\"\n          },\n          \"openstack\": {\n            \"default\": \"hdc-hdp--1710161226\"\n          }\n        },\n        \"os\": \"centos7\",\n        \"os_type\": \"redhat7\",\n        \"uuid\": \"f6e778fc-7f17-4535-9021-515351df3691\"\n      }\n    ]\n},\n  \"versions\": {\n    \"cloudbreak\": [\n      {\n        \"images\": [\n          \"44b140a4-bd0b-457d-b174-e988bee3ca47\",\n          \"f6e778fc-7f17-4535-9021-515351df3692\"\n        ],\n        \"versions\": [\n          \"2.4.0\"\n        ]\n      }\n    ]\n  }\n}", 
            "title": "Example Image Catalog JSON File"
        }, 
        {
            "location": "/images/index.html#register-image-catalog", 
            "text": "Now that you have created your image catalog JSON file, register it with your Cloudbreak instance.", 
            "title": "Register Image Catalog"
        }, 
        {
            "location": "/images/index.html#register-image-catalog-in-the-ui", 
            "text": "Use these steps to register your custom image catalog in the Cloudbreak UI.   Steps   In the Cloudbreak UI, select  Settings     Image Catalogs  from the navigation menu.     Click  Create Image Catalog .\n        Enter name for your image catalog and the URL to the location where it is stored.    Click  Create .    After performing these steps, the image catalog will be available and automatically selected as the default entry in the image catalog drop-down list in the create cluster wizard.", 
            "title": "Register Image Catalog in the UI"
        }, 
        {
            "location": "/images/index.html#register-image-catalog-in-the-cli", 
            "text": "To register your custom image catalog using the CLI, use the  cb imagecatalog create  command. Refer to  CLI documentation .", 
            "title": "Register Image Catalog in the CLI"
        }, 
        {
            "location": "/images/index.html#select-a-custom-image-when-creating-a-cluster", 
            "text": "Once you have registered your image catalog, you can use your custom image(s) when creating a cluster.", 
            "title": "Select a Custom Image When Creating a Cluster"
        }, 
        {
            "location": "/images/index.html#select-a-custom-image-in-cloudbreak-ui", 
            "text": "Perform these steps in the advanced  General Configuration  section of the create wizard wizard.  Steps      Under  Choose Image Catalog , select your custom image catalog.    Under  Base Images     Choose Image , select the provider-specific image that you would like to use.  \n    The \"os\" that you specified in the image catalog will be displayed in the selection and the content of the \"description\" will be displayed in green.       You can leave the default entries for the Ambari and HDP repositories, or you can customize to point to specific versions of Ambari and HDP that you want to use for the cluster.", 
            "title": "Select a Custom Image in Cloudbreak UI"
        }, 
        {
            "location": "/images/index.html#select-a-custom-image-in-cli", 
            "text": "To use the custom image when creating a cluster via CLI, perform these steps.    Steps       Obtain the image ID. For example:   cb imagecatalog images aws --imagecatalog custom-catalog\n[\n  {\n\"Date\": \"2017-10-13\",\n\"Description\": \"Cloudbreak official base image\",\n\"Version\": \"2.5.1.0\",\n\"ImageID\": \"44b140a4-bd0b-457d-b174-e988bee3ca47\"\n  },\n  {\n\"Date\": \"2017-11-16\",\n\"Description\": \"Official Cloudbreak image\",\n\"Version\": \"2.5.1.0\",\n\"ImageID\": \"3c7598a4-ebd6-4a02-5638-882f5c7f7add\"\n  }\n]    When preparing a CLI JSON template for your cluster, set the \"ImageCatalog\" parameter to the image catalog that you would like to use, and set the \"ImageId\" parameter to the uuid of the image from that catalog that you would like to use. For example:   ...\n  \"name\": \"aszegedi-cli-ci\",\n  \"network\": {\n\"subnetCIDR\": \"10.0.0.0/16\"\n  },\n  \"orchestrator\": {\n\"type\": \"SALT\"\n  },\n  \"parameters\": {\n\"instanceProfileStrategy\": \"CREATE\"\n  },\n  \"region\": \"eu-west-1\",\n  \"stackAuthentication\": {\n\"publicKeyId\": \"seq-master\"\n  },\n  \"userDefinedTags\": {\n\"owner\": \"aszegedi\"\n  },\n  \"imageCatalog\": \"custom-catalog\",\n  \"imageId\": \"3c7598a4-ebd6-4a02-5638-882f5c7f7add\"\n}    Related Links  CLI Reference", 
            "title": "Select a Custom Image in CLI"
        }, 
        {
            "location": "/autoscaling/index.html", 
            "text": "Autoscaling\n\n\nAutoscaling allows you to adjust cluster capacity based on Ambari metrics and alerts, as well as schedule time-based capacity adjustment. When creating an autoscaling policy, you define:\n\n\n\n\nAn \nalert\n that triggers a scaling policy. An alert can be based on an Ambari metric or can be time-based.     \n\n\nA \nscaling policy\n that adds or removes a set number of nodes to a selected host group when the conditions defined in the attached alert are met.   \n\n\n\n\nMetric-based Autoscaling\n\n\nCloudbreak accesses all available Ambari metrics and allows you to define alerts based on these metrics. For example:\n\n\n\n\n\n\n\n\nAlert Definition\n\n\nPolicy Definition\n\n\n\n\n\n\n\n\n\n\nResourceManager CPU\n alert with \nCRITICAL\n status for 5 minutes\n\n\nAdd 10 worker nodes\n\n\n\n\n\n\nHDFS Capacity Utilization\n alert with \nWARN\n status for 20 minutes\n\n\nSet the number of worker nodes to 50\n\n\n\n\n\n\nAmbari Server Alerts\n alert with \nCRITICAL\n status for 15 minutes\n\n\nDecrease the number of worker nodes by 80%\n\n\n\n\n\n\n\n\nTime-based Autoscaling\n\n\nTime-based alerts can be defined by providing a cron expression. For example: \n\n\n\n\n\n\n\n\nAlert Definition\n\n\nPolicy Definition\n\n\n\n\n\n\n\n\n\n\nEvery day at 07:00 AM (GMT-8)\n\n\nAdd 90 worker nodes\n\n\n\n\n\n\nEvery day at 08:00 PM (GMT-8)\n\n\nRemove 90 worker nodes\n\n\n\n\n\n\n\n\nEnable Auto Scaling\n\n\nFor each newly created cluster, autoscaling is disabled by default but it can be enabled once the cluster is in a running state. \n\n\n\n\nAutoscaling configuration is only available in the UI. It is currently not available in the CLI. \n\n\n\n\nSteps\n\n\n\n\nOn the cluster details page, navigate to the \nAutoscaling\n tab.   \n\n\n\n\nClick the toggle button to enable autoscaling:\n\n\n  \n\n\n\n\n\n\nThe toggle button turns green and you can see that \"Autoscaling is enabled\":\n\n\n  \n\n\n\n\n\n\nDefine alerts\n and then \ndefine scaling policies\n. You can also \nadjust the autoscaling settings\n. \n\n\n\n\n\n\nIf you decide to disable autoscaling, your previously defined alerts and policies will be preserved. \n\n\nDefining an Alert\n\n\nAfter you have enabled autoscaling, define a metric-based or time-based alert.  \n\n\nDefine a Metric-based Alert\n\n\nAfter \nenabling autoscaling\n, perform the following steps to create a metric-based alert.  \n\n\n\n\nIf you would like to change default thresholds for an Ambari metric, refer to \nModifying Alerts\n in Ambari documentation. \n\n\nIf you would like to create a custom Ambari alert, refer to \nHow to Create a Custom Ambari Alert and Use it for Cloudbreak Autoscaling Policies\n.\n\n\n\n\nSteps\n\n\n\n\nIn the \nAlert Configuration\n section, select \nMetric Based\n alert type.      \n\n\n\n\nProvide the following information:\n\n\n\n\n\n\n\n\nParameter\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nEnter alert name\n\n\nEnter a unique name for the alert.\n\n\n\n\n\n\nChoose metric type\n\n\nSelect the Ambari metric that should trigger the alert.\n\n\n\n\n\n\nAlert status\n\n\nSelect the alert status that should trigger an alert for the selected metric. One of: OK, CRITICAL, WARNING.\n\n\n\n\n\n\nAlert duration\n\n\nSelect the alert duration that should trigger an alert.\n\n\n\n\n\n\n\n\n\n\n\n\nClick \n+\n to save the alert.  \n\n\n\n\n\n\nOnce you have defined an alert, \ncreate a scaling policy\n that this metric should trigger.\n\n\nRelated Links:\n\n\nHow to Create a Custom Ambari Alert and Use it for Cloudbreak Autoscaling Policies\n (HCC) \n\n\nModifying Alerts\n (Hortonworks)   \n\n\nDefine a Time-based Alert\n\n\nAfter \nenabling autoscaling\n, perform the following steps to create a time-based alert.\n\n\nSteps\n\n\n\n\nIn the \nAlert Configuration\n section, select the \nTime Based\n alert type. \n\n\n\n\nProvide the following information: \n\n\n\n\n\n\n\n\nParameter\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nEnter alert name.\n\n\nEnter a unique name for the alert.\n\n\n\n\n\n\nSelect timezone.\n\n\nSelect your timezone.\n\n\n\n\n\n\nEnter cron expression\n\n\nEnter a cron expression that defines the frequency of the alert. Refer to \nCron Expression Generator\n.\n\n\n\n\n\n\n\n\n\n\n\n\nClick \n+\n to save the alert.   \n\n\n\n\n\n\nOnce you have defined an alert, \ncreate a scaling policy\n that this metric should trigger.\n\n\nCreate a Scaling Policy\n\n\nAfter \nenabling autoscaling\n and \ncreating at least one alert\n, perform the following steps to create a scaling policy.\n\n\nSteps\n\n\n\n\n\n\nIn the \nPolicy Configuration\n section, provide the following information:\n\n\n\n\n\n\n\n\nParameter\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nEnter policy name\n\n\nEnter a unique name for the policy.\n\n\n\n\n\n\nSelect action\n\n\nSelect one of the following actions: Add (to add nodes to a host group) Remove (to delete nodes from a host group), or Set (to set the number of nodes in a host group to the chosen number).\n\n\n\n\n\n\nEnter number or percentage\n\n\nEnter a number defining how many or what percentage of nodes to add or remove. If the action selected is \"set\", this defines the number of nodes that a host group will be set to after scaling.\n\n\n\n\n\n\nSelect nodes of percent\n\n\nSelect \"nodes\" or \"percent\", depending on whether you want to scale to a specific number, or percent of current number of nodes.\n\n\n\n\n\n\nSelect host group\n\n\nSelect the host group to which to apply the scaling.\n\n\n\n\n\n\nChoose an alert\n\n\nSelect the alert based on which the scaling should be applied.\n\n\n\n\n\n\n\n\n\n\n\n\nClick \n+\n to save the alert.   \n\n\n\n\n\n\nConfigure Autoscaling Settings\n\n\nAfter \nenabling autoscaling\n, perform these steps to configure the auto scaling settings for your cluster.   \n\n\nSteps\n\n\n\n\n\n\nIn the \nCluster Scaling Configuration\n, provide the following information: \n\n\n\n\n\n\n\n\nSetting\n\n\nDescription\n\n\nDefault Value\n\n\n\n\n\n\n\n\n\n\nCooldown time\n\n\nAfter an auto scaling event occurs, the amount of time to wait before enforcing another scaling policy.\n\n\n30 minutes\n\n\n\n\n\n\nMinimum Cluster Size\n\n\nThe minimum size allowed for the cluster. Auto scaling policies cannot scale the cluster below or above this size.\n\n\n2 nodes\n\n\n\n\n\n\nMaximum Cluster Size\n\n\nThe maximum size allowed for the cluster. Auto scaling policies cannot scale the cluster below or above this size.\n\n\n100 nodes\n\n\n\n\n\n\n\n\n\n\n\n\nClick \nSave\n to save the changes.", 
            "title": "Autoscaling"
        }, 
        {
            "location": "/autoscaling/index.html#autoscaling", 
            "text": "Autoscaling allows you to adjust cluster capacity based on Ambari metrics and alerts, as well as schedule time-based capacity adjustment. When creating an autoscaling policy, you define:   An  alert  that triggers a scaling policy. An alert can be based on an Ambari metric or can be time-based.       A  scaling policy  that adds or removes a set number of nodes to a selected host group when the conditions defined in the attached alert are met.      Metric-based Autoscaling  Cloudbreak accesses all available Ambari metrics and allows you to define alerts based on these metrics. For example:     Alert Definition  Policy Definition      ResourceManager CPU  alert with  CRITICAL  status for 5 minutes  Add 10 worker nodes    HDFS Capacity Utilization  alert with  WARN  status for 20 minutes  Set the number of worker nodes to 50    Ambari Server Alerts  alert with  CRITICAL  status for 15 minutes  Decrease the number of worker nodes by 80%     Time-based Autoscaling  Time-based alerts can be defined by providing a cron expression. For example:      Alert Definition  Policy Definition      Every day at 07:00 AM (GMT-8)  Add 90 worker nodes    Every day at 08:00 PM (GMT-8)  Remove 90 worker nodes", 
            "title": "Autoscaling"
        }, 
        {
            "location": "/autoscaling/index.html#enable-auto-scaling", 
            "text": "For each newly created cluster, autoscaling is disabled by default but it can be enabled once the cluster is in a running state.    Autoscaling configuration is only available in the UI. It is currently not available in the CLI.    Steps   On the cluster details page, navigate to the  Autoscaling  tab.      Click the toggle button to enable autoscaling:        The toggle button turns green and you can see that \"Autoscaling is enabled\":        Define alerts  and then  define scaling policies . You can also  adjust the autoscaling settings .     If you decide to disable autoscaling, your previously defined alerts and policies will be preserved.", 
            "title": "Enable Auto Scaling"
        }, 
        {
            "location": "/autoscaling/index.html#defining-an-alert", 
            "text": "After you have enabled autoscaling, define a metric-based or time-based alert.", 
            "title": "Defining an Alert"
        }, 
        {
            "location": "/autoscaling/index.html#define-a-metric-based-alert", 
            "text": "After  enabling autoscaling , perform the following steps to create a metric-based alert.     If you would like to change default thresholds for an Ambari metric, refer to  Modifying Alerts  in Ambari documentation.   If you would like to create a custom Ambari alert, refer to  How to Create a Custom Ambari Alert and Use it for Cloudbreak Autoscaling Policies .   Steps   In the  Alert Configuration  section, select  Metric Based  alert type.         Provide the following information:     Parameter  Description      Enter alert name  Enter a unique name for the alert.    Choose metric type  Select the Ambari metric that should trigger the alert.    Alert status  Select the alert status that should trigger an alert for the selected metric. One of: OK, CRITICAL, WARNING.    Alert duration  Select the alert duration that should trigger an alert.       Click  +  to save the alert.      Once you have defined an alert,  create a scaling policy  that this metric should trigger.  Related Links:  How to Create a Custom Ambari Alert and Use it for Cloudbreak Autoscaling Policies  (HCC)   Modifying Alerts  (Hortonworks)", 
            "title": "Define a Metric-based Alert"
        }, 
        {
            "location": "/autoscaling/index.html#define-a-time-based-alert", 
            "text": "After  enabling autoscaling , perform the following steps to create a time-based alert.  Steps   In the  Alert Configuration  section, select the  Time Based  alert type.    Provide the following information:      Parameter  Description      Enter alert name.  Enter a unique name for the alert.    Select timezone.  Select your timezone.    Enter cron expression  Enter a cron expression that defines the frequency of the alert. Refer to  Cron Expression Generator .       Click  +  to save the alert.       Once you have defined an alert,  create a scaling policy  that this metric should trigger.", 
            "title": "Define a Time-based Alert"
        }, 
        {
            "location": "/autoscaling/index.html#create-a-scaling-policy", 
            "text": "After  enabling autoscaling  and  creating at least one alert , perform the following steps to create a scaling policy.  Steps    In the  Policy Configuration  section, provide the following information:     Parameter  Description      Enter policy name  Enter a unique name for the policy.    Select action  Select one of the following actions: Add (to add nodes to a host group) Remove (to delete nodes from a host group), or Set (to set the number of nodes in a host group to the chosen number).    Enter number or percentage  Enter a number defining how many or what percentage of nodes to add or remove. If the action selected is \"set\", this defines the number of nodes that a host group will be set to after scaling.    Select nodes of percent  Select \"nodes\" or \"percent\", depending on whether you want to scale to a specific number, or percent of current number of nodes.    Select host group  Select the host group to which to apply the scaling.    Choose an alert  Select the alert based on which the scaling should be applied.       Click  +  to save the alert.", 
            "title": "Create a Scaling Policy"
        }, 
        {
            "location": "/autoscaling/index.html#configure-autoscaling-settings", 
            "text": "After  enabling autoscaling , perform these steps to configure the auto scaling settings for your cluster.     Steps    In the  Cluster Scaling Configuration , provide the following information:      Setting  Description  Default Value      Cooldown time  After an auto scaling event occurs, the amount of time to wait before enforcing another scaling policy.  30 minutes    Minimum Cluster Size  The minimum size allowed for the cluster. Auto scaling policies cannot scale the cluster below or above this size.  2 nodes    Maximum Cluster Size  The maximum size allowed for the cluster. Auto scaling policies cannot scale the cluster below or above this size.  100 nodes       Click  Save  to save the changes.", 
            "title": "Configure Autoscaling Settings"
        }, 
        {
            "location": "/vm-launch/index.html", 
            "text": "Installing Cloudbreak on Your Own VM\n\n\nThis is an advanced deployment option. Select this option if you have custom VM requirements. Otherwise, you should use one of the pre-built images and follow these instructions:\n\n\n\n\nLaunch on AWS\n  \n\n\nLaunch on Azure\n  \n\n\nLaunch on GCP\n  \n\n\nLaunch on OpenStack\n   \n\n\n\n\nSystem Requirements\n\n\nTo launch the Cloudbreak deployer and install the Cloudbreak application, your system must meet the following requirements:\n\n\n\n\nMinimum VM requirements: 8GB RAM, 10GB disk, 2 cores\n\n\nSupported operating systems: RHEL, CentOS, and Oracle Linux 7 (64-bit)\n\n\nDocker 1.9.1 must be installed \n\n\n\n\n\n\nYou can install Cloudbreak on Mac OS X for evaluation purposes only. Mac OS X is not supported for a production deployment of Cloudbreak.\n\n\n\n\nPrerequisites\n\n\nTo launch the Cloudbreak deployer and install the Cloudbreak application, you must first meet the following prerequisites:\n\n\nPorts\n\n\nPorts 22 (SSH), 80 (HTTPS), and 443 (HTTPS) must be open.\n\n\nRoot Access\n\n\nEvery command must be executed as root. In order to get root privileges execute: \n\n\nsudo -i\n\n\n\nSystem Updates\n\n\nEnsure that your system is up-to-date by executing:\n\n\nyum -y update\n\n\n\nReboot it if necessary.\n\n\nIptables\n\n\nInstall iptables-services:\n\n\nyum -y install iptables-services net-tools\n\n\n\nWithout iptables-services installed the \niptables save\n command will not be available.\n\n\nNext, configure permissive iptables on your machine:\n\n\n\niptables --flush INPUT \n&\n&\n \\\niptables --flush FORWARD \n&\n&\n \\\nservice iptables save\n\n\n\n\nMore\n\n\nAdditionally, review the following prerequisites: \n\n\n\n\nPrerequisites on AWS\n\n\nPrerequisites on Azure\n\n\nPrerequisites on GCP\n\n\nPrerequisites on OpenStack\n \n\n\n\n\nInstall Cloudbreak on Your Own VM\n\n\nInstall Cloudbreak using the following steps.\n\n\nSteps\n\n\n\n\n\n\nInstall the Cloudbreak deployer and unzip the platform-specific single binary to your PATH. For example:\n\n\nyum -y install unzip tar\ncurl -Ls s3.amazonaws.com/public-repo-1.hortonworks.com/HDP/cloudbreak/cloudbreak-deployer_2.2.0_Linux_x86_64.tgz | sudo tar -xz -C /bin cbd\ncbd --version\n\n\nOnce the Cloudbreak deployer is installed, you can set up the Cloudbreak application.\n\n\n\n\n\n\nCreate a Cloudbreak deployment directory and navigate to it:\n\n\nmkdir cloudbreak-deployment\ncd cloudbreak-deployment\n\n\n\n\n\n\nIn the directory, create a file called \nProfile\n with the following content:\n\n\nexport UAA_DEFAULT_SECRET=MY-SECRET\nexport UAA_DEFAULT_USER_PW=MY-PASSWORD\nexport UAA_DEFAULT_USER_EMAIL=MY-EMAIL\n\n\nFor example:\n\n\nexport UAA_DEFAULT_SECRET=MySecret123\nexport UAA_DEFAULT_USER_PW=MySecurePassword123\nexport UAA_DEFAULT_USER_EMAIL=dbialek@hortonworks.com\n\n\n\n\nYou will need to provide the email and password when logging in to the Cloudbreak web UI and when using the Cloudbreak CLI. The secret will be used by Cloudbreak for authentication.\n\n\n\n\n\n\n\n\nGenerate configurations by executing:\n\n\nrm *.yml\ncbd generate\n   \n\n\nThe cbd start command includes the cbd generate command which applies the following steps:\n\n\n\n\nCreates the \ndocker-compose.yml\n file, which describes the configuration of all the Docker containers required for the Cloudbreak deployment.  \n\n\nCreates the \nuaa.yml\n file, which holds the configuration of the identity server used to authenticate users with Cloudbreak.   \n\n\n\n\n\n\n\n\nStart the Cloudbreak application by using the following commands:\n\n\ncbd pull\ncbd start\n\n\nThis will start the Docker containers and initialize the application. The first time you start the Coudbreak app, the process will take longer than usual due to the download of all the necessary docker images.\n\n\n\n\n\n\nNext, check Cloudbreak application logs: \n\n\ncbd logs cloudbreak\n\n\nYou should see a message like this in the log: \nStarted CloudbreakApplication in 36.823 seconds.\n Cloudbreak normally takes less than a minute to start.\n\n\n\n\n\n\nNext Steps After Installing on Your Own VM\n\n\nLog in to the Cloudbreak web UI and create a credential for Cloubdreak using the following platform-specific instructions:\n\n\n\n\nAccess Cloudbreak UI on AWS\n  \n\n\nAccess Cloudbreak UI on Azure\n  \n\n\nAccess Cloudbreak UI on GCP\n  \n\n\nAccess Cloudbreak UI on OpenStack", 
            "title": "Install on Your Own VM"
        }, 
        {
            "location": "/vm-launch/index.html#installing-cloudbreak-on-your-own-vm", 
            "text": "This is an advanced deployment option. Select this option if you have custom VM requirements. Otherwise, you should use one of the pre-built images and follow these instructions:   Launch on AWS     Launch on Azure     Launch on GCP     Launch on OpenStack", 
            "title": "Installing Cloudbreak on Your Own VM"
        }, 
        {
            "location": "/vm-launch/index.html#system-requirements", 
            "text": "To launch the Cloudbreak deployer and install the Cloudbreak application, your system must meet the following requirements:   Minimum VM requirements: 8GB RAM, 10GB disk, 2 cores  Supported operating systems: RHEL, CentOS, and Oracle Linux 7 (64-bit)  Docker 1.9.1 must be installed     You can install Cloudbreak on Mac OS X for evaluation purposes only. Mac OS X is not supported for a production deployment of Cloudbreak.", 
            "title": "System Requirements"
        }, 
        {
            "location": "/vm-launch/index.html#prerequisites", 
            "text": "To launch the Cloudbreak deployer and install the Cloudbreak application, you must first meet the following prerequisites:", 
            "title": "Prerequisites"
        }, 
        {
            "location": "/vm-launch/index.html#ports", 
            "text": "Ports 22 (SSH), 80 (HTTPS), and 443 (HTTPS) must be open.", 
            "title": "Ports"
        }, 
        {
            "location": "/vm-launch/index.html#root-access", 
            "text": "Every command must be executed as root. In order to get root privileges execute:   sudo -i", 
            "title": "Root Access"
        }, 
        {
            "location": "/vm-launch/index.html#system-updates", 
            "text": "Ensure that your system is up-to-date by executing:  yum -y update  Reboot it if necessary.", 
            "title": "System Updates"
        }, 
        {
            "location": "/vm-launch/index.html#iptables", 
            "text": "Install iptables-services:  yum -y install iptables-services net-tools  Without iptables-services installed the  iptables save  command will not be available.  Next, configure permissive iptables on your machine:  \niptables --flush INPUT  & &  \\\niptables --flush FORWARD  & &  \\\nservice iptables save", 
            "title": "Iptables"
        }, 
        {
            "location": "/vm-launch/index.html#more", 
            "text": "Additionally, review the following prerequisites:    Prerequisites on AWS  Prerequisites on Azure  Prerequisites on GCP  Prerequisites on OpenStack", 
            "title": "More"
        }, 
        {
            "location": "/vm-launch/index.html#install-cloudbreak-on-your-own-vm", 
            "text": "Install Cloudbreak using the following steps.  Steps    Install the Cloudbreak deployer and unzip the platform-specific single binary to your PATH. For example:  yum -y install unzip tar\ncurl -Ls s3.amazonaws.com/public-repo-1.hortonworks.com/HDP/cloudbreak/cloudbreak-deployer_2.2.0_Linux_x86_64.tgz | sudo tar -xz -C /bin cbd\ncbd --version  Once the Cloudbreak deployer is installed, you can set up the Cloudbreak application.    Create a Cloudbreak deployment directory and navigate to it:  mkdir cloudbreak-deployment\ncd cloudbreak-deployment    In the directory, create a file called  Profile  with the following content:  export UAA_DEFAULT_SECRET=MY-SECRET\nexport UAA_DEFAULT_USER_PW=MY-PASSWORD\nexport UAA_DEFAULT_USER_EMAIL=MY-EMAIL  For example:  export UAA_DEFAULT_SECRET=MySecret123\nexport UAA_DEFAULT_USER_PW=MySecurePassword123\nexport UAA_DEFAULT_USER_EMAIL=dbialek@hortonworks.com   You will need to provide the email and password when logging in to the Cloudbreak web UI and when using the Cloudbreak CLI. The secret will be used by Cloudbreak for authentication.     Generate configurations by executing:  rm *.yml\ncbd generate      The cbd start command includes the cbd generate command which applies the following steps:   Creates the  docker-compose.yml  file, which describes the configuration of all the Docker containers required for the Cloudbreak deployment.    Creates the  uaa.yml  file, which holds the configuration of the identity server used to authenticate users with Cloudbreak.        Start the Cloudbreak application by using the following commands:  cbd pull\ncbd start  This will start the Docker containers and initialize the application. The first time you start the Coudbreak app, the process will take longer than usual due to the download of all the necessary docker images.    Next, check Cloudbreak application logs:   cbd logs cloudbreak  You should see a message like this in the log:  Started CloudbreakApplication in 36.823 seconds.  Cloudbreak normally takes less than a minute to start.", 
            "title": "Install Cloudbreak on Your Own VM"
        }, 
        {
            "location": "/vm-launch/index.html#next-steps-after-installing-on-your-own-vm", 
            "text": "Log in to the Cloudbreak web UI and create a credential for Cloubdreak using the following platform-specific instructions:   Access Cloudbreak UI on AWS     Access Cloudbreak UI on Azure     Access Cloudbreak UI on GCP     Access Cloudbreak UI on OpenStack", 
            "title": "Next Steps After Installing on Your Own VM"
        }, 
        {
            "location": "/cb-upgrade/index.html", 
            "text": "Upgrading Cloudbreak\n\n\nUpgrading Cloudbreak consists of upgrading Cloudbreak deployer and upgrading existing clusters, if needed. \n\n\nUpdate Cloudbreak\n\n\nTo upgrade Cloudbreak to the newest version, perform the following steps.\n\n\nWe recommend that you back up the Cloudbreak databases before upgrading. Refer to \nBack up Cloudbreak Databases\n.\n\n\nSteps\n\n\n\n\n\n\nOn the VM where Cloudbreak is running, navigate to the directory where your Profile file is located:\n\n\ncd /var/lib/cloudbreak-deployment/\n\n\n\n\n\n\nStop all of the running Cloudbreak components:\n\n\ncbd kill\n\n\n\n\n\n\nUpdate Cloudbreak deployer:\n\n\ncbd update\n\n\n\n\n\n\nUpdate the \ndocker-compose.yml\n file with new Docker containers needed for the cbd:\n\n\ncbd regenerate\n\n\n\n\n\n\nIf there are no other Cloudbreak instances that still use old Cloudbreak versions, remove the obsolete containers:\n\n\ncbd util cleanup\n\n\n\n\n\n\nCheck the health and version of the updated cbd:\n\n\ncbd doctor\n\n\n\n\n\n\nStart the new version of the cbd:\n\n\ncbd start\n\n\nCloudbreak needs to download updated docker images for the new version, so this step may take a while.\n\n\n\n\n\n\nIn addition, if you have any clusters running, you must update them using the following steps. \n\n\nUpdate Existing Clusters\n\n\nUpgrading from version 1.4.0 to the newest version does not require any manual modification from the users.\n\n\nUpgrading from version 1.3.0 to the newest version requires that you update existing clusters. To update existing clusters, run the following commands on the \ncbgateway\n node of the cluster.\n\n\nSteps\n\n\n\n\n\n\nUpdate the version of the Salt-Bootsrap tool on the nodes:\n    \nsalt '*' cmd.run 'curl -Ls https://github.com/sequenceiq/salt-bootstrap/releases/download/v0.1.2/salt-bootstrap_0.1.2_Linux_x86_64.tgz | tar -zx -C /usr/sbin/ salt-bootstrap'\n\n\n\n\n\n\nTrigger restart of the tool on the nodes:\n\n\nsalt '*' service.dead salt-bootstrap\n\n\n\n\nTo check the version of the Salt-Bootsrap on the nodes, use \nsalt '*' cmd.run 'salt-bootstrap --version'", 
            "title": "Upgrade Cloudbreak"
        }, 
        {
            "location": "/cb-upgrade/index.html#upgrading-cloudbreak", 
            "text": "Upgrading Cloudbreak consists of upgrading Cloudbreak deployer and upgrading existing clusters, if needed.", 
            "title": "Upgrading Cloudbreak"
        }, 
        {
            "location": "/cb-upgrade/index.html#update-cloudbreak", 
            "text": "To upgrade Cloudbreak to the newest version, perform the following steps.  We recommend that you back up the Cloudbreak databases before upgrading. Refer to  Back up Cloudbreak Databases .  Steps    On the VM where Cloudbreak is running, navigate to the directory where your Profile file is located:  cd /var/lib/cloudbreak-deployment/    Stop all of the running Cloudbreak components:  cbd kill    Update Cloudbreak deployer:  cbd update    Update the  docker-compose.yml  file with new Docker containers needed for the cbd:  cbd regenerate    If there are no other Cloudbreak instances that still use old Cloudbreak versions, remove the obsolete containers:  cbd util cleanup    Check the health and version of the updated cbd:  cbd doctor    Start the new version of the cbd:  cbd start  Cloudbreak needs to download updated docker images for the new version, so this step may take a while.    In addition, if you have any clusters running, you must update them using the following steps.", 
            "title": "Update Cloudbreak"
        }, 
        {
            "location": "/cb-upgrade/index.html#update-existing-clusters", 
            "text": "Upgrading from version 1.4.0 to the newest version does not require any manual modification from the users.  Upgrading from version 1.3.0 to the newest version requires that you update existing clusters. To update existing clusters, run the following commands on the  cbgateway  node of the cluster.  Steps    Update the version of the Salt-Bootsrap tool on the nodes:\n     salt '*' cmd.run 'curl -Ls https://github.com/sequenceiq/salt-bootstrap/releases/download/v0.1.2/salt-bootstrap_0.1.2_Linux_x86_64.tgz | tar -zx -C /usr/sbin/ salt-bootstrap'    Trigger restart of the tool on the nodes:  salt '*' service.dead salt-bootstrap   To check the version of the Salt-Bootsrap on the nodes, use  salt '*' cmd.run 'salt-bootstrap --version'", 
            "title": "Update Existing Clusters"
        }, 
        {
            "location": "/cb-credentials/index.html", 
            "text": "Managing Cloudbreak Credentials\n\n\nYou can view and manage Cloudbreak credentials in the \nCredentials\n tab by clicking \nCreate credential\n and providing required parameters. You must create at least one credential in order to be able to create a cluster. \n\n\n \n\n\nCreate Cloudbreak Credental\n\n\nFor steps, refer to:\n\n\n\n\nCreate Credential on AWS\n  \n\n\nCreate Credential on Azure\n  \n\n\nCreate Credential on GCP\n \n\n\nCreate Credential on OpenStack\n\n\n\n\nView Credential Details\n\n\nTo view credential details, follow these steps.\n\n\nSteps\n\n\n\n\nIn the Cloudbreak UI, select \nCredentials\n from the navigation pane.  \n\n\nClick on the name of a credential. \n\n\n\n\nDetails for AWS Credentials\n\n\nThe following information is available for previously created AWS credentials:\n\n\n\n\nCredential Name  \n\n\nDescription   \n\n\nSelector: One of \nkey-based\n or \nrole-based\n, depending on credential type   \n\n\n\n\nSensitive information such as AWS access key and secret key (in case of a key-based credential) or role ARN (in case of a role-based credential) are not displayed.\n\n\nDetails for Azure Credentials\n\n\nThe following information is available for previously created Azure credentials:\n\n\n\n\nCredential Name   \n\n\nDescription  \n\n\nTenant Id: The \nDirectory ID\n from your Azure Active Directory   \n\n\nSubscription Id: Your Azure \nSubscription ID\n  \n\n\n\n\nIn case of the interactive credential, the following are also displayed:\n\n\n\n\nSp Display Name: Service principal name     \n\n\nRole Type: role selected during credential creation \n\n\n\n\nIn case of the app-based credential, the Application ID and key that you provided when creating the credential are not displayed.\n\n\nDetails for GCP Credentials\n\n\nThe following information is available for previously created GCP credentials:\n\n\n\n\nCredential Name   \n\n\nDescription  \n\n\nService Account Id\n\n\nProject Id \n\n\n\n\nThe P12 key that you attached when creating the credential is not displayed.\n\n\nDetails for OpenStack Credentials\n\n\nThe following information is available for previously created OpenStack credentials:\n\n\n\n\nCredential Name   \n\n\nDescription  \n\n\nFacing: \"internal\" or \"public\", as specified under \"Api Facing\"     \n\n\nEndpoint  \n\n\nSelector: \"cb-keystone-v2\" or \"cb-keystone-v3\"  \n\n\nKeystone Version: \"cb-keystone-v2\" or \"cb-keystone-v3\"   \n\n\nUser Name    \n\n\nTenant Name   \n\n\n\n\nThe password parameter is not displayed. \n\n\nSet a Default Credential\n\n\nIf using multiple Cloudbreak credentials, you can select one credential and use it as default for creating clusters. This default credential will be pre-selected in the create cluster wizard.\n\n\nSteps\n\n\n\n\nIn the Cloudbreak UI, select \nCredentials\n from the navigation pane.  \n\n\nClick \nSet as default\n next to the credential that you would like to set as default.  \n\n\nClick \nYes\n to confirm. \n\n\n\n\nDelete a Credential\n\n\nTo delete a credential, follow these steps.\n\n\nSteps\n\n\n\n\nIn the Cloudbreak UI, select \nCredentials\n from the navigation pane.  \n\n\nSelect one or more credentials by checking their corresponding checkboxes.\n\n\nClick \nDelete\n. \n\n\nClick \nYes\n to confirm.", 
            "title": "Manage Cloudbreak Credentials"
        }, 
        {
            "location": "/cb-credentials/index.html#managing-cloudbreak-credentials", 
            "text": "You can view and manage Cloudbreak credentials in the  Credentials  tab by clicking  Create credential  and providing required parameters. You must create at least one credential in order to be able to create a cluster.", 
            "title": "Managing Cloudbreak Credentials"
        }, 
        {
            "location": "/cb-credentials/index.html#create-cloudbreak-credental", 
            "text": "For steps, refer to:   Create Credential on AWS     Create Credential on Azure     Create Credential on GCP    Create Credential on OpenStack", 
            "title": "Create Cloudbreak Credental"
        }, 
        {
            "location": "/cb-credentials/index.html#view-credential-details", 
            "text": "To view credential details, follow these steps.  Steps   In the Cloudbreak UI, select  Credentials  from the navigation pane.    Click on the name of a credential.", 
            "title": "View Credential Details"
        }, 
        {
            "location": "/cb-credentials/index.html#details-for-aws-credentials", 
            "text": "The following information is available for previously created AWS credentials:   Credential Name    Description     Selector: One of  key-based  or  role-based , depending on credential type      Sensitive information such as AWS access key and secret key (in case of a key-based credential) or role ARN (in case of a role-based credential) are not displayed.", 
            "title": "Details for AWS Credentials"
        }, 
        {
            "location": "/cb-credentials/index.html#details-for-azure-credentials", 
            "text": "The following information is available for previously created Azure credentials:   Credential Name     Description    Tenant Id: The  Directory ID  from your Azure Active Directory     Subscription Id: Your Azure  Subscription ID      In case of the interactive credential, the following are also displayed:   Sp Display Name: Service principal name       Role Type: role selected during credential creation    In case of the app-based credential, the Application ID and key that you provided when creating the credential are not displayed.", 
            "title": "Details for Azure Credentials"
        }, 
        {
            "location": "/cb-credentials/index.html#details-for-gcp-credentials", 
            "text": "The following information is available for previously created GCP credentials:   Credential Name     Description    Service Account Id  Project Id    The P12 key that you attached when creating the credential is not displayed.", 
            "title": "Details for GCP Credentials"
        }, 
        {
            "location": "/cb-credentials/index.html#details-for-openstack-credentials", 
            "text": "The following information is available for previously created OpenStack credentials:   Credential Name     Description    Facing: \"internal\" or \"public\", as specified under \"Api Facing\"       Endpoint    Selector: \"cb-keystone-v2\" or \"cb-keystone-v3\"    Keystone Version: \"cb-keystone-v2\" or \"cb-keystone-v3\"     User Name      Tenant Name      The password parameter is not displayed.", 
            "title": "Details for OpenStack Credentials"
        }, 
        {
            "location": "/cb-credentials/index.html#set-a-default-credential", 
            "text": "If using multiple Cloudbreak credentials, you can select one credential and use it as default for creating clusters. This default credential will be pre-selected in the create cluster wizard.  Steps   In the Cloudbreak UI, select  Credentials  from the navigation pane.    Click  Set as default  next to the credential that you would like to set as default.    Click  Yes  to confirm.", 
            "title": "Set a Default Credential"
        }, 
        {
            "location": "/cb-credentials/index.html#delete-a-credential", 
            "text": "To delete a credential, follow these steps.  Steps   In the Cloudbreak UI, select  Credentials  from the navigation pane.    Select one or more credentials by checking their corresponding checkboxes.  Click  Delete .   Click  Yes  to confirm.", 
            "title": "Delete a Credential"
        }, 
        {
            "location": "/cb-disable-provider/index.html", 
            "text": "Disable Providers\n\n\nIf you are planning to use Cloubdreak with a specific cloud provider or a specific set of cloud providers, you may want to disable the remaining providers. For example, if you are planning to use Cloubdreak with Azure only, you may want to disable AWS, Google Cloud, and OpenStack. \n\n\nSteps\n\n\n\n\n\n\nNavigate to the Cloudbreak deployment directory and edit Profile. For example:\n\n\ncd /var/lib/cloudbreak-deployment/\nvi Profile\n\n\n\n\n\n\nAdd the following entry, setting it to the provider that you  would like to see. For example, if you would like to see Azure only, set this to \"AZURE\":\n\n\nexport CB_ENABLEDPLATFORMS=AZURE\n\n\nAccepted values are:\n\n\n\n\nAZURE\n\n\nAWS\n\n\nGCP\n\n\nOPENSTACK\n\n\n\n\nAny combination of platforms can be used; for example if you would like to see AWS and OpenStack, then use:\n\n\nexport CB_ENABLEDPLATFORMS=AWS,OPENSTACK\n\n\nIf you want to reverse the change and see all providers, then either delete CB_ENABLEDPLATFORMS from the Profile or add the following: \n\n\nexport CB_ENABLEDPLATFORMS=AZURE,AWS,GCP,OPENSTACK\n\n\n\n\n\n\nRestart Cloudbreak by using \ncbd restart\n.", 
            "title": "Disable Providers"
        }, 
        {
            "location": "/cb-disable-provider/index.html#disable-providers", 
            "text": "If you are planning to use Cloubdreak with a specific cloud provider or a specific set of cloud providers, you may want to disable the remaining providers. For example, if you are planning to use Cloubdreak with Azure only, you may want to disable AWS, Google Cloud, and OpenStack.   Steps    Navigate to the Cloudbreak deployment directory and edit Profile. For example:  cd /var/lib/cloudbreak-deployment/\nvi Profile    Add the following entry, setting it to the provider that you  would like to see. For example, if you would like to see Azure only, set this to \"AZURE\":  export CB_ENABLEDPLATFORMS=AZURE  Accepted values are:   AZURE  AWS  GCP  OPENSTACK   Any combination of platforms can be used; for example if you would like to see AWS and OpenStack, then use:  export CB_ENABLEDPLATFORMS=AWS,OPENSTACK  If you want to reverse the change and see all providers, then either delete CB_ENABLEDPLATFORMS from the Profile or add the following:   export CB_ENABLEDPLATFORMS=AZURE,AWS,GCP,OPENSTACK    Restart Cloudbreak by using  cbd restart .", 
            "title": "Disable Providers"
        }, 
        {
            "location": "/cb-db/index.html", 
            "text": "Configuring External Cloudbreak Database\n\n\nBy default, Cloudbreak uses an embedded PostgreSQL database to persist data related to Cloudbreak\nconfiguration, setup and so on. For a production Cloudbreak deployment, we suggest that you configure an external database. \n\n\nExternal Database Support Matrix\n\n\nAn embedded PostgreSQL 9.6.1 database is used by Cloudbreak by default. If you would like to\nuse an external database for Cloudbreak, you may use the following supported database types and versions: \n\n\n\n\n\n\n\n\nDatabase Type\n\n\nSupported Version\n\n\n\n\n\n\n\n\n\n\nExternal PostgreSQL\n\n\n9.x\n\n\n\n\n\n\nExternal MySQL\n\n\nNot supported\n\n\n\n\n\n\nExternal MariaDB\n\n\nNot supported\n\n\n\n\n\n\nExternal Oracle\n\n\nNot supported\n\n\n\n\n\n\nExternal SQL Server\n\n\nNot supported\n\n\n\n\n\n\n\n\nConfigure External Cloudbreak Database\n\n\nThe following section describes how to use Cloudbreak with an existing external database, other than\nthe embedded PostgreSQL database instance that Cloudbreak uses by default. To configure an external PostgreSQL database for Cloudbreak, perform these steps. \n\n\nSteps\n\n\n\n\n\n\nOn your Cloudbreak host machine, set the following environment variables according to the settings of your external database: \n\n\nexport DATABASE_HOST=my.database.host\nexport DATABASE_PORT=5432\nexport DATABASE_USERNAME=admin\nexport DATABASE_PASSWORD=Admin123!\n\n\n\n\n\n\n\nOn your external database, create three databases: \ncbdb, uaadb, periscopedb\n. You can create these databases using the \ncreatedb\n utility with the following commands:\n\n\ncreatedb -h $DATABASE_HOST -p $DATABASE_PORT -U $DATABASE_USERNAME cbdb\ncreatedb -h $DATABASE_HOST -p $DATABASE_PORT -U $DATABASE_USERNAME uaadb\ncreatedb -h $DATABASE_HOST -p $DATABASE_PORT -U $DATABASE_USERNAME periscopedb\n\n\nFor more information refer to the \nPostgreSQL documentation\n. \n\nAlternatively, you can log in to the management interface of your external database and execute \ncreate database\n commands directly. \n\n\n\n\n\n\nSet the following variables in your Cloudbreak Profile file. Modify the database parameters according to your external database.\n\n\nexport DATABASE_HOST=my.database.host\nexport DATABASE_PORT=5432\nexport DATABASE_USERNAME=admin\nexport DATABASE_PASSWORD=Admin123!\n\n\nexport CB_DB_PORT_5432_TCP_ADDR=$DATABASE_HOST\nexport CB_DB_PORT_5432_TCP_PORT=$DATABASE_PORT\nexport CB_DB_ENV_USER=$DATABASE_USERNAME\nexport CB_DB_ENV_PASS=$DATABASE_PASSWORD\nexport CB_DB_ENV_DB=cbdb\n\n\nexport PERISCOPE_DB_TCP_ADDR=$DATABASE_HOST\nexport PERISCOPE_DB_TCP_PORT=$DATABASE_PORT\nexport PERISCOPE_DB_USER=$DATABASE_USERNAME\nexport PERISCOPE_DB_PASS=$DATABASE_PASSWORD\nexport PERISCOPE_DB_NAME=periscopedb\nexport PERISCOPE_DB_SCHEMA_NAME=public\n\n\nexport IDENTITY_DB_URL=$DATABASE_HOST:$DATABASE_PORT\nexport IDENTITY_DB_USER=$DATABASE_USERNAME\nexport IDENTITY_DB_PASS=$DATABASE_PASSWORD\nexport IDENTITY_DB_NAME=uaadb\n\n\n\n\n\n\nRestart Cloudbreak application by using the \ncbd restart\n command. \n\n\n\n\n\n\nAfter performing these steps, your external database will be used for Cloudbreak instead of the built-in database. \n\n\n      \n\nData Migration\n  \n\n If you want to migrate your existing data (such as  blueprints, recipes, and so on) from the embedded database to the external one, then after completing these steps, you should also create a \nbackup\n of your original database and then \nrestore\n it in the external database.", 
            "title": "Configure External Cloudbreak Database"
        }, 
        {
            "location": "/cb-db/index.html#configuring-external-cloudbreak-database", 
            "text": "By default, Cloudbreak uses an embedded PostgreSQL database to persist data related to Cloudbreak\nconfiguration, setup and so on. For a production Cloudbreak deployment, we suggest that you configure an external database.   External Database Support Matrix  An embedded PostgreSQL 9.6.1 database is used by Cloudbreak by default. If you would like to\nuse an external database for Cloudbreak, you may use the following supported database types and versions:      Database Type  Supported Version      External PostgreSQL  9.x    External MySQL  Not supported    External MariaDB  Not supported    External Oracle  Not supported    External SQL Server  Not supported", 
            "title": "Configuring External Cloudbreak Database"
        }, 
        {
            "location": "/cb-db/index.html#configure-external-cloudbreak-database", 
            "text": "The following section describes how to use Cloudbreak with an existing external database, other than\nthe embedded PostgreSQL database instance that Cloudbreak uses by default. To configure an external PostgreSQL database for Cloudbreak, perform these steps.   Steps    On your Cloudbreak host machine, set the following environment variables according to the settings of your external database:   export DATABASE_HOST=my.database.host\nexport DATABASE_PORT=5432\nexport DATABASE_USERNAME=admin\nexport DATABASE_PASSWORD=Admin123!    On your external database, create three databases:  cbdb, uaadb, periscopedb . You can create these databases using the  createdb  utility with the following commands:  createdb -h $DATABASE_HOST -p $DATABASE_PORT -U $DATABASE_USERNAME cbdb\ncreatedb -h $DATABASE_HOST -p $DATABASE_PORT -U $DATABASE_USERNAME uaadb\ncreatedb -h $DATABASE_HOST -p $DATABASE_PORT -U $DATABASE_USERNAME periscopedb  For more information refer to the  PostgreSQL documentation .  \nAlternatively, you can log in to the management interface of your external database and execute  create database  commands directly.     Set the following variables in your Cloudbreak Profile file. Modify the database parameters according to your external database.  export DATABASE_HOST=my.database.host\nexport DATABASE_PORT=5432\nexport DATABASE_USERNAME=admin\nexport DATABASE_PASSWORD=Admin123!  export CB_DB_PORT_5432_TCP_ADDR=$DATABASE_HOST\nexport CB_DB_PORT_5432_TCP_PORT=$DATABASE_PORT\nexport CB_DB_ENV_USER=$DATABASE_USERNAME\nexport CB_DB_ENV_PASS=$DATABASE_PASSWORD\nexport CB_DB_ENV_DB=cbdb  export PERISCOPE_DB_TCP_ADDR=$DATABASE_HOST\nexport PERISCOPE_DB_TCP_PORT=$DATABASE_PORT\nexport PERISCOPE_DB_USER=$DATABASE_USERNAME\nexport PERISCOPE_DB_PASS=$DATABASE_PASSWORD\nexport PERISCOPE_DB_NAME=periscopedb\nexport PERISCOPE_DB_SCHEMA_NAME=public  export IDENTITY_DB_URL=$DATABASE_HOST:$DATABASE_PORT\nexport IDENTITY_DB_USER=$DATABASE_USERNAME\nexport IDENTITY_DB_PASS=$DATABASE_PASSWORD\nexport IDENTITY_DB_NAME=uaadb    Restart Cloudbreak application by using the  cbd restart  command.     After performing these steps, your external database will be used for Cloudbreak instead of the built-in database.          Data Migration     If you want to migrate your existing data (such as  blueprints, recipes, and so on) from the embedded database to the external one, then after completing these steps, you should also create a  backup  of your original database and then  restore  it in the external database.", 
            "title": "Configure External Cloudbreak Database"
        }, 
        {
            "location": "/cb-migrate/index.html", 
            "text": "Moving a Cloudbreak Instance\n\n\nTo transfer a Cloudbreak instance from one host to another, perform these tasks:\n\n\n\n\nIf you are using the embedded PostgreSQL database, \nback up current Cloudbreak database\n data  \n\n\nLaunch a new Cloudbreak instance and start Cloudbreak. Refer to \nLaunch Cloudbreak\n   \n\n\nIf you are using the embedded PostgreSQL database, \npopulate the new Cloudbreak instance database with the dump from the original Cloudbreak instance\n on the new host.  \n\n\nModify Cloudbreak Profile\n  \n\n\n\n\nBack up Cloudbreak Database\n\n\nTo create a backup of the embedded PostgreSQL database, perform these steps.\n\n\nSteps\n \n\n\n\n\n\n\nOn your Cloudbreak host machine, execute the following  command to enter the container of the database:\n\n\ndocker exec -it cbreak_commondb_1 bash\n \nIf it is not running, start the database container by using the \ndocker start cbreak_commondb_1\n command.\n\n\n\n\n\n\nCreate three database dumps (cbdb, uaadb, periscopedb):  \n\n\npg_dump -Fc -U postgres cbdb \n cbdb.dump\npg_dump -Fc -U postgres uaadb \n uaadb.dump\npg_dump -Fc -U postgres periscopedb \n periscopedb.dump\n\n\n\n\n\n\nQuit from the container with shortcut \nCTRL+d\n.\n\n\n\n\n\n\nSave the previously created dumps to the host instance:               \n\n\ndocker cp cbreak_commondb_1:/cbdb.dump ./cbdb.dump\ndocker cp cbreak_commondb_1:/uaadb.dump ./uaadb.dump\ndocker cp cbreak_commondb_1:/periscopedb.dump ./periscopedb.dump\n\n\n\n\n\n\nPopulate Database with Dump from Original Cloudbreak Instance\n\n\nPerform these steps to populate databases with information from the Cloudbreak server.\n\n\nSteps\n \n\n\n\n\n\n\nCopy the saved database files from \nBackup Cloudbreak Database\n to the new Cloudbreak server host.\n\n\n\n\n\n\nCopy the dump files into the database container with the following commands. Modify the location as necessary (The example below assumes that the files are in \n/tmp\n):\n\n\ndocker cp /tmp/cbdb.dump cbreak_commondb_1:/cbdb.dump\ndocker cp /tmp/uaadb.dump cbreak_commondb_1:/uaadb.dump\ndocker cp /tmp/periscopedb.dump cbreak_commondb_1:/periscopedb.dump\n\n\n\n\n\n\nExecute the following command to stop the container:\n\n\ndocker stop cbreak_identity_1\n\n\n\n\n\n\nExecute the following command to enter the container of the database:\n\n\ndocker exec -it cbreak_commondb_1 bash\n\n\n\n\n\n\nExecute the following commands:\n\n\npsql -U postgres\ndrop database uaadb;\ndrop database cbdb;\ndrop database periscopedb;\ncreate database uaadb;\ncreate database cbdb;\ncreate database periscopedb;\n\n\n\n\nIf you get \nERROR:  database \"uaadb\" is being accessed by other users\n error, ensure that    cbreak_identity_1 container is not running and then retry dropping uaadb.  \n\n\n\n\n\n\n\n\nExit the PostgreSQL interactive terminal.\n    \n\\q\n \n\n\n\n\n\n\nRestore the databases from the original backups:\n\n\npg_restore -U postgres -d periscopedb periscopedb.dump\npg_restore -U postgres -d cbdb cbdb.dump\npg_restore -U postgres -d uaadb uaadb.dump\n\n\n\n\n\n\nQuit from the container with the shortcut \nCTRL+d\n.     \n\n\n\n\n\n\nModify Cloudbreak Profile\n\n\nPerform these steps to ensure that your new Profile file is correctly set up. \n\n\nSteps\n \n\n\n\n\n\n\nEnsure that the following parameter values match in the origin and target Profile files and modify Profile file of the target environment if necessary:\n\n\nexport UAA_DEFAULT_USER_EMAIL=admin@example.com\nexport UAA_DEFAULT_SECRET=cbsecret\nexport UAA_DEFAULT_USER_PW=cbuser\n\n\n\n\n\n\nRestart Cloudbreak application by using the \ncbd restart\n command.  \n\n\nAfter performing these steps the migration is complete. To verify, log in to the UI of your new Cloudbreak instance and make sure that it contains the information from your old instance.", 
            "title": "Move Cloudbreak Instance"
        }, 
        {
            "location": "/cb-migrate/index.html#moving-a-cloudbreak-instance", 
            "text": "To transfer a Cloudbreak instance from one host to another, perform these tasks:   If you are using the embedded PostgreSQL database,  back up current Cloudbreak database  data    Launch a new Cloudbreak instance and start Cloudbreak. Refer to  Launch Cloudbreak      If you are using the embedded PostgreSQL database,  populate the new Cloudbreak instance database with the dump from the original Cloudbreak instance  on the new host.    Modify Cloudbreak Profile", 
            "title": "Moving a Cloudbreak Instance"
        }, 
        {
            "location": "/cb-migrate/index.html#back-up-cloudbreak-database", 
            "text": "To create a backup of the embedded PostgreSQL database, perform these steps.  Steps      On your Cloudbreak host machine, execute the following  command to enter the container of the database:  docker exec -it cbreak_commondb_1 bash  \nIf it is not running, start the database container by using the  docker start cbreak_commondb_1  command.    Create three database dumps (cbdb, uaadb, periscopedb):    pg_dump -Fc -U postgres cbdb   cbdb.dump\npg_dump -Fc -U postgres uaadb   uaadb.dump\npg_dump -Fc -U postgres periscopedb   periscopedb.dump    Quit from the container with shortcut  CTRL+d .    Save the previously created dumps to the host instance:                 docker cp cbreak_commondb_1:/cbdb.dump ./cbdb.dump\ndocker cp cbreak_commondb_1:/uaadb.dump ./uaadb.dump\ndocker cp cbreak_commondb_1:/periscopedb.dump ./periscopedb.dump", 
            "title": "Back up Cloudbreak Database"
        }, 
        {
            "location": "/cb-migrate/index.html#populate-database-with-dump-from-original-cloudbreak-instance", 
            "text": "Perform these steps to populate databases with information from the Cloudbreak server.  Steps      Copy the saved database files from  Backup Cloudbreak Database  to the new Cloudbreak server host.    Copy the dump files into the database container with the following commands. Modify the location as necessary (The example below assumes that the files are in  /tmp ):  docker cp /tmp/cbdb.dump cbreak_commondb_1:/cbdb.dump\ndocker cp /tmp/uaadb.dump cbreak_commondb_1:/uaadb.dump\ndocker cp /tmp/periscopedb.dump cbreak_commondb_1:/periscopedb.dump    Execute the following command to stop the container:  docker stop cbreak_identity_1    Execute the following command to enter the container of the database:  docker exec -it cbreak_commondb_1 bash    Execute the following commands:  psql -U postgres\ndrop database uaadb;\ndrop database cbdb;\ndrop database periscopedb;\ncreate database uaadb;\ncreate database cbdb;\ncreate database periscopedb;   If you get  ERROR:  database \"uaadb\" is being accessed by other users  error, ensure that    cbreak_identity_1 container is not running and then retry dropping uaadb.       Exit the PostgreSQL interactive terminal.\n     \\q      Restore the databases from the original backups:  pg_restore -U postgres -d periscopedb periscopedb.dump\npg_restore -U postgres -d cbdb cbdb.dump\npg_restore -U postgres -d uaadb uaadb.dump    Quit from the container with the shortcut  CTRL+d .", 
            "title": "Populate Database with Dump from Original Cloudbreak Instance"
        }, 
        {
            "location": "/cb-migrate/index.html#modify-cloudbreak-profile", 
            "text": "Perform these steps to ensure that your new Profile file is correctly set up.   Steps      Ensure that the following parameter values match in the origin and target Profile files and modify Profile file of the target environment if necessary:  export UAA_DEFAULT_USER_EMAIL=admin@example.com\nexport UAA_DEFAULT_SECRET=cbsecret\nexport UAA_DEFAULT_USER_PW=cbuser    Restart Cloudbreak application by using the  cbd restart  command.    After performing these steps the migration is complete. To verify, log in to the UI of your new Cloudbreak instance and make sure that it contains the information from your old instance.", 
            "title": "Modify Cloudbreak Profile"
        }, 
        {
            "location": "/cb-profile/index.html", 
            "text": "Editing Cloudbreak Profile\n\n\nCloudbreak deployer configuration is based on environment variables.  \n\n\nDuring startup, Cloudbreak deployer tries to determine the underlying infrastructure and then sets required environment variables with appropriate default values. If these environment variables are not sufficient for your use case, you can set additional environment variables in your \nProfile\n file. \n\n\nSet Profile Variables\n\n\nTo set environment variables relevant for Cloudbreak Deployer, add them to a file called \nProfile\n located in the Cloudbreak deployment directory (typically \n/var/lib/cloudbreak-deployment\n).\n\n\nThe \nProfile\n file is sourced, so you can use the usual syntax to set configuration values:\n\n\nexport MY_VAR=some_value\nexport MY_OTHER_VAR=another_value \n\n\n\n\nCheck Available Profile Variables\n\n\nTo see all available environment variables with their default values, use:\n\n\ncbd env show\n\n\n\n\nCreate Environment Specific Profiles\n\n\nIf you would like to use a different versions of Cloudbreak for prod and qa profile, you must create two environment specific configurations that can be sourced. For example:\n\n\n\n\nProfile.prod  \n\n\nProfile.qa   \n\n\n\n\nFor example, to create and use a prod profile, you need to:\n\n\n\n\nCreate a file called \nProfile.prod\n  \n\n\nWrite the environment-specific \nexport DOCKER_TAG_CLOUDBREAK=0.3.99\n into \nProfile.prod\n to specify Docker image.  \n\n\nSet the environment variable: \nCBD_DEFAULT_PROFILE=prod\n  \n\n\n\n\nTo use the prod specific profile once, set:  \n\n\nCBD_DEFAULT_PROFILE=prod cbd some_commands\n\n\n\nTo permanently use the prod profile, set \nexport CBD_DEFAULT_PROFILE=prod\n in your \n.bash_profile\n.\n\n\nProfile Variables\n\n\nCloudbreak Variables\n\n\nRefer to this list for available environment variables. The variables are listed with their default values. If default is unset, no value is listed. \n\n\n\n\n\n\n\n\nVariable Name\n\n\nDefault Value\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nADDRESS_RESOLVING_TIMEOUT\n\n\n120000\n\n\nDNS lookup timeout for internal service discovery\n\n\n\n\n\n\nCAPTURE_CRON_EXPRESSION\n\n\n\n\nSmartSense bundle generation time interval in cron format\n\n\n\n\n\n\nCBD_CERT_ROOT_PATH\n\n\n\"${PWD}/certs\"\n\n\nPath where deployer stores Cloudbreak certificates. ${PWD} refers to the Cloudbreak deployment directory\n\n\n\n\n\n\nCBD_LOG_NAME\n\n\ncbreak\n\n\nName of the Cloudbreak log file\n\n\n\n\n\n\nCBD_TRAEFIK_TLS\n\n\n\"/certs/traefik/client.pem,/certs/traefik/client-key.pem\"\n\n\nPath inside of the Traefik container where TLS files are located\n\n\n\n\n\n\nCB_BLUEPRINT_DEFAULTS\n\n\n\"hdp-small-default;hdp-spark-cluster;hdp-streaming-cluster\"\n\n\nComma separated list of the default blueprints that Cloudbreak initializes in its database\n\n\n\n\n\n\nCB_BYOS_DFS_DATA_DIR\n\n\n\"/hadoop/hdfs/data\"\n\n\n(Deprecated) Default data directory for BYOS orchestrators\n\n\n\n\n\n\nCB_COMPONENT_CLUSTER_ID\n\n\n\n\nSmartSense component cluster ID\n\n\n\n\n\n\nCB_COMPONENT_ID\n\n\n\n\nSmartSense component ID\n\n\n\n\n\n\nCB_COMPOSE_PROJECT\n\n\ncbreak\n\n\nName of the Docker Compose project; it will appear in container names\n\n\n\n\n\n\nCB_DB_ENV_DB\n\n\n\"cbdb\"\n\n\nName of the Cloudbreak database\n\n\n\n\n\n\nCB_DB_ENV_PASS\n\n\n\"\"\n\n\nPassword for the Cloudbreak database authentication\n\n\n\n\n\n\nCB_DB_ENV_SCHEMA\n\n\n\"public\"\n\n\nSchema used in the Cloudbreak database\n\n\n\n\n\n\nCB_DB_ENV_USER\n\n\n\"postgres\"\n\n\nUser for the Cloudbreak database authentication\n\n\n\n\n\n\nCB_DB_ROOT_PATH\n\n\n\"/var/lib/cloudbreak\"\n\n\n(Deprecated) Location of the database volume on Cloudbreak host\n\n\n\n\n\n\nCB_DEFAULT_GATEWAY_CIDR\n\n\n\n\nCIDR address range which is used by Cloudbreak to communicate with its managed clusters. For more information, refer to \nRestricting Inbound Access to Clusters\n.\n\n\n\n\n\n\nCB_DEFAULT_SUBSCRIPTION_ADDRESS\n\n\nhttp://uluwatu.service.consul:3000/notifications\n\n\nURL of the default subscription for Cloudbreak notifications\n\n\n\n\n\n\nCB_ENABLEDPLATFORMS\n\n\n\n\nSet this to disable specific cloud providers. Accepted values are: AZURE, AWS, GCP, OPENSTACK.\n\n\n\n\n\n\nCB_ENABLE_CUSTOM_IMAGE\n\n\n\"false\"\n\n\nSet to \"true\" to enable custom cloud images\n\n\n\n\n\n\nCBD_FORCE_START\n\n\n\n\nSet this to disable docker-compose.yml and uaa.yml validation\n\n\n\n\n\n\nCB_HBM2DDL_STRATEGY\n\n\n\"validate\"\n\n\nConfigures hibernate.hbm2ddl.auto in Cloudbreak\n\n\n\n\n\n\nCB_HOST_DISCOVERY_CUSTOM_DOMAIN\n\n\n\"\"\n\n\nCustom domain of the provisioned cluster\n\n\n\n\n\n\nCB_HTTPS_PROXY\n\n\n\"\"\n\n\nHTTPS proxy URL\n\n\n\n\n\n\nCB_HTTP_PROXY\n\n\n\"\"\n\n\nHTTP proxy URL\n\n\n\n\n\n\nCB_IMAGE_CATALOG_URL\n\n\n\"https://s3-eu-west-1.amazonaws.com/cloudbreak-info/cb-image-catalog.json\"\n\n\nImage catalog URL\n\n\n\n\n\n\nCB_INSTANCE_NODE_ID\n\n\n\n\nUnique identifier of the Cloudbreak node\n\n\n\n\n\n\nCB_INSTANCE_PROVIDER\n\n\n\n\nCloud provider of the Cloudbreak instance\n\n\n\n\n\n\nCB_INSTANCE_REGION\n\n\n\n\nCloud region of the Cloudbreak instance\n\n\n\n\n\n\nCB_INSTANCE_UUID\n\n\n\n\nUnique identifier of Cloudbreak deployment\n\n\n\n\n\n\nCB_JAVA_OPTS\n\n\n\"\"\n\n\nExtra Java options for Autoscale and Cloudbreak\n\n\n\n\n\n\nCB_LOG_LEVEL\n\n\n\"INFO\"\n\n\nLog level of the Cloudbreak service\n\n\n\n\n\n\nCB_MAX_SALT_NEW_SERVICE_RETRY\n\n\n90\n\n\nSalt orchestrator max retry count\n\n\n\n\n\n\nCB_MAX_SALT_RECIPE_EXECUTION_RETRY\n\n\n90\n\n\nSalt orchestrator max retry count for recipes\n\n\n\n\n\n\nCB_PLATFORM_DEFAULT_REGIONS\n\n\n\n\nComma separated list of default regions by platform. For example: \nAWS:eu-west-1\n.\n\n\n\n\n\n\nCB_PRODUCT_ID\n\n\n\n\nSmartSense product ID\n\n\n\n\n\n\nCB_SCHEMA_MIGRATION_AUTO\n\n\ntrue\n\n\nWhen set to true, enables Cloudbreak automatic database schema update\n\n\n\n\n\n\nCB_SMARTSENSE_CONFIGURE\n\n\n\"false\"\n\n\nSet to \u201ctrue\u201d to install and configure SmartSense on cluster nodes\n\n\n\n\n\n\nCB_SMARTSENSE_CLUSTER_NAME_PREFIX\n\n\n\n\nSmartSense Cloudbreak cluster name prefix\n\n\n\n\n\n\nCB_SMARTSENSE_ID\n\n\n\"\"\n\n\nSmartSense subscription ID\n\n\n\n\n\n\nCB_TEMPLATE_DEFAULTS\n\n\n\"minviable-gcp,minviable-azure,minviable-aws\"\n\n\nComma separated list of the default templates that Cloudbreak initializes in its database\n\n\n\n\n\n\nCB_UI_MAX_WAIT\n\n\n400\n\n\nWait timeout for \ncbd start-wait\n command\n\n\n\n\n\n\nCERT_VALIDATION\n\n\n\"true\"\n\n\nWhen set to \"true\", enables cert validation in Cloudbreak and Autoscale\n\n\n\n\n\n\nCLOUDBREAK_SMTP_AUTH\n\n\n\"true\"\n\n\nWhen set to \"true\", configures mail.smtp.auth in Cloudbreak\n\n\n\n\n\n\nCLOUDBREAK_SMTP_SENDER_FROM\n\n\n\"noreply@hortonworks.com\"\n\n\nEmail address of the sender\n\n\n\n\n\n\nCLOUDBREAK_SMTP_SENDER_HOST\n\n\n\"smtp.service.consul\"\n\n\nSMTP server address of the hostname\n\n\n\n\n\n\nCLOUDBREAK_SMTP_SENDER_PASSWORD\n\n\n\"$LOCAL_SMTP_PASSWORD\"\n\n\nSMTP server password\n\n\n\n\n\n\nCLOUDBREAK_SMTP_SENDER_PORT\n\n\n25\n\n\nPort of the SMTP server\n\n\n\n\n\n\nCLOUDBREAK_SMTP_SENDER_USERNAME\n\n\n\"admin\"\n\n\nUsername for SMTP authentication\n\n\n\n\n\n\nCLOUDBREAK_SMTP_STARTTLS_ENABLE\n\n\n\"false\"\n\n\nSet to \"true\" to configure mail.smtp.starttls.enable in Cloudbreak\n\n\n\n\n\n\nCLOUDBREAK_SMTP_TYPE\n\n\n\"smtp\"\n\n\nDefines mail.transport.protocol in CLoudbreak\n\n\n\n\n\n\nCOMMON_DB\n\n\ncommondb\n\n\nName of the database container\n\n\n\n\n\n\nCOMMON_DB_VOL\n\n\ncommon\n\n\nName of the database volume\n\n\n\n\n\n\nCOMPOSE_HTTP_TIMEOUT\n\n\n120\n\n\nDocker Compose execution timeout\n\n\n\n\n\n\nDB_DUMP_VOLUME\n\n\ncbreak_dump\n\n\nName of the database dump volume\n\n\n\n\n\n\nDB_MIGRATION_LOG\n\n\n\"db_migration.log\"\n\n\nDatabase migration log file\n\n\n\n\n\n\nDEFAULT_INBOUND_ACCESS_IP\n\n\n\"\"\n\n\nOpens default ports on AWS instances for Cloudbreak\n\n\n\n\n\n\nDOCKER_CONSUL_OPTIONS\n\n\n\"\"\n\n\nExtra options for Consul\n\n\n\n\n\n\nDOCKER_IMAGE_CBD_SMARTSENSE\n\n\nhortonworks/cbd-smartsense\n\n\nSmartSense Docker image name\n\n\n\n\n\n\nDOCKER_IMAGE_CLOUDBREAK\n\n\nhortonworks/cloudbreak\n\n\nCloudbreak Docker image name\n\n\n\n\n\n\nDOCKER_IMAGE_CLOUDBREAK_AUTH\n\n\nhortonworks/cloudbreak-auth\n\n\nAuthentication service Docker image name\n\n\n\n\n\n\nDOCKER_IMAGE_CLOUDBREAK_PERISCOPE\n\n\nhortonworks/cloudbreak-autoscale\n\n\nAutoscale Docker image name\n\n\n\n\n\n\nDOCKER_IMAGE_CLOUDBREAK_SHELL\n\n\nhortonworks/cloudbreak-shell\n\n\nCloudbreak Shell Docker image name\n\n\n\n\n\n\nDOCKER_IMAGE_CLOUDBREAK_WEB\n\n\nhortonworks/cloudbreak-web\n\n\nWeb UI Docker image name\n\n\n\n\n\n\nDOCKER_TAG_ALPINE\n\n\n3.1\n\n\nAlpine container version\n\n\n\n\n\n\nDOCKER_TAG_CBD_SMARTSENSE\n\n\n0.10.0\n\n\nSmartSense container version\n\n\n\n\n\n\nDOCKER_TAG_CERT_TOOL\n\n\n0.2.0\n\n\nCert tool container version\n\n\n\n\n\n\nDOCKER_TAG_CLOUDBREAK\n\n\n2.1.0-dev.70\n\n\nCloudbreak container version\n\n\n\n\n\n\nDOCKER_TAG_CLOUDBREAK_SHELL\n\n\n2.1.0-dev.70\n\n\nCloudbreak Shell container version\n\n\n\n\n\n\nDOCKER_TAG_CONSUL\n\n\n0.5\n\n\nConsul container version\n\n\n\n\n\n\nDOCKER_TAG_HAVEGED\n\n\n1.1.0\n\n\nHaveged container version\n\n\n\n\n\n\nDOCKER_TAG_LOGROTATE\n\n\n1.0.0\n\n\nLogrotate container version\n\n\n\n\n\n\nDOCKER_TAG_MIGRATION\n\n\n1.0.0\n\n\nMigration container version\n\n\n\n\n\n\nDOCKER_TAG_PERISCOPE\n\n\n2.1.0-dev.70\n\n\nAutoscale container version\n\n\n\n\n\n\nDOCKER_TAG_POSTFIX\n\n\nlatest\n\n\nPostfix container version\n\n\n\n\n\n\nDOCKER_TAG_POSTGRES\n\n\n9.6.1-alpine\n\n\nPostgresql container version\n\n\n\n\n\n\nDOCKER_TAG_REGISTRATOR\n\n\nv5\n\n\nRegistrator container version\n\n\n\n\n\n\nDOCKER_TAG_SULTANS\n\n\n2.1.0-dev.70\n\n\nAuthentication service container version\n\n\n\n\n\n\nDOCKER_TAG_TRAEFIK\n\n\nv1.2.0\n\n\nTraefik container version\n\n\n\n\n\n\nDOCKER_TAG_UAA\n\n\n3.6.5\n\n\nIdentity container version\n\n\n\n\n\n\nDOCKER_TAG_ULUWATU\n\n\n2.1.0-dev.70\n\n\nWeb UI container version\n\n\n\n\n\n\nIDENTITY_DB_NAME\n\n\n\"uaadb\"\n\n\nName of the Identity database\n\n\n\n\n\n\nIDENTITY_DB_PASS\n\n\n\"\"\n\n\nPassword for the Identity database authentication\n\n\n\n\n\n\nIDENTITY_DB_URL\n\n\n\"${COMMON_DB}.service.consul:5432\"\n\n\nURL for the Identity database connection, including the port number\n\n\n\n\n\n\nIDENTITY_DB_USER\n\n\n\"postgres\"\n\n\nUser for the Identity database authentication\n\n\n\n\n\n\nLOCAL_SMTP_PASSWORD\n\n\n\"$UAA_DEFAULT_USER_PW\"\n\n\nDefault password for the internal mail server\n\n\n\n\n\n\nPERISCOPE_DB_ENV_DB\n\n\n\"periscopedb\"\n\n\nName of the Autoscale database\n\n\n\n\n\n\nPERISCOPE_DB_ENV_PASS\n\n\n\"\"\n\n\nPassword for the Autoscale database authentication\n\n\n\n\n\n\nPERISCOPE_DB_ENV_SCHEMA\n\n\n\"public\"\n\n\nSchema used in the Autoscale database\n\n\n\n\n\n\nPERISCOPE_DB_ENV_USER\n\n\n\"postgres\"\n\n\nUser for the Autoscale database authentication\n\n\n\n\n\n\nPERISCOPE_DB_PORT_5432_TCP_ADDR\n\n\n\n\nAddress of the Autoscale database\n\n\n\n\n\n\nPERISCOPE_DB_PORT_5432_TCP_PORT\n\n\n\n\nPort number of the Autoscale database\n\n\n\n\n\n\nPERISCOPE_HBM2DDL_STRATEGY\n\n\n\"validate\"\n\n\nConfigures hibernate.hbm2ddl.auto in Autoscale\n\n\n\n\n\n\nPERISCOPE_LOG_LEVEL\n\n\n\"INFO\"\n\n\nLog level of the Autoscale service\n\n\n\n\n\n\nPERISCOPE_SCHEMA_MIGRATION_AUTO\n\n\ntrue\n\n\nWhen set to \"true\", enables Autoscale automatic database schema update\n\n\n\n\n\n\nPUBLIC_IP\n\n\n\n\nIP address or hostname of the public interface\n\n\n\n\n\n\nREST_DEBUG\n\n\n\"false\"\n\n\nSet to \"true\" to enable REST call debug level in Cloudbreak and Autoscale\n\n\n\n\n\n\nSL_ADDRESS_RESOLVING_TIMEOUT\n\n\n\n\nDNS lookup timeout of Authentication service for internal service discovery\n\n\n\n\n\n\nSL_NODE_TLS_REJECT_UNAUTHORIZED\n\n\n\"0\"\n\n\nWhen set to \"0\", enables self-signed certifications in Authentication service\n\n\n\n\n\n\nSULTANS_CONTAINER_PATH\n\n\n/sultans\n\n\nDefault project location in Authentication service container\n\n\n\n\n\n\nTRAEFIK_MAX_IDLE_CONNECTION\n\n\n100\n\n\nSets --maxidleconnsperhost for Traefik to the value entered\n\n\n\n\n\n\nUAA_CLOUDBREAK_ID\n\n\ncloudbreak\n\n\nIdentity of the Cloudbreak scope in Identity\n\n\n\n\n\n\nUAA_CLOUDBREAK_SECRET\n\n\n$UAA_DEFAULT_SECRET\n\n\nSecret of the Cloudbreak scope in Identity\n\n\n\n\n\n\nUAA_CLOUDBREAK_SHELL_ID\n\n\ncloudbreak_shell\n\n\nIdentity of the Cloudbreak Shell scope in Identity\n\n\n\n\n\n\nUAA_DEFAULT_ACCOUNT\n\n\n\"seq1234567.SequenceIQ\"\n\n\nDefault account for users as an Identity group\n\n\n\n\n\n\nUAA_DEFAULT_SECRET\n\n\n\n\nDefault secret for all the scopes and encryptions\n\n\n\n\n\n\nUAA_DEFAULT_USER_EMAIL\n\n\nadmin@example.com\n\n\nEmail address of default admin user\n\n\n\n\n\n\nUAA_DEFAULT_USER_FIRSTNAME\n\n\nJoe\n\n\nFirst name of default admin user\n\n\n\n\n\n\nUAA_DEFAULT_USER_GROUPS\n\n\nSee \nhere\n\n\nDefault user groups of the users\n\n\n\n\n\n\nUAA_DEFAULT_USER_LASTNAME\n\n\nAdmin\n\n\nLast name of default admin user\n\n\n\n\n\n\nUAA_DEFAULT_USER_PW\n\n\n\n\nPassword of default admin user\n\n\n\n\n\n\nUAA_FLEX_USAGE_CLIENT_ID\n\n\nflex_usage_client\n\n\nIdentity of the Flex usage generator scope in Identity\n\n\n\n\n\n\nUAA_FLEX_USAGE_CLIENT_SECRET\n\n\n$UAA_DEFAULT_SECRET\n\n\nSecret of the Flex usage generator scope in Identity\n\n\n\n\n\n\nUAA_PERISCOPE_ID\n\n\nperiscope\n\n\nIdentity of the Autoscale scope in Identity\n\n\n\n\n\n\nUAA_PERISCOPE_SECRET\n\n\n$UAA_DEFAULT_SECRET\n\n\nSecret of the Autoscale scope in Identity\n\n\n\n\n\n\nUAA_PORT\n\n\n8089\n\n\nIdentity service public port\n\n\n\n\n\n\nUAA_SULTANS_ID\n\n\nsultans\n\n\nIdentity of the Authentication service scope in Identity\n\n\n\n\n\n\nUAA_SULTANS_SECRET\n\n\n$UAA_DEFAULT_SECRET\n\n\nSecret of the Authentication service scope in Identity\n\n\n\n\n\n\nUAA_ULUWATU_ID\n\n\nuluwatu\n\n\nIdentity of the Web UI scope in Identity\n\n\n\n\n\n\nUAA_ULUWATU_SECRET\n\n\n$UAA_DEFAULT_SECRET\n\n\nSecret of the Web UI scope in Identity\n\n\n\n\n\n\nUAA_ZONE_DOMAIN\n\n\nexample.com\n\n\nExternal domain name for zone in Identity\n\n\n\n\n\n\nULUWATU_CONTAINER_PATH\n\n\n/uluwatu\n\n\nDefault project location in the Web UI container\n\n\n\n\n\n\nULU_DEFAULT_SSH_KEY\n\n\n\"\"\n\n\nDefault SSH key for the credentials in Cloudbreak\n\n\n\n\n\n\nULU_HOST_ADDRESS\n\n\n\"https://$PUBLIC_IP\"\n\n\nURL for the Web UI host\n\n\n\n\n\n\nULU_NODE_TLS_REJECT_UNAUTHORIZED\n\n\n\"0\"\n\n\nWhen set to \"0\", enables self-signed certifications in Web UI\n\n\n\n\n\n\nULU_OAUTH_REDIRECT_URI\n\n\n\"$ULU_HOST_ADDRESS/authorize\"\n\n\nAuthorization page on Web UI\n\n\n\n\n\n\nULU_SUBSCRIBE_TO_NOTIFICATIONS\n\n\n\"false\"\n\n\nSet to \u201ctrue\u201d to enable email notifications for Cloudbreak events\n\n\n\n\n\n\nULU_SULTANS_ADDRESS\n\n\n\"https://$PUBLIC_IP/sl\"\n\n\nAuthentication service URL\n\n\n\n\n\n\nVERBOSE_MIGRATION\n\n\nfalse\n\n\nWhen set to true, enables verbose database migration\n\n\n\n\n\n\n\n\nAWS Variables\n\n\n\n\n\n\n\n\nVariable Name\n\n\nDefault Value\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nAWS_ACCESS_KEY_ID\n\n\n\"\"\n\n\nAccess key of the AWS account\n\n\n\n\n\n\nAWS_ROLE_NAME\n\n\ncbreak-deployer\n\n\nName of the AWS role for the \ncbd aws [generate-rol, show role]\n commands\n\n\n\n\n\n\nAWS_SECRET_ACCESS_KEY\n\n\n\"\"\n\n\nSecret access key of the AWS account\n\n\n\n\n\n\nCB_AWS_CUSTOM_CF_TAGS\n\n\n\"\"\n\n\nComma separated list of AWS CloudFormation stack tags\n\n\n\n\n\n\nCB_AWS_DEFAULT_CF_TAG\n\n\n\"\"\n\n\nDefault tag for AWS CloudFormation stack\n\n\n\n\n\n\nCB_AWS_DEFAULT_INBOUND_SECURITY_GROUP\n\n\n\"\"\n\n\nDefault inbound policy name for AWS CloudFormation stack\n\n\n\n\n\n\nCB_AWS_EXTERNAL_ID\n\n\nprovision-ambari\n\n\nExternal ID of the assume role policy\n\n\n\n\n\n\nCB_AWS_HOSTKEY_VERIFY\n\n\n\"false\"\n\n\nEnables host fingerprint verification on AWS\n\n\n\n\n\n\nCB_AWS_VPC\n\n\n\"\"\n\n\nConfigures the VPC ID on AWS. Set this variable if you are provisioning cluster to the same VPC where Cloudbreak is deployed on AWS.\n\n\n\n\n\n\nCERTS_BUCKET\n\n\n\"\"\n\n\nS3 bucket name for backup and restore certificates via \ncbd aws [certs-restore-s3  certs-upload-s3]\n commands\n\n\n\n\n\n\n\n\nAzure Variables\n\n\n\n\n\n\n\n\nVariable Name\n\n\nDefault Value\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nAZURE_SUBSCRIPTION_ID\n\n\n\n\nAzure subscription ID for interactive login in the web UI\n\n\n\n\n\n\nAZURE_TENANT_ID\n\n\n\n\nAzure tenant ID for interactive login in the web UI\n\n\n\n\n\n\n\n\nGCP Variables\n\n\n\n\n\n\n\n\nVariable Name\n\n\nDefault Value\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nCB_GCP_HOSTKEY_VERIFY\n\n\n\"false\"\n\n\nWhen set to \"true\", enables host fingerprint verification on GCP\n\n\n\n\n\n\n\n\nLocal Development Variables\n\n\n\n\n\n\n\n\nVariable Name\n\n\nDefault Value\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nCB_LOCAL_DEV_BIND_ADDR\n\n\n\"192.168.64.1\"\n\n\nAmbassador external address for local development of Cloudbreak and Autoscale\n\n\n\n\n\n\nCB_SCHEMA_SCRIPTS_LOCATION\n\n\n\"container\"\n\n\nLocation of Cloudbreak schema update files\n\n\n\n\n\n\nDOCKER_TAG_AMBASSADOR\n\n\n0.5.0\n\n\nAmbassador container version for local development\n\n\n\n\n\n\nPERISCOPE_SCHEMA_SCRIPTS_LOCATION\n\n\n\"container\"\n\n\nLocation of Cloudbreak schema update files\n\n\n\n\n\n\nPRIVATE_IP\n\n\n$BRIDGE_IP\n\n\nIP address or hostname of the private interface\n\n\n\n\n\n\nREMOVE_CONTAINER\n\n\n\"--rm\"\n\n\nWhen set to \"--rm\" (default), removes side effect containers for debug purpose. Set to \" \" to keep side effect containers for debug purpose\n\n\n\n\n\n\nSULTANS_VOLUME_HOST\n\n\n/dev/null\n\n\nLocation of the locally developed Authentication service project\n\n\n\n\n\n\nUAA_SCHEMA_SCRIPTS_LOCATION\n\n\n\"container\"\n\n\nLocation of Identity schema update files\n\n\n\n\n\n\nULUWATU_VOLUME_HOST\n\n\n/dev/null\n\n\nLocation of the locally developed web UI project\n\n\n\n\n\n\n\n\nMacOS Variables\n\n\n\n\n\n\n\n\nVariable Name\n\n\nDefault Value\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nDOCKER_MACHINE\n\n\n\"\"\n\n\nName of the Docker Machine where Cloudbreak runs\n\n\n\n\n\n\nDOCKER_PROFILE\n\n\nProfile\n\n\nProfile file for environment variables related to Docker Machine\n\n\n\n\n\n\nMACHINE_CPU\n\n\n2\n\n\nNumber of the CPU cores on the Docker Machine instance\n\n\n\n\n\n\nMACHINE_MEM\n\n\n4096\n\n\nAmount of RAM on the Docker Machine instance\n\n\n\n\n\n\nMACHINE_NAME\n\n\ncbd\n\n\nName of the Docker Machine instance\n\n\n\n\n\n\nMACHINE_OPTS\n\n\n\"--xhyve-virtio-9p\"\n\n\nExtra options for Docker Machine instance\n\n\n\n\n\n\nMACHINE_STORAGE_PATH\n\n\n$HOME/.docker/machine\n\n\nDocker Machine storage path\n\n\n\n\n\n\n\n\nUAA_DEFAULT_USER_GROUPS\n\n\nDefault value fro \nUAA_DEFAULT_USER_GROUPS\n is:\n\n\n\"openid,cloudbreak.networks,cloudbreak.securitygroups,cloudbreak.templates,cloudbreak.blueprints,cloudbreak.credentials,cloudbreak.stacks,sequenceiq.cloudbreak.admin,sequenceiq.cloudbreak.user,sequenceiq.account.${UAA_DEFAULT_ACCOUNT},cloudbreak.events,cloudbreak.usages.global,cloudbreak.usages.account,cloudbreak.usages.user,periscope.cluster,cloudbreak.recipes,cloudbreak.blueprints.read,cloudbreak.templates.read,cloudbreak.credentials.read,cloudbreak.recipes.read,cloudbreak.networks.read,cloudbreak.securitygroups.read,cloudbreak.stacks.read,cloudbreak.sssdconfigs,cloudbreak.sssdconfigs.read,cloudbreak.platforms,cloudbreak.platforms.read\"\n\n\n\nChange SMTP Parameters\n\n\nIf you want to change SMTP parameters, add them your \nProfile\n.\n\n\nThe default values of the SMTP parameters are:\n\n\nexport CLOUDBREAK_SMTP_SENDER_USERNAME=\nexport CLOUDBREAK_SMTP_SENDER_PASSWORD=\nexport CLOUDBREAK_SMTP_SENDER_HOST=\nexport CLOUDBREAK_SMTP_SENDER_PORT=25\nexport CLOUDBREAK_SMTP_SENDER_FROM=\nexport CLOUDBREAK_SMTP_AUTH=true\nexport CLOUDBREAK_SMTP_STARTTLS_ENABLE=true\nexport CLOUDBREAK_SMTP_TYPE=smtp\n\n\n\n\nIf your SMTP server uses SMTPS, you must set the protocol in your \nProfile\n to smtps:\n\n\nexport CLOUDBREAK_SMTP_TYPE=smtps\n\n\n\n\nConfigure Consul\n\n\nCloudbreak uses \nConsul\n for DNS resolution. All Cloudbreak related services are registered as someservice.service.consul.\n\n\nConsul\u2019s built-in DNS server is able to fallback on another DNS server. This option is called \n-recursor\n. Clodbreak Deployer first tries to discover the DNS settings of the host by looking for nameserver entry in the \n/etc/resolv.conf\n file. If it finds one, consul will use it as a recursor. Otherwise, it will use \n8.8.8.8\n.\n\n\nFor a full list of available consul config options, refer to \nConsul documentation\n.\n\n\nTo pass any additional Consul configuration, define the \nDOCKER_CONSUL_OPTIONS\n variable in the Profile file.\n\n\nAdd Tags in Profile (AWS)\n\n\nIn order to differentiate launched instances, you can optionally define custom tags for your AWS resources deployed by Cloudbreak. \n\n\n\n\n\n\nIf you want just one custom tag for your CloudFormation resources, set this variable in the \nProfile\n:\n\n\nexport CB_AWS_DEFAULT_CF_TAG=mytagcontent\n\n\nIn this example, the name of the tag will be \nCloudbreakId\n and the value will be \nmytagcontent\n.\n\n\n\n\n\n\nIf you prefer to customize the tag name, set this variable:\n\n\nexport CB_AWS_CUSTOM_CF_TAGS=mytagname:mytagvalue\n\n\nIn this example the name of the tag will be \nmytagname\n and the value will be \nmytagvalue\n. \n\n\n\n\n\n\nYou can specify a list of tags with a comma separated list: \n\n\nexport CB_AWS_CUSTOM_CF_TAGS=tag1:value1,tag2:value2,tag3:value3", 
            "title": "Edit Cloudbreak Profile"
        }, 
        {
            "location": "/cb-profile/index.html#editing-cloudbreak-profile", 
            "text": "Cloudbreak deployer configuration is based on environment variables.    During startup, Cloudbreak deployer tries to determine the underlying infrastructure and then sets required environment variables with appropriate default values. If these environment variables are not sufficient for your use case, you can set additional environment variables in your  Profile  file.", 
            "title": "Editing Cloudbreak Profile"
        }, 
        {
            "location": "/cb-profile/index.html#set-profile-variables", 
            "text": "To set environment variables relevant for Cloudbreak Deployer, add them to a file called  Profile  located in the Cloudbreak deployment directory (typically  /var/lib/cloudbreak-deployment ).  The  Profile  file is sourced, so you can use the usual syntax to set configuration values:  export MY_VAR=some_value\nexport MY_OTHER_VAR=another_value", 
            "title": "Set Profile Variables"
        }, 
        {
            "location": "/cb-profile/index.html#check-available-profile-variables", 
            "text": "To see all available environment variables with their default values, use:  cbd env show", 
            "title": "Check Available Profile Variables"
        }, 
        {
            "location": "/cb-profile/index.html#create-environment-specific-profiles", 
            "text": "If you would like to use a different versions of Cloudbreak for prod and qa profile, you must create two environment specific configurations that can be sourced. For example:   Profile.prod    Profile.qa      For example, to create and use a prod profile, you need to:   Create a file called  Profile.prod     Write the environment-specific  export DOCKER_TAG_CLOUDBREAK=0.3.99  into  Profile.prod  to specify Docker image.    Set the environment variable:  CBD_DEFAULT_PROFILE=prod      To use the prod specific profile once, set:    CBD_DEFAULT_PROFILE=prod cbd some_commands  To permanently use the prod profile, set  export CBD_DEFAULT_PROFILE=prod  in your  .bash_profile .", 
            "title": "Create Environment Specific Profiles"
        }, 
        {
            "location": "/cb-profile/index.html#profile-variables", 
            "text": "", 
            "title": "Profile Variables"
        }, 
        {
            "location": "/cb-profile/index.html#cloudbreak-variables", 
            "text": "Refer to this list for available environment variables. The variables are listed with their default values. If default is unset, no value is listed.      Variable Name  Default Value  Description      ADDRESS_RESOLVING_TIMEOUT  120000  DNS lookup timeout for internal service discovery    CAPTURE_CRON_EXPRESSION   SmartSense bundle generation time interval in cron format    CBD_CERT_ROOT_PATH  \"${PWD}/certs\"  Path where deployer stores Cloudbreak certificates. ${PWD} refers to the Cloudbreak deployment directory    CBD_LOG_NAME  cbreak  Name of the Cloudbreak log file    CBD_TRAEFIK_TLS  \"/certs/traefik/client.pem,/certs/traefik/client-key.pem\"  Path inside of the Traefik container where TLS files are located    CB_BLUEPRINT_DEFAULTS  \"hdp-small-default;hdp-spark-cluster;hdp-streaming-cluster\"  Comma separated list of the default blueprints that Cloudbreak initializes in its database    CB_BYOS_DFS_DATA_DIR  \"/hadoop/hdfs/data\"  (Deprecated) Default data directory for BYOS orchestrators    CB_COMPONENT_CLUSTER_ID   SmartSense component cluster ID    CB_COMPONENT_ID   SmartSense component ID    CB_COMPOSE_PROJECT  cbreak  Name of the Docker Compose project; it will appear in container names    CB_DB_ENV_DB  \"cbdb\"  Name of the Cloudbreak database    CB_DB_ENV_PASS  \"\"  Password for the Cloudbreak database authentication    CB_DB_ENV_SCHEMA  \"public\"  Schema used in the Cloudbreak database    CB_DB_ENV_USER  \"postgres\"  User for the Cloudbreak database authentication    CB_DB_ROOT_PATH  \"/var/lib/cloudbreak\"  (Deprecated) Location of the database volume on Cloudbreak host    CB_DEFAULT_GATEWAY_CIDR   CIDR address range which is used by Cloudbreak to communicate with its managed clusters. For more information, refer to  Restricting Inbound Access to Clusters .    CB_DEFAULT_SUBSCRIPTION_ADDRESS  http://uluwatu.service.consul:3000/notifications  URL of the default subscription for Cloudbreak notifications    CB_ENABLEDPLATFORMS   Set this to disable specific cloud providers. Accepted values are: AZURE, AWS, GCP, OPENSTACK.    CB_ENABLE_CUSTOM_IMAGE  \"false\"  Set to \"true\" to enable custom cloud images    CBD_FORCE_START   Set this to disable docker-compose.yml and uaa.yml validation    CB_HBM2DDL_STRATEGY  \"validate\"  Configures hibernate.hbm2ddl.auto in Cloudbreak    CB_HOST_DISCOVERY_CUSTOM_DOMAIN  \"\"  Custom domain of the provisioned cluster    CB_HTTPS_PROXY  \"\"  HTTPS proxy URL    CB_HTTP_PROXY  \"\"  HTTP proxy URL    CB_IMAGE_CATALOG_URL  \"https://s3-eu-west-1.amazonaws.com/cloudbreak-info/cb-image-catalog.json\"  Image catalog URL    CB_INSTANCE_NODE_ID   Unique identifier of the Cloudbreak node    CB_INSTANCE_PROVIDER   Cloud provider of the Cloudbreak instance    CB_INSTANCE_REGION   Cloud region of the Cloudbreak instance    CB_INSTANCE_UUID   Unique identifier of Cloudbreak deployment    CB_JAVA_OPTS  \"\"  Extra Java options for Autoscale and Cloudbreak    CB_LOG_LEVEL  \"INFO\"  Log level of the Cloudbreak service    CB_MAX_SALT_NEW_SERVICE_RETRY  90  Salt orchestrator max retry count    CB_MAX_SALT_RECIPE_EXECUTION_RETRY  90  Salt orchestrator max retry count for recipes    CB_PLATFORM_DEFAULT_REGIONS   Comma separated list of default regions by platform. For example:  AWS:eu-west-1 .    CB_PRODUCT_ID   SmartSense product ID    CB_SCHEMA_MIGRATION_AUTO  true  When set to true, enables Cloudbreak automatic database schema update    CB_SMARTSENSE_CONFIGURE  \"false\"  Set to \u201ctrue\u201d to install and configure SmartSense on cluster nodes    CB_SMARTSENSE_CLUSTER_NAME_PREFIX   SmartSense Cloudbreak cluster name prefix    CB_SMARTSENSE_ID  \"\"  SmartSense subscription ID    CB_TEMPLATE_DEFAULTS  \"minviable-gcp,minviable-azure,minviable-aws\"  Comma separated list of the default templates that Cloudbreak initializes in its database    CB_UI_MAX_WAIT  400  Wait timeout for  cbd start-wait  command    CERT_VALIDATION  \"true\"  When set to \"true\", enables cert validation in Cloudbreak and Autoscale    CLOUDBREAK_SMTP_AUTH  \"true\"  When set to \"true\", configures mail.smtp.auth in Cloudbreak    CLOUDBREAK_SMTP_SENDER_FROM  \"noreply@hortonworks.com\"  Email address of the sender    CLOUDBREAK_SMTP_SENDER_HOST  \"smtp.service.consul\"  SMTP server address of the hostname    CLOUDBREAK_SMTP_SENDER_PASSWORD  \"$LOCAL_SMTP_PASSWORD\"  SMTP server password    CLOUDBREAK_SMTP_SENDER_PORT  25  Port of the SMTP server    CLOUDBREAK_SMTP_SENDER_USERNAME  \"admin\"  Username for SMTP authentication    CLOUDBREAK_SMTP_STARTTLS_ENABLE  \"false\"  Set to \"true\" to configure mail.smtp.starttls.enable in Cloudbreak    CLOUDBREAK_SMTP_TYPE  \"smtp\"  Defines mail.transport.protocol in CLoudbreak    COMMON_DB  commondb  Name of the database container    COMMON_DB_VOL  common  Name of the database volume    COMPOSE_HTTP_TIMEOUT  120  Docker Compose execution timeout    DB_DUMP_VOLUME  cbreak_dump  Name of the database dump volume    DB_MIGRATION_LOG  \"db_migration.log\"  Database migration log file    DEFAULT_INBOUND_ACCESS_IP  \"\"  Opens default ports on AWS instances for Cloudbreak    DOCKER_CONSUL_OPTIONS  \"\"  Extra options for Consul    DOCKER_IMAGE_CBD_SMARTSENSE  hortonworks/cbd-smartsense  SmartSense Docker image name    DOCKER_IMAGE_CLOUDBREAK  hortonworks/cloudbreak  Cloudbreak Docker image name    DOCKER_IMAGE_CLOUDBREAK_AUTH  hortonworks/cloudbreak-auth  Authentication service Docker image name    DOCKER_IMAGE_CLOUDBREAK_PERISCOPE  hortonworks/cloudbreak-autoscale  Autoscale Docker image name    DOCKER_IMAGE_CLOUDBREAK_SHELL  hortonworks/cloudbreak-shell  Cloudbreak Shell Docker image name    DOCKER_IMAGE_CLOUDBREAK_WEB  hortonworks/cloudbreak-web  Web UI Docker image name    DOCKER_TAG_ALPINE  3.1  Alpine container version    DOCKER_TAG_CBD_SMARTSENSE  0.10.0  SmartSense container version    DOCKER_TAG_CERT_TOOL  0.2.0  Cert tool container version    DOCKER_TAG_CLOUDBREAK  2.1.0-dev.70  Cloudbreak container version    DOCKER_TAG_CLOUDBREAK_SHELL  2.1.0-dev.70  Cloudbreak Shell container version    DOCKER_TAG_CONSUL  0.5  Consul container version    DOCKER_TAG_HAVEGED  1.1.0  Haveged container version    DOCKER_TAG_LOGROTATE  1.0.0  Logrotate container version    DOCKER_TAG_MIGRATION  1.0.0  Migration container version    DOCKER_TAG_PERISCOPE  2.1.0-dev.70  Autoscale container version    DOCKER_TAG_POSTFIX  latest  Postfix container version    DOCKER_TAG_POSTGRES  9.6.1-alpine  Postgresql container version    DOCKER_TAG_REGISTRATOR  v5  Registrator container version    DOCKER_TAG_SULTANS  2.1.0-dev.70  Authentication service container version    DOCKER_TAG_TRAEFIK  v1.2.0  Traefik container version    DOCKER_TAG_UAA  3.6.5  Identity container version    DOCKER_TAG_ULUWATU  2.1.0-dev.70  Web UI container version    IDENTITY_DB_NAME  \"uaadb\"  Name of the Identity database    IDENTITY_DB_PASS  \"\"  Password for the Identity database authentication    IDENTITY_DB_URL  \"${COMMON_DB}.service.consul:5432\"  URL for the Identity database connection, including the port number    IDENTITY_DB_USER  \"postgres\"  User for the Identity database authentication    LOCAL_SMTP_PASSWORD  \"$UAA_DEFAULT_USER_PW\"  Default password for the internal mail server    PERISCOPE_DB_ENV_DB  \"periscopedb\"  Name of the Autoscale database    PERISCOPE_DB_ENV_PASS  \"\"  Password for the Autoscale database authentication    PERISCOPE_DB_ENV_SCHEMA  \"public\"  Schema used in the Autoscale database    PERISCOPE_DB_ENV_USER  \"postgres\"  User for the Autoscale database authentication    PERISCOPE_DB_PORT_5432_TCP_ADDR   Address of the Autoscale database    PERISCOPE_DB_PORT_5432_TCP_PORT   Port number of the Autoscale database    PERISCOPE_HBM2DDL_STRATEGY  \"validate\"  Configures hibernate.hbm2ddl.auto in Autoscale    PERISCOPE_LOG_LEVEL  \"INFO\"  Log level of the Autoscale service    PERISCOPE_SCHEMA_MIGRATION_AUTO  true  When set to \"true\", enables Autoscale automatic database schema update    PUBLIC_IP   IP address or hostname of the public interface    REST_DEBUG  \"false\"  Set to \"true\" to enable REST call debug level in Cloudbreak and Autoscale    SL_ADDRESS_RESOLVING_TIMEOUT   DNS lookup timeout of Authentication service for internal service discovery    SL_NODE_TLS_REJECT_UNAUTHORIZED  \"0\"  When set to \"0\", enables self-signed certifications in Authentication service    SULTANS_CONTAINER_PATH  /sultans  Default project location in Authentication service container    TRAEFIK_MAX_IDLE_CONNECTION  100  Sets --maxidleconnsperhost for Traefik to the value entered    UAA_CLOUDBREAK_ID  cloudbreak  Identity of the Cloudbreak scope in Identity    UAA_CLOUDBREAK_SECRET  $UAA_DEFAULT_SECRET  Secret of the Cloudbreak scope in Identity    UAA_CLOUDBREAK_SHELL_ID  cloudbreak_shell  Identity of the Cloudbreak Shell scope in Identity    UAA_DEFAULT_ACCOUNT  \"seq1234567.SequenceIQ\"  Default account for users as an Identity group    UAA_DEFAULT_SECRET   Default secret for all the scopes and encryptions    UAA_DEFAULT_USER_EMAIL  admin@example.com  Email address of default admin user    UAA_DEFAULT_USER_FIRSTNAME  Joe  First name of default admin user    UAA_DEFAULT_USER_GROUPS  See  here  Default user groups of the users    UAA_DEFAULT_USER_LASTNAME  Admin  Last name of default admin user    UAA_DEFAULT_USER_PW   Password of default admin user    UAA_FLEX_USAGE_CLIENT_ID  flex_usage_client  Identity of the Flex usage generator scope in Identity    UAA_FLEX_USAGE_CLIENT_SECRET  $UAA_DEFAULT_SECRET  Secret of the Flex usage generator scope in Identity    UAA_PERISCOPE_ID  periscope  Identity of the Autoscale scope in Identity    UAA_PERISCOPE_SECRET  $UAA_DEFAULT_SECRET  Secret of the Autoscale scope in Identity    UAA_PORT  8089  Identity service public port    UAA_SULTANS_ID  sultans  Identity of the Authentication service scope in Identity    UAA_SULTANS_SECRET  $UAA_DEFAULT_SECRET  Secret of the Authentication service scope in Identity    UAA_ULUWATU_ID  uluwatu  Identity of the Web UI scope in Identity    UAA_ULUWATU_SECRET  $UAA_DEFAULT_SECRET  Secret of the Web UI scope in Identity    UAA_ZONE_DOMAIN  example.com  External domain name for zone in Identity    ULUWATU_CONTAINER_PATH  /uluwatu  Default project location in the Web UI container    ULU_DEFAULT_SSH_KEY  \"\"  Default SSH key for the credentials in Cloudbreak    ULU_HOST_ADDRESS  \"https://$PUBLIC_IP\"  URL for the Web UI host    ULU_NODE_TLS_REJECT_UNAUTHORIZED  \"0\"  When set to \"0\", enables self-signed certifications in Web UI    ULU_OAUTH_REDIRECT_URI  \"$ULU_HOST_ADDRESS/authorize\"  Authorization page on Web UI    ULU_SUBSCRIBE_TO_NOTIFICATIONS  \"false\"  Set to \u201ctrue\u201d to enable email notifications for Cloudbreak events    ULU_SULTANS_ADDRESS  \"https://$PUBLIC_IP/sl\"  Authentication service URL    VERBOSE_MIGRATION  false  When set to true, enables verbose database migration", 
            "title": "Cloudbreak Variables"
        }, 
        {
            "location": "/cb-profile/index.html#aws-variables", 
            "text": "Variable Name  Default Value  Description      AWS_ACCESS_KEY_ID  \"\"  Access key of the AWS account    AWS_ROLE_NAME  cbreak-deployer  Name of the AWS role for the  cbd aws [generate-rol, show role]  commands    AWS_SECRET_ACCESS_KEY  \"\"  Secret access key of the AWS account    CB_AWS_CUSTOM_CF_TAGS  \"\"  Comma separated list of AWS CloudFormation stack tags    CB_AWS_DEFAULT_CF_TAG  \"\"  Default tag for AWS CloudFormation stack    CB_AWS_DEFAULT_INBOUND_SECURITY_GROUP  \"\"  Default inbound policy name for AWS CloudFormation stack    CB_AWS_EXTERNAL_ID  provision-ambari  External ID of the assume role policy    CB_AWS_HOSTKEY_VERIFY  \"false\"  Enables host fingerprint verification on AWS    CB_AWS_VPC  \"\"  Configures the VPC ID on AWS. Set this variable if you are provisioning cluster to the same VPC where Cloudbreak is deployed on AWS.    CERTS_BUCKET  \"\"  S3 bucket name for backup and restore certificates via  cbd aws [certs-restore-s3  certs-upload-s3]  commands", 
            "title": "AWS Variables"
        }, 
        {
            "location": "/cb-profile/index.html#azure-variables", 
            "text": "Variable Name  Default Value  Description      AZURE_SUBSCRIPTION_ID   Azure subscription ID for interactive login in the web UI    AZURE_TENANT_ID   Azure tenant ID for interactive login in the web UI", 
            "title": "Azure Variables"
        }, 
        {
            "location": "/cb-profile/index.html#gcp-variables", 
            "text": "Variable Name  Default Value  Description      CB_GCP_HOSTKEY_VERIFY  \"false\"  When set to \"true\", enables host fingerprint verification on GCP", 
            "title": "GCP Variables"
        }, 
        {
            "location": "/cb-profile/index.html#local-development-variables", 
            "text": "Variable Name  Default Value  Description      CB_LOCAL_DEV_BIND_ADDR  \"192.168.64.1\"  Ambassador external address for local development of Cloudbreak and Autoscale    CB_SCHEMA_SCRIPTS_LOCATION  \"container\"  Location of Cloudbreak schema update files    DOCKER_TAG_AMBASSADOR  0.5.0  Ambassador container version for local development    PERISCOPE_SCHEMA_SCRIPTS_LOCATION  \"container\"  Location of Cloudbreak schema update files    PRIVATE_IP  $BRIDGE_IP  IP address or hostname of the private interface    REMOVE_CONTAINER  \"--rm\"  When set to \"--rm\" (default), removes side effect containers for debug purpose. Set to \" \" to keep side effect containers for debug purpose    SULTANS_VOLUME_HOST  /dev/null  Location of the locally developed Authentication service project    UAA_SCHEMA_SCRIPTS_LOCATION  \"container\"  Location of Identity schema update files    ULUWATU_VOLUME_HOST  /dev/null  Location of the locally developed web UI project", 
            "title": "Local Development Variables"
        }, 
        {
            "location": "/cb-profile/index.html#macos-variables", 
            "text": "Variable Name  Default Value  Description      DOCKER_MACHINE  \"\"  Name of the Docker Machine where Cloudbreak runs    DOCKER_PROFILE  Profile  Profile file for environment variables related to Docker Machine    MACHINE_CPU  2  Number of the CPU cores on the Docker Machine instance    MACHINE_MEM  4096  Amount of RAM on the Docker Machine instance    MACHINE_NAME  cbd  Name of the Docker Machine instance    MACHINE_OPTS  \"--xhyve-virtio-9p\"  Extra options for Docker Machine instance    MACHINE_STORAGE_PATH  $HOME/.docker/machine  Docker Machine storage path", 
            "title": "MacOS Variables"
        }, 
        {
            "location": "/cb-profile/index.html#uaa_default_user_groups", 
            "text": "Default value fro  UAA_DEFAULT_USER_GROUPS  is:  \"openid,cloudbreak.networks,cloudbreak.securitygroups,cloudbreak.templates,cloudbreak.blueprints,cloudbreak.credentials,cloudbreak.stacks,sequenceiq.cloudbreak.admin,sequenceiq.cloudbreak.user,sequenceiq.account.${UAA_DEFAULT_ACCOUNT},cloudbreak.events,cloudbreak.usages.global,cloudbreak.usages.account,cloudbreak.usages.user,periscope.cluster,cloudbreak.recipes,cloudbreak.blueprints.read,cloudbreak.templates.read,cloudbreak.credentials.read,cloudbreak.recipes.read,cloudbreak.networks.read,cloudbreak.securitygroups.read,cloudbreak.stacks.read,cloudbreak.sssdconfigs,cloudbreak.sssdconfigs.read,cloudbreak.platforms,cloudbreak.platforms.read\"", 
            "title": "UAA_DEFAULT_USER_GROUPS"
        }, 
        {
            "location": "/cb-profile/index.html#change-smtp-parameters", 
            "text": "If you want to change SMTP parameters, add them your  Profile .  The default values of the SMTP parameters are:  export CLOUDBREAK_SMTP_SENDER_USERNAME=\nexport CLOUDBREAK_SMTP_SENDER_PASSWORD=\nexport CLOUDBREAK_SMTP_SENDER_HOST=\nexport CLOUDBREAK_SMTP_SENDER_PORT=25\nexport CLOUDBREAK_SMTP_SENDER_FROM=\nexport CLOUDBREAK_SMTP_AUTH=true\nexport CLOUDBREAK_SMTP_STARTTLS_ENABLE=true\nexport CLOUDBREAK_SMTP_TYPE=smtp  If your SMTP server uses SMTPS, you must set the protocol in your  Profile  to smtps:  export CLOUDBREAK_SMTP_TYPE=smtps", 
            "title": "Change SMTP Parameters"
        }, 
        {
            "location": "/cb-profile/index.html#configure-consul", 
            "text": "Cloudbreak uses  Consul  for DNS resolution. All Cloudbreak related services are registered as someservice.service.consul.  Consul\u2019s built-in DNS server is able to fallback on another DNS server. This option is called  -recursor . Clodbreak Deployer first tries to discover the DNS settings of the host by looking for nameserver entry in the  /etc/resolv.conf  file. If it finds one, consul will use it as a recursor. Otherwise, it will use  8.8.8.8 .  For a full list of available consul config options, refer to  Consul documentation .  To pass any additional Consul configuration, define the  DOCKER_CONSUL_OPTIONS  variable in the Profile file.", 
            "title": "Configure Consul"
        }, 
        {
            "location": "/cb-profile/index.html#add-tags-in-profile-aws", 
            "text": "In order to differentiate launched instances, you can optionally define custom tags for your AWS resources deployed by Cloudbreak.     If you want just one custom tag for your CloudFormation resources, set this variable in the  Profile :  export CB_AWS_DEFAULT_CF_TAG=mytagcontent  In this example, the name of the tag will be  CloudbreakId  and the value will be  mytagcontent .    If you prefer to customize the tag name, set this variable:  export CB_AWS_CUSTOM_CF_TAGS=mytagname:mytagvalue  In this example the name of the tag will be  mytagname  and the value will be  mytagvalue .     You can specify a list of tags with a comma separated list:   export CB_AWS_CUSTOM_CF_TAGS=tag1:value1,tag2:value2,tag3:value3", 
            "title": "Add Tags in Profile (AWS)"
        }, 
        {
            "location": "/cb-delete/index.html", 
            "text": "Deleting Resources\n\n\nYou must terminate all clusters associated with a Cloudbreak before you can terminate the Cloudbreak instance. In general, you should delete clusters from the Cloudbreak UI. If needed, you can also delete the cluster resources manually via the cloud provider tools. \n\n\nDeleting Clusters\n\n\nThe proper way to delete clusters is to use the the \nTerminate\n option available in the Cloudbreak UI. If the terminate process fails, try the \nTerminate\n \n \nForce terminate\n option.\n\n\nIf the force termination does not delete all cluster resources, delete the resources manually:\n\n\n\n\nTo find the VMs, click on the links available in the cluster details. \n\n\nTo find the network and subnet, see the \nCluster Information\n in the cluster details. \n\n\nOn Azure, you can delete the cluster manually by deleting the whole resource group created when the cluster was deployed. The name of the resource group, under which the cluster-related resources are organized always includes the name of the cluster, so you should be able to find the group by searching for that name in the \nResource groups\n.\n\n\n\n\nUpon cluster termination, Cloudbreak only terminates the resources that it created. It does not terminate any resources (such as networks, subnets, roles, and so on) which existed prior to cluster creation. \n\n\nDelete Cloudbreak on AWS\n\n\nIf you want to delete the Cloudbreak instance, you can do so by deleting the EC2 instance on which it is running.\n\n\nSteps\n\n\n\n\n\n\nLog in to the AWS Management Console.\n\n\n\n\n\n\nBrowse to the EC2 Management Console.\n\n\n\n\n\n\nnavigate to \nInstances\n.\n\n\n\n\n\n\nSelect the instance that you want to delete and then select \nActions\n \n \nInstance State\n \n \nTerminate\n.\n\n\n\n\n\n\nClick \nYes, Terminate\n to confirm.\n\n\n  \n\n\n\n\n\n\nDelete Cloudbreak on Azure\n\n\nYou can delete Cloudbreak instance from your Azure account by deleting related resources. To delete a Cloudbreak instance:\n\n\n\n\n\n\nIf you deployed Cloudbreak in a new resource group: to delete Cloudbreak, delete the whole related resource group.\n\n\n\n\n\n\nIf you deployed Cloudbreak in an existing resource group: navigate to the group and delete only Cloubdreak-related resources such as the VM.\n\n\n\n\n\n\nSteps\n\n\n\n\n\n\nFrom the Microsoft Azure Portal dashboard, select \nResource groups\n.\n\n\n\n\n\n\nFind the resource group that you want to delete.\n\n\n\n\n\n\nIf you deployed Cloudbreak in a new resource group, you can delete the whole resource group. Click on \n...\n and select \nDelete\n:\n\n\n  \n\n\nNext, type the name of the resource group to delete and click \nDelete\n.\n\n\n\n\n\n\nIf you deployed Cloudbreak in an existing resource group, navigate to the details of the resource group and delete only Cloudbreak-related resources such as the VM.    \n\n\n\n\n\n\nDelete Cloudbreak on GCP\n\n\nYou can delete Cloudbreak instance from your Google Cloud account. \n\n\nSteps\n\n\n\n\n\n\nNavigate to your Google Cloud account.\n\n\n\n\n\n\nNavigate to \nCompute Engine\n \n \nVM instances\n.\n\n\n\n\n\n\nSelect the instance that you want to delete:\n\n\n     \n\n\n\n\n\n\nClick on the delete icon and then confirm delete. \n\n\n\n\n\n\nDelete Cloudbreak on OpenStack\n\n\nYou can delete Cloudbreak instance from your OpeenStack console. \n\n\nSteps\n\n\n\n\n\n\nNavigate to your OpenStack account.\n\n\n\n\n\n\nNavigate to \nInstances\n.\n\n\n\n\n\n\nSelect the instance to delete, click \nTerminate Instances\n, and confirm.", 
            "title": "Delete Cloudbreak"
        }, 
        {
            "location": "/cb-delete/index.html#deleting-resources", 
            "text": "You must terminate all clusters associated with a Cloudbreak before you can terminate the Cloudbreak instance. In general, you should delete clusters from the Cloudbreak UI. If needed, you can also delete the cluster resources manually via the cloud provider tools.", 
            "title": "Deleting Resources"
        }, 
        {
            "location": "/cb-delete/index.html#deleting-clusters", 
            "text": "The proper way to delete clusters is to use the the  Terminate  option available in the Cloudbreak UI. If the terminate process fails, try the  Terminate     Force terminate  option.  If the force termination does not delete all cluster resources, delete the resources manually:   To find the VMs, click on the links available in the cluster details.   To find the network and subnet, see the  Cluster Information  in the cluster details.   On Azure, you can delete the cluster manually by deleting the whole resource group created when the cluster was deployed. The name of the resource group, under which the cluster-related resources are organized always includes the name of the cluster, so you should be able to find the group by searching for that name in the  Resource groups .   Upon cluster termination, Cloudbreak only terminates the resources that it created. It does not terminate any resources (such as networks, subnets, roles, and so on) which existed prior to cluster creation.", 
            "title": "Deleting Clusters"
        }, 
        {
            "location": "/cb-delete/index.html#delete-cloudbreak-on-aws", 
            "text": "If you want to delete the Cloudbreak instance, you can do so by deleting the EC2 instance on which it is running.  Steps    Log in to the AWS Management Console.    Browse to the EC2 Management Console.    navigate to  Instances .    Select the instance that you want to delete and then select  Actions     Instance State     Terminate .    Click  Yes, Terminate  to confirm.", 
            "title": "Delete Cloudbreak on AWS"
        }, 
        {
            "location": "/cb-delete/index.html#delete-cloudbreak-on-azure", 
            "text": "You can delete Cloudbreak instance from your Azure account by deleting related resources. To delete a Cloudbreak instance:    If you deployed Cloudbreak in a new resource group: to delete Cloudbreak, delete the whole related resource group.    If you deployed Cloudbreak in an existing resource group: navigate to the group and delete only Cloubdreak-related resources such as the VM.    Steps    From the Microsoft Azure Portal dashboard, select  Resource groups .    Find the resource group that you want to delete.    If you deployed Cloudbreak in a new resource group, you can delete the whole resource group. Click on  ...  and select  Delete :      Next, type the name of the resource group to delete and click  Delete .    If you deployed Cloudbreak in an existing resource group, navigate to the details of the resource group and delete only Cloudbreak-related resources such as the VM.", 
            "title": "Delete Cloudbreak on Azure"
        }, 
        {
            "location": "/cb-delete/index.html#delete-cloudbreak-on-gcp", 
            "text": "You can delete Cloudbreak instance from your Google Cloud account.   Steps    Navigate to your Google Cloud account.    Navigate to  Compute Engine     VM instances .    Select the instance that you want to delete:           Click on the delete icon and then confirm delete.", 
            "title": "Delete Cloudbreak on GCP"
        }, 
        {
            "location": "/cb-delete/index.html#delete-cloudbreak-on-openstack", 
            "text": "You can delete Cloudbreak instance from your OpeenStack console.   Steps    Navigate to your OpenStack account.    Navigate to  Instances .    Select the instance to delete, click  Terminate Instances , and confirm.", 
            "title": "Delete Cloudbreak on OpenStack"
        }, 
        {
            "location": "/security/index.html", 
            "text": "Security Overview\n\n\nCloudbreak utilizes cloud provider security resources such as virtual networks, security groups, and identity and access management:\n\n\n\n\nNetwork isolation\n is achieved via user-configured virtual networks and subnets.\n\n    Read more about \nVirtual Networks\n.  \n\n\nNetwork security\n is achieved via out-of-the-box security group settings.\n\n    Read more about \nNetwork Security\n.   \n\n\nControlled use of cloud resources\n using IAM roles (AWS, GCP) or Active Directory (in case of Azure). \n    Read more about \nIdentity Management\n.    \n\n\n\n\nVirtual Networks\n\n\nCloud providers use virtual networks which resemble traditional networks. Depending on the options that you selected during deployment, your Cloudbreak instance and clusters are launched into new or existing cloud provider networking infrastructure (virtual networks and subnets). For more information about virtual networks, refer to the cloud-provider documentation:\n\n\n\n\n\n\n\n\nCloud Provider\n\n\nDocumentation Link\n\n\n\n\n\n\n\n\n\n\nAWS\n\n\nAmazon Virtual Private Cloud (Amazon VPC)\n\n\n\n\n\n\nAzure\n\n\nMicrosoft Azure Virtual Network\n\n\n\n\n\n\nGoogle Cloud Platform\n\n\nVirtual Private Cloud (VPC) network\n\n\n\n\n\n\nOpenStack\n\n\nNetwork\n\n\n\n\n\n\n\n\nNetwork Security\n\n\nSecurity groups are setup to control network traffic to the instances in the system.\n\n\nCloudbreak Instance Security Group\n\n\nThe following table lists the minimum security group port configuration required for the Cloudbreak instance:\n\n\n\n\n\n\n\n\nInbound Port\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\n22\n\n\nSSH access to the Cloudbreak VM.\n\n\n\n\n\n\n80\n\n\nHTTP access to the Cloudbreak UI. This is automatically redirected to the HTTPS (443) port.\n\n\n\n\n\n\n443\n\n\nHTTPS access to the Cloudbreak UI.\n\n\n\n\n\n\n\n\nDefault Cluster Security Groups\n\n\nFor clusters, Cloudbreak provides the following security group settings. If you do not modify these settings, these default security rules will be created. You can modify these rules either when creating or, if you don't want to use security group, remove them. \n\n\nAs an alternative to creating new security groups, you can select from your existing set of security groups, which can be modified using the cloud provider tools. \n\n\nCluster Host Group with Ambari Server\n\n\n\n\n\n\n\n\nInbound Port\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\n22\n\n\nSSH access to the VM instance. This port is also used by Cloudbreak to communicate with the cluster.\n\n\n\n\n\n\n443\n\n\nHTTPS access to the Ambari UI.\n\n\n\n\n\n\n9443\n\n\nManagement port, used by Cloudbreak to communicate with the cluster node VM.\n\n\n\n\n\n\n\n\n\n\nImportant\n\n\n\nBy default, ports 22, 443, and 9443 are set to 0.0.0.0/0 CIDR for inbound access on the Ambari node security group. We strongly recommend that you limit this CIDR, considering the following restrictions:\n\nPorts 22 and 9443 must be open to Cloudbreak's CIDR. You can set CB_DEFAULT_GATEWAY_CIDR in your Cloudbreak's Profile file in order to automatically open ports 22 and 9443 to your Cloudbrak IP. Refer to \nRestricting Inbound Access to Clusters\n.\n\n\nPort 22 must be open to your CIDR if you would like to access the master node via SSH.\n\n\nPort 443 must be open to your CIDR if you would like to access Cloudbreak web UI in a browser.\n  \n\n\n\n\n\n\nCluster Host Groups without the Ambari Server\n\n\n\n\n\n\n\n\nInbound Port\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\n22\n\n\nSSH access to the VM instance.\n\n\n\n\n\n\n\n\n\n\nImportant\n\n\n\nBy default, port 22 is set to 0.0.0.0/0 CIDR for inbound access on non-Ambari node security groups. We strongly recommend that you remove it.\n\n\n\n\n\nWhen creating a new security group, Cloudbreak uses the following naming convention: \nclustername\n-ClusterNodeSecurityGroup\nhostgroupname\n \n\n\nIdentity Management\n\n\nTo securely control access to cloud resources, cloud providers use identity management services such as IAM roles (AWS and GCP) and Active Directory (Azure). \n\n\n\n\n\n\n\n\nCloud Provider\n\n\nDocumentation Link\n\n\n\n\n\n\n\n\n\n\nAWS\n\n\nAWS Identity and Access Management (IAM)\n\n\n\n\n\n\nAzure\n\n\nAzure Active Directory ((Azure AD))\n\n\n\n\n\n\nGoogle\n\n\nGoogle Cloud Identity and Access Management (IAM)\n\n\n\n\n\n\nOpenStack\n\n\nKeystone\n\n\n\n\n\n\n\n\nCloudbreak utilizes cloud provider\u2019s identity management services via Cloudbreak credential. After launching Cloudbreak on your chosen cloud provider, you must create a Cloudbreak credential, which allows Cloudbreak to authenticate with your cloud provider identity management service. Only after you have completed this step, Cloudbreak can create resources on your behalf. \n\n\nAuthentication with AWS\n\n\nWhen launching Cloudbreak on AWS, you must select a way for Cloudbreak to authenticate with your AWS account and create resources on your behalf. While key-based authentication uses your AWS access key and secret key, role-based authentication uses IAM roles.\n\n\nIf you are using role-based authentication for Cloudbreak on AWS, you must create two IAM roles: one to grant Cloudbreak access to allow Cloudbreak to assume AWS roles (using the \"AssumeRole\" policy) and the second one to provide Cloudbreak with the capabilities required for cluster creation (using the \"cb-policy\" policy).\n\n\nThe following table provides contextual information about the two roles required: \n\n\n\n\n\n\n\n\nRole\n\n\nPurpose\n\n\nOverview of Steps\n\n\nConfiguration\n\n\n\n\n\n\n\n\n\n\nCloudbreakRole\n\n\nAllows Cloudbreak to assume other IAM roles - specifically the CredentialRole.\n\n\nCreate a role called \"CloudbreakRole\" and attach the \"AssumeRole\" policy. The \"AssumeRole\" policy definition and steps for creating the CloudbreakRole are provided below.\n\n\nWhen launching your Cloudbreak VM, during \nStep 3: Configure Instance Details\n \n \nIAM\n, you will attach the \"CloudbreakRole\" IAM role to the VM.\n\n\n\n\n\n\nCredentialRole\n\n\nAllows Cloudbreak to create AWS resources required for clusters.\n\n\nCreate a new IAM role called \"CredentialRole\" and attach the \"cb-policy\" policy to it. The \"cb-policy\" policy definition and steps for creating the CredentialRole are provided below.\n When creating this role using the AWS Console, make sure that that it is a role for cross-account access and that the trust-relation is set up as follows: 'Account ID' is your own 12-digit AWS account ID and 'External ID' is \u201cprovision-ambari\u201d. See steps below.\n\n\nOnce you log in to the Cloudbreak UI and are ready to create clusters, you will use this role to create the Cloudbreak credential.\n\n\n\n\n\n\n\n\nRelated Links\n\n\nMeet the Prerequisites: Authentication\n  \n\n\nAuthentication with Azure\n\n\nAfter launching Cloudbread on Azure, you are required to create a Cloudbreak credential, which allows Cloudbreak to authenticate with your Azure Active Directory. \n\n\nYou have two options:\n\n\n\n\n\n\nInteractive: The app and service principal creation and role assignment are fully automated, so the only input that you need to provide to Cloudbreak is your Subscription ID and Directory ID. \n\n\n\n\n\n\nApp-based: The app and service principal creation and role assignment are not automated You must create an Azure Active Directory application registration and then provide its parameters to Cloudbreak, in addition to providing your Subscription ID and Directory ID. \n\n\n\n\n\n\nRelated Links\n\n\nCreate Cloudbreak Credential\n  \n\n\nAuthentication with GCP\n\n\nAfter launching Cloudbreak on GCP, you are required to register a service account in Cloudbrak via creating a Cloudbreak credential. Cloudbreak uses this account to authenticate with the GCP identity management service.\n\n\nRelated Links\n\n\nMeet the Prerequisites: Service Account\n  \n\n\nAuthentication with OpenStack\n\n\nAfter launching Cloudbread on OpenStack, you are required to create a Cloudbreak credential, which allows Cloudbreak to authenticate with keystone. \n\n\nRelated Links\n\n\nCreate Cloudbreak Credential", 
            "title": "Security Overview"
        }, 
        {
            "location": "/security/index.html#security-overview", 
            "text": "Cloudbreak utilizes cloud provider security resources such as virtual networks, security groups, and identity and access management:   Network isolation  is achieved via user-configured virtual networks and subnets. \n    Read more about  Virtual Networks .    Network security  is achieved via out-of-the-box security group settings. \n    Read more about  Network Security .     Controlled use of cloud resources  using IAM roles (AWS, GCP) or Active Directory (in case of Azure). \n    Read more about  Identity Management .", 
            "title": "Security Overview"
        }, 
        {
            "location": "/security/index.html#virtual-networks", 
            "text": "Cloud providers use virtual networks which resemble traditional networks. Depending on the options that you selected during deployment, your Cloudbreak instance and clusters are launched into new or existing cloud provider networking infrastructure (virtual networks and subnets). For more information about virtual networks, refer to the cloud-provider documentation:     Cloud Provider  Documentation Link      AWS  Amazon Virtual Private Cloud (Amazon VPC)    Azure  Microsoft Azure Virtual Network    Google Cloud Platform  Virtual Private Cloud (VPC) network    OpenStack  Network", 
            "title": "Virtual Networks"
        }, 
        {
            "location": "/security/index.html#network-security", 
            "text": "Security groups are setup to control network traffic to the instances in the system.", 
            "title": "Network Security"
        }, 
        {
            "location": "/security/index.html#cloudbreak-instance-security-group", 
            "text": "The following table lists the minimum security group port configuration required for the Cloudbreak instance:     Inbound Port  Description      22  SSH access to the Cloudbreak VM.    80  HTTP access to the Cloudbreak UI. This is automatically redirected to the HTTPS (443) port.    443  HTTPS access to the Cloudbreak UI.", 
            "title": "Cloudbreak Instance Security Group"
        }, 
        {
            "location": "/security/index.html#default-cluster-security-groups", 
            "text": "For clusters, Cloudbreak provides the following security group settings. If you do not modify these settings, these default security rules will be created. You can modify these rules either when creating or, if you don't want to use security group, remove them.   As an alternative to creating new security groups, you can select from your existing set of security groups, which can be modified using the cloud provider tools.   Cluster Host Group with Ambari Server     Inbound Port  Description      22  SSH access to the VM instance. This port is also used by Cloudbreak to communicate with the cluster.    443  HTTPS access to the Ambari UI.    9443  Management port, used by Cloudbreak to communicate with the cluster node VM.      Important  \nBy default, ports 22, 443, and 9443 are set to 0.0.0.0/0 CIDR for inbound access on the Ambari node security group. We strongly recommend that you limit this CIDR, considering the following restrictions: Ports 22 and 9443 must be open to Cloudbreak's CIDR. You can set CB_DEFAULT_GATEWAY_CIDR in your Cloudbreak's Profile file in order to automatically open ports 22 and 9443 to your Cloudbrak IP. Refer to  Restricting Inbound Access to Clusters .  Port 22 must be open to your CIDR if you would like to access the master node via SSH.  Port 443 must be open to your CIDR if you would like to access Cloudbreak web UI in a browser.      Cluster Host Groups without the Ambari Server     Inbound Port  Description      22  SSH access to the VM instance.      Important  \nBy default, port 22 is set to 0.0.0.0/0 CIDR for inbound access on non-Ambari node security groups. We strongly recommend that you remove it.   When creating a new security group, Cloudbreak uses the following naming convention:  clustername -ClusterNodeSecurityGroup hostgroupname", 
            "title": "Default Cluster Security Groups"
        }, 
        {
            "location": "/security/index.html#identity-management", 
            "text": "To securely control access to cloud resources, cloud providers use identity management services such as IAM roles (AWS and GCP) and Active Directory (Azure).      Cloud Provider  Documentation Link      AWS  AWS Identity and Access Management (IAM)    Azure  Azure Active Directory ((Azure AD))    Google  Google Cloud Identity and Access Management (IAM)    OpenStack  Keystone     Cloudbreak utilizes cloud provider\u2019s identity management services via Cloudbreak credential. After launching Cloudbreak on your chosen cloud provider, you must create a Cloudbreak credential, which allows Cloudbreak to authenticate with your cloud provider identity management service. Only after you have completed this step, Cloudbreak can create resources on your behalf.", 
            "title": "Identity Management"
        }, 
        {
            "location": "/security/index.html#authentication-with-aws", 
            "text": "When launching Cloudbreak on AWS, you must select a way for Cloudbreak to authenticate with your AWS account and create resources on your behalf. While key-based authentication uses your AWS access key and secret key, role-based authentication uses IAM roles.  If you are using role-based authentication for Cloudbreak on AWS, you must create two IAM roles: one to grant Cloudbreak access to allow Cloudbreak to assume AWS roles (using the \"AssumeRole\" policy) and the second one to provide Cloudbreak with the capabilities required for cluster creation (using the \"cb-policy\" policy).  The following table provides contextual information about the two roles required:      Role  Purpose  Overview of Steps  Configuration      CloudbreakRole  Allows Cloudbreak to assume other IAM roles - specifically the CredentialRole.  Create a role called \"CloudbreakRole\" and attach the \"AssumeRole\" policy. The \"AssumeRole\" policy definition and steps for creating the CloudbreakRole are provided below.  When launching your Cloudbreak VM, during  Step 3: Configure Instance Details     IAM , you will attach the \"CloudbreakRole\" IAM role to the VM.    CredentialRole  Allows Cloudbreak to create AWS resources required for clusters.  Create a new IAM role called \"CredentialRole\" and attach the \"cb-policy\" policy to it. The \"cb-policy\" policy definition and steps for creating the CredentialRole are provided below.  When creating this role using the AWS Console, make sure that that it is a role for cross-account access and that the trust-relation is set up as follows: 'Account ID' is your own 12-digit AWS account ID and 'External ID' is \u201cprovision-ambari\u201d. See steps below.  Once you log in to the Cloudbreak UI and are ready to create clusters, you will use this role to create the Cloudbreak credential.     Related Links  Meet the Prerequisites: Authentication", 
            "title": "Authentication with AWS"
        }, 
        {
            "location": "/security/index.html#authentication-with-azure", 
            "text": "After launching Cloudbread on Azure, you are required to create a Cloudbreak credential, which allows Cloudbreak to authenticate with your Azure Active Directory.   You have two options:    Interactive: The app and service principal creation and role assignment are fully automated, so the only input that you need to provide to Cloudbreak is your Subscription ID and Directory ID.     App-based: The app and service principal creation and role assignment are not automated You must create an Azure Active Directory application registration and then provide its parameters to Cloudbreak, in addition to providing your Subscription ID and Directory ID.     Related Links  Create Cloudbreak Credential", 
            "title": "Authentication with Azure"
        }, 
        {
            "location": "/security/index.html#authentication-with-gcp", 
            "text": "After launching Cloudbreak on GCP, you are required to register a service account in Cloudbrak via creating a Cloudbreak credential. Cloudbreak uses this account to authenticate with the GCP identity management service.  Related Links  Meet the Prerequisites: Service Account", 
            "title": "Authentication with GCP"
        }, 
        {
            "location": "/security/index.html#authentication-with-openstack", 
            "text": "After launching Cloudbread on OpenStack, you are required to create a Cloudbreak credential, which allows Cloudbreak to authenticate with keystone.   Related Links  Create Cloudbreak Credential", 
            "title": "Authentication with OpenStack"
        }, 
        {
            "location": "/security-cb/index.html", 
            "text": "Securing Cloudbreak After Launch\n\n\nCloudbreak comes with default settings designed for easy first experience rather than strict security. To secure Cloudbreak, follow these recommendations. \n\n\nRestricting Inbound Access to Cloudbreak\n\n\nWe recommend that you block all communication ports except 22, 80, and 443 on the Cloudbreak's security group. If you have to log in to the Cloudbreak host remotely, use the SSH port (usually 22).\n\n\nRestricting Inbound Access to Clusters\n\n\nWe recommend that after launching Cloudbreak you set  CB_DEFAULT_GATEWAY_CIDR in your Cloudbreak's Profile file in order to automatically open ports 9443 and 22 to your Cloudbrak IP on your Ambari node security group. \n\n\nSteps\n \n\n\n\n\n\n\nSet CB_DEFAULT_GATEWAY_CIDR to the CIDR address range which is used by Cloudbreak to communicate with the cluster:\n\n\nexport CB_DEFAULT_GATEWAY_CIDR=14.15.16.17/32\n\n\nOr, if your Cloudbreak communicates with the cluster through multiple addresses, set multiple addresses separated with a comma:\n\n\nexport CB_DEFAULT_GATEWAY_CIDR=14.15.16.17/32,18.17.16.15/32\n\n\n\n\n\n\nIf Cloudbreak has already been started, restart it using \ncbd restart\n.     \n\n\n\n\n\n\nWhen CB_DEFAULT_GATEWAY_CIDR is set, two additional rules are added to your Ambari node security group: (1) port 9443 open to your Cloudbrak IP, and (2) port 22 open to your Cloudbrak IP. You can view and edit these default rules in the create cluster wizard. \n\n\n\n\n\n\nSecure the Profile\n\n\nBefore starting Cloudbreak for the first time, configure the Profile file as directed below. Changes are applied during startup so a restart (\ncbd restart\n) is required after each change.\n\n\n\n\n\n\nExecute the following command in the directory where you want to store Cloudbreak-related files:\n\n\n\necho export PUBLIC_IP=[the ip or hostname to bind] \n Profile\n\n\n\n\n\n\n\nAfter you have a base Profile file, add the following custom properties to it:\n\n\n\nexport UAA_DEFAULT_SECRET='[custom secret]'\nexport UAA_DEFAULT_USER_EMAIL='[default admin email address]'\nexport UAA_DEFAULT_USER_PW='[default admin password]'\nexport UAA_DEFAULT_USER_FIRSTNAME='[default admin first name]'\nexport UAA_DEFAULT_USER_LASTNAME='[default admin last name]'\n\n\n\nCloudbreak has additional secrets which by default inherit their values from \nUAA_DEFAULT_SECRET\n. Instead of using the default, you can define different values in the Profile for each of these service clients:\n\n\n\nexport UAA_CLOUDBREAK_SECRET='[cloudbreak secret]'\nexport UAA_PERISCOPE_SECRET='[auto scaling secret]'\nexport UAA_ULUWATU_SECRET='[web ui secret]'\nexport UAA_SULTANS_SECRET='[authenticator secret]'\n\n\n\nYou can change these secrets at any time, except \nUAA_CLOUDBREAK_SECRET\n which is used to encrypt sensitive information at database level. \n\n\nUAA_DEFAULT_USER_PW\n is stored in plain text format, but if \nUAA_DEFAULT_USER_PW\n is missing from the Profile, it gets a default value. Because default password is not an option, if you set an empty password explicitly in the Profile Cloudbreak deployer will ask for password all the time when it is needed for the operation.\n\n\n\nexport UAA_DEFAULT_USER_PW=''\n\n\n\nIn this case, Cloudbreak deployer wouldn't be able to add the default user, so you have to do it manually by executing the following command:\n\n\n\ncbd util add-default-user\n\n\n\n\n\n\n\nFor more information about setting environment variables in Profile, refer to \nConfigure Profile Variables\n.\n\n\nAdd SSL Certificate for Cloudbreak UI\n\n\nBy default Cloudbreak has been configured with a self-signed certificate for access via HTTPS. This is sufficient for many deployments such as trials, development, testing, or staging. However, for production deployments, a trusted certificate is preferred and can be configured in the controller. Follow these steps to configure the cloud controller to use your own trusted certificate. \n\n\nPrerequisites\n\n\nTo use your own certificate, you must have:\n\n\n\n\nA resolvable fully qualified domain name (FQDN) for the controller host IP address. For example, this can be set up in \nAmazon Route 53\n.  \n\n\nA valid SSL certificate for this fully qualified domain name. The certificate can be obtained from a number of certificate providers.  \n\n\n\n\nSteps\n\n\n\n\n\n\nSSH to the Cloudbreak host instance:\n\n\nssh -i mykeypair.pem cloudbreak@[CONTROLLER-IP-ADDRESS]\n\n\n\n\n\n\nMake sure that the target fully qualified domain name (FQDN) which you plan to use for Cloudbreak is resolvable:\n\n\nnslookup [TARGET-CONTROLLER-FQDN]\n\n\nFor example:\n\n\nnslookup hdcloud.example.com\n\n\n\n\n\n\nBrowse to the Cloudbreak deployment directory and edit the \nProfile\n file:\n\n\nvi /var/lib/cloudbreak-deployment/Profile\n\n\n\n\n\n\nReplace the value of the \nPUBLIC_IP\n variable with the \nTARGET-CONTROLLER-FQDN\n value:\n\n\nPUBLIC_IP=[TARGET-CONTROLLER-FQDN]\n\n\n\n\n\n\nCopy your private key and certificate files for the FQDN onto the Cloudbreak host. These files must be placed under \n/var/lib/cloudbreak-deployment/certs/traefik/\n directory.\n\n\n\n\nFile permissions for the private key and certificate files can be set to 600.\n\n\n\n\n\n\n\n\n\n\nFile\n\n\nExample\n\n\n\n\n\n\n\n\n\n\nPRIV-KEY-LOCATION\n\n\n/var/lib/cloudbreak-deployment/certs/traefik/hdcloud.example.com.key\n\n\n\n\n\n\nCERT-LOCATION\n\n\n/var/lib/cloudbreak-deployment/certs/traefik/hdcloud.example.com.crt\n\n\n\n\n\n\n\n\n\n\n\n\nConfigure TLS details in your \nProfile\n by adding the following line at the end of the file.\n\n\n\n\nNotice that \nCERT-LOCATION\n and \nPRIV-KEY-LOCATION\n are file locations from Step 5, starting at the \n/certs/...\n path.\n\n\n\n\nexport CBD_TRAEFIK_TLS=\u201d[CERT-LOCATION],[PRIV-KEY-LOCATION]\u201d\n\n\nFor example:\n\n\nexport CBD_TRAEFIK_TLS=\"/certs/traefik/hdcloud.example.com.crt,/certs/traefik/hdcloud.example.com.key\"\n\n\n\n\n\n\nRestart Cloudbreak deployer:\n\n\ncbd restart\n\n\n\n\n\n\nUsing your web browser, access to the Cloudbreak UI using the new resolvable fully qualified domain name.\n\n\n\n\n\n\nConfirm that the connection is SSL-protected and that the certificate used is the certificate that you provided to Cloudbreak.\n\n\n\n\n\n\nConfigure Access from Custom Domains\n\n\nCloudbreak deployer, which uses UAA as an identity provider, supports multitenancy. In UAA, multitenancy is managed through identity zones. An identity zone is accessed through a unique subdomain. For example, if the standard UAA responds to \nhttps://uaa.10.244.0.34.xip.io\n, a zone on this UAA can be accessed through a unique subdomain \nhttps://testzone1.uaa.10.244.0.34.xip.io\n.\n\n\nIf you want to use a custom domain for your identity or deployment, add the \nUAA_ZONE_DOMAIN\n line to your \nProfile\n:\n\n\nexport UAA_ZONE_DOMAIN=my-subdomain.example.com\n\n\n\nThis variable is necessary for UAA to identify which zone provider should handle the requests that arrive to that domain.", 
            "title": "Configure Basic Security"
        }, 
        {
            "location": "/security-cb/index.html#securing-cloudbreak-after-launch", 
            "text": "Cloudbreak comes with default settings designed for easy first experience rather than strict security. To secure Cloudbreak, follow these recommendations.", 
            "title": "Securing Cloudbreak After Launch"
        }, 
        {
            "location": "/security-cb/index.html#restricting-inbound-access-to-cloudbreak", 
            "text": "We recommend that you block all communication ports except 22, 80, and 443 on the Cloudbreak's security group. If you have to log in to the Cloudbreak host remotely, use the SSH port (usually 22).", 
            "title": "Restricting Inbound Access to Cloudbreak"
        }, 
        {
            "location": "/security-cb/index.html#restricting-inbound-access-to-clusters", 
            "text": "We recommend that after launching Cloudbreak you set  CB_DEFAULT_GATEWAY_CIDR in your Cloudbreak's Profile file in order to automatically open ports 9443 and 22 to your Cloudbrak IP on your Ambari node security group.   Steps      Set CB_DEFAULT_GATEWAY_CIDR to the CIDR address range which is used by Cloudbreak to communicate with the cluster:  export CB_DEFAULT_GATEWAY_CIDR=14.15.16.17/32  Or, if your Cloudbreak communicates with the cluster through multiple addresses, set multiple addresses separated with a comma:  export CB_DEFAULT_GATEWAY_CIDR=14.15.16.17/32,18.17.16.15/32    If Cloudbreak has already been started, restart it using  cbd restart .         When CB_DEFAULT_GATEWAY_CIDR is set, two additional rules are added to your Ambari node security group: (1) port 9443 open to your Cloudbrak IP, and (2) port 22 open to your Cloudbrak IP. You can view and edit these default rules in the create cluster wizard.", 
            "title": "Restricting Inbound Access to Clusters"
        }, 
        {
            "location": "/security-cb/index.html#secure-the-profile", 
            "text": "Before starting Cloudbreak for the first time, configure the Profile file as directed below. Changes are applied during startup so a restart ( cbd restart ) is required after each change.    Execute the following command in the directory where you want to store Cloudbreak-related files:  \necho export PUBLIC_IP=[the ip or hostname to bind]   Profile    After you have a base Profile file, add the following custom properties to it:  \nexport UAA_DEFAULT_SECRET='[custom secret]'\nexport UAA_DEFAULT_USER_EMAIL='[default admin email address]'\nexport UAA_DEFAULT_USER_PW='[default admin password]'\nexport UAA_DEFAULT_USER_FIRSTNAME='[default admin first name]'\nexport UAA_DEFAULT_USER_LASTNAME='[default admin last name]'  Cloudbreak has additional secrets which by default inherit their values from  UAA_DEFAULT_SECRET . Instead of using the default, you can define different values in the Profile for each of these service clients:  \nexport UAA_CLOUDBREAK_SECRET='[cloudbreak secret]'\nexport UAA_PERISCOPE_SECRET='[auto scaling secret]'\nexport UAA_ULUWATU_SECRET='[web ui secret]'\nexport UAA_SULTANS_SECRET='[authenticator secret]'  You can change these secrets at any time, except  UAA_CLOUDBREAK_SECRET  which is used to encrypt sensitive information at database level.   UAA_DEFAULT_USER_PW  is stored in plain text format, but if  UAA_DEFAULT_USER_PW  is missing from the Profile, it gets a default value. Because default password is not an option, if you set an empty password explicitly in the Profile Cloudbreak deployer will ask for password all the time when it is needed for the operation.  \nexport UAA_DEFAULT_USER_PW=''  In this case, Cloudbreak deployer wouldn't be able to add the default user, so you have to do it manually by executing the following command:  \ncbd util add-default-user    For more information about setting environment variables in Profile, refer to  Configure Profile Variables .", 
            "title": "Secure the Profile"
        }, 
        {
            "location": "/security-cb/index.html#add-ssl-certificate-for-cloudbreak-ui", 
            "text": "By default Cloudbreak has been configured with a self-signed certificate for access via HTTPS. This is sufficient for many deployments such as trials, development, testing, or staging. However, for production deployments, a trusted certificate is preferred and can be configured in the controller. Follow these steps to configure the cloud controller to use your own trusted certificate.   Prerequisites  To use your own certificate, you must have:   A resolvable fully qualified domain name (FQDN) for the controller host IP address. For example, this can be set up in  Amazon Route 53 .    A valid SSL certificate for this fully qualified domain name. The certificate can be obtained from a number of certificate providers.     Steps    SSH to the Cloudbreak host instance:  ssh -i mykeypair.pem cloudbreak@[CONTROLLER-IP-ADDRESS]    Make sure that the target fully qualified domain name (FQDN) which you plan to use for Cloudbreak is resolvable:  nslookup [TARGET-CONTROLLER-FQDN]  For example:  nslookup hdcloud.example.com    Browse to the Cloudbreak deployment directory and edit the  Profile  file:  vi /var/lib/cloudbreak-deployment/Profile    Replace the value of the  PUBLIC_IP  variable with the  TARGET-CONTROLLER-FQDN  value:  PUBLIC_IP=[TARGET-CONTROLLER-FQDN]    Copy your private key and certificate files for the FQDN onto the Cloudbreak host. These files must be placed under  /var/lib/cloudbreak-deployment/certs/traefik/  directory.   File permissions for the private key and certificate files can be set to 600.      File  Example      PRIV-KEY-LOCATION  /var/lib/cloudbreak-deployment/certs/traefik/hdcloud.example.com.key    CERT-LOCATION  /var/lib/cloudbreak-deployment/certs/traefik/hdcloud.example.com.crt       Configure TLS details in your  Profile  by adding the following line at the end of the file.   Notice that  CERT-LOCATION  and  PRIV-KEY-LOCATION  are file locations from Step 5, starting at the  /certs/...  path.   export CBD_TRAEFIK_TLS=\u201d[CERT-LOCATION],[PRIV-KEY-LOCATION]\u201d  For example:  export CBD_TRAEFIK_TLS=\"/certs/traefik/hdcloud.example.com.crt,/certs/traefik/hdcloud.example.com.key\"    Restart Cloudbreak deployer:  cbd restart    Using your web browser, access to the Cloudbreak UI using the new resolvable fully qualified domain name.    Confirm that the connection is SSL-protected and that the certificate used is the certificate that you provided to Cloudbreak.", 
            "title": "Add SSL Certificate for Cloudbreak UI"
        }, 
        {
            "location": "/security-cb/index.html#configure-access-from-custom-domains", 
            "text": "Cloudbreak deployer, which uses UAA as an identity provider, supports multitenancy. In UAA, multitenancy is managed through identity zones. An identity zone is accessed through a unique subdomain. For example, if the standard UAA responds to  https://uaa.10.244.0.34.xip.io , a zone on this UAA can be accessed through a unique subdomain  https://testzone1.uaa.10.244.0.34.xip.io .  If you want to use a custom domain for your identity or deployment, add the  UAA_ZONE_DOMAIN  line to your  Profile :  export UAA_ZONE_DOMAIN=my-subdomain.example.com  This variable is necessary for UAA to identify which zone provider should handle the requests that arrive to that domain.", 
            "title": "Configure Access from Custom Domains"
        }, 
        {
            "location": "/security-kerberos/index.html", 
            "text": "Enabling Kerberos Security\n\n\nWhen creating a cluster, you can optionally enable Kerberos security in that cluster and provide your Kerberos configuration details. Cloudbreak will automatically extend your blueprint configuration with the defined properties.\n\n\nKerberos Overview\n\n\nKerberos is a third party authentication mechanism, in which users and services that users wish to access Hadoop rely on a third party - the Kerberos server - to authenticate each to the other. The Kerberos server itself is known as the \nKey Distribution Center\n, or \nKDC\n. At a high level, the KDC has three parts:\n\n\n\n\nA database of the users and services (known as \nprincipals\n) and their respective Kerberos passwords  \n\n\nAn \nAuthentication Server (AS)\n which performs the initial authentication and issues a Ticket Granting Ticket (TGT)  \n\n\nA \nTicket Granting Server (TGS)\n that issues subsequent service tickets based on the initial TGT  \n\n\n\n\nA user principal requests authentication from the AS. The AS returns a TGT that is encrypted using the user principal's Kerberos password, which is known only to the user principal and the AS. The user principal decrypts the TGT locally using its Kerberos password, and from that point forward, until the ticket expires, the user principal can use the TGT to get service tickets from the TGS. Service tickets are what allow the principal to access various services. \n\n\nSince cluster resources (hosts or services) cannot provide a password each time to decrypt the TGT, they use a special file, called a \nkeytab\n, which contains the resource principal authentication credentials. The set of hosts, users, and services over which the Kerberos server has control is called a \nrealm\n.\n\n\nThe following table explains the Kerberos related terminology:\n\n\n\n\n\n\n\n\nTerm\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nKey Distribution Center, or KDC\n\n\nThe trusted source for authentication in a Kerberos-enabled environment.\n\n\n\n\n\n\nKerberos KDC Server\n\n\nThe machine, or server, that serves as the Key Distribution Center (KDC).\n\n\n\n\n\n\nKerberos Client\n\n\nAny machine in the cluster that authenticates against the KDC.\n\n\n\n\n\n\nPrincipal\n\n\nThe unique name of a user or service that authenticates against the KDC.\n\n\n\n\n\n\nKeytab\n\n\nA file that includes one or more principals and their keys.\n\n\n\n\n\n\nRealm\n\n\nThe Kerberos network that includes a KDC and a number of clients.\n\n\n\n\n\n\n\n\nEnabling Kerberos\n\n\nThe option to enable Kerberos is available in the advanced \nSecurity\n section of the create cluster wizard.  \n\n\nYou have the following options for enabling Kerberos in a Cloudbreak  managed cluster:\n\n\n\n\n\n\n\n\nOption\n\n\nDescription\n\n\nEnvironment\n\n\n\n\n\n\n\n\n\n\nUse Existing KDC\n\n\nAllows you to leverage an existing MIT KDC or Active Directory for enabling Kerberos with the cluster.\nYou can either provide the required parameters and Cloudbreak will generate the descriptors on your behalf, or provide the exact Ambari Kerberos descriptors to be injected into your blueprint in JSON format.\n\n\nSuitable for production\n\n\n\n\n\n\nUse Test KDC\n\n\nInstalls a new MIT KDC on the master node and configures the cluster to leverage that KDC.\n\n\nSuitable for evaluation and testing only, not suitable for production\n\n\n\n\n\n\n\n\nUse Existing KDC\n\n\nTo use an existing KDC, in the advanced \nSecurity\n section of the create cluster wizard select \nEnable Kerberos Security\n. By default, \nUse Existing KDC\n option is selected.  \n\n\nYou must provide the following information about your MIT KDC or Active Directory. Based on these parameters, kerberos-env and krb5-conf JSON descriptors for Ambari are generated and injected into your Blueprint:\n\n\n\n\nBefore proceeding with the configuration, you must confirm that you met the requirements by checking the boxes next to all requirements listed. The configuration options are displayed only after you have confirmed all the requirements by checking every box.    \n\n\n\n\n\n\n\n\n\n\nParameter\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nKerberos Admin Principal\n\n\nThe admin principal in your existing MIT KDC or AD.\n\n\n\n\n\n\nKerberos Admin Password\n\n\nThe admin principal password in your existing MIT KDC or AD.\n\n\n\n\n\n\nMIT KDC or Active Directory\n\n\nSelect MIT KDC or Active Directory.\n\n\n\n\n\n\n\n\nUse Basic Configuration\n\n\n\n\n\n\n\n\nParameter\n\n\nRequired if using...\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nKerberos Url\n\n\nMIT, AD\n\n\nIP address or FQDN for the KDC host. Optionally a port number may be included. Example: \"kdc.example1.com:88\" or \"kdc.example1.com\"\n\n\n\n\n\n\nKerberos Admin URL\n\n\nMIT, AD\n\n\n(Optional) IP address or FQDN for the KDC admin host. Optionally a port number may be included. Example: \"kdc.example2.com:88\" or \"kdc.example2.com\"\n\n\n\n\n\n\nKerberos Realm\n\n\nMIT, AD\n\n\nThe default realm to use when creating service principals. Example: \"EXAMPLE.COM\"\n\n\n\n\n\n\nKerberos AD Ldap Url\n\n\nAD\n\n\nThe URL to the Active Directory LDAP Interface. This value must indicate a secure channel using LDAPS since it is required for creating and updating passwords for Active Directory accounts. Example: \"ldaps://ad.example.com:636\"\n\n\n\n\n\n\nKerberos AD Container DN\n\n\nAD\n\n\nThe distinguished name (DN) of the container used store service principals. Example:  \"OU=hadoop,DC=example,DC=com\"\n\n\n\n\n\n\nUse TCP Connection\n\n\nOptional\n\n\nBy default, Kerberos uses UDP. Checkmark this box to use TCP instead.\n\n\n\n\n\n\n\n\nUse Advanced Configuration\n \n\n\nChecking the \nUse Custom Configuration\n option allows you to provide the actual Ambari Kerberos descriptors to be injected into your blueprint (instead of Cloudbreak generating the descriptors on your behalf). This is the most powerful option which gives you full control of the Ambari Kerberos options that are available. You must provide: \n\n\n\n\nKerberos-env JSON Descriptor (required)\n\n\nkrb5-conf JSON Descriptor (optional)\n\n\n\n\nTo learn more about the Ambari Kerberos JSON descriptors, refer to \nApache cwiki\n.  \n\n\nUse Test KDC\n\n\nTo use a test KDC, in the advanced \nSecurity\n section of the create cluster wizard select \nEnable Kerberos Security\n and then select \nUse Test KDC\n.\n\n\n\n\nImportant\n\n\n\nUsing the Test KDC is for evaluation and testing purposes only, and cannot be used for production clusters. To enable Kerberos for production use, you must use the \nUse Existing KDC\n option. \n\n\n\n\n\nYou must provide the following parameters for your new test KDC:  \n\n\n\n\n\n\n\n\nParameter\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nKerberos Master Key\n\n\nThe master key for the KDC database.\n\n\n\n\n\n\nKerberos Admin Username\n\n\nThe admin principal to create that can administer the KDC.\n\n\n\n\n\n\nKerberos Admin Password\n\n\nThe admin principal password.\n\n\n\n\n\n\nConfirm Kerberos Admin Password\n\n\nThe admin principal password.\n\n\n\n\n\n\n\n\nWhen using the test KDC option:\n\n\n\n\nCloudbreak installs an MIT KDC instance on the Ambari server node.  \n\n\nKerberos clients are installed on all cluster nodes, and the krb5.conf is configured to use the MIT KDC.  \n\n\nThe cluster is configured for Kerberos to use the MIT KDC. Very basic Ambari KSON Kerberos descriptors are generated and used accordingly.\n\n\n\n\nExample kerberos-env JSON descriptor file:\n\n\n{\n      \"kerberos-env\" : {\n        \"properties\" : {\n          \"kdc_type\" : \"mit-kdc\",\n          \"kdc_hosts\" : \"ip-10-0-121-81.ec2.internal\",\n          \"realm\" : \"EC2.INTERNAL\",\n          \"encryption_types\" : \"aes des3-cbc-sha1 rc4 des-cbc-md5\",\n          \"ldap_url\" : \"\",\n          \"admin_server_host\" : \"ip-10-0-121-81.ec2.internal\",\n          \"container_dn\" : \"\"\n        }\n      }\n    }\n\n\n\nExample krb5-conf JSON  descriptor file: \n\n\n {\n      \"krb5-conf\" : {\n        \"properties\" : {\n          \"domains\" : \".ec2.internal\",\n          \"manage_krb5_conf\" : \"true\"\n        }\n      }\n    }\n\n\n\nRelated Links\n \n\n\nApache cwiki", 
            "title": "Enable Kerberos Security"
        }, 
        {
            "location": "/security-kerberos/index.html#enabling-kerberos-security", 
            "text": "When creating a cluster, you can optionally enable Kerberos security in that cluster and provide your Kerberos configuration details. Cloudbreak will automatically extend your blueprint configuration with the defined properties.", 
            "title": "Enabling Kerberos Security"
        }, 
        {
            "location": "/security-kerberos/index.html#kerberos-overview", 
            "text": "Kerberos is a third party authentication mechanism, in which users and services that users wish to access Hadoop rely on a third party - the Kerberos server - to authenticate each to the other. The Kerberos server itself is known as the  Key Distribution Center , or  KDC . At a high level, the KDC has three parts:   A database of the users and services (known as  principals ) and their respective Kerberos passwords    An  Authentication Server (AS)  which performs the initial authentication and issues a Ticket Granting Ticket (TGT)    A  Ticket Granting Server (TGS)  that issues subsequent service tickets based on the initial TGT     A user principal requests authentication from the AS. The AS returns a TGT that is encrypted using the user principal's Kerberos password, which is known only to the user principal and the AS. The user principal decrypts the TGT locally using its Kerberos password, and from that point forward, until the ticket expires, the user principal can use the TGT to get service tickets from the TGS. Service tickets are what allow the principal to access various services.   Since cluster resources (hosts or services) cannot provide a password each time to decrypt the TGT, they use a special file, called a  keytab , which contains the resource principal authentication credentials. The set of hosts, users, and services over which the Kerberos server has control is called a  realm .  The following table explains the Kerberos related terminology:     Term  Description      Key Distribution Center, or KDC  The trusted source for authentication in a Kerberos-enabled environment.    Kerberos KDC Server  The machine, or server, that serves as the Key Distribution Center (KDC).    Kerberos Client  Any machine in the cluster that authenticates against the KDC.    Principal  The unique name of a user or service that authenticates against the KDC.    Keytab  A file that includes one or more principals and their keys.    Realm  The Kerberos network that includes a KDC and a number of clients.", 
            "title": "Kerberos Overview"
        }, 
        {
            "location": "/security-kerberos/index.html#enabling-kerberos", 
            "text": "The option to enable Kerberos is available in the advanced  Security  section of the create cluster wizard.    You have the following options for enabling Kerberos in a Cloudbreak  managed cluster:     Option  Description  Environment      Use Existing KDC  Allows you to leverage an existing MIT KDC or Active Directory for enabling Kerberos with the cluster. You can either provide the required parameters and Cloudbreak will generate the descriptors on your behalf, or provide the exact Ambari Kerberos descriptors to be injected into your blueprint in JSON format.  Suitable for production    Use Test KDC  Installs a new MIT KDC on the master node and configures the cluster to leverage that KDC.  Suitable for evaluation and testing only, not suitable for production", 
            "title": "Enabling Kerberos"
        }, 
        {
            "location": "/security-kerberos/index.html#use-existing-kdc", 
            "text": "To use an existing KDC, in the advanced  Security  section of the create cluster wizard select  Enable Kerberos Security . By default,  Use Existing KDC  option is selected.    You must provide the following information about your MIT KDC or Active Directory. Based on these parameters, kerberos-env and krb5-conf JSON descriptors for Ambari are generated and injected into your Blueprint:   Before proceeding with the configuration, you must confirm that you met the requirements by checking the boxes next to all requirements listed. The configuration options are displayed only after you have confirmed all the requirements by checking every box.          Parameter  Description      Kerberos Admin Principal  The admin principal in your existing MIT KDC or AD.    Kerberos Admin Password  The admin principal password in your existing MIT KDC or AD.    MIT KDC or Active Directory  Select MIT KDC or Active Directory.     Use Basic Configuration     Parameter  Required if using...  Description      Kerberos Url  MIT, AD  IP address or FQDN for the KDC host. Optionally a port number may be included. Example: \"kdc.example1.com:88\" or \"kdc.example1.com\"    Kerberos Admin URL  MIT, AD  (Optional) IP address or FQDN for the KDC admin host. Optionally a port number may be included. Example: \"kdc.example2.com:88\" or \"kdc.example2.com\"    Kerberos Realm  MIT, AD  The default realm to use when creating service principals. Example: \"EXAMPLE.COM\"    Kerberos AD Ldap Url  AD  The URL to the Active Directory LDAP Interface. This value must indicate a secure channel using LDAPS since it is required for creating and updating passwords for Active Directory accounts. Example: \"ldaps://ad.example.com:636\"    Kerberos AD Container DN  AD  The distinguished name (DN) of the container used store service principals. Example:  \"OU=hadoop,DC=example,DC=com\"    Use TCP Connection  Optional  By default, Kerberos uses UDP. Checkmark this box to use TCP instead.     Use Advanced Configuration    Checking the  Use Custom Configuration  option allows you to provide the actual Ambari Kerberos descriptors to be injected into your blueprint (instead of Cloudbreak generating the descriptors on your behalf). This is the most powerful option which gives you full control of the Ambari Kerberos options that are available. You must provide:    Kerberos-env JSON Descriptor (required)  krb5-conf JSON Descriptor (optional)   To learn more about the Ambari Kerberos JSON descriptors, refer to  Apache cwiki .", 
            "title": "Use Existing KDC"
        }, 
        {
            "location": "/security-kerberos/index.html#use-test-kdc", 
            "text": "To use a test KDC, in the advanced  Security  section of the create cluster wizard select  Enable Kerberos Security  and then select  Use Test KDC .   Important  \nUsing the Test KDC is for evaluation and testing purposes only, and cannot be used for production clusters. To enable Kerberos for production use, you must use the  Use Existing KDC  option.    You must provide the following parameters for your new test KDC:       Parameter  Description      Kerberos Master Key  The master key for the KDC database.    Kerberos Admin Username  The admin principal to create that can administer the KDC.    Kerberos Admin Password  The admin principal password.    Confirm Kerberos Admin Password  The admin principal password.     When using the test KDC option:   Cloudbreak installs an MIT KDC instance on the Ambari server node.    Kerberos clients are installed on all cluster nodes, and the krb5.conf is configured to use the MIT KDC.    The cluster is configured for Kerberos to use the MIT KDC. Very basic Ambari KSON Kerberos descriptors are generated and used accordingly.   Example kerberos-env JSON descriptor file:  {\n      \"kerberos-env\" : {\n        \"properties\" : {\n          \"kdc_type\" : \"mit-kdc\",\n          \"kdc_hosts\" : \"ip-10-0-121-81.ec2.internal\",\n          \"realm\" : \"EC2.INTERNAL\",\n          \"encryption_types\" : \"aes des3-cbc-sha1 rc4 des-cbc-md5\",\n          \"ldap_url\" : \"\",\n          \"admin_server_host\" : \"ip-10-0-121-81.ec2.internal\",\n          \"container_dn\" : \"\"\n        }\n      }\n    }  Example krb5-conf JSON  descriptor file:    {\n      \"krb5-conf\" : {\n        \"properties\" : {\n          \"domains\" : \".ec2.internal\",\n          \"manage_krb5_conf\" : \"true\"\n        }\n      }\n    }  Related Links    Apache cwiki", 
            "title": "Use Test KDC"
        }, 
        {
            "location": "/cli-install/index.html", 
            "text": "Installing Cloudbreak CLI\n\n\nThe Cloudbreak Command Line Interface (CLI) is a tool to help you manage your Cloudbreak cluster instances. This tool can be used to interact with Cloudbreak for automating cluster creation, management, monitoring, and termination. \n\n\nThe CLI is available for Linux, Mac OS X, and Windows. \n\n\nInstall the CLI\n\n\nAfter you have launched Cloudbreak, the CLI is available for download from that Cloudbreak instance.\n\n\nSteps\n\n\n\n\nBrowse to your Cloudbreak instance and log in to the Cloubdreak web UI.  \n\n\nSelect \nDownload CLI\n from the navigation pane. \n\n\nSelect your operating system. The CLI is available for Linux, Mac OS X, and Windows:\n\n    \n  \n\n\nDownload the selected bundle to your local machine.  \n\n\nExtract the bundle.  \n\n\nYou can optionally add \ncb\n to your system path.\n\n\n\n\nRun the executable to verify the CLI: \n\n\ncb --version\n\n\n\n\n\n\nConfigure the CLI\n\n\nOnce you have installed the CLI, you need to configure the CLI to work with Cloudbreak.\n\n\nSteps\n\n\n\n\n\n\nUse the \ncb configure\n command to set up the CLI configuration file. The configuration options are:  \n\n\n\n\n--server\n server address [$CB_SERVER_ADDRESS]  \n\n\n--username\n user name (e-mail address) [$CB_USER_NAME]  \n\n\n--password\n password [$CB_PASSWORD]  \n\n\n\n\nThe password configuration is optional. If you do not provide the password, no password is stored in the CLI configuration file. Therefore, you will need to provide the password with each command you execute or via an environment variable.\n\n\nFor example:\n\n\ncb configure --server https://ec2-11-111-111-11.compute-1.amazonaws.com --username admin@hortonworks.com\n\n\n\n\n\n\nThe CLI configuration file will be saved at \n~/.cb/config\n. The content will look similar to the following:\n\n\ndefault:\n  username: admin@hortonworks.com\n  server: https://ec2-11-111-111-11.compute-1.amazonaws.com\n\n\n\n\n\n\nRun any command to verify that you can connect to the Cloudbreak instance via CLI. For example:\n\n\ncb list-clusters\n  \n\n\n\n\n\n\n\n    \nConfiguration Precedence\n\n    \n\n    The CLI can look for configuration options from different locations. You can optionally\n    pass the configuration options on each command or from environment variables. The following\n    order is used for the CLI to look for configuration options: \nCommand Line\n, \nEnvironment Variables\n\n    and the \nConfiguration File\n.\n    \n\n\n\n\n\nAdd Multiple Configurations\n\n\nIf you are using multiple profiles for multiple environments, you can configure them using the \ncb configure\n command and passing the name of your environment-specific profile file using the \n--profile\n parameter. After running the command, the configuration will be added as a new entry to the \nconfig\n file. For example, running the following command \ncb configure --server https://192.167.65.4 --username test@hortonworks.com --profile staging\n will add the \"staging\" entry:\n\n\ndefault:\n  username: admin@hortonworks.com\n  server: https://192.167.65.4\nstaging:\n  username: test@hortonworks.com\n  server: https://192.167.65.4  \n\n\n\n\nFor example:\n\n\n#cb configure --server https://192.167.65.4 --username test@hortonworks.com --profile staging\nINFO:  [writeConfigToFile] dir already exists: /Users/rkovacs/.cb\nINFO:  [writeConfigToFile] writing credentials to file: /Users/rkovacs/.cb/config\n# cat /Users/rkovacs/.cb/config\ndefault:\n  username: admin@example.com\n  server: https://192.167.65.4\n  output: table\nstaging:\n  username: test@hortonworks.com\n  server: https://192.167.65.4\n\n\n\nConfigure Default Output\n\n\nBy default, JSON format is used in command output. For example, if you run \ncb list-clusters\n without specifying output type, the output will be JSON. If you would like to change default output, add it to the config file. For example:\n\n\ndefault:\n  username: admin@hortonworks.com\n  server: https://192.167.65.4\n  output: table\n\n\n\n\n\nNext: Get Started with CLI", 
            "title": "Install the CLI"
        }, 
        {
            "location": "/cli-install/index.html#installing-cloudbreak-cli", 
            "text": "The Cloudbreak Command Line Interface (CLI) is a tool to help you manage your Cloudbreak cluster instances. This tool can be used to interact with Cloudbreak for automating cluster creation, management, monitoring, and termination.   The CLI is available for Linux, Mac OS X, and Windows.", 
            "title": "Installing Cloudbreak CLI"
        }, 
        {
            "location": "/cli-install/index.html#install-the-cli", 
            "text": "After you have launched Cloudbreak, the CLI is available for download from that Cloudbreak instance.  Steps   Browse to your Cloudbreak instance and log in to the Cloubdreak web UI.    Select  Download CLI  from the navigation pane.   Select your operating system. The CLI is available for Linux, Mac OS X, and Windows: \n         Download the selected bundle to your local machine.    Extract the bundle.    You can optionally add  cb  to your system path.   Run the executable to verify the CLI:   cb --version", 
            "title": "Install the CLI"
        }, 
        {
            "location": "/cli-install/index.html#configure-the-cli", 
            "text": "Once you have installed the CLI, you need to configure the CLI to work with Cloudbreak.  Steps    Use the  cb configure  command to set up the CLI configuration file. The configuration options are:     --server  server address [$CB_SERVER_ADDRESS]    --username  user name (e-mail address) [$CB_USER_NAME]    --password  password [$CB_PASSWORD]     The password configuration is optional. If you do not provide the password, no password is stored in the CLI configuration file. Therefore, you will need to provide the password with each command you execute or via an environment variable.  For example:  cb configure --server https://ec2-11-111-111-11.compute-1.amazonaws.com --username admin@hortonworks.com    The CLI configuration file will be saved at  ~/.cb/config . The content will look similar to the following:  default:\n  username: admin@hortonworks.com\n  server: https://ec2-11-111-111-11.compute-1.amazonaws.com    Run any command to verify that you can connect to the Cloudbreak instance via CLI. For example:  cb list-clusters       \n     Configuration Precedence \n     \n    The CLI can look for configuration options from different locations. You can optionally\n    pass the configuration options on each command or from environment variables. The following\n    order is used for the CLI to look for configuration options:  Command Line ,  Environment Variables \n    and the  Configuration File .", 
            "title": "Configure the CLI"
        }, 
        {
            "location": "/cli-install/index.html#add-multiple-configurations", 
            "text": "If you are using multiple profiles for multiple environments, you can configure them using the  cb configure  command and passing the name of your environment-specific profile file using the  --profile  parameter. After running the command, the configuration will be added as a new entry to the  config  file. For example, running the following command  cb configure --server https://192.167.65.4 --username test@hortonworks.com --profile staging  will add the \"staging\" entry:  default:\n  username: admin@hortonworks.com\n  server: https://192.167.65.4\nstaging:\n  username: test@hortonworks.com\n  server: https://192.167.65.4    For example:  #cb configure --server https://192.167.65.4 --username test@hortonworks.com --profile staging\nINFO:  [writeConfigToFile] dir already exists: /Users/rkovacs/.cb\nINFO:  [writeConfigToFile] writing credentials to file: /Users/rkovacs/.cb/config\n# cat /Users/rkovacs/.cb/config\ndefault:\n  username: admin@example.com\n  server: https://192.167.65.4\n  output: table\nstaging:\n  username: test@hortonworks.com\n  server: https://192.167.65.4", 
            "title": "Add Multiple Configurations"
        }, 
        {
            "location": "/cli-install/index.html#configure-default-output", 
            "text": "By default, JSON format is used in command output. For example, if you run  cb list-clusters  without specifying output type, the output will be JSON. If you would like to change default output, add it to the config file. For example:  default:\n  username: admin@hortonworks.com\n  server: https://192.167.65.4\n  output: table   Next: Get Started with CLI", 
            "title": "Configure Default Output"
        }, 
        {
            "location": "/cli-get-started/index.html", 
            "text": "Getting Started with the CLI\n\n\nGet Started with the CLI\n\n\nAfter \ninstalling\n and \nconfiguring\n the CLI, you can use it to perform the same tasks as are available in the Cloudbreak UI: create and manage clusters, credentials, blueprints, and recipes.\n\n\nSteps\n\n\n\n\n\n\nBefore you start using the CLI, familiarize yourself with Cloudbreak concepts and the Cloudbreak web UI. \n\n\n\n\n\n\nIf you haven't already, create at least one Cloudbreak credential by using Cloudbreak UI or the \ncredential create\n command. \n\n\n\n\n\n\nTo create a cluster, you must first generate a JSON skeleton. Although it is possible to generate it by using the \ncluster generate-template\n command, it is easiest to obtain it from the Cloudbreak UI, as described in \nObtain Cluster JSON Template from the UI\n.\n\n\n\n\n\n\nSave the template in the JSON format and edit it if needed.\n\n\n\n\n\n\nOnce your JSON file is ready, you can use it to create a cluster via the \ncluster create\n command.\n\n\n\n\n\n\nOnce your cluster is running, use can use the CLI to manage and monitor your cluster. For a full list of commands, refer to \nCLI Reference\n.    \n\n\n\n\n\n\nObtain Cluster JSON Template from the UI\n\n\nThe simplest way to obtain a valid JSON template for your cluster is to get it from the Cloudbreak UI. You can do this in two ways:\n\n\nFrom Create Cluster\n\n\nOnce you've provided all the cluster parameters, on the last page of the create cluster wizard, click \nShow CLI Command\n to obtain the JSON template:\n\n\n    \n\n\nClick \nCopy the JSON\n to copy the content and then use a text editor to edit and save it. \n\n\nFrom Cluster Details\n\n\nYou can obtain the JSON template for a cluster from the cluster details page by selecting \nActions\n \n \nShow CLI Command\n. This option is available for all clusters that have been initiated, so the cluster does not need to be in the running state to obtain this information. In fact, this option is useful when troubleshooting cluster failures.  \n\n\n   \n\n\nClick \nCopy the JSON\n to copy the content and then use a text editor to edit and save it. \n\n\n \n\n\nObtain CLI Command from the UI\n\n\nCloudbreak web UI includes an option in the UI which allows you to generate the  \ncreate\n command for resources such as credentials, blueprints, clusters, and recipes. This option is available when creating a resource and for existing resources, from the resource details page.   \n\n\nFrom Create Resource\n\n\nWhen creating a resource (credential, blueprint, cluster, or recipe), provide all information and then click \nShow CLI Command\n. The UI will display the \ncreate\n CLI command for the resource.\n\n\nFrom Resource Details\n\n\nNavigate to credential, blueprint, cluster, or recipe details and  click \nShow CLI Command\n. The UI will display the \ncreate\n CLI command for the resource.\n\n\nGet Help\n\n\nTo get CLI help, you can add help to the end of a command. The following will list help for the CLI at the top-level:\n\n\ncb --help\n\n\n\nor \n\n\ncb --h\n\n\n\nThe following will list help for the create-cluster command, including its command options and global options:\n\n\ncb cluster --help\n\n\n\nor\n\n\ncb cluster --h\n\n\n\n\n\nNext: CLI Reference", 
            "title": "Get Started with the CLI"
        }, 
        {
            "location": "/cli-get-started/index.html#getting-started-with-the-cli", 
            "text": "", 
            "title": "Getting Started with the CLI"
        }, 
        {
            "location": "/cli-get-started/index.html#get-started-with-the-cli", 
            "text": "After  installing  and  configuring  the CLI, you can use it to perform the same tasks as are available in the Cloudbreak UI: create and manage clusters, credentials, blueprints, and recipes.  Steps    Before you start using the CLI, familiarize yourself with Cloudbreak concepts and the Cloudbreak web UI.     If you haven't already, create at least one Cloudbreak credential by using Cloudbreak UI or the  credential create  command.     To create a cluster, you must first generate a JSON skeleton. Although it is possible to generate it by using the  cluster generate-template  command, it is easiest to obtain it from the Cloudbreak UI, as described in  Obtain Cluster JSON Template from the UI .    Save the template in the JSON format and edit it if needed.    Once your JSON file is ready, you can use it to create a cluster via the  cluster create  command.    Once your cluster is running, use can use the CLI to manage and monitor your cluster. For a full list of commands, refer to  CLI Reference .", 
            "title": "Get Started with the CLI"
        }, 
        {
            "location": "/cli-get-started/index.html#obtain-cluster-json-template-from-the-ui", 
            "text": "The simplest way to obtain a valid JSON template for your cluster is to get it from the Cloudbreak UI. You can do this in two ways:  From Create Cluster  Once you've provided all the cluster parameters, on the last page of the create cluster wizard, click  Show CLI Command  to obtain the JSON template:        Click  Copy the JSON  to copy the content and then use a text editor to edit and save it.   From Cluster Details  You can obtain the JSON template for a cluster from the cluster details page by selecting  Actions     Show CLI Command . This option is available for all clusters that have been initiated, so the cluster does not need to be in the running state to obtain this information. In fact, this option is useful when troubleshooting cluster failures.         Click  Copy the JSON  to copy the content and then use a text editor to edit and save it.", 
            "title": "Obtain Cluster JSON Template from the UI"
        }, 
        {
            "location": "/cli-get-started/index.html#obtain-cli-command-from-the-ui", 
            "text": "Cloudbreak web UI includes an option in the UI which allows you to generate the   create  command for resources such as credentials, blueprints, clusters, and recipes. This option is available when creating a resource and for existing resources, from the resource details page.     From Create Resource  When creating a resource (credential, blueprint, cluster, or recipe), provide all information and then click  Show CLI Command . The UI will display the  create  CLI command for the resource.  From Resource Details  Navigate to credential, blueprint, cluster, or recipe details and  click  Show CLI Command . The UI will display the  create  CLI command for the resource.", 
            "title": "Obtain CLI Command from the UI"
        }, 
        {
            "location": "/cli-get-started/index.html#get-help", 
            "text": "To get CLI help, you can add help to the end of a command. The following will list help for the CLI at the top-level:  cb --help  or   cb --h  The following will list help for the create-cluster command, including its command options and global options:  cb cluster --help  or  cb cluster --h   Next: CLI Reference", 
            "title": "Get Help"
        }, 
        {
            "location": "/cli-reference/index.html", 
            "text": "Cloudbreak CLI Reference\n\n\nThis section will help you get started with the Cloudbreak CLI after you have \ninstalled and configured it\n.\n\n\nCommand Structure\n\n\nThe CLI command can contain multiple parts. The first part is a set of global options. The next part is the command. The next part is a set of command options and arguments which could include sub-commands.\n\n\ncb [global options] command [command options] [arguments...]\n\n\n\nCommand Output\n\n\nYou can control the output from the CLI using the --output argument. The possible output formats include:\n\n\n\n\nJSON (\njson\n)\n\n\nYAML (\nyaml\n)\n\n\nFormatted table (\ntable\n)\n\n\n\n\nFor example:\n\n\ncb cluster list --output json\n\n\n\ncb clusters list --output yaml\n\n\n\ncb cluster list --output table\n\n\n\nCommands\n\n\nConfigure CLI:  \n\n\n\n\nconfigure\n  \n\n\n\n\nCloud Provider:\n\n\n\n\ncloud availability-zones\n  \n\n\ncloud regions\n       \n\n\ncloud volumes\n   \n\n\ncloud instances\n   \n\n\n\n\nCredential:  \n\n\n\n\ncredential create\n   \n\n\ncredential delete\n        \n\n\ncredential describe\n        \n\n\ncredential list\n         \n\n\n\n\nBlueprint:   \n\n\n\n\nblueprint create\n  \n\n\nblueprint delete\n           \n\n\nblueprint describe\n         \n\n\nblueprint list\n           \n\n\n\n\nCluster: \n\n\n\n\ncluster change-ambari-password\n   \n\n\ncluster create\n \n\n\ncluster delete\n              \n\n\ncluster describe\n    \n\n\ncluster generate-template\n   \n\n\ncluster generate-reinstall-template\n  \n\n\ncluster list\n    \n\n\ncluster repair\n             \n\n\ncluster scale\n      \n\n\ncluster start\n        \n\n\ncluster stop\n              \n\n\ncluster sync\n \n\n\n\n\nImage Catalog\n\n\n\n\nimagecatalog create\n     \n\n\nimagecatalog delete\n \n\n\nimagecatalog images\n                      \n\n\nimagecatalog list\n  \n\n\nimagecatalog set-default\n            \n\n\n\n\nRecipe:  \n\n\n\n\nrecipe create\n     \n\n\nrecipe delete\n            \n\n\nrecipe describe\n            \n\n\nrecipe list\n              \n\n\n\n\n\n\nblueprint create\n\n\nAdds a new blueprint from a file or from a URL.\n\n\nSub-commands\n\n\nfrom-url\n Creates a blueprint by downloading it from a URL location\n\n\nfrom-file\n Creates a blueprint by reading it from a local file\n\n\nRequired Options\n\n\nfrom-url\n \n\n\n--name \nvalue\n Name of the blueprint\n\n\n--url \nvalue\n URL location of the Ambari blueprint JSON file\n\n\nfrom-file\n \n\n\n--name \nvalue\n Name of the blueprint\n\n\n--file \nvalue\n Location of the Ambari blueprint JSON file on the local machine\n\n\nOptions\n\n\n--description \nvalue\n  Description of the resource\n\n\n--public\n  Public in account\n\n\n--server \nvalue\n Server address [$CB_SERVER_ADDRESS]\n\n\n--username \nvalue\n User name (e-mail address) [$CB_USER_NAME]\n\n\n--password \nvalue\n Password [$CB_PASSWORD]\n\n\n--profile \nvalue\n Selects a config profile to use [$CB_PROFILE] \n\n\nExamples\n\n\nAdds a blueprint from a URL:\n\n\ncb blueprint create from-url --url https://someurl.com/test.bp --name test1\n\n\n\nAdds a blueprint from a local file:\n\n\ncb blueprint create from-file --file /Users/test/Documents/blueprints/test.bp --name test2\n\n\n\nRelated Links\n\n\nBlueprints\n\n\n\n\nblueprint delete\n\n\nDeletes an existing blueprint.\n\n\nRequired Options\n\n\n--name \nvalue\n Name of the blueprint \n\n\nOptions\n\n\n--server \nvalue\n Server address [$CB_SERVER_ADDRESS]\n\n\n--username \nvalue\n User name (e-mail address) [$CB_USER_NAME]\n\n\n--password \nvalue\n Password [$CB_PASSWORD]\n\n\n--profile \nvalue\n Selects a config profile to use [$CB_PROFILE] \n\n\nExamples\n\n\ncb blueprint delete --name \"testbp\"\n\n\n\n\n\nblueprint describe\n\n\nDescribes an existing blueprint.\n\n\nRequired Options\n\n\n--name \nvalue\n Name of the blueprint \n\n\nOptions\n\n\n--server \nvalue\n Server address [$CB_SERVER_ADDRESS]\n\n\n--username \nvalue\n User name (e-mail address) [$CB_USER_NAME]\n\n\n--password \nvalue\n Password [$CB_PASSWORD]\n\n\n--profile \nvalue\n Selects a config profile to use [$CB_PROFILE]\n\n\n--output \nvalue\n  Supported formats: json, yaml, table (default: \"json\") [$CB_OUT_FORMAT]\n\n\nExamples\n\n\ncb blueprint describe --name \"Data Science: Apache Spark 2.1, Apache Zeppelin 0.7.0\"\n{\n  \"Name\": \"Data Science: Apache Spark 2.1, Apache Zeppelin 0.7.0\",\n  \"Description\": \"Data Science: Apache Spark 2.1, Apache Zeppelin 0.7.0\",\n  \"HDPVersion\": \"2.6\",\n  \"HostgroupCount\": \"3\",\n  \"Tags\": \"DEFAULT\"\n}\n\n\n\n\n\nblueprint list\n\n\nLists available blueprints.\n\n\nRequired Options\n\n\nNome\n\n\nOptions\n\n\n--server \nvalue\n Server address [$CB_SERVER_ADDRESS]\n\n\n--username \nvalue\n User name (e-mail address) [$CB_USER_NAME]\n\n\n--password \nvalue\n Password [$CB_PASSWORD]\n\n\n--profile \nvalue\n Selects a config profile to use [$CB_PROFILE]\n\n\n--output \nvalue\n Supported formats: json, yaml, table (default: \"json\") [$CB_OUT_FORMAT]  \n\n\nExamples\n\n\ncb blueprint list\n[\n  {\n    \"Name\": \"Data Science: Apache Spark 2.1, Apache Zeppelin 0.7.0\",\n    \"Description\": \"Data Science: Apache Spark 2.1, Apache Zeppelin 0.7.0\",\n    \"HDPVersion\": \"2.6\",\n    \"HostgroupCount\": \"3\",\n    \"Tags\": \"DEFAULT\"\n  },\n  {\n    \"Name\": \"EDW-ETL: Apache Hive 1.2.1, Apache Spark 2.1\",\n    \"Description\": \"EDW-ETL: Apache Hive 1.2.1, Apache Spark 2.1\",\n    \"HDPVersion\": \"2.6\",\n    \"HostgroupCount\": \"3\",\n    \"Tags\": \"DEFAULT\"\n  },\n  {\n    \"Name\": \"26EDW-ETL: Apache Hive 1.2.1, Apache Spark 1.6\",\n    \"Description\": \"26EDW-ETL: Apache Hive 1.2.1, Apache Spark 1.6\",\n    \"HDPVersion\": \"2.6\",\n    \"HostgroupCount\": \"3\",\n    \"Tags\": \"DEFAULT\"\n  },\n  {\n    \"Name\": \"Data Science: Apache Spark 1.6, Apache Zeppelin 0.7.0\",\n    \"Description\": \"Data Science: Apache Spark 1.6, Apache Zeppelin 0.7.0\",\n    \"HDPVersion\": \"2.6\",\n    \"HostgroupCount\": \"3\",\n    \"Tags\": \"DEFAULT\"\n  },\n  {\n    \"Name\": \"BI: Druid 0.9.2 (Technical Preview)\",\n    \"Description\": \"BI: Druid 0.9.2 (Technical Preview)\",\n    \"HDPVersion\": \"2.6\",\n    \"HostgroupCount\": \"3\",\n    \"Tags\": \"DEFAULT\"\n  },\n  {\n    \"Name\": \"EDW-Analytics: Apache Hive 2 LLAP, Apache Zeppelin 0.7.0\",\n    \"Description\": \"EDW-Analytics: Apache Hive 2 LLAP, Apache Zeppelin 0.7.0\",\n    \"HDPVersion\": \"2.6\",\n    \"HostgroupCount\": \"3\",\n    \"Tags\": \"DEFAULT\"\n  },\n  {\n    \"Name\": \"EDW-ETL: Apache Hive 1.2.1, Apache Spark 1.6\",\n    \"Description\": \"EDW-ETL: Apache Hive 1.2.1, Apache Spark 1.6\",\n    \"HDPVersion\": \"2.6\",\n    \"HostgroupCount\": \"3\",\n    \"Tags\": \"DEFAULT\"\n  }\n]\n\n\n\n\n\ncloud availability-zones\n\n\nLists the available availability zones in a region. \n\n\nRequired Options\n\n\n--credential \nvalue\n  Name of the credential\n\n\n--region \nvalue\n   Name of the region  \n\n\nOptions\n\n\n--server \nvalue\n  Server address [$CB_SERVER_ADDRESS]\n\n\n--username \nvalue\n  User name (e-mail address) [$CB_USER_NAME]\n\n\n--password \nvalue\n Password [$CB_PASSWORD] \n\n\n--profile \nvalue\n  Selects a config profile to use [$CB_PROFILE] \n\n\n--output \nvalue\n  Supported formats: json, yaml, table (default: \"json\") [$CB_OUT_FORMAT]  \n\n\nExamples\n\n\nLists availability zones in the us-west-2 (Oregon) region on the AWS account identified by the credential called \"aws-cred\":\n\n\ncb cloud availability-zones --credential aws-cred --region us-west-2\n[\n  {\n    \"Name\": \"us-west-2a\"\n  },\n  {\n    \"Name\": \"us-west-2b\"\n  },\n  {\n    \"Name\": \"us-west-2c\"\n  }\n]\n\n\n\n\n\ncloud regions\n\n\nLists available regions. \n\n\nRequired Options\n\n\n--credential \nvalue\n  Name of the credential  \n\n\nOptions\n\n\n--server \nvalue\n  Server address [$CB_SERVER_ADDRESS]\n\n\n--username \nvalue\n  User name (e-mail address) [$CB_USER_NAME]\n\n\n--password \nvalue\n Password [$CB_PASSWORD] \n\n\n--profile \nvalue\n  Selects a config profile to use [$CB_PROFILE] \n\n\n--output \nvalue\n  Supported formats: json, yaml, table (default: \"json\") [$CB_OUT_FORMAT]  \n\n\nExamples\n\n\nLists regions available on the AWS account identified by the credential called \"aws-cred\":\n\n\ncb cloud regions --credential aws-cred\n[\n  {\n    \"Name\": \"ap-northeast-1\",\n    \"Description\": \"Asia Pacific (Tokyo)\"\n  },\n  {\n    \"Name\": \"ap-northeast-2\",\n    \"Description\": \"Asia Pacific (Seoul)\"\n  },\n  ...\n\n\n\n\n\ncloud volumes\n\n\nLists the available volume types. \n\n\nSub-commands\n\n\naws\n     Lists the available aws volume types\n\n\nazure\n   Lists the available azure volume types\n\n\ngcp\n     Lists the available gcp volume types  \n\n\nRequired Options\n\n\nNone\n\n\nOptions\n\n\n--server \nvalue\n  Server address [$CB_SERVER_ADDRESS]\n\n\n--username \nvalue\n  User name (e-mail address) [$CB_USER_NAME]\n\n\n--password \nvalue\n Password [$CB_PASSWORD] \n\n\n--profile \nvalue\n  Selects a config profile to use [$CB_PROFILE] \n\n\n--output \nvalue\n  Supported formats: json, yaml, table (default: \"json\") [$CB_OUT_FORMAT]  \n\n\nExamples\n\n\nLists volumes available on AWS:\n\n\ncb cloud volumes aws\n[\n  {\n    \"Name\": \"ephemeral\",\n    \"Description\": \"Ephemeral\"\n  },\n  {\n    \"Name\": \"gp2\",\n    \"Description\": \"General Purpose (SSD)\"\n  },\n  {\n    \"Name\": \"st1\",\n    \"Description\": \"Throughput Optimized HDD\"\n  },\n  {\n    \"Name\": \"standard\",\n    \"Description\": \"Magnetic\"\n  }\n]\n\n\n\n\n\ncloud instances\n\n\nLists the available instance types.\n\n\nRequired Options\n\n\n--credential \nvalue\n  Name of the credential\n\n\n--region \nvalue\n   Name of the region  \n\n\nOptions\n\n\n--server \nvalue\n      Server address [$CB_SERVER_ADDRESS]\n\n\n--username \nvalue\n    User name (e-mail address) [$CB_USER_NAME]\n\n\n--password \nvalue\n    Password [$CB_PASSWORD] \n\n\n--profile \nvalue\n     Selects a config profile to use [$CB_PROFILE] \n\n\n--availability-zone \nvalue\n  Name of the availability zone \n\n\n--output \nvalue\n      Supported formats: json, yaml, table (default: \"json\") [$CB_OUT_FORMAT] \n\n\nExamples\n\n\nLists instance types available in the us-west-2 (Oregon) region on the AWS account identified by the credential called \"aws-cred\":\n\n\ncb cloud instances --credential aws-cred --region us-west-2\n\n  {\n    \"Name\": \"c3.2xlarge\",\n    \"Cpu\": \"8\",\n    \"Memory\": \"15.0\",\n    \"AvailabilityZone\": \"us-west-2b\"\n  },\n  {\n    \"Name\": \"c3.4xlarge\",\n    \"Cpu\": \"16\",\n    \"Memory\": \"30.0\",\n    \"AvailabilityZone\": \"us-west-2b\"\n  },\n  ...\n\n\n\n\n\ncluster change-ambari-password\n\n\nChanges Ambari password.\n\n\nRequired Options\n\n\n--name \nvalue\n  Cluster name\n\n\n--old-password \nvalue\n Old Ambari password \n\n\n--new-password \nvalue\n  New Ambari password  \n\n\n--ambari-user\n  Ambari user   \n\n\nOptions\n\n\n--server \nvalue\n  Server address [$CB_SERVER_ADDRESS]\n\n\n--username \nvalue\n  User name (e-mail address) [$CB_USER_NAME]\n\n\n--password \nvalue\n Password [$CB_PASSWORD]\n\n\n--profile \nvalue\n  Selects a config profile to use [$CB_PROFILE]\n\n\n--output \nvalue\n  Supported formats: json, yaml, table (default: \"json\") [$CB_OUT_FORMAT]\n\n\nExamples\n\n\nChanges password for Ambari user called \"admin\" for a cluster called \"test1234\":\n\n\ncb cluster change-ambari-password --name test1234 --old-password 123456 --new-password Ambari123456 --ambari-user admin\n\n\n\n\n\ncluster create\n\n\nCreates a new cluster based on a JSON template.\n\n\nRequired Options\n\n\n--cli-input-json \nvalue\n  User provided file in JSON format  \n\n\nOptions\n\n\n--name \nvalue\n  Name for the cluster\n\n\n--description \nvalue\n  Description of resource \n\n\n--public\n  Public in account \n\n\n--input-json-param-password \nvalue\n  Password for the cluster and Ambari \n\n\n--wait\n  Wait for the operation to finish. No argument is required \n\n\n--server \nvalue\n   Server address   [$CB_SERVER_ADDRESS] \n\n\n--username \nvalue\n  User name (e-mail address) [$CB_USER_NAME]\n\n\n--password \nvalue\n  Password [$CB_PASSWORD]\n\n\n--profile \nvalue\n  Selects a config profile to use [$CB_PROFILE]  \n\n\nExamples\n\n\nCreates a cluster called \"testcluster\" based on a local JSON file called \"mytemplate.json\" located in the /Users/test/Documents directory:   \n\n\ncb cluster create --name testcluster --cli-input-json /Users/test/Documents/mytemplate.json\n\n\n\nRelated Links\n\n\nObtain Cluster JSON Template from the UI\n   \n\n\n\n\ncluster delete\n\n\nDeletes an existing cluster. \n\n\nRequired Options\n\n\n--name \nvalue\n  Cluster name\n\n\nOptions\n\n\n--force\n  Force the operation\n\n\n--wait\n  Wait for the operation to finish. No argument is required\n\n\n--server \nvalue\n  Server address [$CB_SERVER_ADDRESS]\n\n\n--username \nvalue\n  User name (e-mail address) [$CB_USER_NAME]\n\n\n--password \nvalue\n Password [$CB_PASSWORD]\n\n\n--profile \nvalue\n  Selects a config profile to use [$CB_PROFILE]   \n\n\nExamples\n\n\ncb cluster delete --name test1234\n\n\n\n\n\ncluster describe\n\n\nDescribes an existing cluster.\n\n\nRequired Options\n\n\n--name \nvalue\n  Cluster name\n\n\nOptions\n\n\n--server \nvalue\n  Server address [$CB_SERVER_ADDRESS]\n\n\n--username \nvalue\n  User name (e-mail address) [$CB_USER_NAME]\n\n\n--password \nvalue\n  Password [$CB_PASSWORD]\n\n\n--profile \nvalue\n  Selects a config profile to use [$CB_PROFILE]\n\n\n--output \nvalue\n  Supported formats: json, yaml, table (default: \"json\") [$CB_OUT_FORMAT]  \n\n\nExamples\n\n\nReturns a JSON file describing an existing cluster called \"test1234\":\n\n\n./cb cluster describe --name test1234\n\n\n\nThe command returns JSON output which due to space limitation was not captured in the example.\n\n\n\n\ncluster generate-template\n\n\nGenerates a provider-specific cluster template in JSON format.\n\n\nSub-commands\n\n\naws new-network\n Generates an AWS cluster JSON template with new network\n\n\naws existing-network\n Generates an AWS cluster JSON template with existing network\n\n\naws existing-subnet\n Generates an AWS cluster JSON template with existing network and subnet  \n\n\nazure new-network\n Generates an Azure cluster JSON template with new network\n\n\nazure existing-subnet\n Generates an Azure cluster JSON template with existing network and subnet  \n\n\ngcp new-network\n Generates an GCP cluster JSON template with new network\n\n\ngcp existing-network\n Generates an GCP cluster JSON template with existing network \n\n\ngcp existing-subnet\n Generates an GCP cluster JSON template with existing network and subnet\n\n\ngcp legacy-network\n Generates an GCP cluster JSON template with legacy network without subnets    \n\n\nopenstack new-network\n Generates an OS cluster JSON template with new network \n\n\nopenstack existing-network\n Generates an OS cluster JSON template with existing network \n\n\nopenstack existing-subnet\n Generates an OS cluster JSON template with existing network and subnet   \n\n\nExamples\n\n\ncb cluster generate-template aws new-network\n\n\n\nRelated Commands\n\n\nRelated Links\n\n\nObtain Cluster JSON Template from the UI\n   \n\n\n\n\ncluster generate-reinstall-template\n\n\nGenerates a cluster template that you can use to reinstall the cluster if installation went fail. \n\n\nRequired Options\n\n\n--blueprint-name \nvalue\n  Name of the blueprint \n\n\nOptions\n\n\n--server \nvalue\n  Server address [$CB_SERVER_ADDRESS]\n\n\n--username \nvalue\n  User name (e-mail address) [$CB_USER_NAME]\n\n\n--password \nvalue\n Password [$CB_PASSWORD]\n\n\n--profile \nvalue\n  Selects a config profile to use [$CB_PROFILE]\n\n\n--output \nvalue\n  Supported formats: json, yaml, table (default: \"json\") [$CB_OUT_FORMAT]\n\n\nExamples\n\n\ncb cluster generate-reinstall-template --blueprint-name \"EDW-ETL: Apache Hive 1.2.1, Apache Spark 2.1\"\n\n\n\n\n\ncluster list\n\n\nLists all clusters which are currently associated with the Cloudbreak instance.\n\n\nRequired Options\n\n\nNone\n\n\nOptions\n\n\n--server \nvalue\n  Server address [$CB_SERVER_ADDRESS]\n\n\n--username \nvalue\n  User name (e-mail address) [$CB_USER_NAME]\n\n\n--password \nvalue\n Password [$CB_PASSWORD]\n\n\n--profile \nvalue\n  Selects a config profile to use [$CB_PROFILE]\n\n\n--output \nvalue\n  Supported formats: json, yaml, table (default: \"json\") [$CB_OUT_FORMAT]\n\n\nExamples\n\n\nLists available clusters: \n\n\ncb cluster list\n[\n  {\n    \"Name\": \"test1234\",\n    \"Description\": \"\",\n    \"CloudPlatform\": \"AZURE\",\n    \"StackStatus\": \"UPDATE_IN_PROGRESS\",\n    \"ClusterStatus\": \"REQUESTED\"\n  }\n]\n\n\n\nLists available clusters, with output in a table format:\n\n\ncb cluster list --output table\n+----------+-------------+---------------+--------------------+---------------+\n|   NAME   | DESCRIPTION | CLOUDPLATFORM |    STACKSTATUS     | CLUSTERSTATUS |\n+----------+-------------+---------------+--------------------+---------------+\n| test1234 |             | AZURE         | UPDATE_IN_PROGRESS | REQUESTED     |\n+----------+-------------+---------------+--------------------+---------------+\n\n\n\n\n\ncluster repair\n\n\nRepairs a cluster if cluster installation failed.\n\n\nRequired Options\n\n\n--name \nvalue\n  Cluster name\n\n\nOptions\n\n\n--wait\n  Wait for the operation to finish. No argument is required\n\n\n--server \nvalue\n  Server address [$CB_SERVER_ADDRESS]\n\n\n--username \nvalue\n  User name (e-mail address) [$CB_USER_NAME]\n\n\n--password \nvalue\n Password [$CB_PASSWORD]\n\n\n--profile \nvalue\n  Selects a config profile to use [$CB_PROFILE]\n\n\n--output \nvalue\n  Supported formats: json, yaml, table (default: \"json\") [$CB_OUT_FORMAT]\n\n\nExamples\n\n\ncb cluster repair --name test1234\n\n\n\n\n\ncluster scale\n\n\nScales a cluster by adding or removing nodes.\n\n\nRequired Options\n\n\n--name \nvalue\n  Name of the cluster\n\n--group-name \nvalue\n  Name of the group to scale\n\n--desired-node-count \nvalue\n  Desired number of nodes\n\n\nOptions\n\n\n--wait\n  Wait for the operation to finish. No argument is required\n\n\n--server \nvalue\n  Server address [$CB_SERVER_ADDRESS]\n\n\n--username \nvalue\n  User name (e-mail address) [$CB_USER_NAME]\n\n\n--password \nvalue\n Password [$CB_PASSWORD]\n\n\n--profile \nvalue\n  Selects a config profile to use [$CB_PROFILE]\n\n\n--output \nvalue\n  Supported formats: json, yaml, table (default: \"json\") [$CB_OUT_FORMAT]  \n\n\nExamples\n\n\ncb cluster scale --name test1234 --group-name worker --desired node-count 3\n\n\n\n\n\ncluster start\n\n\nStarts a cluster which has previously been stopped.\n\n\nRequired Options\n\n\n--name \nvalue\n  Cluster name\n\n\nOptions\n\n\n--wait\n  Wait for the operation to finish. No argument is required\n\n\n--server \nvalue\n  Server address [$CB_SERVER_ADDRESS]\n\n\n--username \nvalue\n  User name (e-mail address) [$CB_USER_NAME]\n\n\n--password \nvalue\n Password [$CB_PASSWORD]\n\n\n--profile \nvalue\n  Selects a config profile to use [$CB_PROFILE]\n\n\n--output \nvalue\n  Supported formats: json, yaml, table (default: \"json\") [$CB_OUT_FORMAT]\n\n\nExamples\n\n\ncb cluster start --name test1234\n\n\n\n\n\ncluster stop\n\n\nStops a cluster.\n\n\nRequired Options\n\n\n--name \nvalue\n  Cluster name\n\n\nOptions\n\n\n--wait\n  Wait for the operation to finish. No argument is required\n\n\n--server \nvalue\n  Server address [$CB_SERVER_ADDRESS]\n\n\n--username \nvalue\n  User name (e-mail address) [$CB_USER_NAME]\n\n\n--password \nvalue\n Password [$CB_PASSWORD]\n\n\n--profile \nvalue\n  Selects a config profile to use [$CB_PROFILE]\n\n\n--output \nvalue\n  Supported formats: json, yaml, table (default: \"json\") [$CB_OUT_FORMAT]\n\n\nExamples\n\n\ncb cluster stop --name test1234\n\n\n\n\n\ncluster sync\n\n\nSynchronizes a cluster with the cloud provider.\n\n\nRequired Options\n\n\n--name \nvalue\n  Cluster name\n\n\nOptions\n\n\n--server \nvalue\n  Server address [$CB_SERVER_ADDRESS]\n\n\n--username \nvalue\n  User name (e-mail address) [$CB_USER_NAME]\n\n\n--password \nvalue\n Password [$CB_PASSWORD]\n\n\n--profile \nvalue\n  Selects a config profile to use [$CB_PROFILE]\n\n\n--output \nvalue\n  Supported formats: json, yaml, table (default: \"json\") [$CB_OUT_FORMAT]\n\n\nExamples\n\n\ncb cluster sync --name test1234\n\n\n\n\n\nconfigure\n\n\nConfigures the Cloudbreak server address and credentials used to communicate with this server.\n\n\nRequired Options\n\n\n--server value\n  Server address [$CB_SERVER_ADDRESS]\n\n\n--username value\n   User name (e-mail address) [$CB_USER_NAME]  \n\n\nOptions\n\n\n--password value\n  Password [$CB_PASSWORD]\n\n\n--profile value\n  Select a config profile to use [$CB_PROFILE] \n\n\n--output value\n  Supported formats: json, yaml, table (default: \"json\") [$CB_OUT_FORMAT]  \n\n\nExamples\n\n\nThis example configures the server address with username and password:\n\n\ncb configure --server https://ec2-11-111-111-11.compute-1.amazonaws.com --username admin@hortonworks.com --password MySecurePassword123\n\n\n\nThis example configures the server address with username but without a password:\n\n\ncb configure --server https://ec2-11-111-111-11.compute-1.amazonaws.com --username admin@hortonworks.com\n\n\n\nRelated Links\n\n\nConfigure CLI\n\n\n\n\ncredential create\n\n\nCreates a new Cloudbreak credential.\n\n\nSub-commands\n\n\naws role-based\n  Creates a new AWS credential\n\n\naws key-based\n  Creates a new AWS credential\n\n\nazure app-based\n Creates a new app-based Azure credential\n\n\ngcp\n Creates a new gcp credential\n\n\nopenstack keystone-v2\n Creates a new OpenStack credential\n\n\nopenstack keystone-v3\n Creates a new OpenStack credential \n\n\nRequired Options\n\n\naws role-based\n \n\n\n--name \nvalue\n  Credential name\n\n\n--role-arn \nvalue\n IAM Role ARN of the role used for Cloudbreak credential  \n\n\naws key-based\n \n\n\n--name \nvalue\n  Credential name\n\n\n--access-key \nvalue\n  AWS Access Key\n\n\n--secret-key \nvalue\n  AWS Secret Key  \n\n\nazure app-based\n \n\n\n--name \nvalue\n  Credential name\n\n\n--subscription-id \nvalue\n  Subscription ID from your Azure Subscriptions\n\n\n--tenant-id \nvalue\n  Directory ID from your Azure Active Directory \n Properties    \n\n\n--app-id \nvalue\n  Application ID of your app from your Azure Active Directory \n App Registrations          \n\n\n--app-password \nvalue\n  Your application key from app registration's Settings \n Keys  \n\n\ngcp\n \n\n\n--name \nvalue\n  Credential name \n\n\n--project-id \nvalue\n  Project ID from your GCP account                      \n\n\n--service-account-id \nvalue\n  Your GCP Service account ID from IAM \n Admin \n Service accounts               \n\n\n--service-account-private-key-file \nvalue\n  P12 key from your GCP service account  \n\n\nopenstack keystone-v2\n  \n\n\n--name \nvalue\n  Credential name\n\n\n--tenant-user \nvalue\n  OpenStack user name   \n\n\n--tenant-password \nvalue\n  OpenStack password\n\n\n--tenant-name \nvalue\n  OpenStack tenant name    \n\n\n--endpoint \nvalue\n   OpenStack endpoint   \n\n\nopenstack keystone-v3\n \n\n\n--name \nvalue\n  Credential name\n\n\n--tenant-user \nvalue\n  OpenStack user name   \n\n\n--tenant-password \nvalue\n  OpenStack password\n\n\n--user-domain \nvalue\n  OpenStack user domain    \n\n\n--endpoint \nvalue\n   OpenStack endpoint  \n\n\nOptions\n\n\n--description \nvalue\n  Description of the resource\n\n\n--public\n  Public in account\n\n\n--server \nvalue\n Server address [$CB_SERVER_ADDRESS]\n\n\n--username \nvalue\n User name (e-mail address) [$CB_USER_NAME]\n\n\n--password \nvalue\n Password [$CB_PASSWORD]\n\n\n--profile \nvalue\n Selects a config profile to use [$CB_PROFILE]  \n\n\nAdditionally, the following option is available for OpenStack Keystone2 and Keystone3:\n\n\n--facing \nvalue\n API facing. One of: public, admin, internal\n\n\nAdditionally, the following option is available for OpenStack Keystone3:\n\n\n--keystone-scope \nvalue\n OpenStack keystone scope. One of: default, domain, project \n\n\nExamples\n\n\nCreates a role-based credential on AWS:\n\n\ncb credential create aws role-based --name my-credential1 --role-arn arn:aws:iam::517127065441:role/CredentialRole\n\n\n\nCreates a key-based credential on AWS:\n\n\ncb credential create aws key-based --name my-credential2 --access-key ABDVIRDFV3K4HLJ45SKA --secret-key D89L5pOPM+426Rtj3curKzJEJL3lYoNcP8GvguBV\n\n\n\nCreates an app-based credential on Azure:\n\n\ncb credential create azure app-based --name my-credential3 --subscription-id b8e7379e-568g-55d3-na82-45b8d421e998 --tenant-id  c79n5399-3231-65ba-8dgg-2g4e2a40085e --app-id 6d147d89-48d2-5de2-eef8-b89775bbfcg1 --app-password 4a8hBgfI52s/C8R5Sea2YHGnBFrD3fRONfdG8w7F2Ua=\n\n\n\nCreates a credential on Google Cloud:\n\n\ncb credential create gcp --name my-credential4 --project-id test-proj --service-account-id test@test-proj.iam.gserviceaccount.com --service-account-private-key-file /Users/test/3fff57a6f68e.p12\n\n\n\nCreates a role-based credential on OpenStack with Keystone-v2:\n\n\ncb credential create openstack keystone-v2 --name my-credential5 --tenant-user test --tenant-password MySecurePass123 --tenant-name test --endpoint http://openstack.test.organization.com:5000/v2.0\n\n\n\nRelated Links\n  \n\n\nCreate Credential on AWS\n\n\nCreate Credential on Azure\n\n\nCreate Credential on GCP\n\n\nCreate Credential on OpenStack\n  \n\n\n\n\ncredential delete\n\n\nDeletes an existing Cloudbreak credential.\n\n\nRequired Options\n\n\n--name \nvalue\n  Name of the credential \n\n\nOptions\n\n\n--server \nvalue\n Server address [$CB_SERVER_ADDRESS]\n\n\n--username \nvalue\n User name (e-mail address) [$CB_USER_NAME]\n\n\n--password \nvalue\n Password [$CB_PASSWORD]\n\n\n--profile \nvalue\n Selects a config profile to use [$CB_PROFILE] \n\n\nExamples\n\n\ncb credential delete --name test-cred\n\n\n\n\n\ncredential describe\n\n\nDescribes an existing credential.\n\n\nRequired Options\n\n\n--name \nvalue\n  Name of the credential \n\n\nOptions\n\n\n--server \nvalue\n Server address [$CB_SERVER_ADDRESS]\n\n\n--username \nvalue\n User name (e-mail address) [$CB_USER_NAME]\n\n\n--password \nvalue\n Password [$CB_PASSWORD]\n\n\n--profile \nvalue\n Selects a config profile to use [$CB_PROFILE]\n\n\n--output \nvalue\n  Supported formats: json, yaml, table (default: \"json\") [$CB_OUT_FORMAT]\n\n\nExamples\n\n\ncb credential describe --name testcred\n{\n  \"Name\": \"testcred\",\n  \"Description\": \"\",\n  \"CloudPlatform\": \"AZURE\"\n}\n\n\n\n\n\ncredential list\n\n\nLists existing Cloudbreak credentials.\n\n\nRequired Options\n\n\nNone\n\n\nOptions\n\n\n--server \nvalue\n Server address [$CB_SERVER_ADDRESS]\n\n\n--username \nvalue\n User name (e-mail address) [$CB_USER_NAME]\n\n\n--password \nvalue\n Password [$CB_PASSWORD]\n\n\n--profile \nvalue\n Selects a config profile to use [$CB_PROFILE]\n\n\n--output \nvalue\n  Supported formats: json, yaml, table (default: \"json\") [$CB_OUT_FORMAT]\n\n\nExamples\n\n\nLists credentials:\n\n\ncb credential list \n[\n  {\n    \"Name\": \"testcred\",\n    \"Description\": \"\",\n    \"CloudPlatform\": \"AZURE\"\n  }\n]\n\n\n\n\nLists credentials, with output formatted in a table format:\n\n\ncb credential list --output table\n+---------+-------------+---------------+\n|  NAME   | DESCRIPTION | CLOUDPLATFORM |\n+---------+-------------+---------------+\n| armcred |             | AZURE         |\n+---------+-------------+---------------+\n\n\n\n\n\nimagecatalog create\n\n\nRegisters a new custom image catalog based on the URL provided.  \n\n\nRequired Options\n  \n\n\n--name \nvalue\n  Name for the image catalog\n\n\n--url \nvalue\n   URL location of the image catalog JSON file  \n\n\nOptions\n\n\n--description \nvalue\n  Description for the recipe \n\n\n--public\n   Public in account\n\n\n--server \nvalue\n  Server address [$CB_SERVER_ADDRESS]\n\n\n--username \nvalue\n  User name (e-mail address) [$CB_USER_NAME]\n\n\n--password \nvalue\n  Password [$CB_PASSWORD]\n\n\n--profile \nvalue\n  Selects a config profile to use [$CB_PROFILE]  \n\n\nExamples\n\n\nRegisters an image catalog called \"mycustomcatalog\" which is available at https://example.com/myimagecatalog.json: \n\n\ncb imagecatalog create --name mycustomcatalog --url https://example.com/myimagecatalog.json\n\n\n\nRelated Links\n  \n\n\nCustom Images\n   \n\n\n\n\nimagecatalog delete\n\n\nDeletes a previously registered custom image catalog. \n\n\nRequired Options\n  \n\n\n--name \nvalue\n  Name for the image catalog   \n\n\nOptions\n\n\n--output \nvalue\n  Supported formats: json, yaml, table (default: \"json\") [$CB_OUT_FORMAT]\n\n\n--server \nvalue\n  Server address [$CB_SERVER_ADDRESS]\n\n\n--username \nvalue\n  User name (e-mail address) [$CB_USER_NAME]\n\n\n--password \nvalue\n  Password [$CB_PASSWORD]\n\n\n--profile \nvalue\n  Selects a config profile to use [$CB_PROFILE] \n\n\nExamples\n\n\nDeletes an image catalog called \"mycustomcatalog\":\n\n\ncb imagecatalog delete --name mycustomcatalog\n\n\n\nRelated Links\n  \n\n\nCustom Images\n  \n\n\n\n\nimagecatalog images\n\n\nLists images from the specified image catalog available for the specified cloud provider.   \n\n\nSub-commands\n  \n\n\naws\n         Lists available aws images   \n\n\nazure\n       Lists available azure images   \n\n\ngcp\n         Lists available gcp images  \n\n\nopenstack\n   Lists available openstack images    \n\n\nRequired Options\n\n\n--imagecatalog \nvalue\n  Name of the imagecatalog     \n\n\nOptions\n\n\n--output \nvalue\n  Supported formats: json, yaml, table (default: \"json\") [$CB_OUT_FORMAT]  \n\n\n--server \nvalue\n  Server address [$CB_SERVER_ADDRESS]\n\n\n--username \nvalue\n  User name (e-mail address) [$CB_USER_NAME]\n\n\n--password \nvalue\n Password [$CB_PASSWORD] \n\n\n--profile \nvalue\n  Selects a config profile to use [$CB_PROFILE]   \n\n\nExamples\n  \n\n\nReturns date, description, Ambari version, and image ID for all AWS images from an image catalog called \"myimagecatalog\":\n\n\n./cb imagecatalog images aws --imagecatalog cloudbreak-default\n[\n  {\n    \"Date\": \"2017-10-13\",\n    \"Description\": \"Cloudbreak official base image\",\n    \"Version\": \"2.6.0.0\",\n    \"ImageID\": \"44b140a4-bd0b-457d-b174-e988bee3ca47\"\n  },\n  {\n    \"Date\": \"2017-11-16\",\n    \"Description\": \"Official Cloudbreak image\",\n    \"Version\": \"2.6.0.0\",\n    \"ImageID\": \"3c7598a4-ebd6-4a02-5638-882f5c7f7add\"\n  }\n]\n\n\n\nRelated Links\n  \n\n\nCustom Images\n  \n\n\n\n\nimagecatalog list\n\n\nLists default and custom image catalogs registered with Cloudbreak instance.   \n\n\nRequired Options\n  \n\n\nNone  \n\n\nOptions\n  \n\n\n--server \nvalue\n  Server address [$CB_SERVER_ADDRESS]\n\n\n--username \nvalue\n  User name (e-mail address) [$CB_USER_NAME]\n\n\n--password \nvalue\n  Password [$CB_PASSWORD]\n\n\n--profile \nvalue\n  Selects a config profile to use [$CB_PROFILE]\n\n\n--output \nvalue\n  Supported formats: json, yaml, table (default: \"json\") [$CB_OUT_FORMAT]   \n\n\nExamples\n\n\nLists existing image catalogs:  \n\n\ncb  imagecatalog list \n[\n  {\n    \"Name\": \"mycustomcatalog\",\n    \"Default\": false,\n    \"URL\": \"https://example.com/imagecatalog.json\"\n  },\n  {\n    \"Name\": \"cloudbreak-default\",\n    \"Default\": true,\n    \"URL\": \"https://s3-eu-west-1.amazonaws.com/cloudbreak-info/v2-dev-cb-image-catalog.json\"\n  }\n]\n\n\n\nRelated Links\n  \n\n\nCustom Images\n  \n\n\n\n\nimagecatalog set-default\n\n\nSets the specified image catalog as default.  \n\n\nRequired Options\n   \n\n\n--name \nvalue\n  Name for the image catalog    \n\n\nOptions\n\n\n--output \nvalue\n  Supported formats: json, yaml, table (default: \"json\") [$CB_OUT_FORMAT]\n\n\n--server \nvalue\n  Server address [$CB_SERVER_ADDRESS]\n\n\n--username \nvalue\n  User name (e-mail address) [$CB_USER_NAME]\n\n\n--password \nvalue\n  Password [$CB_PASSWORD]\n\n\n--profile \nvalue\n  Selects a config profile to use [$CB_PROFILE] \n\n\nExamples\n\n\nSets \"mycustomcatalog\" as default:  \n\n\nimagecatalog set-default --name mycustomcatalog\n\n\n\nRelated Links\n  \n\n\nCustom Images\n  \n\n\n\n\nrecipe create\n\n\nAdds a new recipe from a file or from a URL.\n\n\nSub-commands\n\n\nfrom-url\n  Creates a recipe by downloading it from a URL location\n\n\nfrom-file\n  Creates a recipe by reading it from a local file  \n\n\nRequired Options\n\n\nfrom-url\n  \n\n\n--name \nvalue\n  Name for the recipe \n\n\n--execution-type \nvalue\n  Type of execution [pre-ambari-start, pre-termination, post-ambari-start, post-cluster-install]  \n\n\n--url \nvalue\n  URL location of the Ambari blueprint JSON file  \n\n\nfrom-file\n \n\n\n--name \nvalue\n  Name for the recipe\n\n\n--execution-type \nvalue\n  Type of execution [pre-ambari-start, pre-termination, post-ambari-start, post-cluster-install] \n\n\n--file \nvalue\n  Location of the Ambari blueprint JSON file  \n\n\nOptions\n\n\n--description \nvalue\n  Description for the recipe \n\n\n--public\n   Public in account\n\n\n--server \nvalue\n  Server address [$CB_SERVER_ADDRESS]\n\n\n--username \nvalue\n  User name (e-mail address) [$CB_USER_NAME]\n\n\n--password \nvalue\n  Password [$CB_PASSWORD]\n\n\n--profile \nvalue\n  Selects a config profile to use [$CB_PROFILE]  \n\n\nExamples\n\n\nAdds a new recipe called \"test1\" from a URL:\n\n\ncb recipe create from-url --name \"test1\" --execution-type post-ambari-start --url http://some-site.com/test.sh\n\n\n\nAdds a new recipe called \"test2\" from a file:\n\n\ncb recipe create from-url --name \"test2\" --execution-type post-ambari-start --file /Users/test/Documents/test.sh\n\n\n\nRelated Links\n\n\nRecipes\n\n\n\n\nrecipe delete\n\n\nDeletes an existing recipe.\n\n\nRequired Options\n\n\n--name \nvalue\n  Name for the recipe  \n\n\nOptions\n\n\n--server \nvalue\n  Server address [$CB_SERVER_ADDRESS]\n\n\n--username \nvalue\n  User name (e-mail address) [$CB_USER_NAME]\n\n\n--password \nvalue\n  Password [$CB_PASSWORD]\n\n\n--profile \nvalue\n  Selects a config profile to use [$CB_PROFILE]\n\n\n--output \nvalue\n  Supported formats: json, yaml, table (default: \"json\") [$CB_OUT_FORMAT]\n\n\nExamples\n\n\ncb recipe delete --name test\n\n\n\n\n\nrecipe describe\n\n\nDescribes an existing recipe.\n\n\nRequired Options\n\n\n--name \nvalue\n  Name for the recipe    \n\n\nOptions\n\n\n--server \nvalue\n  Server address [$CB_SERVER_ADDRESS]\n\n\n--username \nvalue\n  User name (e-mail address) [$CB_USER_NAME]\n\n\n--password \nvalue\n  Password [$CB_PASSWORD]\n\n\n--profile \nvalue\n  Selects a config profile to use [$CB_PROFILE]\n\n\n--output \nvalue\n  Supported formats: json, yaml, table (default: \"json\") [$CB_OUT_FORMAT]\n\n\nExamples\n\n\nDescribes a recipe called \"test\":\n\n\ncb recipe describe --name test\n{\n  \"Name\": \"test\",\n  \"Description\": \"\",\n  \"ExecutionType\": \"POST\"\n}\n\n\n\nDescribes a recipe called \"test\", with output presented in a table format:\n\n\ncb describe-recipe --name test --output table\n+------+-------------+----------------+\n| NAME | DESCRIPTION | EXECUTION TYPE |\n+------+-------------+----------------+\n| test |             | POST           |\n+------+-------------+----------------+\n\n\n\n\n\nrecipe list\n\n\nLists all available recipes.\n\n\nRequired Options\n\n\nNone\n\n\nOptions\n\n\n--server \nvalue\n  Server address [$CB_SERVER_ADDRESS]\n\n\n--username \nvalue\n  User name (e-mail address) [$CB_USER_NAME]\n\n\n--password \nvalue\n  Password [$CB_PASSWORD]\n\n\n--profile \nvalue\n  Selects a config profile to use [$CB_PROFILE]\n\n\n--output \nvalue\n  Supported formats: json, yaml, table (default: \"json\") [$CB_OUT_FORMAT]  \n\n\nExamples\n\n\nLists existing recipes:\n\n\ncb recipe list\n[\n  {\n    \"Name\": \"test\",\n    \"Description\": \"\",\n    \"ExecutionType\": \"POST\"\n  }\n]\n\n\n\nLists existing recipes, with output presented in a table format:\n\n\ncb recipe list --output table\n+------+-------------+-------------------+\n| NAME | DESCRIPTION | EXECUTION TYPE    |\n+------+-------------+-------------------+\n| test |             | POST-AMBARI-START |\n+------+-------------+-------------------+\n\n\n\n\n\n\nDebugging\n\n\nTo use debugging mode, pass the \n--debug\n option. \n\n\nChecking CLI Version\n\n\nTo check CLI version, use \ncb --version\n.", 
            "title": "CLI Reference"
        }, 
        {
            "location": "/cli-reference/index.html#cloudbreak-cli-reference", 
            "text": "This section will help you get started with the Cloudbreak CLI after you have  installed and configured it .", 
            "title": "Cloudbreak CLI Reference"
        }, 
        {
            "location": "/cli-reference/index.html#command-structure", 
            "text": "The CLI command can contain multiple parts. The first part is a set of global options. The next part is the command. The next part is a set of command options and arguments which could include sub-commands.  cb [global options] command [command options] [arguments...]", 
            "title": "Command Structure"
        }, 
        {
            "location": "/cli-reference/index.html#command-output", 
            "text": "You can control the output from the CLI using the --output argument. The possible output formats include:   JSON ( json )  YAML ( yaml )  Formatted table ( table )   For example:  cb cluster list --output json  cb clusters list --output yaml  cb cluster list --output table", 
            "title": "Command Output"
        }, 
        {
            "location": "/cli-reference/index.html#commands", 
            "text": "Configure CLI:     configure      Cloud Provider:   cloud availability-zones     cloud regions          cloud volumes      cloud instances       Credential:     credential create      credential delete           credential describe           credential list             Blueprint:      blueprint create     blueprint delete              blueprint describe            blueprint list               Cluster:    cluster change-ambari-password      cluster create    cluster delete                 cluster describe       cluster generate-template      cluster generate-reinstall-template     cluster list       cluster repair                cluster scale         cluster start           cluster stop                 cluster sync     Image Catalog   imagecatalog create        imagecatalog delete    imagecatalog images                         imagecatalog list     imagecatalog set-default                Recipe:     recipe create        recipe delete               recipe describe               recipe list", 
            "title": "Commands"
        }, 
        {
            "location": "/cli-reference/index.html#blueprint-create", 
            "text": "Adds a new blueprint from a file or from a URL.  Sub-commands  from-url  Creates a blueprint by downloading it from a URL location  from-file  Creates a blueprint by reading it from a local file  Required Options  from-url    --name  value  Name of the blueprint  --url  value  URL location of the Ambari blueprint JSON file  from-file    --name  value  Name of the blueprint  --file  value  Location of the Ambari blueprint JSON file on the local machine  Options  --description  value   Description of the resource  --public   Public in account  --server  value  Server address [$CB_SERVER_ADDRESS]  --username  value  User name (e-mail address) [$CB_USER_NAME]  --password  value  Password [$CB_PASSWORD]  --profile  value  Selects a config profile to use [$CB_PROFILE]   Examples  Adds a blueprint from a URL:  cb blueprint create from-url --url https://someurl.com/test.bp --name test1  Adds a blueprint from a local file:  cb blueprint create from-file --file /Users/test/Documents/blueprints/test.bp --name test2  Related Links  Blueprints", 
            "title": "blueprint create"
        }, 
        {
            "location": "/cli-reference/index.html#blueprint-delete", 
            "text": "Deletes an existing blueprint.  Required Options  --name  value  Name of the blueprint   Options  --server  value  Server address [$CB_SERVER_ADDRESS]  --username  value  User name (e-mail address) [$CB_USER_NAME]  --password  value  Password [$CB_PASSWORD]  --profile  value  Selects a config profile to use [$CB_PROFILE]   Examples  cb blueprint delete --name \"testbp\"", 
            "title": "blueprint delete"
        }, 
        {
            "location": "/cli-reference/index.html#blueprint-describe", 
            "text": "Describes an existing blueprint.  Required Options  --name  value  Name of the blueprint   Options  --server  value  Server address [$CB_SERVER_ADDRESS]  --username  value  User name (e-mail address) [$CB_USER_NAME]  --password  value  Password [$CB_PASSWORD]  --profile  value  Selects a config profile to use [$CB_PROFILE]  --output  value   Supported formats: json, yaml, table (default: \"json\") [$CB_OUT_FORMAT]  Examples  cb blueprint describe --name \"Data Science: Apache Spark 2.1, Apache Zeppelin 0.7.0\"\n{\n  \"Name\": \"Data Science: Apache Spark 2.1, Apache Zeppelin 0.7.0\",\n  \"Description\": \"Data Science: Apache Spark 2.1, Apache Zeppelin 0.7.0\",\n  \"HDPVersion\": \"2.6\",\n  \"HostgroupCount\": \"3\",\n  \"Tags\": \"DEFAULT\"\n}", 
            "title": "blueprint describe"
        }, 
        {
            "location": "/cli-reference/index.html#blueprint-list", 
            "text": "Lists available blueprints.  Required Options  Nome  Options  --server  value  Server address [$CB_SERVER_ADDRESS]  --username  value  User name (e-mail address) [$CB_USER_NAME]  --password  value  Password [$CB_PASSWORD]  --profile  value  Selects a config profile to use [$CB_PROFILE]  --output  value  Supported formats: json, yaml, table (default: \"json\") [$CB_OUT_FORMAT]    Examples  cb blueprint list\n[\n  {\n    \"Name\": \"Data Science: Apache Spark 2.1, Apache Zeppelin 0.7.0\",\n    \"Description\": \"Data Science: Apache Spark 2.1, Apache Zeppelin 0.7.0\",\n    \"HDPVersion\": \"2.6\",\n    \"HostgroupCount\": \"3\",\n    \"Tags\": \"DEFAULT\"\n  },\n  {\n    \"Name\": \"EDW-ETL: Apache Hive 1.2.1, Apache Spark 2.1\",\n    \"Description\": \"EDW-ETL: Apache Hive 1.2.1, Apache Spark 2.1\",\n    \"HDPVersion\": \"2.6\",\n    \"HostgroupCount\": \"3\",\n    \"Tags\": \"DEFAULT\"\n  },\n  {\n    \"Name\": \"26EDW-ETL: Apache Hive 1.2.1, Apache Spark 1.6\",\n    \"Description\": \"26EDW-ETL: Apache Hive 1.2.1, Apache Spark 1.6\",\n    \"HDPVersion\": \"2.6\",\n    \"HostgroupCount\": \"3\",\n    \"Tags\": \"DEFAULT\"\n  },\n  {\n    \"Name\": \"Data Science: Apache Spark 1.6, Apache Zeppelin 0.7.0\",\n    \"Description\": \"Data Science: Apache Spark 1.6, Apache Zeppelin 0.7.0\",\n    \"HDPVersion\": \"2.6\",\n    \"HostgroupCount\": \"3\",\n    \"Tags\": \"DEFAULT\"\n  },\n  {\n    \"Name\": \"BI: Druid 0.9.2 (Technical Preview)\",\n    \"Description\": \"BI: Druid 0.9.2 (Technical Preview)\",\n    \"HDPVersion\": \"2.6\",\n    \"HostgroupCount\": \"3\",\n    \"Tags\": \"DEFAULT\"\n  },\n  {\n    \"Name\": \"EDW-Analytics: Apache Hive 2 LLAP, Apache Zeppelin 0.7.0\",\n    \"Description\": \"EDW-Analytics: Apache Hive 2 LLAP, Apache Zeppelin 0.7.0\",\n    \"HDPVersion\": \"2.6\",\n    \"HostgroupCount\": \"3\",\n    \"Tags\": \"DEFAULT\"\n  },\n  {\n    \"Name\": \"EDW-ETL: Apache Hive 1.2.1, Apache Spark 1.6\",\n    \"Description\": \"EDW-ETL: Apache Hive 1.2.1, Apache Spark 1.6\",\n    \"HDPVersion\": \"2.6\",\n    \"HostgroupCount\": \"3\",\n    \"Tags\": \"DEFAULT\"\n  }\n]", 
            "title": "blueprint list"
        }, 
        {
            "location": "/cli-reference/index.html#cloud-availability-zones", 
            "text": "Lists the available availability zones in a region.   Required Options  --credential  value   Name of the credential  --region  value    Name of the region    Options  --server  value   Server address [$CB_SERVER_ADDRESS]  --username  value   User name (e-mail address) [$CB_USER_NAME]  --password  value  Password [$CB_PASSWORD]   --profile  value   Selects a config profile to use [$CB_PROFILE]   --output  value   Supported formats: json, yaml, table (default: \"json\") [$CB_OUT_FORMAT]    Examples  Lists availability zones in the us-west-2 (Oregon) region on the AWS account identified by the credential called \"aws-cred\":  cb cloud availability-zones --credential aws-cred --region us-west-2\n[\n  {\n    \"Name\": \"us-west-2a\"\n  },\n  {\n    \"Name\": \"us-west-2b\"\n  },\n  {\n    \"Name\": \"us-west-2c\"\n  }\n]", 
            "title": "cloud availability-zones"
        }, 
        {
            "location": "/cli-reference/index.html#cloud-regions", 
            "text": "Lists available regions.   Required Options  --credential  value   Name of the credential    Options  --server  value   Server address [$CB_SERVER_ADDRESS]  --username  value   User name (e-mail address) [$CB_USER_NAME]  --password  value  Password [$CB_PASSWORD]   --profile  value   Selects a config profile to use [$CB_PROFILE]   --output  value   Supported formats: json, yaml, table (default: \"json\") [$CB_OUT_FORMAT]    Examples  Lists regions available on the AWS account identified by the credential called \"aws-cred\":  cb cloud regions --credential aws-cred\n[\n  {\n    \"Name\": \"ap-northeast-1\",\n    \"Description\": \"Asia Pacific (Tokyo)\"\n  },\n  {\n    \"Name\": \"ap-northeast-2\",\n    \"Description\": \"Asia Pacific (Seoul)\"\n  },\n  ...", 
            "title": "cloud regions"
        }, 
        {
            "location": "/cli-reference/index.html#cloud-volumes", 
            "text": "Lists the available volume types.   Sub-commands  aws      Lists the available aws volume types  azure    Lists the available azure volume types  gcp      Lists the available gcp volume types    Required Options  None  Options  --server  value   Server address [$CB_SERVER_ADDRESS]  --username  value   User name (e-mail address) [$CB_USER_NAME]  --password  value  Password [$CB_PASSWORD]   --profile  value   Selects a config profile to use [$CB_PROFILE]   --output  value   Supported formats: json, yaml, table (default: \"json\") [$CB_OUT_FORMAT]    Examples  Lists volumes available on AWS:  cb cloud volumes aws\n[\n  {\n    \"Name\": \"ephemeral\",\n    \"Description\": \"Ephemeral\"\n  },\n  {\n    \"Name\": \"gp2\",\n    \"Description\": \"General Purpose (SSD)\"\n  },\n  {\n    \"Name\": \"st1\",\n    \"Description\": \"Throughput Optimized HDD\"\n  },\n  {\n    \"Name\": \"standard\",\n    \"Description\": \"Magnetic\"\n  }\n]", 
            "title": "cloud volumes"
        }, 
        {
            "location": "/cli-reference/index.html#cloud-instances", 
            "text": "Lists the available instance types.  Required Options  --credential  value   Name of the credential  --region  value    Name of the region    Options  --server  value       Server address [$CB_SERVER_ADDRESS]  --username  value     User name (e-mail address) [$CB_USER_NAME]  --password  value     Password [$CB_PASSWORD]   --profile  value      Selects a config profile to use [$CB_PROFILE]   --availability-zone  value   Name of the availability zone   --output  value       Supported formats: json, yaml, table (default: \"json\") [$CB_OUT_FORMAT]   Examples  Lists instance types available in the us-west-2 (Oregon) region on the AWS account identified by the credential called \"aws-cred\":  cb cloud instances --credential aws-cred --region us-west-2\n\n  {\n    \"Name\": \"c3.2xlarge\",\n    \"Cpu\": \"8\",\n    \"Memory\": \"15.0\",\n    \"AvailabilityZone\": \"us-west-2b\"\n  },\n  {\n    \"Name\": \"c3.4xlarge\",\n    \"Cpu\": \"16\",\n    \"Memory\": \"30.0\",\n    \"AvailabilityZone\": \"us-west-2b\"\n  },\n  ...", 
            "title": "cloud instances"
        }, 
        {
            "location": "/cli-reference/index.html#cluster-change-ambari-password", 
            "text": "Changes Ambari password.  Required Options  --name  value   Cluster name  --old-password  value  Old Ambari password   --new-password  value   New Ambari password    --ambari-user   Ambari user     Options  --server  value   Server address [$CB_SERVER_ADDRESS]  --username  value   User name (e-mail address) [$CB_USER_NAME]  --password  value  Password [$CB_PASSWORD]  --profile  value   Selects a config profile to use [$CB_PROFILE]  --output  value   Supported formats: json, yaml, table (default: \"json\") [$CB_OUT_FORMAT]  Examples  Changes password for Ambari user called \"admin\" for a cluster called \"test1234\":  cb cluster change-ambari-password --name test1234 --old-password 123456 --new-password Ambari123456 --ambari-user admin", 
            "title": "cluster change-ambari-password"
        }, 
        {
            "location": "/cli-reference/index.html#cluster-create", 
            "text": "Creates a new cluster based on a JSON template.  Required Options  --cli-input-json  value   User provided file in JSON format    Options  --name  value   Name for the cluster  --description  value   Description of resource   --public   Public in account   --input-json-param-password  value   Password for the cluster and Ambari   --wait   Wait for the operation to finish. No argument is required   --server  value    Server address   [$CB_SERVER_ADDRESS]   --username  value   User name (e-mail address) [$CB_USER_NAME]  --password  value   Password [$CB_PASSWORD]  --profile  value   Selects a config profile to use [$CB_PROFILE]    Examples  Creates a cluster called \"testcluster\" based on a local JSON file called \"mytemplate.json\" located in the /Users/test/Documents directory:     cb cluster create --name testcluster --cli-input-json /Users/test/Documents/mytemplate.json  Related Links  Obtain Cluster JSON Template from the UI", 
            "title": "cluster create"
        }, 
        {
            "location": "/cli-reference/index.html#cluster-delete", 
            "text": "Deletes an existing cluster.   Required Options  --name  value   Cluster name  Options  --force   Force the operation  --wait   Wait for the operation to finish. No argument is required  --server  value   Server address [$CB_SERVER_ADDRESS]  --username  value   User name (e-mail address) [$CB_USER_NAME]  --password  value  Password [$CB_PASSWORD]  --profile  value   Selects a config profile to use [$CB_PROFILE]     Examples  cb cluster delete --name test1234", 
            "title": "cluster delete"
        }, 
        {
            "location": "/cli-reference/index.html#cluster-describe", 
            "text": "Describes an existing cluster.  Required Options  --name  value   Cluster name  Options  --server  value   Server address [$CB_SERVER_ADDRESS]  --username  value   User name (e-mail address) [$CB_USER_NAME]  --password  value   Password [$CB_PASSWORD]  --profile  value   Selects a config profile to use [$CB_PROFILE]  --output  value   Supported formats: json, yaml, table (default: \"json\") [$CB_OUT_FORMAT]    Examples  Returns a JSON file describing an existing cluster called \"test1234\":  ./cb cluster describe --name test1234  The command returns JSON output which due to space limitation was not captured in the example.", 
            "title": "cluster describe"
        }, 
        {
            "location": "/cli-reference/index.html#cluster-generate-template", 
            "text": "Generates a provider-specific cluster template in JSON format.  Sub-commands  aws new-network  Generates an AWS cluster JSON template with new network  aws existing-network  Generates an AWS cluster JSON template with existing network  aws existing-subnet  Generates an AWS cluster JSON template with existing network and subnet    azure new-network  Generates an Azure cluster JSON template with new network  azure existing-subnet  Generates an Azure cluster JSON template with existing network and subnet    gcp new-network  Generates an GCP cluster JSON template with new network  gcp existing-network  Generates an GCP cluster JSON template with existing network   gcp existing-subnet  Generates an GCP cluster JSON template with existing network and subnet  gcp legacy-network  Generates an GCP cluster JSON template with legacy network without subnets      openstack new-network  Generates an OS cluster JSON template with new network   openstack existing-network  Generates an OS cluster JSON template with existing network   openstack existing-subnet  Generates an OS cluster JSON template with existing network and subnet     Examples  cb cluster generate-template aws new-network  Related Commands  Related Links  Obtain Cluster JSON Template from the UI", 
            "title": "cluster generate-template"
        }, 
        {
            "location": "/cli-reference/index.html#cluster-generate-reinstall-template", 
            "text": "Generates a cluster template that you can use to reinstall the cluster if installation went fail.   Required Options  --blueprint-name  value   Name of the blueprint   Options  --server  value   Server address [$CB_SERVER_ADDRESS]  --username  value   User name (e-mail address) [$CB_USER_NAME]  --password  value  Password [$CB_PASSWORD]  --profile  value   Selects a config profile to use [$CB_PROFILE]  --output  value   Supported formats: json, yaml, table (default: \"json\") [$CB_OUT_FORMAT]  Examples  cb cluster generate-reinstall-template --blueprint-name \"EDW-ETL: Apache Hive 1.2.1, Apache Spark 2.1\"", 
            "title": "cluster generate-reinstall-template"
        }, 
        {
            "location": "/cli-reference/index.html#cluster-list", 
            "text": "Lists all clusters which are currently associated with the Cloudbreak instance.  Required Options  None  Options  --server  value   Server address [$CB_SERVER_ADDRESS]  --username  value   User name (e-mail address) [$CB_USER_NAME]  --password  value  Password [$CB_PASSWORD]  --profile  value   Selects a config profile to use [$CB_PROFILE]  --output  value   Supported formats: json, yaml, table (default: \"json\") [$CB_OUT_FORMAT]  Examples  Lists available clusters:   cb cluster list\n[\n  {\n    \"Name\": \"test1234\",\n    \"Description\": \"\",\n    \"CloudPlatform\": \"AZURE\",\n    \"StackStatus\": \"UPDATE_IN_PROGRESS\",\n    \"ClusterStatus\": \"REQUESTED\"\n  }\n]  Lists available clusters, with output in a table format:  cb cluster list --output table\n+----------+-------------+---------------+--------------------+---------------+\n|   NAME   | DESCRIPTION | CLOUDPLATFORM |    STACKSTATUS     | CLUSTERSTATUS |\n+----------+-------------+---------------+--------------------+---------------+\n| test1234 |             | AZURE         | UPDATE_IN_PROGRESS | REQUESTED     |\n+----------+-------------+---------------+--------------------+---------------+", 
            "title": "cluster list"
        }, 
        {
            "location": "/cli-reference/index.html#cluster-repair", 
            "text": "Repairs a cluster if cluster installation failed.  Required Options  --name  value   Cluster name  Options  --wait   Wait for the operation to finish. No argument is required  --server  value   Server address [$CB_SERVER_ADDRESS]  --username  value   User name (e-mail address) [$CB_USER_NAME]  --password  value  Password [$CB_PASSWORD]  --profile  value   Selects a config profile to use [$CB_PROFILE]  --output  value   Supported formats: json, yaml, table (default: \"json\") [$CB_OUT_FORMAT]  Examples  cb cluster repair --name test1234", 
            "title": "cluster repair"
        }, 
        {
            "location": "/cli-reference/index.html#cluster-scale", 
            "text": "Scales a cluster by adding or removing nodes.  Required Options  --name  value   Name of the cluster --group-name  value   Name of the group to scale --desired-node-count  value   Desired number of nodes  Options  --wait   Wait for the operation to finish. No argument is required  --server  value   Server address [$CB_SERVER_ADDRESS]  --username  value   User name (e-mail address) [$CB_USER_NAME]  --password  value  Password [$CB_PASSWORD]  --profile  value   Selects a config profile to use [$CB_PROFILE]  --output  value   Supported formats: json, yaml, table (default: \"json\") [$CB_OUT_FORMAT]    Examples  cb cluster scale --name test1234 --group-name worker --desired node-count 3", 
            "title": "cluster scale"
        }, 
        {
            "location": "/cli-reference/index.html#cluster-start", 
            "text": "Starts a cluster which has previously been stopped.  Required Options  --name  value   Cluster name  Options  --wait   Wait for the operation to finish. No argument is required  --server  value   Server address [$CB_SERVER_ADDRESS]  --username  value   User name (e-mail address) [$CB_USER_NAME]  --password  value  Password [$CB_PASSWORD]  --profile  value   Selects a config profile to use [$CB_PROFILE]  --output  value   Supported formats: json, yaml, table (default: \"json\") [$CB_OUT_FORMAT]  Examples  cb cluster start --name test1234", 
            "title": "cluster start"
        }, 
        {
            "location": "/cli-reference/index.html#cluster-stop", 
            "text": "Stops a cluster.  Required Options  --name  value   Cluster name  Options  --wait   Wait for the operation to finish. No argument is required  --server  value   Server address [$CB_SERVER_ADDRESS]  --username  value   User name (e-mail address) [$CB_USER_NAME]  --password  value  Password [$CB_PASSWORD]  --profile  value   Selects a config profile to use [$CB_PROFILE]  --output  value   Supported formats: json, yaml, table (default: \"json\") [$CB_OUT_FORMAT]  Examples  cb cluster stop --name test1234", 
            "title": "cluster stop"
        }, 
        {
            "location": "/cli-reference/index.html#cluster-sync", 
            "text": "Synchronizes a cluster with the cloud provider.  Required Options  --name  value   Cluster name  Options  --server  value   Server address [$CB_SERVER_ADDRESS]  --username  value   User name (e-mail address) [$CB_USER_NAME]  --password  value  Password [$CB_PASSWORD]  --profile  value   Selects a config profile to use [$CB_PROFILE]  --output  value   Supported formats: json, yaml, table (default: \"json\") [$CB_OUT_FORMAT]  Examples  cb cluster sync --name test1234", 
            "title": "cluster sync"
        }, 
        {
            "location": "/cli-reference/index.html#configure", 
            "text": "Configures the Cloudbreak server address and credentials used to communicate with this server.  Required Options  --server value   Server address [$CB_SERVER_ADDRESS]  --username value    User name (e-mail address) [$CB_USER_NAME]    Options  --password value   Password [$CB_PASSWORD]  --profile value   Select a config profile to use [$CB_PROFILE]   --output value   Supported formats: json, yaml, table (default: \"json\") [$CB_OUT_FORMAT]    Examples  This example configures the server address with username and password:  cb configure --server https://ec2-11-111-111-11.compute-1.amazonaws.com --username admin@hortonworks.com --password MySecurePassword123  This example configures the server address with username but without a password:  cb configure --server https://ec2-11-111-111-11.compute-1.amazonaws.com --username admin@hortonworks.com  Related Links  Configure CLI", 
            "title": "configure"
        }, 
        {
            "location": "/cli-reference/index.html#credential-create", 
            "text": "Creates a new Cloudbreak credential.  Sub-commands  aws role-based   Creates a new AWS credential  aws key-based   Creates a new AWS credential  azure app-based  Creates a new app-based Azure credential  gcp  Creates a new gcp credential  openstack keystone-v2  Creates a new OpenStack credential  openstack keystone-v3  Creates a new OpenStack credential   Required Options  aws role-based    --name  value   Credential name  --role-arn  value  IAM Role ARN of the role used for Cloudbreak credential    aws key-based    --name  value   Credential name  --access-key  value   AWS Access Key  --secret-key  value   AWS Secret Key    azure app-based    --name  value   Credential name  --subscription-id  value   Subscription ID from your Azure Subscriptions  --tenant-id  value   Directory ID from your Azure Active Directory   Properties      --app-id  value   Application ID of your app from your Azure Active Directory   App Registrations            --app-password  value   Your application key from app registration's Settings   Keys    gcp    --name  value   Credential name   --project-id  value   Project ID from your GCP account                        --service-account-id  value   Your GCP Service account ID from IAM   Admin   Service accounts                 --service-account-private-key-file  value   P12 key from your GCP service account    openstack keystone-v2     --name  value   Credential name  --tenant-user  value   OpenStack user name     --tenant-password  value   OpenStack password  --tenant-name  value   OpenStack tenant name      --endpoint  value    OpenStack endpoint     openstack keystone-v3    --name  value   Credential name  --tenant-user  value   OpenStack user name     --tenant-password  value   OpenStack password  --user-domain  value   OpenStack user domain      --endpoint  value    OpenStack endpoint    Options  --description  value   Description of the resource  --public   Public in account  --server  value  Server address [$CB_SERVER_ADDRESS]  --username  value  User name (e-mail address) [$CB_USER_NAME]  --password  value  Password [$CB_PASSWORD]  --profile  value  Selects a config profile to use [$CB_PROFILE]    Additionally, the following option is available for OpenStack Keystone2 and Keystone3:  --facing  value  API facing. One of: public, admin, internal  Additionally, the following option is available for OpenStack Keystone3:  --keystone-scope  value  OpenStack keystone scope. One of: default, domain, project   Examples  Creates a role-based credential on AWS:  cb credential create aws role-based --name my-credential1 --role-arn arn:aws:iam::517127065441:role/CredentialRole  Creates a key-based credential on AWS:  cb credential create aws key-based --name my-credential2 --access-key ABDVIRDFV3K4HLJ45SKA --secret-key D89L5pOPM+426Rtj3curKzJEJL3lYoNcP8GvguBV  Creates an app-based credential on Azure:  cb credential create azure app-based --name my-credential3 --subscription-id b8e7379e-568g-55d3-na82-45b8d421e998 --tenant-id  c79n5399-3231-65ba-8dgg-2g4e2a40085e --app-id 6d147d89-48d2-5de2-eef8-b89775bbfcg1 --app-password 4a8hBgfI52s/C8R5Sea2YHGnBFrD3fRONfdG8w7F2Ua=  Creates a credential on Google Cloud:  cb credential create gcp --name my-credential4 --project-id test-proj --service-account-id test@test-proj.iam.gserviceaccount.com --service-account-private-key-file /Users/test/3fff57a6f68e.p12  Creates a role-based credential on OpenStack with Keystone-v2:  cb credential create openstack keystone-v2 --name my-credential5 --tenant-user test --tenant-password MySecurePass123 --tenant-name test --endpoint http://openstack.test.organization.com:5000/v2.0  Related Links     Create Credential on AWS  Create Credential on Azure  Create Credential on GCP  Create Credential on OpenStack", 
            "title": "credential create"
        }, 
        {
            "location": "/cli-reference/index.html#credential-delete", 
            "text": "Deletes an existing Cloudbreak credential.  Required Options  --name  value   Name of the credential   Options  --server  value  Server address [$CB_SERVER_ADDRESS]  --username  value  User name (e-mail address) [$CB_USER_NAME]  --password  value  Password [$CB_PASSWORD]  --profile  value  Selects a config profile to use [$CB_PROFILE]   Examples  cb credential delete --name test-cred", 
            "title": "credential delete"
        }, 
        {
            "location": "/cli-reference/index.html#credential-describe", 
            "text": "Describes an existing credential.  Required Options  --name  value   Name of the credential   Options  --server  value  Server address [$CB_SERVER_ADDRESS]  --username  value  User name (e-mail address) [$CB_USER_NAME]  --password  value  Password [$CB_PASSWORD]  --profile  value  Selects a config profile to use [$CB_PROFILE]  --output  value   Supported formats: json, yaml, table (default: \"json\") [$CB_OUT_FORMAT]  Examples  cb credential describe --name testcred\n{\n  \"Name\": \"testcred\",\n  \"Description\": \"\",\n  \"CloudPlatform\": \"AZURE\"\n}", 
            "title": "credential describe"
        }, 
        {
            "location": "/cli-reference/index.html#credential-list", 
            "text": "Lists existing Cloudbreak credentials.  Required Options  None  Options  --server  value  Server address [$CB_SERVER_ADDRESS]  --username  value  User name (e-mail address) [$CB_USER_NAME]  --password  value  Password [$CB_PASSWORD]  --profile  value  Selects a config profile to use [$CB_PROFILE]  --output  value   Supported formats: json, yaml, table (default: \"json\") [$CB_OUT_FORMAT]  Examples  Lists credentials:  cb credential list \n[\n  {\n    \"Name\": \"testcred\",\n    \"Description\": \"\",\n    \"CloudPlatform\": \"AZURE\"\n  }\n]  Lists credentials, with output formatted in a table format:  cb credential list --output table\n+---------+-------------+---------------+\n|  NAME   | DESCRIPTION | CLOUDPLATFORM |\n+---------+-------------+---------------+\n| armcred |             | AZURE         |\n+---------+-------------+---------------+", 
            "title": "credential list"
        }, 
        {
            "location": "/cli-reference/index.html#imagecatalog-create", 
            "text": "Registers a new custom image catalog based on the URL provided.    Required Options     --name  value   Name for the image catalog  --url  value    URL location of the image catalog JSON file    Options  --description  value   Description for the recipe   --public    Public in account  --server  value   Server address [$CB_SERVER_ADDRESS]  --username  value   User name (e-mail address) [$CB_USER_NAME]  --password  value   Password [$CB_PASSWORD]  --profile  value   Selects a config profile to use [$CB_PROFILE]    Examples  Registers an image catalog called \"mycustomcatalog\" which is available at https://example.com/myimagecatalog.json:   cb imagecatalog create --name mycustomcatalog --url https://example.com/myimagecatalog.json  Related Links     Custom Images", 
            "title": "imagecatalog create"
        }, 
        {
            "location": "/cli-reference/index.html#imagecatalog-delete", 
            "text": "Deletes a previously registered custom image catalog.   Required Options     --name  value   Name for the image catalog     Options  --output  value   Supported formats: json, yaml, table (default: \"json\") [$CB_OUT_FORMAT]  --server  value   Server address [$CB_SERVER_ADDRESS]  --username  value   User name (e-mail address) [$CB_USER_NAME]  --password  value   Password [$CB_PASSWORD]  --profile  value   Selects a config profile to use [$CB_PROFILE]   Examples  Deletes an image catalog called \"mycustomcatalog\":  cb imagecatalog delete --name mycustomcatalog  Related Links     Custom Images", 
            "title": "imagecatalog delete"
        }, 
        {
            "location": "/cli-reference/index.html#imagecatalog-images", 
            "text": "Lists images from the specified image catalog available for the specified cloud provider.     Sub-commands     aws          Lists available aws images     azure        Lists available azure images     gcp          Lists available gcp images    openstack    Lists available openstack images      Required Options  --imagecatalog  value   Name of the imagecatalog       Options  --output  value   Supported formats: json, yaml, table (default: \"json\") [$CB_OUT_FORMAT]    --server  value   Server address [$CB_SERVER_ADDRESS]  --username  value   User name (e-mail address) [$CB_USER_NAME]  --password  value  Password [$CB_PASSWORD]   --profile  value   Selects a config profile to use [$CB_PROFILE]     Examples     Returns date, description, Ambari version, and image ID for all AWS images from an image catalog called \"myimagecatalog\":  ./cb imagecatalog images aws --imagecatalog cloudbreak-default\n[\n  {\n    \"Date\": \"2017-10-13\",\n    \"Description\": \"Cloudbreak official base image\",\n    \"Version\": \"2.6.0.0\",\n    \"ImageID\": \"44b140a4-bd0b-457d-b174-e988bee3ca47\"\n  },\n  {\n    \"Date\": \"2017-11-16\",\n    \"Description\": \"Official Cloudbreak image\",\n    \"Version\": \"2.6.0.0\",\n    \"ImageID\": \"3c7598a4-ebd6-4a02-5638-882f5c7f7add\"\n  }\n]  Related Links     Custom Images", 
            "title": "imagecatalog images"
        }, 
        {
            "location": "/cli-reference/index.html#imagecatalog-list", 
            "text": "Lists default and custom image catalogs registered with Cloudbreak instance.     Required Options     None    Options     --server  value   Server address [$CB_SERVER_ADDRESS]  --username  value   User name (e-mail address) [$CB_USER_NAME]  --password  value   Password [$CB_PASSWORD]  --profile  value   Selects a config profile to use [$CB_PROFILE]  --output  value   Supported formats: json, yaml, table (default: \"json\") [$CB_OUT_FORMAT]     Examples  Lists existing image catalogs:    cb  imagecatalog list \n[\n  {\n    \"Name\": \"mycustomcatalog\",\n    \"Default\": false,\n    \"URL\": \"https://example.com/imagecatalog.json\"\n  },\n  {\n    \"Name\": \"cloudbreak-default\",\n    \"Default\": true,\n    \"URL\": \"https://s3-eu-west-1.amazonaws.com/cloudbreak-info/v2-dev-cb-image-catalog.json\"\n  }\n]  Related Links     Custom Images", 
            "title": "imagecatalog list"
        }, 
        {
            "location": "/cli-reference/index.html#imagecatalog-set-default", 
            "text": "Sets the specified image catalog as default.    Required Options      --name  value   Name for the image catalog      Options  --output  value   Supported formats: json, yaml, table (default: \"json\") [$CB_OUT_FORMAT]  --server  value   Server address [$CB_SERVER_ADDRESS]  --username  value   User name (e-mail address) [$CB_USER_NAME]  --password  value   Password [$CB_PASSWORD]  --profile  value   Selects a config profile to use [$CB_PROFILE]   Examples  Sets \"mycustomcatalog\" as default:    imagecatalog set-default --name mycustomcatalog  Related Links     Custom Images", 
            "title": "imagecatalog set-default"
        }, 
        {
            "location": "/cli-reference/index.html#recipe-create", 
            "text": "Adds a new recipe from a file or from a URL.  Sub-commands  from-url   Creates a recipe by downloading it from a URL location  from-file   Creates a recipe by reading it from a local file    Required Options  from-url     --name  value   Name for the recipe   --execution-type  value   Type of execution [pre-ambari-start, pre-termination, post-ambari-start, post-cluster-install]    --url  value   URL location of the Ambari blueprint JSON file    from-file    --name  value   Name for the recipe  --execution-type  value   Type of execution [pre-ambari-start, pre-termination, post-ambari-start, post-cluster-install]   --file  value   Location of the Ambari blueprint JSON file    Options  --description  value   Description for the recipe   --public    Public in account  --server  value   Server address [$CB_SERVER_ADDRESS]  --username  value   User name (e-mail address) [$CB_USER_NAME]  --password  value   Password [$CB_PASSWORD]  --profile  value   Selects a config profile to use [$CB_PROFILE]    Examples  Adds a new recipe called \"test1\" from a URL:  cb recipe create from-url --name \"test1\" --execution-type post-ambari-start --url http://some-site.com/test.sh  Adds a new recipe called \"test2\" from a file:  cb recipe create from-url --name \"test2\" --execution-type post-ambari-start --file /Users/test/Documents/test.sh  Related Links  Recipes", 
            "title": "recipe create"
        }, 
        {
            "location": "/cli-reference/index.html#recipe-delete", 
            "text": "Deletes an existing recipe.  Required Options  --name  value   Name for the recipe    Options  --server  value   Server address [$CB_SERVER_ADDRESS]  --username  value   User name (e-mail address) [$CB_USER_NAME]  --password  value   Password [$CB_PASSWORD]  --profile  value   Selects a config profile to use [$CB_PROFILE]  --output  value   Supported formats: json, yaml, table (default: \"json\") [$CB_OUT_FORMAT]  Examples  cb recipe delete --name test", 
            "title": "recipe delete"
        }, 
        {
            "location": "/cli-reference/index.html#recipe-describe", 
            "text": "Describes an existing recipe.  Required Options  --name  value   Name for the recipe      Options  --server  value   Server address [$CB_SERVER_ADDRESS]  --username  value   User name (e-mail address) [$CB_USER_NAME]  --password  value   Password [$CB_PASSWORD]  --profile  value   Selects a config profile to use [$CB_PROFILE]  --output  value   Supported formats: json, yaml, table (default: \"json\") [$CB_OUT_FORMAT]  Examples  Describes a recipe called \"test\":  cb recipe describe --name test\n{\n  \"Name\": \"test\",\n  \"Description\": \"\",\n  \"ExecutionType\": \"POST\"\n}  Describes a recipe called \"test\", with output presented in a table format:  cb describe-recipe --name test --output table\n+------+-------------+----------------+\n| NAME | DESCRIPTION | EXECUTION TYPE |\n+------+-------------+----------------+\n| test |             | POST           |\n+------+-------------+----------------+", 
            "title": "recipe describe"
        }, 
        {
            "location": "/cli-reference/index.html#recipe-list", 
            "text": "Lists all available recipes.  Required Options  None  Options  --server  value   Server address [$CB_SERVER_ADDRESS]  --username  value   User name (e-mail address) [$CB_USER_NAME]  --password  value   Password [$CB_PASSWORD]  --profile  value   Selects a config profile to use [$CB_PROFILE]  --output  value   Supported formats: json, yaml, table (default: \"json\") [$CB_OUT_FORMAT]    Examples  Lists existing recipes:  cb recipe list\n[\n  {\n    \"Name\": \"test\",\n    \"Description\": \"\",\n    \"ExecutionType\": \"POST\"\n  }\n]  Lists existing recipes, with output presented in a table format:  cb recipe list --output table\n+------+-------------+-------------------+\n| NAME | DESCRIPTION | EXECUTION TYPE    |\n+------+-------------+-------------------+\n| test |             | POST-AMBARI-START |\n+------+-------------+-------------------+", 
            "title": "recipe list"
        }, 
        {
            "location": "/cli-reference/index.html#debugging", 
            "text": "To use debugging mode, pass the  --debug  option.", 
            "title": "Debugging"
        }, 
        {
            "location": "/cli-reference/index.html#checking-cli-version", 
            "text": "To check CLI version, use  cb --version .", 
            "title": "Checking CLI Version"
        }, 
        {
            "location": "/trouble-cb-logs/index.html", 
            "text": "Checking Cloudbreak Logs\n\n\nWhen troubleshooting, you can access the following Cloudbreak logs.\n\n\nCloudbreak Logs\n\n\nWhen installing Cloudbreak using a pre-built cloud image, the  Cloudbreak deployer location and the cbd root folder is \n/var/lib/cloudbreak-deployment\n. You must execute all cbd actions from the cbd root folder as a cloudbreak user. \n\n\n\n\nYour cbd root directory may be different if you installed Cloudbreak on your own VM. \n\n\n\n\nAggregated Logs\n\n\nCloudbreak consists of multiple microservices deployed into Docker containers. \n\n\nTo check aggregated service logs, use the following commands:\n\n\ncbd logs\n shows all service logs.\n\n\ncbd logs | tee cloudbreak.log\n allows you to redirect the input into a file for sharing these logs.\n\n\nIndividual Service Logs\n\n\nTo check individual service logs, use the following commands:\n\n\ncbd logs cloudbreak\n shows Cloudbreak logs. This service is the backend service that handles all deployments.\n\n\ncbd logs uluwatu\n shows Cloudbreak UI logs. Uluwatu is the UI component of Cloudbreak.\n\n\ncbd logs identity\n shows Identity logs. Identity is responsible for authentication and authorization.\n\n\ncbd logs periscope\n shows Periscope logs. Periscope is responsible for triggering autoscaling rules.\n\n\nDocker Logs\n\n\nThe same logs can be accessed via Docker commands:\n\n\ndocker logs cbreak_cloudbreak_1\n shows the same logs as \ncbd logs cloudbreak\n.\n\n\nCloudbreak logs are rotated and can be accessed later from the Cloudbreak deployment folder. Each time you restart the application via cbd restart a new log file is created with a timestamp in the name (for example, cbreak-20170821-105900.log). \n\n\n\n\nThere is a symlink called \ncbreak.log\n which points to the latest log file. Sharing this symlink does not share the log itself.\n\n\n\n\nSaltstack Logs\n\n\nCloudbreak uses Saltstack to install Ambari and the necessary packages for the HDP provisioning. Salt Master always runs alongside the Ambari Server node. Each instance in the cluster runs a Salt Minion, which connects to the Salt Master. There can be multiple Salt Masters if the cluster is configured to run in HA (High Availability) mode and in this case each Salt Minion connects to each Salt Master.\n\n\nCloudbreak also uses SaltStack to execute user-provided customization scripts called \"recipes\". \n\n\nSalt Master and Salt Minion logs can be found at the following location: \n/var/log/salt\n\n\nAmbari Logs\n\n\nCloudbreak uses Ambari to orchestrate the installation of the different HDP components. Each instance in the cluster runs an Ambari agent which connects to the Ambari server. Ambari server is declared by the user during the cluster installation wizard. \n\n\nAmbari Server Logs\n\n\nAmbari server logs can be found on the nodes where Ambari server is installed in the following locations:\n\n\n/var/log/ambari-server/ambari-server.log\n\n\n/var/log/ambari-server/ambari-server.out\n\n\nBoth files contain important information about the root cause of a certain issue so it is advised to check both.\n\n\nAmbari Agent Logs\n\n\nAmbari agent logs can be found on the nodes where Ambari agent is installed in the following locations:\n\n\n/var/log/ambari-agent/ambari-agent.log\n\n\nRecipe Logs\n\n\nCloudbreak supports \"recipes\" - user-provided customization scripts that can be run prior to or after cluster installation. It is the user\u2019s responsibility to provide an idempotent well tested script. If the execution fails, the recipe logs can be found at \n/var/log/recipes\n on the nodes on which the recipes were executed.\n\n\nIt is advised, but not required to have an advanced logging mechanism in the script, as Cloudbreak always logs every script that are run. Recipes are often the sources of installation failures as users might try to remove necessary packages or reconfigure services.\n\n\nRelated Links\n\n\nTroubleshooting Cloudbreak", 
            "title": "Cloudbreak Logs"
        }, 
        {
            "location": "/trouble-cb-logs/index.html#checking-cloudbreak-logs", 
            "text": "When troubleshooting, you can access the following Cloudbreak logs.", 
            "title": "Checking Cloudbreak Logs"
        }, 
        {
            "location": "/trouble-cb-logs/index.html#cloudbreak-logs", 
            "text": "When installing Cloudbreak using a pre-built cloud image, the  Cloudbreak deployer location and the cbd root folder is  /var/lib/cloudbreak-deployment . You must execute all cbd actions from the cbd root folder as a cloudbreak user.    Your cbd root directory may be different if you installed Cloudbreak on your own VM.", 
            "title": "Cloudbreak Logs"
        }, 
        {
            "location": "/trouble-cb-logs/index.html#aggregated-logs", 
            "text": "Cloudbreak consists of multiple microservices deployed into Docker containers.   To check aggregated service logs, use the following commands:  cbd logs  shows all service logs.  cbd logs | tee cloudbreak.log  allows you to redirect the input into a file for sharing these logs.", 
            "title": "Aggregated Logs"
        }, 
        {
            "location": "/trouble-cb-logs/index.html#individual-service-logs", 
            "text": "To check individual service logs, use the following commands:  cbd logs cloudbreak  shows Cloudbreak logs. This service is the backend service that handles all deployments.  cbd logs uluwatu  shows Cloudbreak UI logs. Uluwatu is the UI component of Cloudbreak.  cbd logs identity  shows Identity logs. Identity is responsible for authentication and authorization.  cbd logs periscope  shows Periscope logs. Periscope is responsible for triggering autoscaling rules.", 
            "title": "Individual Service Logs"
        }, 
        {
            "location": "/trouble-cb-logs/index.html#docker-logs", 
            "text": "The same logs can be accessed via Docker commands:  docker logs cbreak_cloudbreak_1  shows the same logs as  cbd logs cloudbreak .  Cloudbreak logs are rotated and can be accessed later from the Cloudbreak deployment folder. Each time you restart the application via cbd restart a new log file is created with a timestamp in the name (for example, cbreak-20170821-105900.log).    There is a symlink called  cbreak.log  which points to the latest log file. Sharing this symlink does not share the log itself.", 
            "title": "Docker Logs"
        }, 
        {
            "location": "/trouble-cb-logs/index.html#saltstack-logs", 
            "text": "Cloudbreak uses Saltstack to install Ambari and the necessary packages for the HDP provisioning. Salt Master always runs alongside the Ambari Server node. Each instance in the cluster runs a Salt Minion, which connects to the Salt Master. There can be multiple Salt Masters if the cluster is configured to run in HA (High Availability) mode and in this case each Salt Minion connects to each Salt Master.  Cloudbreak also uses SaltStack to execute user-provided customization scripts called \"recipes\".   Salt Master and Salt Minion logs can be found at the following location:  /var/log/salt", 
            "title": "Saltstack Logs"
        }, 
        {
            "location": "/trouble-cb-logs/index.html#ambari-logs", 
            "text": "Cloudbreak uses Ambari to orchestrate the installation of the different HDP components. Each instance in the cluster runs an Ambari agent which connects to the Ambari server. Ambari server is declared by the user during the cluster installation wizard.", 
            "title": "Ambari Logs"
        }, 
        {
            "location": "/trouble-cb-logs/index.html#ambari-server-logs", 
            "text": "Ambari server logs can be found on the nodes where Ambari server is installed in the following locations:  /var/log/ambari-server/ambari-server.log  /var/log/ambari-server/ambari-server.out  Both files contain important information about the root cause of a certain issue so it is advised to check both.", 
            "title": "Ambari Server Logs"
        }, 
        {
            "location": "/trouble-cb-logs/index.html#ambari-agent-logs", 
            "text": "Ambari agent logs can be found on the nodes where Ambari agent is installed in the following locations:  /var/log/ambari-agent/ambari-agent.log", 
            "title": "Ambari Agent Logs"
        }, 
        {
            "location": "/trouble-cb-logs/index.html#recipe-logs", 
            "text": "Cloudbreak supports \"recipes\" - user-provided customization scripts that can be run prior to or after cluster installation. It is the user\u2019s responsibility to provide an idempotent well tested script. If the execution fails, the recipe logs can be found at  /var/log/recipes  on the nodes on which the recipes were executed.  It is advised, but not required to have an advanced logging mechanism in the script, as Cloudbreak always logs every script that are run. Recipes are often the sources of installation failures as users might try to remove necessary packages or reconfigure services.  Related Links  Troubleshooting Cloudbreak", 
            "title": "Recipe Logs"
        }, 
        {
            "location": "/trouble-cb/index.html", 
            "text": "Troubleshooting Cloudbreak\n\n\nThis section includes common errors and steps to resolve them. \n\n\nQuota Limitations\n\n\nEach cloud provider has quota limitations on various cloud resources, and these quotas can usually be increased on request. If there is an error message in Cloudbreak saying that there are no more available EIPs (Elastic IP Address) or VPCs, you need to request more of these resources. \n\n\nTo see the limitations visit the cloud provider\u2019s site:\n\n\n\n\nAWS Service Limits\n \n\n\nAzure subscription and service limits, quotas, and constraints\n\n\nGCP Resource Quotas\n \n\n\n\n\nConnection Timeout When Ports Are Not Open\n\n\nIn the cluster installation wizard, you must specify on which node you want to run the Ambari server. Cloudbreak communicates with this node to orchestrate the installation.\n\n\nA common reason for connection timeout is security group misconfiguration. Cloudbreak allows configuring different security groups for the different instance groups; however, there are certain requirements for the Ambari server node. Specifically, the following ports must be open in order to communicate with that node:\n\n\n\n\n22 (SSH)  \n\n\n9443 (two-way-ssl through nginx) \n\n\n\n\nBlueprint Errors\n\n\nInvalid Services and Configurations\n\n\nAmbari blueprints are a declarative definition of a cluster. With a blueprint, you specify a stack, the component layout, and the configurations to materialize a Hadoop cluster instance via a REST API without having to use the Ambari cluster install wizard. \n\n\nCloudbreak supports any type of blueprints, which is a common source of errors. These errors are only visible once the core infrastructure is up and running and Cloudbreak tries to initiate the cluster installation through Ambari. Ambari validates the blueprint and  rejects it if it's invalid. \n\n\nFor example, if there are configurations for a certain service like Hive but Hive as a service is not mapped to any host group, the blueprint is invalid.\n\n\nTo fix these type of issues, edit your blueprint and then reinstall your cluster. Cloudbreak UI has support for this so the infrastructure does not have to be terminated.\n\n\nThere are some cases when Ambari cannot validate your blueprint beforehand. In these cases, the issues are only visible in the Ambari server logs. To troubleshoot, check Ambari server logs.\n\n\nWrong HDP Version\n\n\nIn the blueprint, only the major and minor HDP version should be defined (for example, \"2.6\"). If wrong version number is provided, the following error can be found in the logs:\n\n\n5/15/2017 12:23:19 PM testcluster26 - create failed: Cannot use the specified Ambari stack: HDPRepo\n{stack='null'; utils='null'}\n. Error: org.apache.ambari.server.controller.spi.NoSuchResourceException: The specified resource doesn't exist: Stack data, Stack HDP 2.6.0.3 is not found in Ambari metainfo\n\n\n\n\nFor correct blueprint layout, refer to the \nAmbari cwiki\n page.\n\n\nRecipe Errors\n\n\nRecipe Execution Times Out\n\n\nIf the scripts are taking too much time to execute, the processes will time out, as the threshold for all recipes is set to 15 minutes. To change this threshold, you must override the default value by adding the following to the cbd Profile file:\n\n\nexport CB_JAVA_OPTS=\u201d -Dcb.max.salt.recipe.execution.retry=90\u201d\n\n\n\n\nThis property indicates the number of tries for checking if the scripts have finished with a sleep time (i.e. the wait time between two polling attempts) of 10 seconds. The default value is 90. To increase the threshold, provide a number greater than 90. You must restart Cloudbreak after changing properties in the Profile file.\n\n\nRecipe Execution Fails\n\n\nIt often happens that a script cannot be executed successfully because there are typos or errors in the script. To verify this you can check the recipe logs at\n\n/var/log/recipes\n. For each script, there will be a separate log file with the name of the script that you provided on the Cloudbreak UI.\n\n\nInvalid PUBLIC_IP in CBD Profile\n\n\nThe \nPUBLIC_IP\n property must be set in the cbd Profile file or else you won\u2019t be able to log in on the Cloudbreak UI. \n\n\nIf you are migrating your instance, make sure that after the start the IP remains valid. If you need to edit the \nPUBLIC_IP\n property in Profile, make sure to restart Cloudbreak using \ncbd restart\n.\n\n\nCbd Cannot Get VM's Public IP\n\n\nBy default the \ncbd\n tool tries to get the VM's public IP to bind Cloudbreak UI to it. But if \ncbd\n cannot get the IP address during the initialization, you must set it manually. Check your \nProfile\n and if \nPUBLIC_IP\n is not set, add the \nPUBLIC_IP\n variable and set it to the public IP of the VM. For example: \n\n\nexport PUBLIC_IP=192.134.23.10\n\n\n\nPermission or Connection Problems\n\n\nIf you face permission or connection issues, disable SELinux:\n\n\n\n\nSet \nSELINUX=disabled\n in \n/etc/selinux/config\n.  \n\n\nReboot the machine.  \n\n\n\n\nEnsure the SELinux is not turned on afterwards:\n\n\n\n\n\n\n\n\nChanging Properties in the Profile\n\n\nThere are many properties that can be changed in the Cloudbreak application. These values must be changed in the Cloudbreak \nProfil\ne file. To see all possible options, use the following command:\n\ncbd env show\n.\n\n\nAfter changing a property, you must regenerate the config file and restart the application. There are two ways to do this:\n\n\nIn version 1.4.0 and newer of the cbd command line, you can regenerate the config file and restart the application with a single command:\n\n\ncbd restart\n - same as cbd regenerate/kill/start.\n\n\nIn versions earlier than 1.4.0, you must run the following three commands:\n\n\ncbd regenerate\n regenerates the Docker compose file\n\ncbd kill\n removes all Docker containers (there is no stop command for this).\n\ncbd start\n starts the application with the new compose file.\n\n\nRelated Links\n\n\nCloudbreak Logs\n\n\nTroubleshooting AWS\n\n\nTroubleshooting Azure\n\n\nTroubleshooting GCP\n\n\nTroubleshooting OpenStack", 
            "title": "General Troubleshooting"
        }, 
        {
            "location": "/trouble-cb/index.html#troubleshooting-cloudbreak", 
            "text": "This section includes common errors and steps to resolve them.", 
            "title": "Troubleshooting Cloudbreak"
        }, 
        {
            "location": "/trouble-cb/index.html#quota-limitations", 
            "text": "Each cloud provider has quota limitations on various cloud resources, and these quotas can usually be increased on request. If there is an error message in Cloudbreak saying that there are no more available EIPs (Elastic IP Address) or VPCs, you need to request more of these resources.   To see the limitations visit the cloud provider\u2019s site:   AWS Service Limits    Azure subscription and service limits, quotas, and constraints  GCP Resource Quotas", 
            "title": "Quota Limitations"
        }, 
        {
            "location": "/trouble-cb/index.html#connection-timeout-when-ports-are-not-open", 
            "text": "In the cluster installation wizard, you must specify on which node you want to run the Ambari server. Cloudbreak communicates with this node to orchestrate the installation.  A common reason for connection timeout is security group misconfiguration. Cloudbreak allows configuring different security groups for the different instance groups; however, there are certain requirements for the Ambari server node. Specifically, the following ports must be open in order to communicate with that node:   22 (SSH)    9443 (two-way-ssl through nginx)", 
            "title": "Connection Timeout When Ports Are Not Open"
        }, 
        {
            "location": "/trouble-cb/index.html#blueprint-errors", 
            "text": "", 
            "title": "Blueprint Errors"
        }, 
        {
            "location": "/trouble-cb/index.html#invalid-services-and-configurations", 
            "text": "Ambari blueprints are a declarative definition of a cluster. With a blueprint, you specify a stack, the component layout, and the configurations to materialize a Hadoop cluster instance via a REST API without having to use the Ambari cluster install wizard.   Cloudbreak supports any type of blueprints, which is a common source of errors. These errors are only visible once the core infrastructure is up and running and Cloudbreak tries to initiate the cluster installation through Ambari. Ambari validates the blueprint and  rejects it if it's invalid.   For example, if there are configurations for a certain service like Hive but Hive as a service is not mapped to any host group, the blueprint is invalid.  To fix these type of issues, edit your blueprint and then reinstall your cluster. Cloudbreak UI has support for this so the infrastructure does not have to be terminated.  There are some cases when Ambari cannot validate your blueprint beforehand. In these cases, the issues are only visible in the Ambari server logs. To troubleshoot, check Ambari server logs.", 
            "title": "Invalid Services and Configurations"
        }, 
        {
            "location": "/trouble-cb/index.html#wrong-hdp-version", 
            "text": "In the blueprint, only the major and minor HDP version should be defined (for example, \"2.6\"). If wrong version number is provided, the following error can be found in the logs:  5/15/2017 12:23:19 PM testcluster26 - create failed: Cannot use the specified Ambari stack: HDPRepo\n{stack='null'; utils='null'}\n. Error: org.apache.ambari.server.controller.spi.NoSuchResourceException: The specified resource doesn't exist: Stack data, Stack HDP 2.6.0.3 is not found in Ambari metainfo  For correct blueprint layout, refer to the  Ambari cwiki  page.", 
            "title": "Wrong HDP Version"
        }, 
        {
            "location": "/trouble-cb/index.html#recipe-errors", 
            "text": "", 
            "title": "Recipe Errors"
        }, 
        {
            "location": "/trouble-cb/index.html#recipe-execution-times-out", 
            "text": "If the scripts are taking too much time to execute, the processes will time out, as the threshold for all recipes is set to 15 minutes. To change this threshold, you must override the default value by adding the following to the cbd Profile file:  export CB_JAVA_OPTS=\u201d -Dcb.max.salt.recipe.execution.retry=90\u201d  This property indicates the number of tries for checking if the scripts have finished with a sleep time (i.e. the wait time between two polling attempts) of 10 seconds. The default value is 90. To increase the threshold, provide a number greater than 90. You must restart Cloudbreak after changing properties in the Profile file.", 
            "title": "Recipe Execution Times Out"
        }, 
        {
            "location": "/trouble-cb/index.html#recipe-execution-fails", 
            "text": "It often happens that a script cannot be executed successfully because there are typos or errors in the script. To verify this you can check the recipe logs at /var/log/recipes . For each script, there will be a separate log file with the name of the script that you provided on the Cloudbreak UI.", 
            "title": "Recipe Execution Fails"
        }, 
        {
            "location": "/trouble-cb/index.html#invalid-public_ip-in-cbd-profile", 
            "text": "The  PUBLIC_IP  property must be set in the cbd Profile file or else you won\u2019t be able to log in on the Cloudbreak UI.   If you are migrating your instance, make sure that after the start the IP remains valid. If you need to edit the  PUBLIC_IP  property in Profile, make sure to restart Cloudbreak using  cbd restart .", 
            "title": "Invalid PUBLIC_IP in CBD Profile"
        }, 
        {
            "location": "/trouble-cb/index.html#cbd-cannot-get-vms-public-ip", 
            "text": "By default the  cbd  tool tries to get the VM's public IP to bind Cloudbreak UI to it. But if  cbd  cannot get the IP address during the initialization, you must set it manually. Check your  Profile  and if  PUBLIC_IP  is not set, add the  PUBLIC_IP  variable and set it to the public IP of the VM. For example:   export PUBLIC_IP=192.134.23.10", 
            "title": "Cbd Cannot Get VM's Public IP"
        }, 
        {
            "location": "/trouble-cb/index.html#permission-or-connection-problems", 
            "text": "If you face permission or connection issues, disable SELinux:   Set  SELINUX=disabled  in  /etc/selinux/config .    Reboot the machine.     Ensure the SELinux is not turned on afterwards:", 
            "title": "Permission or Connection Problems"
        }, 
        {
            "location": "/trouble-cb/index.html#changing-properties-in-the-profile", 
            "text": "There are many properties that can be changed in the Cloudbreak application. These values must be changed in the Cloudbreak  Profil e file. To see all possible options, use the following command: cbd env show .  After changing a property, you must regenerate the config file and restart the application. There are two ways to do this:  In version 1.4.0 and newer of the cbd command line, you can regenerate the config file and restart the application with a single command:  cbd restart  - same as cbd regenerate/kill/start.  In versions earlier than 1.4.0, you must run the following three commands:  cbd regenerate  regenerates the Docker compose file cbd kill  removes all Docker containers (there is no stop command for this). cbd start  starts the application with the new compose file.  Related Links  Cloudbreak Logs  Troubleshooting AWS  Troubleshooting Azure  Troubleshooting GCP  Troubleshooting OpenStack", 
            "title": "Changing Properties in the Profile"
        }, 
        {
            "location": "/trouble-aws/index.html", 
            "text": "Troubleshooting Cloudbreak on AWS\n\n\nComing soon! Meanwhile, check out \nHDCloud\n troubleshooting docs.", 
            "title": "Troubleshooting AWS"
        }, 
        {
            "location": "/trouble-aws/index.html#troubleshooting-cloudbreak-on-aws", 
            "text": "Coming soon! Meanwhile, check out  HDCloud  troubleshooting docs.", 
            "title": "Troubleshooting Cloudbreak on AWS"
        }, 
        {
            "location": "/trouble-azure/index.html", 
            "text": "Troubleshooting Cloudbreak on Azure\n\n\nCloudbreak Deployment Errors\n\n\nInvalid Resource Reference\n\n\nExample error message:\n\n\nResource /subscriptions/.../resourceGroups//providers/Microsoft.Network/virtualNetworks/cbdeployerVnet/\n\nsubnets/cbdeployerSubnet referenced by resource /subscriptions/.../resourceGroups/Manulife-ADLS/providers/\n\nMicrosoft.Network/networkInterfaces/cbdeployerNic was not found.\n\nPlease make sure that the referenced resource exists, and that both resources are in the same region.\n\n\nSymptom\n: The most common reason for this error is that you did not provide the Vnet RG Name (last parameter in the template).  \n\n\nSolution\n: When launching Cloudbreak, under \"Vnet RG Name\" provide the name of the resource group in which the selected VNet is located. If using a new VNet, enter the same resource group name as in \"Resource group\". \n\n\nCredential Creation Errors\n\n\nRole already exists\n\n\nExample error message: \nRole already exists in Azure with the name: CloudbreakCustom50\n\n\nSymptom\n: You specified that you want to create a new role for Cloudbreak credential, but an existing role with the same name already exists in Azure. \n\n\nSolution\n: You should either rename the role during credential creation or select the \nReuse existing custom role\n option. \n\n\nRole does not exist\n\n\nExample error message: \nRole does not exist in Azure with the name: CloudbreakCustom60\n\n\nSymptom\n: You specified that you want to reuse an existing role for your Cloudbreak credential, but that particular role does not exist in Azure.\n\n\nSolution\n: You should either rename the new role during the credential creation to match the existing role's name or select the \nLet Cloudbreak create a custom role\n option. \n\n\nRole does not have enough privileges\n\n\nExample error message: \nCloudbreakCustom 50 role does not have enough privileges to be used by Cloudbreak!\n\n\n\n\nSymptom\n: You specified that you want to reuse an  existing role for your Cloudbreak credential, but that particular role does not have the necessary privileges for Cloudbreak cluster management.\n\n\nSolution\n: You should either select an existing role with enough privileges or select the \nLet Cloudbreak create a custom role\n option.\n\n\nThe necessary action set for Cloudbreak to be able to manage the clusters includes:\n        \n\"Microsoft.Compute/*\",\n        \"Microsoft.Network/*\",\n        \"Microsoft.Storage/*\",\n        \"Microsoft.Resources/*\"\n\n\nClient does not have authorization\n\n\nExample error message:\n\n\nFailed to verify credential: Status code 403, {\"error\":{\"code\":\"AuthorizationFailed\",\n\n\"message\":\"The client 'X' with object id 'z' does not have authorization to perform action\n\n'Microsoft.Storage/storageAccounts/read' over scope 'subscriptions/...'\"}\n\n\nSymptom\n: Your Azure account does not have sufficient permissions to create a Coudbreak credential. \n\n\nSolution\n: If you get this error during interactive credential creation, please ensure that your Azure account has \nMicrosoft.Authorization/*/Write\n permission. Otherwise contact your Azure administrator to either give your account that permission or create the necessary resources for the app-based credential creation method.  \n\n\nCloud not validate publickey certificate\n\n\nExample error message:\n\n\nCould not validate publickey certificate [certificate: 'fdfdsf'], detailed message: \n\nCorrupt or unknown public key file format\n\n\nSymptom\n: The syntax of your SSH public key is incorrect.\n\n\nSolution\n: You must correct the syntax of your SSH key. For information about the correct syntax, refer to \nthis\n page.", 
            "title": "Troubleshooting Azure"
        }, 
        {
            "location": "/trouble-azure/index.html#troubleshooting-cloudbreak-on-azure", 
            "text": "", 
            "title": "Troubleshooting Cloudbreak on Azure"
        }, 
        {
            "location": "/trouble-azure/index.html#cloudbreak-deployment-errors", 
            "text": "", 
            "title": "Cloudbreak Deployment Errors"
        }, 
        {
            "location": "/trouble-azure/index.html#invalid-resource-reference", 
            "text": "Example error message:  Resource /subscriptions/.../resourceGroups//providers/Microsoft.Network/virtualNetworks/cbdeployerVnet/ \nsubnets/cbdeployerSubnet referenced by resource /subscriptions/.../resourceGroups/Manulife-ADLS/providers/ \nMicrosoft.Network/networkInterfaces/cbdeployerNic was not found. \nPlease make sure that the referenced resource exists, and that both resources are in the same region.  Symptom : The most common reason for this error is that you did not provide the Vnet RG Name (last parameter in the template).    Solution : When launching Cloudbreak, under \"Vnet RG Name\" provide the name of the resource group in which the selected VNet is located. If using a new VNet, enter the same resource group name as in \"Resource group\".", 
            "title": "Invalid Resource Reference"
        }, 
        {
            "location": "/trouble-azure/index.html#credential-creation-errors", 
            "text": "", 
            "title": "Credential Creation Errors"
        }, 
        {
            "location": "/trouble-azure/index.html#role-already-exists", 
            "text": "Example error message:  Role already exists in Azure with the name: CloudbreakCustom50  Symptom : You specified that you want to create a new role for Cloudbreak credential, but an existing role with the same name already exists in Azure.   Solution : You should either rename the role during credential creation or select the  Reuse existing custom role  option.", 
            "title": "Role already exists"
        }, 
        {
            "location": "/trouble-azure/index.html#role-does-not-exist", 
            "text": "Example error message:  Role does not exist in Azure with the name: CloudbreakCustom60  Symptom : You specified that you want to reuse an existing role for your Cloudbreak credential, but that particular role does not exist in Azure.  Solution : You should either rename the new role during the credential creation to match the existing role's name or select the  Let Cloudbreak create a custom role  option.", 
            "title": "Role does not exist"
        }, 
        {
            "location": "/trouble-azure/index.html#role-does-not-have-enough-privileges", 
            "text": "Example error message:  CloudbreakCustom 50 role does not have enough privileges to be used by Cloudbreak!   Symptom : You specified that you want to reuse an  existing role for your Cloudbreak credential, but that particular role does not have the necessary privileges for Cloudbreak cluster management.  Solution : You should either select an existing role with enough privileges or select the  Let Cloudbreak create a custom role  option.  The necessary action set for Cloudbreak to be able to manage the clusters includes:\n         \"Microsoft.Compute/*\",\n        \"Microsoft.Network/*\",\n        \"Microsoft.Storage/*\",\n        \"Microsoft.Resources/*\"", 
            "title": "Role does not have enough privileges"
        }, 
        {
            "location": "/trouble-azure/index.html#client-does-not-have-authorization", 
            "text": "Example error message:  Failed to verify credential: Status code 403, {\"error\":{\"code\":\"AuthorizationFailed\", \n\"message\":\"The client 'X' with object id 'z' does not have authorization to perform action \n'Microsoft.Storage/storageAccounts/read' over scope 'subscriptions/...'\"}  Symptom : Your Azure account does not have sufficient permissions to create a Coudbreak credential.   Solution : If you get this error during interactive credential creation, please ensure that your Azure account has  Microsoft.Authorization/*/Write  permission. Otherwise contact your Azure administrator to either give your account that permission or create the necessary resources for the app-based credential creation method.", 
            "title": "Client does not have authorization"
        }, 
        {
            "location": "/trouble-azure/index.html#cloud-not-validate-publickey-certificate", 
            "text": "Example error message:  Could not validate publickey certificate [certificate: 'fdfdsf'], detailed message:  \nCorrupt or unknown public key file format  Symptom : The syntax of your SSH public key is incorrect.  Solution : You must correct the syntax of your SSH key. For information about the correct syntax, refer to  this  page.", 
            "title": "Cloud not validate publickey certificate"
        }, 
        {
            "location": "/trouble-gcp/index.html", 
            "text": "Troubleshooting Cloudbreak on GCP\n\n\nComing soon!", 
            "title": "Troubleshooting GCP"
        }, 
        {
            "location": "/trouble-gcp/index.html#troubleshooting-cloudbreak-on-gcp", 
            "text": "Coming soon!", 
            "title": "Troubleshooting Cloudbreak on GCP"
        }, 
        {
            "location": "/trouble-os/index.html", 
            "text": "Troubleshooting Cloudbreak on OpenStack\n\n\nComing soon!", 
            "title": "Troubleshooting OpenStack"
        }, 
        {
            "location": "/trouble-os/index.html#troubleshooting-cloudbreak-on-openstack", 
            "text": "Coming soon!", 
            "title": "Troubleshooting Cloudbreak on OpenStack"
        }, 
        {
            "location": "/dev/index.html", 
            "text": "Developer Documentation\n\n\nThe following table includes links to Cloudbreak developer documentation: \n\n\n\n\n\n\n\n\nDoc Link\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nSet Up Local Development\n\n\nThis documentation will help you set up your local development environment.\n\n\n\n\n\n\nRetrieve OAuth Bearer Token via Cloudbreak REST API\n\n\nDescribes how to retrieve OAuth bearer token via Cloudbreak REST API.\n\n\n\n\n\n\nSPI Reference\n\n\nThis is Cloudbreak SPI reference documentation.\n\n\n\n\n\n\nAPI Reference\n\n\nThis is Cloudbreak API reference documentation. Cloudbreak is a RESTful application development platform whose goal is to help developers deploy HDP clusters in various cloud environments. Once Cloudbreak is deployed in your favorite servlet container, it exposes REST APIs, allowing you to spin up Hadoop clusters of any size with your chosen cloud provider.\n\n\n\n\n\n\nFlow Diagrams\n\n\nThis is Cloudbreak flow diagrams reference documentation.\n\n\n\n\n\n\n\n\nRetrieve OAuth Bearer Token via Cloudbreak REST API\n\n\nIn order to communicate with Cloudbreak's API, you must retrieve a bearer token. For example: \n\n\nTOKEN=$(curl -k -iX POST -H \"accept: application/x-www-form-urlencoded\" -d 'credentials={\"username\":\"admin@example.com\",\"password\":\"pwd\"}' \"https://192.168.99.100/identity/oauth/authorize?response_type=token\n_id=cloudbreak_shell\n=openid\n=login\n_uri=http://cloudbreak.shell\" | grep location | cut -d'=' -f 3 | cut -d'\n&\n' -f 1)\n\n\n\nCloudbreak Service Provider Interface (SPI)\n\n\nIn addition to supporting multiple cloud platforms, Cloudbreak provides an easy way to integrate a new provider trough its \nService Provider Interface (SPI)\n, a plugin mechanism that enables seamless integration with any cloud provider. \n\n\nThis SPI plugin mechanism has been used to integrate all currently supported providers with Cloudbreak. The following links point to the Cloudbreak SPI implementations for AWS, Azure, Google Cloud, and OpenStack. You can use these implementations as a reference:\n\n\n\n\nThe \ncloud-aws\n module integrates Amazon Web Services\n\n\nThe \ncloud-arm\n module integrates Microsoft Azure\n\n\nThe \ncloud-gcp\n module integrates Google Cloud Platform  \n\n\nThe \ncloud-openstack\n module integrates OpenStack\n\n\n\n\nThe Cloubdreak SPI interface is event-based, scalable, and decoupled from Cloudbreak. The core of Cloudbreak uses \nEventBus\n to communicate with the providers, but the complexity of event handling is hidden from the provider implementation.\n\n\nSupported Resource Management Methods\n\n\nCloud providers support two kinds of deployment and resource management methods:\n\n\n\n\nTemplate based deployments\n\n\nIndividual resource based deployments\n\n\n\n\nCloudbreak's SPI supports both of these methods. It provides a well-defined interface, abstract classes, and helper classes, scheduling and polling of resources to aid the integration and to avoid any boilerplate code in the module of cloud provider.\n\n\nTemplate Based Deployments\n\n\nProviders with template-based deployments such as \nAWS CloudFormation\n, \nAzure ARM\n or \nOpenStack Heat\n have the ability to create and manage a collection of related cloud resources, provisioning and updating them in an orderly and predictable fashion. \n\n\nWhen working with providers that use template-based deployments, Cloudbreak needs to be provided with a reference to the template, because every change in the infrastructure (for example, creating a new instance or deleting one) is managed through this templating mechanism.\n\n\nIf a provider has templating support, then the provider's \ngradle\n module depends on the \ncloud-api\n module:\n\n\napply plugin: 'java'\n\nsourceCompatibility = 1.7\n\nrepositories {\n    mavenCentral()\n}\n\njar {\n    baseName = 'cloud-new-provider'\n}\n\ndependencies {\n\n    compile project(':cloud-api')\n\n}\n\n\n\n\nThe entry point for the provider is the  \nCloudConnector\n interface and every interface that needs to be implemented is reachable trough this interface.\n\n\nIndividual Resource Based Deployments\n\n\nThere are providers such as GCP that do not support a templating mechanism, and customizable providers such as OpenStack where the Heat Orchestration (templating) component is optional and individual resources need to be handled separately. \n\n\nWhen working with such providers, resources such as networks, discs, and compute instances need to be created and managed with an ordered sequence of API calls, and Cloudbreak needs to provide a solution to manage the collection of related cloud resources as a whole.\n\n\nIf the provider has no templating support, then the provider's \ngradle\n module typically depends on the \ncloud-template\n module, which includes Cloudbreak-defined abstract template. This template is a set of abstract and utility classes to support provisioning and updating related resources in an orderly and predictable manner trough ordered sequences of cloud API calls:\n\n\napply plugin: 'java'\n\nsourceCompatibility = 1.7\n\nrepositories {\n    mavenCentral()\n}\n\njar {\n    baseName = 'cloud-new-provider'\n}\n\ndependencies {\n\n    compile project(':cloud-template')\n\n}\n\n\n\n\nSupport for Modularity\n\n\nCloudbreak uses \nvariants\n to deal with highly modular providers such as OpenStack, which allows you to install different components (for volume storage, networking, and so on) and to entirely exclude certain components. For example, Nova or Neutron can be used for networking in OpenStack and some components such as Heat may not installed at all in some deployment scenarios. \n\n\nCloudbreak SPI interface uses variants to support this flexibility: if some part of the cloud provider uses a different component, you don't need to re-implement the complete stack, but just use a different variant and re-implement the part that is different.\n\n\nAn example implementation for this feature can be found in the \ncloud-openstack\n module which supports a HEAT and NATIVE variants. While the HEAT variant utilizes the Heat templating to launch a stack, the NATIVE variant starts the cluster by using a sequence of API calls without Heat to achieve the same result. Both of them use the same authentication and credential management.", 
            "title": "Developer Documentation"
        }, 
        {
            "location": "/dev/index.html#developer-documentation", 
            "text": "The following table includes links to Cloudbreak developer documentation:      Doc Link  Description      Set Up Local Development  This documentation will help you set up your local development environment.    Retrieve OAuth Bearer Token via Cloudbreak REST API  Describes how to retrieve OAuth bearer token via Cloudbreak REST API.    SPI Reference  This is Cloudbreak SPI reference documentation.    API Reference  This is Cloudbreak API reference documentation. Cloudbreak is a RESTful application development platform whose goal is to help developers deploy HDP clusters in various cloud environments. Once Cloudbreak is deployed in your favorite servlet container, it exposes REST APIs, allowing you to spin up Hadoop clusters of any size with your chosen cloud provider.    Flow Diagrams  This is Cloudbreak flow diagrams reference documentation.", 
            "title": "Developer Documentation"
        }, 
        {
            "location": "/dev/index.html#retrieve-oauth-bearer-token-via-cloudbreak-rest-api", 
            "text": "In order to communicate with Cloudbreak's API, you must retrieve a bearer token. For example:   TOKEN=$(curl -k -iX POST -H \"accept: application/x-www-form-urlencoded\" -d 'credentials={\"username\":\"admin@example.com\",\"password\":\"pwd\"}' \"https://192.168.99.100/identity/oauth/authorize?response_type=token _id=cloudbreak_shell =openid =login _uri=http://cloudbreak.shell\" | grep location | cut -d'=' -f 3 | cut -d' & ' -f 1)", 
            "title": "Retrieve OAuth Bearer Token via Cloudbreak REST API"
        }, 
        {
            "location": "/dev/index.html#cloudbreak-service-provider-interface-spi", 
            "text": "In addition to supporting multiple cloud platforms, Cloudbreak provides an easy way to integrate a new provider trough its  Service Provider Interface (SPI) , a plugin mechanism that enables seamless integration with any cloud provider.   This SPI plugin mechanism has been used to integrate all currently supported providers with Cloudbreak. The following links point to the Cloudbreak SPI implementations for AWS, Azure, Google Cloud, and OpenStack. You can use these implementations as a reference:   The  cloud-aws  module integrates Amazon Web Services  The  cloud-arm  module integrates Microsoft Azure  The  cloud-gcp  module integrates Google Cloud Platform    The  cloud-openstack  module integrates OpenStack   The Cloubdreak SPI interface is event-based, scalable, and decoupled from Cloudbreak. The core of Cloudbreak uses  EventBus  to communicate with the providers, but the complexity of event handling is hidden from the provider implementation.", 
            "title": "Cloudbreak Service Provider Interface (SPI)"
        }, 
        {
            "location": "/dev/index.html#supported-resource-management-methods", 
            "text": "Cloud providers support two kinds of deployment and resource management methods:   Template based deployments  Individual resource based deployments   Cloudbreak's SPI supports both of these methods. It provides a well-defined interface, abstract classes, and helper classes, scheduling and polling of resources to aid the integration and to avoid any boilerplate code in the module of cloud provider.", 
            "title": "Supported Resource Management Methods"
        }, 
        {
            "location": "/dev/index.html#template-based-deployments", 
            "text": "Providers with template-based deployments such as  AWS CloudFormation ,  Azure ARM  or  OpenStack Heat  have the ability to create and manage a collection of related cloud resources, provisioning and updating them in an orderly and predictable fashion.   When working with providers that use template-based deployments, Cloudbreak needs to be provided with a reference to the template, because every change in the infrastructure (for example, creating a new instance or deleting one) is managed through this templating mechanism.  If a provider has templating support, then the provider's  gradle  module depends on the  cloud-api  module:  apply plugin: 'java'\n\nsourceCompatibility = 1.7\n\nrepositories {\n    mavenCentral()\n}\n\njar {\n    baseName = 'cloud-new-provider'\n}\n\ndependencies {\n\n    compile project(':cloud-api')\n\n}  The entry point for the provider is the   CloudConnector  interface and every interface that needs to be implemented is reachable trough this interface.", 
            "title": "Template Based Deployments"
        }, 
        {
            "location": "/dev/index.html#individual-resource-based-deployments", 
            "text": "There are providers such as GCP that do not support a templating mechanism, and customizable providers such as OpenStack where the Heat Orchestration (templating) component is optional and individual resources need to be handled separately.   When working with such providers, resources such as networks, discs, and compute instances need to be created and managed with an ordered sequence of API calls, and Cloudbreak needs to provide a solution to manage the collection of related cloud resources as a whole.  If the provider has no templating support, then the provider's  gradle  module typically depends on the  cloud-template  module, which includes Cloudbreak-defined abstract template. This template is a set of abstract and utility classes to support provisioning and updating related resources in an orderly and predictable manner trough ordered sequences of cloud API calls:  apply plugin: 'java'\n\nsourceCompatibility = 1.7\n\nrepositories {\n    mavenCentral()\n}\n\njar {\n    baseName = 'cloud-new-provider'\n}\n\ndependencies {\n\n    compile project(':cloud-template')\n\n}", 
            "title": "Individual Resource Based Deployments"
        }, 
        {
            "location": "/dev/index.html#support-for-modularity", 
            "text": "Cloudbreak uses  variants  to deal with highly modular providers such as OpenStack, which allows you to install different components (for volume storage, networking, and so on) and to entirely exclude certain components. For example, Nova or Neutron can be used for networking in OpenStack and some components such as Heat may not installed at all in some deployment scenarios.   Cloudbreak SPI interface uses variants to support this flexibility: if some part of the cloud provider uses a different component, you don't need to re-implement the complete stack, but just use a different variant and re-implement the part that is different.  An example implementation for this feature can be found in the  cloud-openstack  module which supports a HEAT and NATIVE variants. While the HEAT variant utilizes the Heat templating to launch a stack, the NATIVE variant starts the cluster by using a sequence of API calls without Heat to achieve the same result. Both of them use the same authentication and credential management.", 
            "title": "Support for Modularity"
        }, 
        {
            "location": "/releasenotes/index.html", 
            "text": "Release Notes\n\n\n2.4.0\n\n\nCloudbreak 2.4.0 is a general availability release, which is suitable for production deployments. \n\n\n\n\nNew Features\n\n\n\n\nNew UI/UX\n\n\nCloudbreak 2 introduces a new user interface, designed to make user interaction with Cloudbreak easier.\n\n\nNew CLI\n\n\nCloudbreak 2.1.0 TP introduces a new CLI tool. All commands start with a singular object followed by an action, for example, blueprint create instead of create blueprint, and blueprint list instead of list blueprints. Refer to \nInstall CLI\n and \nCLI Reference\n.\n\n\nSupport for Kerberos\n\n\nCreating Kerberos-enabled clusters is supported. Refer to \nEnabling Kerberos Security\n.\n\n\nSupport for Configuring an External RDBMS for Cloudbreak\n\n\nUsing an external RDBMS for Cloubdreak is supported. For configuration instructions, refer to \nConfiguring External Cloudbreak Database\n.\n\n\nSupport for Migrating Cloudbreak Instance\n\n\nMigrating Cloudbreak from one machine to another is supported. For migration instructions, refer to \nMoving a Cloudbreak Instance\n.\n\n\nProviding Your Own JDK\n\n\nProviding your own JDK on a custom base image is supported. For instructions, refer to \"Advanced topics\" in the \nhttps://github.com/hortonworks/cloudbreak-images\n repository.\n\n\nPrewarmed Images\n\n\nTo accelerate cluster creation, CLoudbreak 2.4 introduces prewarmed images, which include the operating system, as well as the default version of Ambari and HDP. By default, Cloudbreak 2.4 launches clusters from these prewarmed images, instead of using base images (which were used by default in earlier versions of Cloudbreak). Default base images are still available in case you would like to use different Ambari and HDP versions than those provided with prewarmed images. For more information, refer to \nPrewardmed and Base Images\n. \n\n\nCLI Templates\n\n\nAfter specifying the parameters for your cluster in the Cloudbreak web UI, you can copy the content of the CLI JSON file that can be used to create a cluster via Cloudbreeak CLI. For more information, refer to \nObtain Cluster JSON Template from the UI\n.    \n\n\nFurthermore, Cloudbreak web UI includes an option in the UI which allows you to generate the  \ncreate\n command for resources such as credentials, blueprints, clusters, and recipes. For more information, refer to \nObtain CLI Command from the UI\n.  \n\n\nNew Recipe Types\n\n\nNew types of recipes are introduced:\n\n\n\n\nPRE-AMBARI-START (new, useful for configuring Ambari prior to start)  \n\n\nPOST-AMBARI-START (formerly known as PRE)  \n\n\nPOST-CLUSTER-INSTALL (formerly known as POST)  \n\n\nPRE-TERMINATION (new, useful for cluster cleanup pre-termination tasks)\n\nRefer to updated \nRecipes\n documentation.\n\n\n\n\nDisabling Cloud Providers\n\n\nYou can hide cloud providers available in Cloudbreak by adding the \nCB_ENABLEDPLATFORMS\n environment variable in Profile and setting it to the provider(s) that you would like to have available. For more information, refer to \nDisable Providers\n.\n\n\nSupport for Ambari 2.6\n\n\nTBD\n\n\nAmbari Master Key\n\n\n\n\nBehavioral Changes\n\n\n\n\nCustom Images\n\n\nThe functionality which enables you to create custom images was changed and improved. Refer to \nCustom Images\n.\n\n\nPrewarmed Images Are Used By Default\n\n\nBy default, Cloudbreak 2.4 launches clusters from these prewarmed images, instead of base images (which were used by default in earlier versions of Cloudbreak). For more information, refer to \nPrewardmed and Base Images\n. \n\n\nRemoval of Cloudbreak Shell\n\n\nCloudbreak Shell is no longer available in Cloudbreak 2.1.0 TP and later. It was replaced by the \nCloudbreak CLI\n.\n\n\nRemoval of Platforms\n\n\nThe \nPlatforms\n feature was removed.\n\n\nRemoval of Mesos\n\n\nCloudbreak 2 does not support Mesos cloud provider.\n\n\nRemoval of Templates\n\n\nEarlier versions of Cloudbreak allowed you to save infrastructure, network, and security group templates. This feature was removed. Instead, you can define VMs, storage, networks, and security groups as part of the create cluster wizard.\n\n\n\n\nFixed Issues\n\n\n\n\n\n\nKnown Issues\n\n\n\n\n(BUG-93548) AWS Region eu-west-3 Is Not Supported\n\n\nThe AWS region eu-west-3 can be selected during cluster creation. However, it is not supported by Cloudbreak.\n\n\nWorkaround\n: \n\n\nDo not use the AWS region eu-west-3. Instead, use eu-west-1 or eu-west-2.\n\n\n\n\n(BUG-91543) Networks With No Subnets Are Not Supported\n\n\nYou cannot create a cluster using an existing network that does not have any subnets. You must use a network that includes at least one subnet. If you try to use a network with no subnets, the cluster fails with the following error:   \n\n\nInfrastructure creation failed. Reason: Invalid value for field 'resource.network': 'https://www.googleapis.com/compute/v1/projects/siq-haas/global/networks/cbd-test'. A subnet mode Network must be specified for Subnetwork creation.: [ resourceType: GCP_SUBNET, resourceName: testgc-20171110211021 ]\n\n\nWorkaround\n: \n\n\nDo not use this option. It will be removed in a future release. \n\n\n\n\n(BUG-92605) Cluster Creation Fails with ResourceInError\n\n\nCluster creation fails with the following error: \n\n\nInfrastructure creation failed. Reason: Failed to create the stack for CloudContext{id=3689, name='test-exisitngnetwork', platform='StringType{value='OPENSTACK'}', owner='e0307f96-bd7d-4641-8c8f-b95f2667d9c6'} due to: Resource CREATE failed: ResourceInError: resources.ambari_volume_master_0_0: Went to status error due to \"Unknown\"\n\n\nWorkaround\n: \n\n\nThis may mean that the volumes that you requested exceed volumes available on your cloud provider account. When creating a cluster, on the advanced \nHardware and Storage\n page of the create cluster wizard, try reducing the amount of requested storage. If you need more storage, try using a different region or ask your cloud provider admin to increase the resource quota for volumes.  \n\n\n\n\n(BUG-93339) HiveServer2 Process Could Not Establish Connection to jdbc:hive2\n\n\nWhen using EDW-Analytics: Apache Hive 2 LLAP, Apache Zeppelin 0.7.0 blueprint with a two-node cluster, Ambari shows the following Alert on Hive: \nHiveServer2 Process CRIT Connection failed on host...\n \n\n\nWorkaround:\n \n\n\n\n\nAlthough it is possible to create a cluster with less than 3 nodes, in order to use the EDW-Analytics: Apache Hive 2 LLAP, Apache Zeppelin 0.7.0 blueprint, you must have at least 3 nodes: 1 Ambari node and 2 non-Ambari nodes. Terminate the cluster and create a new one with at least 3 nodes.   \n\n\nAlternatively, it is possible to resolve this issue by adding an additional NodeManager for the Ambari host group.  \n\n\n\n\n\n\n(BUG-91013) Incorrect Node Status After Cluster Restart\n\n\nYou may sporadically experience an issue where after you stop and restart a cluster, the node status displayed in the \"Hardware\" section is incorrect.   \n\n\n\n\n(BUG-93241) Error When Scaling Multiple Host Groups\n\n\nScaling of multiple host groups fails with the following error: \n\nBatch update returned unexpected row count from update [0]; actual row count: 0; expected: 1; nested exception is org.hibernate.StaleStateException: Batch update returned unexpected row count from update [0]; actual row count: 0; expected: 1\n\n\nWorkaround:\n\n\nScaling multiple host groups at once is not supported. If you would like to scale multiple host groups: scale the first host group and wait until scaling has completed, then scale the second host group, and so on.  \n\n\n\n\n(BUG-95607) Special Characters in Blueprint Name Cause an Error in CLI\n\n\nWhen registering a blueprint via \nblueprint create\n CLI command, if the name of the blueprint includes one or more of the following special characters \n@#$%|:\n*;\n you will get an error similar to:  \n\n\ncb blueprint create from-url --name test@# --url https://myurl.com/myblueprint.bp\n\n[1] 7547\n-bash: application.yml: command not found\n-bash: --url: command not found\n ~ \ue0b1 integration-test \ue0b0 1 \ue0b0 time=\"2018-02-01T12:56:44+01:00\" level=\"error\" msg=\"the following parameters are missing: url\\n\"\n \n\n\nWorkaround:\n\n You have two options:\n\n\n\n\nDo not include any of the following special characters \n@#$%|:\n*;\n in the blueprint name.  \n\n\nIf you want to use special characters in the name, perform the task via the UI.  \n\n\n\n\n\n\n(BUG-93257) Clusters Are Missing From History\n\n\nAfter changing the dates on the \nHistory\n page multiple times, the results displayed may sometimes be incorrect. \n\n\nWorkaround:\n\n\nRefresh the page if you think that the history displayed may be incorrect.", 
            "title": "Release Notes"
        }, 
        {
            "location": "/releasenotes/index.html#release-notes", 
            "text": "", 
            "title": "Release Notes"
        }, 
        {
            "location": "/releasenotes/index.html#240", 
            "text": "Cloudbreak 2.4.0 is a general availability release, which is suitable for production deployments.", 
            "title": "2.4.0"
        }, 
        {
            "location": "/releasenotes/index.html#new-features", 
            "text": "", 
            "title": "New Features"
        }, 
        {
            "location": "/releasenotes/index.html#new-uiux", 
            "text": "Cloudbreak 2 introduces a new user interface, designed to make user interaction with Cloudbreak easier.", 
            "title": "New UI/UX"
        }, 
        {
            "location": "/releasenotes/index.html#new-cli", 
            "text": "Cloudbreak 2.1.0 TP introduces a new CLI tool. All commands start with a singular object followed by an action, for example, blueprint create instead of create blueprint, and blueprint list instead of list blueprints. Refer to  Install CLI  and  CLI Reference .", 
            "title": "New CLI"
        }, 
        {
            "location": "/releasenotes/index.html#support-for-kerberos", 
            "text": "Creating Kerberos-enabled clusters is supported. Refer to  Enabling Kerberos Security .", 
            "title": "Support for Kerberos"
        }, 
        {
            "location": "/releasenotes/index.html#support-for-configuring-an-external-rdbms-for-cloudbreak", 
            "text": "Using an external RDBMS for Cloubdreak is supported. For configuration instructions, refer to  Configuring External Cloudbreak Database .", 
            "title": "Support for Configuring an External RDBMS for Cloudbreak"
        }, 
        {
            "location": "/releasenotes/index.html#support-for-migrating-cloudbreak-instance", 
            "text": "Migrating Cloudbreak from one machine to another is supported. For migration instructions, refer to  Moving a Cloudbreak Instance .", 
            "title": "Support for Migrating Cloudbreak Instance"
        }, 
        {
            "location": "/releasenotes/index.html#providing-your-own-jdk", 
            "text": "Providing your own JDK on a custom base image is supported. For instructions, refer to \"Advanced topics\" in the  https://github.com/hortonworks/cloudbreak-images  repository.", 
            "title": "Providing Your Own JDK"
        }, 
        {
            "location": "/releasenotes/index.html#prewarmed-images", 
            "text": "To accelerate cluster creation, CLoudbreak 2.4 introduces prewarmed images, which include the operating system, as well as the default version of Ambari and HDP. By default, Cloudbreak 2.4 launches clusters from these prewarmed images, instead of using base images (which were used by default in earlier versions of Cloudbreak). Default base images are still available in case you would like to use different Ambari and HDP versions than those provided with prewarmed images. For more information, refer to  Prewardmed and Base Images .", 
            "title": "Prewarmed Images"
        }, 
        {
            "location": "/releasenotes/index.html#cli-templates", 
            "text": "After specifying the parameters for your cluster in the Cloudbreak web UI, you can copy the content of the CLI JSON file that can be used to create a cluster via Cloudbreeak CLI. For more information, refer to  Obtain Cluster JSON Template from the UI .      Furthermore, Cloudbreak web UI includes an option in the UI which allows you to generate the   create  command for resources such as credentials, blueprints, clusters, and recipes. For more information, refer to  Obtain CLI Command from the UI .", 
            "title": "CLI Templates"
        }, 
        {
            "location": "/releasenotes/index.html#new-recipe-types", 
            "text": "New types of recipes are introduced:   PRE-AMBARI-START (new, useful for configuring Ambari prior to start)    POST-AMBARI-START (formerly known as PRE)    POST-CLUSTER-INSTALL (formerly known as POST)    PRE-TERMINATION (new, useful for cluster cleanup pre-termination tasks) \nRefer to updated  Recipes  documentation.", 
            "title": "New Recipe Types"
        }, 
        {
            "location": "/releasenotes/index.html#disabling-cloud-providers", 
            "text": "You can hide cloud providers available in Cloudbreak by adding the  CB_ENABLEDPLATFORMS  environment variable in Profile and setting it to the provider(s) that you would like to have available. For more information, refer to  Disable Providers .", 
            "title": "Disabling Cloud Providers"
        }, 
        {
            "location": "/releasenotes/index.html#support-for-ambari-26", 
            "text": "TBD", 
            "title": "Support for Ambari 2.6"
        }, 
        {
            "location": "/releasenotes/index.html#ambari-master-key", 
            "text": "", 
            "title": "Ambari Master Key"
        }, 
        {
            "location": "/releasenotes/index.html#behavioral-changes", 
            "text": "", 
            "title": "Behavioral Changes"
        }, 
        {
            "location": "/releasenotes/index.html#custom-images", 
            "text": "The functionality which enables you to create custom images was changed and improved. Refer to  Custom Images .", 
            "title": "Custom Images"
        }, 
        {
            "location": "/releasenotes/index.html#prewarmed-images-are-used-by-default", 
            "text": "By default, Cloudbreak 2.4 launches clusters from these prewarmed images, instead of base images (which were used by default in earlier versions of Cloudbreak). For more information, refer to  Prewardmed and Base Images .", 
            "title": "Prewarmed Images Are Used By Default"
        }, 
        {
            "location": "/releasenotes/index.html#removal-of-cloudbreak-shell", 
            "text": "Cloudbreak Shell is no longer available in Cloudbreak 2.1.0 TP and later. It was replaced by the  Cloudbreak CLI .", 
            "title": "Removal of Cloudbreak Shell"
        }, 
        {
            "location": "/releasenotes/index.html#removal-of-platforms", 
            "text": "The  Platforms  feature was removed.", 
            "title": "Removal of Platforms"
        }, 
        {
            "location": "/releasenotes/index.html#removal-of-mesos", 
            "text": "Cloudbreak 2 does not support Mesos cloud provider.", 
            "title": "Removal of Mesos"
        }, 
        {
            "location": "/releasenotes/index.html#removal-of-templates", 
            "text": "Earlier versions of Cloudbreak allowed you to save infrastructure, network, and security group templates. This feature was removed. Instead, you can define VMs, storage, networks, and security groups as part of the create cluster wizard.", 
            "title": "Removal of Templates"
        }, 
        {
            "location": "/releasenotes/index.html#fixed-issues", 
            "text": "", 
            "title": "Fixed Issues"
        }, 
        {
            "location": "/releasenotes/index.html#known-issues", 
            "text": "", 
            "title": "Known Issues"
        }, 
        {
            "location": "/releasenotes/index.html#bug-93548-aws-region-eu-west-3-is-not-supported", 
            "text": "The AWS region eu-west-3 can be selected during cluster creation. However, it is not supported by Cloudbreak.  Workaround :   Do not use the AWS region eu-west-3. Instead, use eu-west-1 or eu-west-2.", 
            "title": "(BUG-93548) AWS Region eu-west-3 Is Not Supported"
        }, 
        {
            "location": "/releasenotes/index.html#bug-91543-networks-with-no-subnets-are-not-supported", 
            "text": "You cannot create a cluster using an existing network that does not have any subnets. You must use a network that includes at least one subnet. If you try to use a network with no subnets, the cluster fails with the following error:     Infrastructure creation failed. Reason: Invalid value for field 'resource.network': 'https://www.googleapis.com/compute/v1/projects/siq-haas/global/networks/cbd-test'. A subnet mode Network must be specified for Subnetwork creation.: [ resourceType: GCP_SUBNET, resourceName: testgc-20171110211021 ]  Workaround :   Do not use this option. It will be removed in a future release.", 
            "title": "(BUG-91543) Networks With No Subnets Are Not Supported"
        }, 
        {
            "location": "/releasenotes/index.html#bug-92605-cluster-creation-fails-with-resourceinerror", 
            "text": "Cluster creation fails with the following error:   Infrastructure creation failed. Reason: Failed to create the stack for CloudContext{id=3689, name='test-exisitngnetwork', platform='StringType{value='OPENSTACK'}', owner='e0307f96-bd7d-4641-8c8f-b95f2667d9c6'} due to: Resource CREATE failed: ResourceInError: resources.ambari_volume_master_0_0: Went to status error due to \"Unknown\"  Workaround :   This may mean that the volumes that you requested exceed volumes available on your cloud provider account. When creating a cluster, on the advanced  Hardware and Storage  page of the create cluster wizard, try reducing the amount of requested storage. If you need more storage, try using a different region or ask your cloud provider admin to increase the resource quota for volumes.", 
            "title": "(BUG-92605) Cluster Creation Fails with ResourceInError"
        }, 
        {
            "location": "/releasenotes/index.html#bug-93339-hiveserver2-process-could-not-establish-connection-to-jdbchive2", 
            "text": "When using EDW-Analytics: Apache Hive 2 LLAP, Apache Zeppelin 0.7.0 blueprint with a two-node cluster, Ambari shows the following Alert on Hive:  HiveServer2 Process CRIT Connection failed on host...    Workaround:     Although it is possible to create a cluster with less than 3 nodes, in order to use the EDW-Analytics: Apache Hive 2 LLAP, Apache Zeppelin 0.7.0 blueprint, you must have at least 3 nodes: 1 Ambari node and 2 non-Ambari nodes. Terminate the cluster and create a new one with at least 3 nodes.     Alternatively, it is possible to resolve this issue by adding an additional NodeManager for the Ambari host group.", 
            "title": "(BUG-93339) HiveServer2 Process Could Not Establish Connection to jdbc:hive2"
        }, 
        {
            "location": "/releasenotes/index.html#bug-91013-incorrect-node-status-after-cluster-restart", 
            "text": "You may sporadically experience an issue where after you stop and restart a cluster, the node status displayed in the \"Hardware\" section is incorrect.", 
            "title": "(BUG-91013) Incorrect Node Status After Cluster Restart"
        }, 
        {
            "location": "/releasenotes/index.html#bug-93241-error-when-scaling-multiple-host-groups", 
            "text": "Scaling of multiple host groups fails with the following error:  Batch update returned unexpected row count from update [0]; actual row count: 0; expected: 1; nested exception is org.hibernate.StaleStateException: Batch update returned unexpected row count from update [0]; actual row count: 0; expected: 1  Workaround:  Scaling multiple host groups at once is not supported. If you would like to scale multiple host groups: scale the first host group and wait until scaling has completed, then scale the second host group, and so on.", 
            "title": "(BUG-93241) Error When Scaling Multiple Host Groups"
        }, 
        {
            "location": "/releasenotes/index.html#bug-95607-special-characters-in-blueprint-name-cause-an-error-in-cli", 
            "text": "When registering a blueprint via  blueprint create  CLI command, if the name of the blueprint includes one or more of the following special characters  @#$%|: *;  you will get an error similar to:    cb blueprint create from-url --name test@# --url https://myurl.com/myblueprint.bp \n[1] 7547\n-bash: application.yml: command not found\n-bash: --url: command not found\n ~ \ue0b1 integration-test \ue0b0 1 \ue0b0 time=\"2018-02-01T12:56:44+01:00\" level=\"error\" msg=\"the following parameters are missing: url\\n\"    Workaround: \n You have two options:   Do not include any of the following special characters  @#$%|: *;  in the blueprint name.    If you want to use special characters in the name, perform the task via the UI.", 
            "title": "(BUG-95607) Special Characters in Blueprint Name Cause an Error in CLI"
        }, 
        {
            "location": "/releasenotes/index.html#bug-93257-clusters-are-missing-from-history", 
            "text": "After changing the dates on the  History  page multiple times, the results displayed may sometimes be incorrect.   Workaround:  Refresh the page if you think that the history displayed may be incorrect.", 
            "title": "(BUG-93257) Clusters Are Missing From History"
        }, 
        {
            "location": "/faq/index.html", 
            "text": "FAQs\n\n\nHow to...\n\n\nGenerate SSH Key Pair\n\n\nAll the instances created by Cloudbreak are configured to allow key-based SSH, so you'll need to provide an SSH public key that can be used later to SSH onto the instances in the clusters you'll create with Cloudbreak. You can use one of your existing keys or you can generate a new one.\n\n\nTo generate a new SSH key pair, execute:\n\n\nssh-keygen -t rsa -b 4096 -C \"your_email@example.com\"\n# Creates a new ssh key, using the provided email as a label\n# Generating public/private rsa key pair.\n# Enter file in which to save the key (/Users/you/.ssh/id_rsa): [Press enter]\n\n\n\nYou'll be asked to enter a passphrase, but you can leave it empty:\n\n\n# Enter passphrase (empty for no passphrase): [Type a passphrase]\n# Enter same passphrase again: [Type passphrase again]\n\n\n\nAfter you enter (or not) a passphrase, the key pair is generated. The output should look similar to:\n\n\n# Your identification has been saved in /Users/you/.ssh/id_rsa.\n# Your public key has been saved in /Users/you/.ssh/id_rsa.pub.\n# The key fingerprint is:\n# 01:0f:f4:3b:ca:85:sd:17:sd:7d:sd:68:9d:sd:a2:sd your_email@example.com\n\n\n\nLater you'll need to pass the content of the \n.pub\n file to Cloudbreak and use the private key file to SSH to the instances. \n\n\nRecover Public SSH Key\n\n\nThe \n-y\n option of \nssh-keygen\n outputs the public key. For example:\n\n\nssh-keygen -y -f ~/.ssh/id_rsa > ~/.ssh/id_rsa.pub\n\n\n\nSSH to the Hosts\n\n\nTo connect to a running VM through SSH, you need to know its public IP address and have your private key available. \n\n\nThe private key that you must use to access the VM is the counterpart of the public key that you specified when creating a Cloudbreak credential.\n\n\nYou can find the IP addresses of all the running VMs in the Cloudbreak UI, on the cluster details page. Only key-based authentication is supported. \n\n\nCloudbreak creates a cloudbreak user which can be used to ssh into the box. This user has passwordless sudo rights.\n\n\nFor example:\n\n\nssh -i ~/.ssh/your-private-key.pem cloudbreak@\npublic-ip\n\n\n\n\n\nCheck Cloudbreak Version\n\n\nTo check Cloudbreak version, navigate to the Cloudbreak home directory and execute the following command:\n\n\ncbd doctor\n\n\n\n\nCheck Available Environment Variables\n\n\nTo see all available environment variables with their default values, use:\n\n\ncbd env show\n\n\n\n\nAccess Cloudbreak Logs\n\n\nRefer to \nTroubleshooting\n.\n\n\nDebug in Cloudbreak Shell\n\n\nTo get more detailed command prompt output, set the DEBUG environment variable to non-zero:\n\n\nDEBUG=1 cbd \nsome_command\n\n\n\n\n\nConfigure and Test Proxy Settings\n\n\nFor cbd\n\n\nTo configure proxy settings for Cloudbreak Deployer, add the following configs to your Profile:\n\n\nexport http_proxy=\nhttp://YOUR_PROXY_ADDRESS:YOUR_PROXY_PORT/\n\nexport https_proxy=\nhttp(s)://YOUR_PROXY_ADDRESS:YOUR_PROXY_PORT/\n\nexport CB_HTTP_PROXY=\nhttp://YOUR_PROXY_ADDRESS:YOUR_PROXY_PORT/\n\nexport CB_HTTPS_PROXY=\nhttp(s)://YOUR_PROXY_ADDRESS:YOUR_PROXY_PORT/\n\nexport CB_JAVA_OPTS=\n-Dhttp.proxyHost=YOUR_PROXY_ADDRESS -Dhttp.proxyPort=YOUR_PROXY_PORT -Dhttps.proxyHost=YOUR_PROXY_ADDRESS -Dhttps.proxyPort=YOUR_PROXY_PORT -Dhttp.nonProxyHosts=172.17.0.1|*.service.consul|*.node.dc1.consul\n\n\n\n\n\nFor Docker\n\n\nTo download newer Docker images from the official repository, you need to configure proxy settings for the Docker service. You can do this by configuring the 'HTTP_PROXY' variable in your environment. Next, restart the docker service. For more information, refer to \nDocker documentation\n.\n\n\nFor Provisioned Clusters\n\n\nFor a cluster to be provisioned to a (virtual) network that is behind a proxy, the yum on the provisioned machines needs to be configured to use that proxy. This is important because the Ambari install needs access to public repositories. You can configure yum proxy settings by using the recipe functionality of Cloudbreak. Use the following bash script to create a 'pre' recipe that will run on all of the nodes before the Ambari install:\n\n\n#!/bin/bash\ncat \n /etc/yum.conf \nENDOF\n\nproxy=http://YOUR_PROXY_ADDRESS:YOUR_PROXY_PORT\n\nENDOF\n\n\n\n\nTest Your Proxy Settings\n\n\nYou can use the following CURL command to test your proxy settings:\n\n\nhttps_proxy=\nYOUR_PROXY_ADDRESS:YOUR_PROXY_PORT\n curl -X GET -I --insecure https://cloudbreak-api.sequenceiq.com/info\n\n\n\n\nIts output should start with:\n\n\nHTTP/1.1 200 OK", 
            "title": "FAQs"
        }, 
        {
            "location": "/faq/index.html#faqs", 
            "text": "How to...", 
            "title": "FAQs"
        }, 
        {
            "location": "/faq/index.html#generate-ssh-key-pair", 
            "text": "All the instances created by Cloudbreak are configured to allow key-based SSH, so you'll need to provide an SSH public key that can be used later to SSH onto the instances in the clusters you'll create with Cloudbreak. You can use one of your existing keys or you can generate a new one.  To generate a new SSH key pair, execute:  ssh-keygen -t rsa -b 4096 -C \"your_email@example.com\"\n# Creates a new ssh key, using the provided email as a label\n# Generating public/private rsa key pair.\n# Enter file in which to save the key (/Users/you/.ssh/id_rsa): [Press enter]  You'll be asked to enter a passphrase, but you can leave it empty:  # Enter passphrase (empty for no passphrase): [Type a passphrase]\n# Enter same passphrase again: [Type passphrase again]  After you enter (or not) a passphrase, the key pair is generated. The output should look similar to:  # Your identification has been saved in /Users/you/.ssh/id_rsa.\n# Your public key has been saved in /Users/you/.ssh/id_rsa.pub.\n# The key fingerprint is:\n# 01:0f:f4:3b:ca:85:sd:17:sd:7d:sd:68:9d:sd:a2:sd your_email@example.com  Later you'll need to pass the content of the  .pub  file to Cloudbreak and use the private key file to SSH to the instances.", 
            "title": "Generate SSH Key Pair"
        }, 
        {
            "location": "/faq/index.html#recover-public-ssh-key", 
            "text": "The  -y  option of  ssh-keygen  outputs the public key. For example:  ssh-keygen -y -f ~/.ssh/id_rsa > ~/.ssh/id_rsa.pub", 
            "title": "Recover Public SSH Key"
        }, 
        {
            "location": "/faq/index.html#ssh-to-the-hosts", 
            "text": "To connect to a running VM through SSH, you need to know its public IP address and have your private key available.   The private key that you must use to access the VM is the counterpart of the public key that you specified when creating a Cloudbreak credential.  You can find the IP addresses of all the running VMs in the Cloudbreak UI, on the cluster details page. Only key-based authentication is supported.   Cloudbreak creates a cloudbreak user which can be used to ssh into the box. This user has passwordless sudo rights.  For example:  ssh -i ~/.ssh/your-private-key.pem cloudbreak@ public-ip", 
            "title": "SSH to the Hosts"
        }, 
        {
            "location": "/faq/index.html#check-cloudbreak-version", 
            "text": "To check Cloudbreak version, navigate to the Cloudbreak home directory and execute the following command:  cbd doctor", 
            "title": "Check Cloudbreak Version"
        }, 
        {
            "location": "/faq/index.html#check-available-environment-variables", 
            "text": "To see all available environment variables with their default values, use:  cbd env show", 
            "title": "Check Available Environment Variables"
        }, 
        {
            "location": "/faq/index.html#access-cloudbreak-logs", 
            "text": "Refer to  Troubleshooting .", 
            "title": "Access Cloudbreak Logs"
        }, 
        {
            "location": "/faq/index.html#debug-in-cloudbreak-shell", 
            "text": "To get more detailed command prompt output, set the DEBUG environment variable to non-zero:  DEBUG=1 cbd  some_command", 
            "title": "Debug in Cloudbreak Shell"
        }, 
        {
            "location": "/faq/index.html#configure-and-test-proxy-settings", 
            "text": "For cbd  To configure proxy settings for Cloudbreak Deployer, add the following configs to your Profile:  export http_proxy= http://YOUR_PROXY_ADDRESS:YOUR_PROXY_PORT/ \nexport https_proxy= http(s)://YOUR_PROXY_ADDRESS:YOUR_PROXY_PORT/ \nexport CB_HTTP_PROXY= http://YOUR_PROXY_ADDRESS:YOUR_PROXY_PORT/ \nexport CB_HTTPS_PROXY= http(s)://YOUR_PROXY_ADDRESS:YOUR_PROXY_PORT/ \nexport CB_JAVA_OPTS= -Dhttp.proxyHost=YOUR_PROXY_ADDRESS -Dhttp.proxyPort=YOUR_PROXY_PORT -Dhttps.proxyHost=YOUR_PROXY_ADDRESS -Dhttps.proxyPort=YOUR_PROXY_PORT -Dhttp.nonProxyHosts=172.17.0.1|*.service.consul|*.node.dc1.consul   For Docker  To download newer Docker images from the official repository, you need to configure proxy settings for the Docker service. You can do this by configuring the 'HTTP_PROXY' variable in your environment. Next, restart the docker service. For more information, refer to  Docker documentation .  For Provisioned Clusters  For a cluster to be provisioned to a (virtual) network that is behind a proxy, the yum on the provisioned machines needs to be configured to use that proxy. This is important because the Ambari install needs access to public repositories. You can configure yum proxy settings by using the recipe functionality of Cloudbreak. Use the following bash script to create a 'pre' recipe that will run on all of the nodes before the Ambari install:  #!/bin/bash\ncat   /etc/yum.conf  ENDOF\n\nproxy=http://YOUR_PROXY_ADDRESS:YOUR_PROXY_PORT\n\nENDOF  Test Your Proxy Settings  You can use the following CURL command to test your proxy settings:  https_proxy= YOUR_PROXY_ADDRESS:YOUR_PROXY_PORT  curl -X GET -I --insecure https://cloudbreak-api.sequenceiq.com/info  Its output should start with:  HTTP/1.1 200 OK", 
            "title": "Configure and Test Proxy Settings"
        }, 
        {
            "location": "/get-help/index.html", 
            "text": "Get Help\n\n\nIf you need help with Cloudbreak, you have two options:\n\n\n\n\n\n\n\n\nOption\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nHortonworks Community Connection\n\n\nThis is free optional support via Hortonworks Community Connection (HCC).\n\n\n\n\n\n\nHortonworks Flex Support Subscription\n\n\nThis is paid Hortonworks enterprise support.\n\n\n\n\n\n\n\n\nHCC\n\n\nYou can register for optional free community support at \nHortonworks Community Connection\n where you can browse articles and previously answered questions, and ask questions of your own. When posting questions related to Cloudbreak, make sure to use the \"Cloudbreak\" tag.\n\n\nFlex Subscription\n\n\nYou can optionally use your existing Hortonworks \nFlex subscription(s)\n to cover the Cloudbreak node and clusters managed by it. \n\n\n\n\nYou must have an existing SmartSense ID and a Flex subscription. For general information about the Hortonworks Flex Support Subscription, visit the Hortonworks Support page at \nhttps://hortonworks.com/services/support/enterprise/\n.\n\n\n\n\nThe general steps are:\n\n\n\n\nConfigure Smart Sense in your \nProfile\n file.   \n\n\nRegister your Flex subscription in the Cloudbreak web UI or via CLI. You can register and manage multiple Flex subscriptions. For example, you can choose to use your Flex subscription to cover the Cloudbreak node.   \n\n\nWhen creating a cluster, you can select the Flex subscription that you want to use for the cluster.  \n\n\n\n\nConfiguring SmartSense\n\n\nTo configure SmartSense in Cloudbreak, enable SmartSense and add your SmartSense ID to the \nProfile\n by adding the following variables:\n\n\nexport CB_SMARTSENSE_CONFIGURE=true\nexport CB_SMARTSENSE_ID=YOUR-SMARTSENSE-ID\n\n\n\nFor example:\n\n\nexport CB_SMARTSENSE_CONFIGURE=true\nexport CB_SMARTSENSE_ID=A-00000000-C-00000000\n\n\n\nYou can do this in one of the two ways:\n\n\n\n\nWhen initiating Cloudbreak deployer  \n\n\nAfter you've already initiated Cloudbreak Deployer. If you choose this option, you must restart Cloudbreak using \ncbd restart\n.\n\n\n\n\n\n\nSmartSense ID defined in the \nProfile\n file always overrides the ID registered via Cloudbreak CLI.\n\n\n\n\nRegister and Manage Flex Subscriptions\n\n\nOnce you log in to the Cloudbreak web UI, you can manage your Flex subscriptions from the \nFlex Subscriptions\n page available in the navigation menu. You can:\n\n\n\n\nRegister a new Flex subscription.  \n\n\nSet a default Flex subscription.  \n\n\nSelect a Flex subscription to be used for the Cloudbreak node.  \n\n\nDelete a Flex subscription.  \n\n\nCheck which clusters are connected to a specific subscription.  \n\n\n\n\nUse Flex Subscription for a Cluster\n\n\nWhen creating a cluster, you can select the Flex subscription that you want to use for the cluster.\n\n\nMore Cloudbreak Resources\n\n\nCheck out the following documentation to learn more:\n\n\n\n\n Resource \nDescription\n\n\nHortonworks documentation \n\n\nDuring cluster create process, Cloudbreak automatically installs Ambari and sets up a cluster for you. After this deployment is complete, refer to the \nAmbari documentation\n and \nHDP documentation\n for help.\n\n\n\n\n\n\nHortonworks tutorials\n\n\n\n\nUse Hortonworks tutorials to get started with Apache Spark, Apache Hive, Apache Zeppelin, and more.\n\n\nApache documentation\n\n\n\n\n In addition to Hortonworks documentation, refer to the Apache Software Foundation documentation to get information on specific Hadoop services. \n\n\n\n\n\nAmbari blueprints\nLearn about Ambari blueprints. Ambari blueprints are a declarative definition of a Hadoop cluster that Ambari can use to create Hadoop clusters.\n\n\nCloudbreak project\nVisit the Hortonworks website to see Cloudbreak-related news and updates.\n\n\nApache Ambari project\nLearn about the Apache Ambari project. Apache Ambari is an operational platform for provisioning, managing, and monitoring Apache Hadoop clusters. Ambari exposes a robust set of REST APIs and a rich web interface for cluster management.", 
            "title": "Getting Help"
        }, 
        {
            "location": "/get-help/index.html#get-help", 
            "text": "If you need help with Cloudbreak, you have two options:     Option  Description      Hortonworks Community Connection  This is free optional support via Hortonworks Community Connection (HCC).    Hortonworks Flex Support Subscription  This is paid Hortonworks enterprise support.", 
            "title": "Get Help"
        }, 
        {
            "location": "/get-help/index.html#hcc", 
            "text": "You can register for optional free community support at  Hortonworks Community Connection  where you can browse articles and previously answered questions, and ask questions of your own. When posting questions related to Cloudbreak, make sure to use the \"Cloudbreak\" tag.", 
            "title": "HCC"
        }, 
        {
            "location": "/get-help/index.html#flex-subscription", 
            "text": "You can optionally use your existing Hortonworks  Flex subscription(s)  to cover the Cloudbreak node and clusters managed by it.    You must have an existing SmartSense ID and a Flex subscription. For general information about the Hortonworks Flex Support Subscription, visit the Hortonworks Support page at  https://hortonworks.com/services/support/enterprise/ .   The general steps are:   Configure Smart Sense in your  Profile  file.     Register your Flex subscription in the Cloudbreak web UI or via CLI. You can register and manage multiple Flex subscriptions. For example, you can choose to use your Flex subscription to cover the Cloudbreak node.     When creating a cluster, you can select the Flex subscription that you want to use for the cluster.", 
            "title": "Flex Subscription"
        }, 
        {
            "location": "/get-help/index.html#configuring-smartsense", 
            "text": "To configure SmartSense in Cloudbreak, enable SmartSense and add your SmartSense ID to the  Profile  by adding the following variables:  export CB_SMARTSENSE_CONFIGURE=true\nexport CB_SMARTSENSE_ID=YOUR-SMARTSENSE-ID  For example:  export CB_SMARTSENSE_CONFIGURE=true\nexport CB_SMARTSENSE_ID=A-00000000-C-00000000  You can do this in one of the two ways:   When initiating Cloudbreak deployer    After you've already initiated Cloudbreak Deployer. If you choose this option, you must restart Cloudbreak using  cbd restart .    SmartSense ID defined in the  Profile  file always overrides the ID registered via Cloudbreak CLI.", 
            "title": "Configuring SmartSense"
        }, 
        {
            "location": "/get-help/index.html#register-and-manage-flex-subscriptions", 
            "text": "Once you log in to the Cloudbreak web UI, you can manage your Flex subscriptions from the  Flex Subscriptions  page available in the navigation menu. You can:   Register a new Flex subscription.    Set a default Flex subscription.    Select a Flex subscription to be used for the Cloudbreak node.    Delete a Flex subscription.    Check which clusters are connected to a specific subscription.", 
            "title": "Register and Manage Flex Subscriptions"
        }, 
        {
            "location": "/get-help/index.html#use-flex-subscription-for-a-cluster", 
            "text": "When creating a cluster, you can select the Flex subscription that you want to use for the cluster.", 
            "title": "Use Flex Subscription for a Cluster"
        }, 
        {
            "location": "/get-help/index.html#more-cloudbreak-resources", 
            "text": "Check out the following documentation to learn more:    Resource  Description  Hortonworks documentation   During cluster create process, Cloudbreak automatically installs Ambari and sets up a cluster for you. After this deployment is complete, refer to the  Ambari documentation  and  HDP documentation  for help.    Hortonworks tutorials   Use Hortonworks tutorials to get started with Apache Spark, Apache Hive, Apache Zeppelin, and more.  Apache documentation    In addition to Hortonworks documentation, refer to the Apache Software Foundation documentation to get information on specific Hadoop services.    Ambari blueprints Learn about Ambari blueprints. Ambari blueprints are a declarative definition of a Hadoop cluster that Ambari can use to create Hadoop clusters.  Cloudbreak project Visit the Hortonworks website to see Cloudbreak-related news and updates.  Apache Ambari project Learn about the Apache Ambari project. Apache Ambari is an operational platform for provisioning, managing, and monitoring Apache Hadoop clusters. Ambari exposes a robust set of REST APIs and a rich web interface for cluster management.", 
            "title": "More Cloudbreak Resources"
        }, 
        {
            "location": "/smartsense/index.html", 
            "text": "SmartSense Telemetry\n\n\nHelp us make a better product by opting in to automatically send information to Hortonworks. This includes enabling\nHortonworks SmartSense and sending performance and usage info. As you use the product,\nSmartSense measures and collects information and then sends these information bundles to Hortonworks.\n\n\nHow to Disable\n\n\nDisable Bundle Upload for Cloudbreak and New Clusters\n\n\n\n    \nImportant\n\n    \n\n    Do not perform these steps when you have clusters currently in the process of being deployed.\n    Wait for all clusters to be deployed.\n\n\n\n\n\n\n\n\n\nSSH into the Cloudbreak host.\n\n\n\n\n\n\nEdit \n/var/lib/cloudbreak-deployment/Profile\n.\n\n\n\n\n\n\nChange \nCB_SMARTSENSE_CONFIGURE\n to \nfalse\n:\n\n    \nexport CB_SMARTSENSE_CONFIGURE=false\n\n\n\n\n\n\nRestart the cloud controller:\n\n    \ncd /var/lib/cloudbreak-deployment\ncbd restart\n\n\n\n\n\n\nDisable Bundle Upload for an Existing Cluster\n\n\n\n\n\n\nSSH into the master node for the cluster.\n\n\n\n\n\n\nEdit \n/etc/hst/conf/hst-server.ini\n.\n\n\n\n\n\n\nChange \n[gateway]\n configuration to \nfalse\n:\n\n    \n[gateway]\nenabled=false\n\n\n\n\n\n\nRestart the SmartSense Server:\n    \nhst restart\n\n\n\n\n\n\n(Optional) Disable SmartSense daily bundle capture:\n\n\n\n\nSmartSense is scheduled to capture a telemetry bundle daily. With the bundle upload disabled, the bundle will still\nbe captured but just saved locally (i.e. not uploaded).\n\n\nTo disable the bundle capture, execute the following:\n\nhst capture-schedule -a pause\n\n\n\n\n\n\n\n\nRepeat on all existing clusters.", 
            "title": "SmartSense"
        }, 
        {
            "location": "/smartsense/index.html#smartsense-telemetry", 
            "text": "Help us make a better product by opting in to automatically send information to Hortonworks. This includes enabling\nHortonworks SmartSense and sending performance and usage info. As you use the product,\nSmartSense measures and collects information and then sends these information bundles to Hortonworks.", 
            "title": "SmartSense Telemetry"
        }, 
        {
            "location": "/smartsense/index.html#how-to-disable", 
            "text": "", 
            "title": "How to Disable"
        }, 
        {
            "location": "/smartsense/index.html#disable-bundle-upload-for-cloudbreak-and-new-clusters", 
            "text": "Important \n     \n    Do not perform these steps when you have clusters currently in the process of being deployed.\n    Wait for all clusters to be deployed.     SSH into the Cloudbreak host.    Edit  /var/lib/cloudbreak-deployment/Profile .    Change  CB_SMARTSENSE_CONFIGURE  to  false : \n     export CB_SMARTSENSE_CONFIGURE=false    Restart the cloud controller: \n     cd /var/lib/cloudbreak-deployment\ncbd restart", 
            "title": "Disable Bundle Upload for Cloudbreak and New Clusters"
        }, 
        {
            "location": "/smartsense/index.html#disable-bundle-upload-for-an-existing-cluster", 
            "text": "SSH into the master node for the cluster.    Edit  /etc/hst/conf/hst-server.ini .    Change  [gateway]  configuration to  false : \n     [gateway]\nenabled=false    Restart the SmartSense Server:\n     hst restart    (Optional) Disable SmartSense daily bundle capture:   SmartSense is scheduled to capture a telemetry bundle daily. With the bundle upload disabled, the bundle will still\nbe captured but just saved locally (i.e. not uploaded).  To disable the bundle capture, execute the following: hst capture-schedule -a pause     Repeat on all existing clusters.", 
            "title": "Disable Bundle Upload for an Existing Cluster"
        }, 
        {
            "location": "/acknowledge/index.html", 
            "text": "Acknowledgements\n\n\nCopyrights and Trademarks\n\n\n\u00a9 2011-2018 Hortonworks Inc. All Rights Reserved. Hortonworks and HDP are registered trademarks\nor trademarks of Hortonworks, Inc. in the United States and other jurisdictions.  All other\ntrademarks and trade names are the property of their respective owners.\n\n\nApache, Hadoop, Falcon, Atlas, Tez, Sqoop, Flume, Kafka, Pig, Hive,\nHBase, Accumulo, Storm, Solr, Spark, Ranger, Knox, Ambari, ZooKeeper,\nOozie, Metron, Druid, and the Hadoop elephant logo are either registered trademarks or\ntrademarks of the \nApache Software Foundation\n in\nthe United States or other countries.\n\n\n\"Amazon Web Services\", \"AWS\", \"Amazon EC2\", \"Amazon S3\", \n\"Amazon VPC\", \"Amazon RDS\", the \"Amazon Web Services\" logo, and other\nAWS service names, graphics, and logos are trademarks of Amazon Technologies, Inc. or its affiliates in the United States and/or other countries.\n\n\n\"Microsoft Azure\", \"Windows Azure\", the \"Azure\" logo, and other\nAzure service names, graphics, and logos are trademarks of Microsoft Corporation or its affiliates in the United States and/or other countries.\n\n\n\"Google Cloud\", the \"Google Cloud\" logo, and other\nGoogle Cloud service names, graphics, and logos are trademarks of Google LLC or its affiliates in the United States and/or other countries.\n\n\n\"OpenStack\", the \"OpenStack\" logo, and other\nOpenStack graphics and logos are trademarks of OpenStack LLC or its affiliates in the United States and/or other countries.\n\n\nDocumentation was built with \nMkDocs\n and the\n\nCinder Theme\n, licensed under\nthe \nMIT license\n.\n\n\nContact Information\n\n\nHortonworks, Inc.\n\n\n5470 Great America Parkway\n\n\nSanta Clara, CA 95054\n\n\n\nWebsite:\n \nwww.hortonworks.com\n\n\n\n\nCommunity:\n \ncommunity.hortonworks.com", 
            "title": "Acknowledgements"
        }, 
        {
            "location": "/acknowledge/index.html#acknowledgements", 
            "text": "", 
            "title": "Acknowledgements"
        }, 
        {
            "location": "/acknowledge/index.html#copyrights-and-trademarks", 
            "text": "\u00a9 2011-2018 Hortonworks Inc. All Rights Reserved. Hortonworks and HDP are registered trademarks\nor trademarks of Hortonworks, Inc. in the United States and other jurisdictions.  All other\ntrademarks and trade names are the property of their respective owners.  Apache, Hadoop, Falcon, Atlas, Tez, Sqoop, Flume, Kafka, Pig, Hive,\nHBase, Accumulo, Storm, Solr, Spark, Ranger, Knox, Ambari, ZooKeeper,\nOozie, Metron, Druid, and the Hadoop elephant logo are either registered trademarks or\ntrademarks of the  Apache Software Foundation  in\nthe United States or other countries.  \"Amazon Web Services\", \"AWS\", \"Amazon EC2\", \"Amazon S3\", \n\"Amazon VPC\", \"Amazon RDS\", the \"Amazon Web Services\" logo, and other\nAWS service names, graphics, and logos are trademarks of Amazon Technologies, Inc. or its affiliates in the United States and/or other countries.  \"Microsoft Azure\", \"Windows Azure\", the \"Azure\" logo, and other\nAzure service names, graphics, and logos are trademarks of Microsoft Corporation or its affiliates in the United States and/or other countries.  \"Google Cloud\", the \"Google Cloud\" logo, and other\nGoogle Cloud service names, graphics, and logos are trademarks of Google LLC or its affiliates in the United States and/or other countries.  \"OpenStack\", the \"OpenStack\" logo, and other\nOpenStack graphics and logos are trademarks of OpenStack LLC or its affiliates in the United States and/or other countries.  Documentation was built with  MkDocs  and the Cinder Theme , licensed under\nthe  MIT license .", 
            "title": "Copyrights and Trademarks"
        }, 
        {
            "location": "/acknowledge/index.html#contact-information", 
            "text": "Hortonworks, Inc. \n5470 Great America Parkway \nSanta Clara, CA 95054  Website:   www.hortonworks.com   Community:   community.hortonworks.com", 
            "title": "Contact Information"
        }
    ]
}