{
    "docs": [
        {
            "location": "/index.html", 
            "text": "Introduction\n\n\nWelcome to the \nCloudbreak 2.6 Technical Preview\n documentation! \n\n\nCloudbreak simplifies the provisioning, management, and monitoring of on-demand HDP and HDF clusters in virtual and cloud environments. It leverages cloud infrastructure to create host instances, and uses Apache Ambari via Ambari blueprints to provision and manage HDP clusters. \n\n\nCloudbreak allows you to create clusters using the Cloudbreak web UI, Cloudbreak CLI, and Cloudbreak REST API. Clusters can be launched on public cloud infrastructure platforms \nMicrosoft Azure\n, \nAmazon Web Services (AWS)\n, and \nGoogle Cloud Platform (GCP)\n, and on the private cloud infrastructure platform \nOpenStack\n.\n\n\n   \n\n\nPrimary use cases\n\n\nCloudbreak allows you to create, manage, and monitor your HDP and HDF clusters on your chosen cloud platform:\n\n\n\n\nDynamically deploy, configure, and manage clusters on public and private clouds (AWS, Azure, Google Cloud, OpenStack).   \n\n\nUse automated scaling to seamlessly manage elasticity requirements as cluster workloads change.  \n\n\nSecure your cluster by enabling Kerberos.   \n\n\n\n\nDefault cluster configurations\n\n\nCloudbreak includes default cluster configurations (in the form of blueprints) and supports using your own custom cluster configurations (in the form of custom blueprints).\n\n\nThe following default cluster configurations are available:\n\n\nPlatform version: \nHDP 2.6\n\n\n\n\n\n\n\n\nCluster type\n\n\nMain services\n\n\nDescription\n\n\nList of all services included\n\n\n\n\n\n\n\n\n\n\nData Science\n\n\n Spark 2,\nZeppelin\n\n\nUseful for data science with Spark 2 and Zeppelin.\n\n\nHDFS, YARN, MapReduce2, Tez, Hive, Pig, Sqoop, ZooKeeper, Ambari Metrics, Spark 2, Zeppelin\n\n\n\n\n\n\nEDW - Analytics\n\n\n Hive 2 LLAP\n,\nZeppelin\n\n\nUseful for EDW analytics using Hive LLAP.\n\n\nHDFS, YARN, MapReduce2, Tez, Hive 2 LLAP, Druid, Pig, ZooKeeper, Ambari Metrics, Spark 2\n\n\n\n\n\n\nEDW - ETL\n\n\n Hive,\n Spark 2\n\n\nUseful for ETL data processing with Hive and Spark 2.\n\n\nHDFS, YARN, MapReduce2, Tez, Hive, Pig, ZooKeeper, Ambari Metrics, Spark 2\n\n\n\n\n\n\n\n\nPlatform version: \nHDF 3.1\n\n\n\n\n\n\n\n\nCluster type\n\n\nMain services\n\n\nDescription\n\n\nList of all services included\n\n\n\n\n\n\n\n\n\n\nFlow Management\n\n\n NiFi\n\n\nUseful for flow management with NiFi.\n\n\nNiFi, NiFi Registry, ZooKeeper, Ambari Metrics\n\n\n\n\n\n\nMessaging Management\n\n\n Kafka\n\n\nUseful for messaging management with Kafka.\n\n\nKafka, ZooKeeper, Ambari Metrics\n\n\n\n\n\n\n\n\nCore concepts\n\n\nRefer to \nArchitecture\n and \nCore concepts\n.\n\n\nGet started\n\n\nTo get started with Cloudbreak:\n\n\n\n\n\n\nSelect the cloud platform on which you would like to launch Cloudbreak:\n\n\n\n\nAmazon Web Services (AWS)  \n\n\nMicrosoft Azure  \n\n\nGoogle Cloud Platform (GCP)  \n\n\nOpenStack  \n\n\n\n\n\n\n\n\nSelect the deployment option. In general, Cloudbreak offers a quickstart option and an advanced deployment option.  \n\n\n\n\n\n\nDeployment options on AWS\n\n\nThere are two basic deployment options:\n\n\n\n\nAWS quickstart option\n: Instantiate Cloudbreak by using the CloudFormation template. This is the basic deployment option and the easiest to get started with.   \n\n\nAdvanced Option\n: Install the Cloudbreak deployer on your own VM. This is an advanced deployment option. Select this option if you have custom requirements. The supported operating systems are RHEL, CentOS, and Oracle Linux 7 (64-bit).\n\n\n\n\nDeployment options on Azure\n\n\n\n\nAzure quickstart option\n: Instantiate Cloudbreak by using ???. This is the basic deployment option and the easiest to get started with. \n\n\nAdvanced Option\n: Install the Cloudbreak deployer on your own VM. This is an advanced deployment option.\n \nSelect this option if you have custom requirements. The supported operating systems are RHEL, CentOS, and Oracle Linux 7 (64-bit).\n\n\n\n\nDeployment options on GCP\n\n\n\n\nGCP quickstart option\n: Instantiate Cloudbreak using ???. This is the basic deployment option and the easiest to get started with.   \n\n\nAdvanced Option\n: Install the Cloudbreak deployer on your own VM. This is an advanced deployment option. Select this option if you have custom requirements. The supported operating systems are RHEL, CentOS, and Oracle Linux 7 (64-bit).\n\n\n\n\nDeployment options on OpenStack\n\n\nYou must launch Cloudbreak manually on \nOpenStack\n. Refer to \nLaunch on OpenStack\n. This is an advanced deployment option. Select this option if you have custom VM requirements. The supported operating systems are RHEL, CentOS, and Oracle Linux 7 (64-bit).\n\n\nLaunch Cloudbreak\n\n\n(Quickstart Option)\n You can launch Cloudbreak from a template:  \n\n\n\n\nLaunch on AWS\n  \n\n\nLaunch on Azure\n \n\n\nLaunch on GCP\n  \n\n\n\n\n\n\nThis option is not available for \nOpenStack\n; you must launch Cloudbreak manually. Refer to \nLaunch on OpenStack\n.    \n\n\n\n\n(Advanced Option)\n Or you can launch Cloudbreak \non your own VM\n on one of these cloud platforms. This is an advanced deployment option that you should only use if you have custom requirements. \n\n\nIn general, the steps include meeting the prerequisites, launching Cloudbreak on a VM, and creating the Cloudbreak credential. After performing these steps, you can create a cluster based on one of the default blueprints or upload your own blueprint and then create a cluster. \n\n\n\n    \nNote\n\n    \nThe Cloudbreak software runs in your cloud environment. You are responsible for cloud infrastructure related charges while running Cloudbreak and the clusters being managed by Cloudbreak.", 
            "title": "Get started"
        }, 
        {
            "location": "/index.html#introduction", 
            "text": "Welcome to the  Cloudbreak 2.6 Technical Preview  documentation!   Cloudbreak simplifies the provisioning, management, and monitoring of on-demand HDP and HDF clusters in virtual and cloud environments. It leverages cloud infrastructure to create host instances, and uses Apache Ambari via Ambari blueprints to provision and manage HDP clusters.   Cloudbreak allows you to create clusters using the Cloudbreak web UI, Cloudbreak CLI, and Cloudbreak REST API. Clusters can be launched on public cloud infrastructure platforms  Microsoft Azure ,  Amazon Web Services (AWS) , and  Google Cloud Platform (GCP) , and on the private cloud infrastructure platform  OpenStack .", 
            "title": "Introduction"
        }, 
        {
            "location": "/index.html#primary-use-cases", 
            "text": "Cloudbreak allows you to create, manage, and monitor your HDP and HDF clusters on your chosen cloud platform:   Dynamically deploy, configure, and manage clusters on public and private clouds (AWS, Azure, Google Cloud, OpenStack).     Use automated scaling to seamlessly manage elasticity requirements as cluster workloads change.    Secure your cluster by enabling Kerberos.", 
            "title": "Primary use cases"
        }, 
        {
            "location": "/index.html#default-cluster-configurations", 
            "text": "Cloudbreak includes default cluster configurations (in the form of blueprints) and supports using your own custom cluster configurations (in the form of custom blueprints).  The following default cluster configurations are available:  Platform version:  HDP 2.6     Cluster type  Main services  Description  List of all services included      Data Science   Spark 2, Zeppelin  Useful for data science with Spark 2 and Zeppelin.  HDFS, YARN, MapReduce2, Tez, Hive, Pig, Sqoop, ZooKeeper, Ambari Metrics, Spark 2, Zeppelin    EDW - Analytics   Hive 2 LLAP , Zeppelin  Useful for EDW analytics using Hive LLAP.  HDFS, YARN, MapReduce2, Tez, Hive 2 LLAP, Druid, Pig, ZooKeeper, Ambari Metrics, Spark 2    EDW - ETL   Hive,  Spark 2  Useful for ETL data processing with Hive and Spark 2.  HDFS, YARN, MapReduce2, Tez, Hive, Pig, ZooKeeper, Ambari Metrics, Spark 2     Platform version:  HDF 3.1     Cluster type  Main services  Description  List of all services included      Flow Management   NiFi  Useful for flow management with NiFi.  NiFi, NiFi Registry, ZooKeeper, Ambari Metrics    Messaging Management   Kafka  Useful for messaging management with Kafka.  Kafka, ZooKeeper, Ambari Metrics", 
            "title": "Default cluster configurations"
        }, 
        {
            "location": "/index.html#core-concepts", 
            "text": "Refer to  Architecture  and  Core concepts .", 
            "title": "Core concepts"
        }, 
        {
            "location": "/index.html#get-started", 
            "text": "To get started with Cloudbreak:    Select the cloud platform on which you would like to launch Cloudbreak:   Amazon Web Services (AWS)    Microsoft Azure    Google Cloud Platform (GCP)    OpenStack       Select the deployment option. In general, Cloudbreak offers a quickstart option and an advanced deployment option.", 
            "title": "Get started"
        }, 
        {
            "location": "/index.html#deployment-options-on-aws", 
            "text": "There are two basic deployment options:   AWS quickstart option : Instantiate Cloudbreak by using the CloudFormation template. This is the basic deployment option and the easiest to get started with.     Advanced Option : Install the Cloudbreak deployer on your own VM. This is an advanced deployment option. Select this option if you have custom requirements. The supported operating systems are RHEL, CentOS, and Oracle Linux 7 (64-bit).", 
            "title": "Deployment options on AWS"
        }, 
        {
            "location": "/index.html#deployment-options-on-azure", 
            "text": "Azure quickstart option : Instantiate Cloudbreak by using ???. This is the basic deployment option and the easiest to get started with.   Advanced Option : Install the Cloudbreak deployer on your own VM. This is an advanced deployment option.   Select this option if you have custom requirements. The supported operating systems are RHEL, CentOS, and Oracle Linux 7 (64-bit).", 
            "title": "Deployment options on Azure"
        }, 
        {
            "location": "/index.html#deployment-options-on-gcp", 
            "text": "GCP quickstart option : Instantiate Cloudbreak using ???. This is the basic deployment option and the easiest to get started with.     Advanced Option : Install the Cloudbreak deployer on your own VM. This is an advanced deployment option. Select this option if you have custom requirements. The supported operating systems are RHEL, CentOS, and Oracle Linux 7 (64-bit).", 
            "title": "Deployment options on GCP"
        }, 
        {
            "location": "/index.html#deployment-options-on-openstack", 
            "text": "You must launch Cloudbreak manually on  OpenStack . Refer to  Launch on OpenStack . This is an advanced deployment option. Select this option if you have custom VM requirements. The supported operating systems are RHEL, CentOS, and Oracle Linux 7 (64-bit).", 
            "title": "Deployment options on OpenStack"
        }, 
        {
            "location": "/index.html#launch-cloudbreak", 
            "text": "(Quickstart Option)  You can launch Cloudbreak from a template:     Launch on AWS     Launch on Azure    Launch on GCP       This option is not available for  OpenStack ; you must launch Cloudbreak manually. Refer to  Launch on OpenStack .       (Advanced Option)  Or you can launch Cloudbreak  on your own VM  on one of these cloud platforms. This is an advanced deployment option that you should only use if you have custom requirements.   In general, the steps include meeting the prerequisites, launching Cloudbreak on a VM, and creating the Cloudbreak credential. After performing these steps, you can create a cluster based on one of the default blueprints or upload your own blueprint and then create a cluster.   \n     Note \n     The Cloudbreak software runs in your cloud environment. You are responsible for cloud infrastructure related charges while running Cloudbreak and the clusters being managed by Cloudbreak.", 
            "title": "Launch Cloudbreak"
        }, 
        {
            "location": "/architecture/index.html", 
            "text": "Architecture\n\n\nCloudbreak deployer\n installs Cloudbreak components on a VM. Once these components are deployed, you can use \nCloudbreak application\n or Cloudbreak CLI to create, manage, and monitor clusters. \n\n\nCloudbreak deployer architecture\n\n\nCloudbreak deployer\n installs Cloudbreak components on a VM. It includes the following components:\n\n\n\n\n\n\n\n\nComponent\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nCloudbreak Application\n\n\nCloudbreak application is built on the foundation of cloud provider APIs and Apache Ambari.\n\n\n\n\n\n\nUluwatu\n\n\nThis is Cloudbreak web UI, which can be used to create, manage, and monitor clusters.\n\n\n\n\n\n\nCloudbreak CLI\n\n\nThis is Cloudbreak's command line tool, which can be used to create, manage, and monitor clusters.\n\n\n\n\n\n\nIdentity\n\n\nThis is Cloudbreak's OAuth identity server implementation, which utilizes UAA.\n\n\n\n\n\n\nSultans\n\n\nThis is Cloudbreak's user management system.\n\n\n\n\n\n\nPeriscope\n\n\nThis is Cloudbreak's autoscaling application, which is responsible for automatically increasing or decreasing the capacity of the cluster when your pre-defined conditions are met.\n\n\n\n\n\n\n\n\nCloudbreak application architecture\n\n\nThe Cloudbreak application is a web application which simplifies cluster provisioning in the cloud. Based on your input, Cloudbreak provisions all required cloud infrastructure and then provisions a cluster on your behalf within your cloud provider account.   \n\n\n \n\n\nCloudbreak application is built on the foundation of cloud provider APIs and Apache Ambari:\n\n\n\n\n\n\nCloudbreak uses \ncloud provider APIs\n to communicate with the cloud providers. \n\n\n\n\n\n\nCloudbreak uses the \nCloudbreak credential\n to authenticate with your cloud provider account and provision cloud resources required for the clusters. \n\n\n\n\n\n\nCloudbreak uses Apache Ambari and \nAmbari blueprints\n to provision, manage, and monitor clusters. Ambari blueprints are a declarative definition of a cluster. With a blueprint, you can specify stack, component layout, and configurations to materialize a cluster instance via Ambari REST API, without having to use the Ambari cluster install wizard.     \n\n\n\n\n\n\nCore concepts\n\n\nBefore using Cloudbreak, you should familiarize yourself with the following concepts.     \n\n\nCloudbreak credential\n\n\nAfter launching Cloudbreak, you must create a Cloudbreak credential for each cloud provider on which you would like to provision clusters. Only after you have completed that step you can start creating clusters. \n\n\nCloudbreak credential allows Cloudbreak to authenticate with the cloud provider and create resources on your behalf. The authentication process varies depending on the cloud provider, but is typically done via assigning a specific IAM role to Cloudbreak which allows Cloudbreak to perform certain actions within your cloud provider account. To learn more, refer to \nIdentity management\n.  \n\n\n \n\n\nRelated links\n\n\nIdentity management\n  \n\n\nAmbari blueprints\n\n\nAmbari blueprints are a declarative definition of a cluster. A blueprint allows you to specify stack, component layout, and configurations to materialize a cluster instance via Ambari REST API, without having to use the Ambari cluster install wizard.  \n\n\nAmbari blueprints are specified in JSON format. After you provide the blueprint to Cloudbreak, the host groups in the JSON are mapped to a set of instances when starting the cluster, and the specified services and components are installed on the corresponding nodes.\n\n\nCloudbreak includes a few default blueprints and allows you to upload your own blueprints.\n\n\n \n\n\nRelated links\n\n\nUsing custom blueprints\n\n\nApache documentation\n (External)  \n\n\nRecipes\n\n\nCloudbreak allows you to upload custom scripts, called \"recipes\". A recipe is a script that runs on all nodes of a selected node group at a specific time. You can use recipes for tasks such as installing additional software or performing advanced cluster configuration. For example, you can use a recipe to put a JAR file on the Hadoop classpath.\n\n\nAvailable recipe execution times are:  \n\n\n\n\nBefore Ambari server start    \n\n\nAfter Ambari server start    \n\n\nAfter cluster installation    \n\n\nBefore cluster termination   \n\n\n\n\nYou can upload your recipes to Cloudbreak via the UI or CLI. Then, when creating a cluster, you can optionally attach one or more \"recipes\" and they will be executed on a specific host group at a specified time. \n\n\nRelated links\n\n\nUsing custom scripts (recipes)", 
            "title": "Architecture"
        }, 
        {
            "location": "/architecture/index.html#architecture", 
            "text": "Cloudbreak deployer  installs Cloudbreak components on a VM. Once these components are deployed, you can use  Cloudbreak application  or Cloudbreak CLI to create, manage, and monitor clusters.", 
            "title": "Architecture"
        }, 
        {
            "location": "/architecture/index.html#cloudbreak-deployer-architecture", 
            "text": "Cloudbreak deployer  installs Cloudbreak components on a VM. It includes the following components:     Component  Description      Cloudbreak Application  Cloudbreak application is built on the foundation of cloud provider APIs and Apache Ambari.    Uluwatu  This is Cloudbreak web UI, which can be used to create, manage, and monitor clusters.    Cloudbreak CLI  This is Cloudbreak's command line tool, which can be used to create, manage, and monitor clusters.    Identity  This is Cloudbreak's OAuth identity server implementation, which utilizes UAA.    Sultans  This is Cloudbreak's user management system.    Periscope  This is Cloudbreak's autoscaling application, which is responsible for automatically increasing or decreasing the capacity of the cluster when your pre-defined conditions are met.", 
            "title": "Cloudbreak deployer architecture"
        }, 
        {
            "location": "/architecture/index.html#cloudbreak-application-architecture", 
            "text": "The Cloudbreak application is a web application which simplifies cluster provisioning in the cloud. Based on your input, Cloudbreak provisions all required cloud infrastructure and then provisions a cluster on your behalf within your cloud provider account.        Cloudbreak application is built on the foundation of cloud provider APIs and Apache Ambari:    Cloudbreak uses  cloud provider APIs  to communicate with the cloud providers.     Cloudbreak uses the  Cloudbreak credential  to authenticate with your cloud provider account and provision cloud resources required for the clusters.     Cloudbreak uses Apache Ambari and  Ambari blueprints  to provision, manage, and monitor clusters. Ambari blueprints are a declarative definition of a cluster. With a blueprint, you can specify stack, component layout, and configurations to materialize a cluster instance via Ambari REST API, without having to use the Ambari cluster install wizard.", 
            "title": "Cloudbreak application architecture"
        }, 
        {
            "location": "/architecture/index.html#core-concepts", 
            "text": "Before using Cloudbreak, you should familiarize yourself with the following concepts.", 
            "title": "Core concepts"
        }, 
        {
            "location": "/architecture/index.html#cloudbreak-credential", 
            "text": "After launching Cloudbreak, you must create a Cloudbreak credential for each cloud provider on which you would like to provision clusters. Only after you have completed that step you can start creating clusters.   Cloudbreak credential allows Cloudbreak to authenticate with the cloud provider and create resources on your behalf. The authentication process varies depending on the cloud provider, but is typically done via assigning a specific IAM role to Cloudbreak which allows Cloudbreak to perform certain actions within your cloud provider account. To learn more, refer to  Identity management .       Related links  Identity management", 
            "title": "Cloudbreak credential"
        }, 
        {
            "location": "/architecture/index.html#ambari-blueprints", 
            "text": "Ambari blueprints are a declarative definition of a cluster. A blueprint allows you to specify stack, component layout, and configurations to materialize a cluster instance via Ambari REST API, without having to use the Ambari cluster install wizard.    Ambari blueprints are specified in JSON format. After you provide the blueprint to Cloudbreak, the host groups in the JSON are mapped to a set of instances when starting the cluster, and the specified services and components are installed on the corresponding nodes.  Cloudbreak includes a few default blueprints and allows you to upload your own blueprints.     Related links  Using custom blueprints  Apache documentation  (External)", 
            "title": "Ambari blueprints"
        }, 
        {
            "location": "/architecture/index.html#recipes", 
            "text": "Cloudbreak allows you to upload custom scripts, called \"recipes\". A recipe is a script that runs on all nodes of a selected node group at a specific time. You can use recipes for tasks such as installing additional software or performing advanced cluster configuration. For example, you can use a recipe to put a JAR file on the Hadoop classpath.  Available recipe execution times are:     Before Ambari server start      After Ambari server start      After cluster installation      Before cluster termination      You can upload your recipes to Cloudbreak via the UI or CLI. Then, when creating a cluster, you can optionally attach one or more \"recipes\" and they will be executed on a specific host group at a specified time.   Related links  Using custom scripts (recipes)", 
            "title": "Recipes"
        }, 
        {
            "location": "/security/index.html", 
            "text": "Security overview\n\n\nCloudbreak utilizes cloud provider security resources such as virtual networks, security groups, and identity and access management:\n\n\n\n\nNetwork isolation\n is achieved via user-configured virtual networks and subnets.\n\n    Read more about \nvirtual networks\n.  \n\n\nNetwork security\n is achieved via out-of-the-box security group settings.\n\n    Read more about \nnetwork security\n.   \n\n\nControlled use of cloud resources\n using IAM roles (AWS, GCP) or Active Directory (in case of Azure). \n    Read more about \nidentity management\n.    \n\n\n\n\nVirtual networks\n\n\nCloud providers use virtual networks which resemble traditional networks. Depending on the options that you selected during deployment, your Cloudbreak instance and clusters are launched into new or existing cloud provider networking infrastructure (virtual networks and subnets). For more information about virtual networks, refer to the cloud-provider documentation:\n\n\n\n\n\n\n\n\nCloud provider\n\n\nExternal documentation link\n\n\n\n\n\n\n\n\n\n\nAWS\n\n\nAmazon Virtual Private Cloud (Amazon VPC)\n\n\n\n\n\n\nAzure\n\n\nMicrosoft Azure Virtual Network\n\n\n\n\n\n\nGoogle Cloud Platform\n\n\nVirtual Private Cloud (VPC) network\n\n\n\n\n\n\nOpenStack\n\n\nNetwork\n\n\n\n\n\n\n\n\nNetwork security\n\n\nSecurity groups are set up to control network traffic to the instances in the system.\n\n\nCloudbreak uses public IP addresses when communicating with cluster nodes. On AWS, you can configure it to use private IPs instead. For instructions, refer to \nConfigure communication via private IPs on AWS\n.  \n\n\nCloudbreak instance security group\n\n\nThe following table lists the minimum security group port configuration required for the Cloudbreak instance:\n\n\n\n\n\n\n\n\nInbound port\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\n22\n\n\nSSH access to the Cloudbreak VM.\n\n\n\n\n\n\n80\n\n\nHTTP access to the Cloudbreak UI. This is automatically redirected to the HTTPS (443) port.\n\n\n\n\n\n\n443\n\n\nHTTPS access to the Cloudbreak UI.\n\n\n\n\n\n\n\n\nDefault cluster security groups\n\n\nFor clusters, Cloudbreak provides the following security group settings. If you do not modify these settings, these default security rules will be created. You can modify these rules either when creating or, if you don't want to use security group, remove them. \n\n\nAs an alternative to creating new security groups, you can select from your existing set of security groups, which can be modified using the cloud provider tools. \n\n\nCluster host group with Ambari server\n\n\n\n\n\n\n\n\nInbound port\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\n22\n\n\nSSH access to the VM instance. This port is also used by Cloudbreak to communicate with the cluster.\n\n\n\n\n\n\n443\n\n\nHTTPS access to the Ambari UI.\n\n\n\n\n\n\n9443\n\n\nManagement port, used by Cloudbreak to communicate with the cluster node VM.\n\n\n\n\n\n\n\n\n\n\nImportant\n\n\n\nBy default, ports 22, 443, and 9443 are set to 0.0.0.0/0 CIDR for inbound access on the Ambari node security group. We strongly recommend that you limit this CIDR, considering the following restrictions:\n\nPorts 22 and 9443 must be open to Cloudbreak's CIDR. You can set CB_DEFAULT_GATEWAY_CIDR in your Cloudbreak's Profile file in order to automatically open ports 22 and 9443 to your Cloudbreak IP. Refer to \nRestricting inbound access from Cloudbreak to cluster\n.\n\n\nPort 22 must be open to your CIDR if you would like to access the master node via SSH.\n\n\nPort 443 must be open to your CIDR if you would like to access Ambari web UI in a browser.\n\n\n  \n\n\n\n\n\n\nCluster host groups without the Ambari server\n\n\n\n\n\n\n\n\nInbound port\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\n22\n\n\nSSH access to the VM instance.\n\n\n\n\n\n\n\n\n\n\nImportant\n\n\n\nBy default, port 22 is set to 0.0.0.0/0 CIDR for inbound access on non-Ambari node security groups. We strongly recommend that you remove it.\n\n\n\n\n\nWhen creating a new security group, Cloudbreak uses the following naming convention: \nclustername\n-ClusterNodeSecurityGroup\nhostgroupname\n \n\n\n\n\nImportant\n\n\n\nDepending on what services you are including, you need to open additional ports as required by these services. For example, when using the Flow Management blueprint, you must open port 9091 for NiFi (on NiFI host group) and port 61443 for NiFI Registry (on the Services host group).  \n\n\n\n\n\n\nIdentity management\n\n\nTo securely control access to cloud resources, cloud providers use identity management services such as IAM roles (AWS and GCP) and Active Directory (Azure). \n\n\n\n\n\n\n\n\nCloud provider\n\n\nExternal documentation link\n\n\n\n\n\n\n\n\n\n\nAWS\n\n\nAWS Identity and Access Management (IAM)\n\n\n\n\n\n\nAzure\n\n\nAzure Active Directory ((Azure AD))\n\n\n\n\n\n\nGoogle\n\n\nGoogle Cloud Identity and Access Management (IAM)\n\n\n\n\n\n\nOpenStack\n\n\nKeystone\n\n\n\n\n\n\n\n\nCloudbreak utilizes cloud provider\u2019s identity management services via Cloudbreak credential. After launching Cloudbreak on your chosen cloud provider, you must create a Cloudbreak credential, which allows Cloudbreak to authenticate with your cloud provider identity management service. Only after you have completed this step, Cloudbreak can create resources on your behalf. \n\n\nAuthentication with AWS\n\n\nWhen launching Cloudbreak on AWS, you must select a way for Cloudbreak to authenticate with your AWS account and create resources on your behalf. While key-based authentication uses your AWS access key and secret key, role-based authentication uses IAM roles.\n\n\nIf you are using role-based authentication for Cloudbreak on AWS, you must create two IAM roles: one to grant Cloudbreak access to allow Cloudbreak to assume AWS roles (using the \"AssumeRole\" policy) and the second one to provide Cloudbreak with the capabilities required for cluster creation (using the \"cb-policy\" policy).\n\n\nThe following table provides contextual information about the two roles required: \n\n\n\n\n\n\n\n\nRole\n\n\nPurpose\n\n\nOverview of steps\n\n\nConfiguration\n\n\n\n\n\n\n\n\n\n\nCloudbreakRole\n\n\nAllows Cloudbreak to assume other IAM roles - specifically the CredentialRole.\n\n\nCreate a role called \"CloudbreakRole\" and attach the \"AssumeRole\" policy. The \"AssumeRole\" policy definition and steps for creating the CloudbreakRole are provided below.\n\n\nWhen launching Cloudbreak, you will attach the \"CloudbreakRole\" IAM role to the VM.\nIf you are using hosted Cloudbreak, you do not need to perform this step.\n\n\n\n\n\n\nCredentialRole\n\n\nAllows Cloudbreak to create AWS resources required for clusters.\n\n\nCreate a new IAM role called \"CredentialRole\" and attach the \"cb-policy\" policy to it. The \"cb-policy\" policy definition and steps for creating the CredentialRole are provided below.\n When creating this role using the AWS Console, make sure that that it is a role for cross-account access and that the trust-relation is set up as follows: 'Account ID' is your own 12-digit AWS account ID and 'External ID' is \u201cprovision-ambari\u201d. See steps below.\n\n\nOnce you log in to the Cloudbreak UI and are ready to create clusters, you will use this role to create the Cloudbreak credential.\n\n\n\n\n\n\n\n\nRelated links\n\n\nMeet the prerequisites: Authentication\n  \n\n\nAuthentication with Azure\n\n\nAfter launching Cloudbread on Azure, you are required to create a Cloudbreak credential, which allows Cloudbreak to authenticate with your Azure Active Directory. \n\n\nYou have two options:\n\n\n\n\n\n\nInteractive: The app and service principal creation and role assignment are fully automated, so the only input that you need to provide to Cloudbreak is your Subscription ID and Directory ID. \n\n\n\n\n\n\nApp-based: The app and service principal creation and role assignment are not automated You must create an Azure Active Directory application registration and then provide its parameters to Cloudbreak, in addition to providing your Subscription ID and Directory ID. \n\n\n\n\n\n\nRelated links\n\n\nCreate Cloudbreak credential\n  \n\n\nAuthentication with GCP\n\n\nAfter launching Cloudbreak on GCP, you are required to register a service account in Cloudbreak via creating a Cloudbreak credential. Cloudbreak uses this account to authenticate with the GCP identity management service.\n\n\nRelated links\n\n\nMeet the prerequisites: Service account\n  \n\n\nAuthentication with OpenStack\n\n\nAfter launching Cloudbreak on OpenStack, you are required to create a Cloudbreak credential, which allows Cloudbreak to authenticate with keystone. \n\n\nRelated links\n\n\nCreate Cloudbreak credential", 
            "title": "Security overview"
        }, 
        {
            "location": "/security/index.html#security-overview", 
            "text": "Cloudbreak utilizes cloud provider security resources such as virtual networks, security groups, and identity and access management:   Network isolation  is achieved via user-configured virtual networks and subnets. \n    Read more about  virtual networks .    Network security  is achieved via out-of-the-box security group settings. \n    Read more about  network security .     Controlled use of cloud resources  using IAM roles (AWS, GCP) or Active Directory (in case of Azure). \n    Read more about  identity management .", 
            "title": "Security overview"
        }, 
        {
            "location": "/security/index.html#virtual-networks", 
            "text": "Cloud providers use virtual networks which resemble traditional networks. Depending on the options that you selected during deployment, your Cloudbreak instance and clusters are launched into new or existing cloud provider networking infrastructure (virtual networks and subnets). For more information about virtual networks, refer to the cloud-provider documentation:     Cloud provider  External documentation link      AWS  Amazon Virtual Private Cloud (Amazon VPC)    Azure  Microsoft Azure Virtual Network    Google Cloud Platform  Virtual Private Cloud (VPC) network    OpenStack  Network", 
            "title": "Virtual networks"
        }, 
        {
            "location": "/security/index.html#network-security", 
            "text": "Security groups are set up to control network traffic to the instances in the system.  Cloudbreak uses public IP addresses when communicating with cluster nodes. On AWS, you can configure it to use private IPs instead. For instructions, refer to  Configure communication via private IPs on AWS .", 
            "title": "Network security"
        }, 
        {
            "location": "/security/index.html#cloudbreak-instance-security-group", 
            "text": "The following table lists the minimum security group port configuration required for the Cloudbreak instance:     Inbound port  Description      22  SSH access to the Cloudbreak VM.    80  HTTP access to the Cloudbreak UI. This is automatically redirected to the HTTPS (443) port.    443  HTTPS access to the Cloudbreak UI.", 
            "title": "Cloudbreak instance security group"
        }, 
        {
            "location": "/security/index.html#default-cluster-security-groups", 
            "text": "For clusters, Cloudbreak provides the following security group settings. If you do not modify these settings, these default security rules will be created. You can modify these rules either when creating or, if you don't want to use security group, remove them.   As an alternative to creating new security groups, you can select from your existing set of security groups, which can be modified using the cloud provider tools.   Cluster host group with Ambari server     Inbound port  Description      22  SSH access to the VM instance. This port is also used by Cloudbreak to communicate with the cluster.    443  HTTPS access to the Ambari UI.    9443  Management port, used by Cloudbreak to communicate with the cluster node VM.      Important  \nBy default, ports 22, 443, and 9443 are set to 0.0.0.0/0 CIDR for inbound access on the Ambari node security group. We strongly recommend that you limit this CIDR, considering the following restrictions: Ports 22 and 9443 must be open to Cloudbreak's CIDR. You can set CB_DEFAULT_GATEWAY_CIDR in your Cloudbreak's Profile file in order to automatically open ports 22 and 9443 to your Cloudbreak IP. Refer to  Restricting inbound access from Cloudbreak to cluster .  Port 22 must be open to your CIDR if you would like to access the master node via SSH.  Port 443 must be open to your CIDR if you would like to access Ambari web UI in a browser.       Cluster host groups without the Ambari server     Inbound port  Description      22  SSH access to the VM instance.      Important  \nBy default, port 22 is set to 0.0.0.0/0 CIDR for inbound access on non-Ambari node security groups. We strongly recommend that you remove it.   When creating a new security group, Cloudbreak uses the following naming convention:  clustername -ClusterNodeSecurityGroup hostgroupname     Important  \nDepending on what services you are including, you need to open additional ports as required by these services. For example, when using the Flow Management blueprint, you must open port 9091 for NiFi (on NiFI host group) and port 61443 for NiFI Registry (on the Services host group).", 
            "title": "Default cluster security groups"
        }, 
        {
            "location": "/security/index.html#identity-management", 
            "text": "To securely control access to cloud resources, cloud providers use identity management services such as IAM roles (AWS and GCP) and Active Directory (Azure).      Cloud provider  External documentation link      AWS  AWS Identity and Access Management (IAM)    Azure  Azure Active Directory ((Azure AD))    Google  Google Cloud Identity and Access Management (IAM)    OpenStack  Keystone     Cloudbreak utilizes cloud provider\u2019s identity management services via Cloudbreak credential. After launching Cloudbreak on your chosen cloud provider, you must create a Cloudbreak credential, which allows Cloudbreak to authenticate with your cloud provider identity management service. Only after you have completed this step, Cloudbreak can create resources on your behalf.", 
            "title": "Identity management"
        }, 
        {
            "location": "/security/index.html#authentication-with-aws", 
            "text": "When launching Cloudbreak on AWS, you must select a way for Cloudbreak to authenticate with your AWS account and create resources on your behalf. While key-based authentication uses your AWS access key and secret key, role-based authentication uses IAM roles.  If you are using role-based authentication for Cloudbreak on AWS, you must create two IAM roles: one to grant Cloudbreak access to allow Cloudbreak to assume AWS roles (using the \"AssumeRole\" policy) and the second one to provide Cloudbreak with the capabilities required for cluster creation (using the \"cb-policy\" policy).  The following table provides contextual information about the two roles required:      Role  Purpose  Overview of steps  Configuration      CloudbreakRole  Allows Cloudbreak to assume other IAM roles - specifically the CredentialRole.  Create a role called \"CloudbreakRole\" and attach the \"AssumeRole\" policy. The \"AssumeRole\" policy definition and steps for creating the CloudbreakRole are provided below.  When launching Cloudbreak, you will attach the \"CloudbreakRole\" IAM role to the VM. If you are using hosted Cloudbreak, you do not need to perform this step.    CredentialRole  Allows Cloudbreak to create AWS resources required for clusters.  Create a new IAM role called \"CredentialRole\" and attach the \"cb-policy\" policy to it. The \"cb-policy\" policy definition and steps for creating the CredentialRole are provided below.  When creating this role using the AWS Console, make sure that that it is a role for cross-account access and that the trust-relation is set up as follows: 'Account ID' is your own 12-digit AWS account ID and 'External ID' is \u201cprovision-ambari\u201d. See steps below.  Once you log in to the Cloudbreak UI and are ready to create clusters, you will use this role to create the Cloudbreak credential.     Related links  Meet the prerequisites: Authentication", 
            "title": "Authentication with AWS"
        }, 
        {
            "location": "/security/index.html#authentication-with-azure", 
            "text": "After launching Cloudbread on Azure, you are required to create a Cloudbreak credential, which allows Cloudbreak to authenticate with your Azure Active Directory.   You have two options:    Interactive: The app and service principal creation and role assignment are fully automated, so the only input that you need to provide to Cloudbreak is your Subscription ID and Directory ID.     App-based: The app and service principal creation and role assignment are not automated You must create an Azure Active Directory application registration and then provide its parameters to Cloudbreak, in addition to providing your Subscription ID and Directory ID.     Related links  Create Cloudbreak credential", 
            "title": "Authentication with Azure"
        }, 
        {
            "location": "/security/index.html#authentication-with-gcp", 
            "text": "After launching Cloudbreak on GCP, you are required to register a service account in Cloudbreak via creating a Cloudbreak credential. Cloudbreak uses this account to authenticate with the GCP identity management service.  Related links  Meet the prerequisites: Service account", 
            "title": "Authentication with GCP"
        }, 
        {
            "location": "/security/index.html#authentication-with-openstack", 
            "text": "After launching Cloudbreak on OpenStack, you are required to create a Cloudbreak credential, which allows Cloudbreak to authenticate with keystone.   Related links  Create Cloudbreak credential", 
            "title": "Authentication with OpenStack"
        }, 
        {
            "location": "/aws-launch/index.html", 
            "text": "Launching Cloudbreak on AWS\n\n\nBefore launching Cloudbreak on AWS, review and meet the prerequisites. Next, launch a VM using a Cloudbreak Amazon Machine Image, access the VM, and then start Cloudbreak. Once Cloudbreak is running, log in to the Cloudbreak UI and create a Cloudbreak credential. \n\n\nMeet the prerequisites\n\n\nBefore launching Cloudbreak on AWS, you must meet the following prerequisites.\n\n\nAWS account\n\n\nIn order to launch Cloudbreak on AWS, you must log in to your AWS account. If you don't have an account, you can create one at \nhttps://aws.amazon.com/\n.\n\n\nAWS region\n\n\nDecide in which AWS region you would like to launch Cloudbreak. The following AWS regions are supported: \n\n\n\n\n\n\n\n\nRegion name\n\n\nRegion\n\n\n\n\n\n\n\n\n\n\nEU (Ireland)\n\n\neu-west-1\n\n\n\n\n\n\nEU (Frankfurt)\n\n\neu-central-1\n\n\n\n\n\n\nUS East (N. Virginia)\n\n\nus-east-1\n\n\n\n\n\n\nUS West (N. California)\n\n\nus-west-1\n\n\n\n\n\n\nUS West (Oregon)\n\n\nus-west-2\n\n\n\n\n\n\nSouth America (S\u00e3o Paulo)\n\n\nsa-east-1\n\n\n\n\n\n\nAsia Pacific (Tokyo)\n\n\nap-northeast-1\n\n\n\n\n\n\nAsia Pacific (Singapore)\n\n\nap-southeast-1\n\n\n\n\n\n\nAsia Pacific (Sydney)\n\n\nap-southeast-2\n\n\n\n\n\n\n\n\nClusters created via Cloudbreak can be in the same or different region as Cloudbreak; when you launch a cluster, you select the region in which to launch it.\n\n\nRelated links\n\n\nAWS regions and endpoints\n (External)   \n\n\nSSH key pair\n\n\nImport an existing key pair or generate a new key pair in the AWS region which you are planning to use for launching Cloudbreak and clusters. You can do this using the following steps.\n\n\nSteps\n \n\n\n\n\nNavigate to the Amazon EC2 console at https://console.aws.amazon.com/ec2/.  \n\n\nCheck the region listed in the top right corner to make sure that you are in the correct region.  \n\n\nIn the left pane, find \nNETWORK AND SECURITY\n and click \nKey Pairs\n.   \n\n\nDo one of the following:\n\n\nClick \nCreate Key Pair\n to create a new key pair. Your private key file will be automatically downloaded onto your computer. Make sure to save it in a secure location. You will need it to SSH to the cluster nodes. You may want to change access settings for the file using \nchmod 400 my-key-pair.pem\n.  \n\n\nClick \nImport Key Pair\n to upload an existing public key and then select it and click \nImport\n. Make sure that you have access to its corresponding private key.    \n\n\n\n\n\n\n\n\nYou need this SSH key pair to SSH to the Cloudbreak instance and start Cloudbreak. \n\n\nRelated links\n\n\nCreating a key pair using Amazon EC2\n (External)  \n\n\nAuthentication\n\n\nBefore you can start using Cloudbreak for provisioning clusters, you must select a way for Cloudbreak to authenticate with your AWS account and create resources on your behalf. There are two ways to do this: \n\n\n\n\n\n\nKey-based\n: This is a simpler option which does not require additional configuration at this point. It requires that you provide your AWS access key and secret key pair in the Cloudbreak web UI later. All you need to do now is check your AWS account and ensure that you can access this key pair.\n\n\n\n\n\n\nRole-based\n: This requires that you or your AWS admin create an IAM role to allow Cloudbreak to assume AWS roles (the \"AssumeRole\" policy).\n\n\n\n\n\n\n(Option 1) Configure key-based authentication\n\n\nIf you are using key-based authentication for Cloudbreak on AWS, you must be able to provide your AWS access key and secret key pair. Cloudbreak will use these keys to launch the resources. You must provide the access and secret keys later in the Cloudbreak web UI later when creating a credential. \n\n\nIf you choose this option, all you need to do at this point is check your AWS account and make sure that you can access this key pair. You can generate new access and secret keys from the \nIAM Console\n \n \nUsers\n. Next, select a user and click on the \nSecurity credentials\n tab:\n\n\n \n\n\nIf you choose this option, you can proceed to \nLaunch Cloudbreak deployer from an image\n.\n\n\n(Option 2) Configure role-based authentication\n\n\nIf you are using role-based authentication for Cloudbreak on AWS, you must create two IAM roles: one to grant Cloudbreak access to allow Cloudbreak to assume AWS roles (using the \"AssumeRole\" policy) and the second one to provide Cloudbreak with the capabilities required for cluster creation (using the \"cb-policy\" policy).\n\n\nThe following table provides contextual information about the two roles required: \n\n\n\n\n\n\n\n\nRole\n\n\nPurpose\n\n\nOverview of steps\n\n\nConfiguration\n\n\n\n\n\n\n\n\n\n\nCloudbreakRole\n\n\nAllows Cloudbreak to assume other IAM roles - specifically the CredentialRole.\n\n\nCreate a role called \"CloudbreakRole\" and attach the \"AssumeRole\" policy. The \"AssumeRole\" policy definition and steps for creating the CloudbreakRole are provided below.\n\n\nWhen launching Cloudbreak, you will attach the \"CloudbreakRole\" IAM role to the VM.\nIf you are using hosted Cloudbreak, you do not need to perform this step.\n\n\n\n\n\n\nCredentialRole\n\n\nAllows Cloudbreak to create AWS resources required for clusters.\n\n\nCreate a new IAM role called \"CredentialRole\" and attach the \"cb-policy\" policy to it. The \"cb-policy\" policy definition and steps for creating the CredentialRole are provided below.\n When creating this role using the AWS Console, make sure that that it is a role for cross-account access and that the trust-relation is set up as follows: 'Account ID' is your own 12-digit AWS account ID and 'External ID' is \u201cprovision-ambari\u201d. See steps below.\n\n\nOnce you log in to the Cloudbreak UI and are ready to create clusters, you will use this role to create the Cloudbreak credential.\n\n\n\n\n\n\n\n\n\n\nThese role and policy names are just examples. You may use different names when creating your resources.  \n\n\nCloudbreakRole: Alternatively, instead of attaching the \"CloudbreakRole\" role during the VM launch, you can assign the \"CloudbreakRole\" to an IAM user and then add the access and secret key of that user to your 'Profile'.\n\n\nCredentialRole: Alternatively you can generate the \"CredentialRole\" role later once your Cloudbreak VM is running by SSHing to the Cloudbreak VM and running the \ncbd aws generate-role\n command. This command creates a role with the name \"cbreak-deployer\" (equivalent to the \"CredentialRole\"). To customize the name of the role, add \nexport AWS_ROLE_NAME=my-cloudbreak-role-name\n (where \"my-cloudbreak-role-name\" is your custom role name) as a new line to your Profile. If you choose this option, you must make sure that the \"CloudbreakRole\" or the IAM user have a permission not only to assume a role but also to create a role.  \n\n\n\n\nYou can create these roles in the \nIAM console\n, on the \nRoles\n page via the \nCreate Role\n option. Detailed steps are provided below. \n\n\nCreate CloudbreakRole\n\n\nUse these steps to create CloudbreakRole. \n\n\nUse the following \"AssumeRole\" policy definition: \n\n\n{\n  \"Version\": \"2012-10-17\",\n  \"Statement\": {\n    \"Sid\": \"Stmt1400068149000\",\n    \"Effect\": \"Allow\",\n    \"Action\": [\"sts:AssumeRole\"],\n    \"Resource\": \"*\"\n  }\n}\n\n\n\n\nSteps\n\n\n\n\n\n\nNavigate to the \nIAM console\n \n \nRoles\n and click \nCreate Role\n.\n\n\n \n\n\n\n\n\n\nIn the \"Create Role\" wizard, select \nAWS service\n role type and then select any service. \n\n\n \n\n\n\n\n\n\nWhen done, click \nNext: Permissions\n to navigate to the next page in the wizard.\n\n\n\n\n\n\nClick \nCreate policy\n.\n\n\n\n\n\n\n\n\nClick \nSelect\n next to \"Create Your Own Policy\".\n\n\n  \n\n\n\n\n\n\nIn the \nPolicy Name\n field, enter \"AssumeRole\" and in the \nPolicy Document\n paste the policy definition. You can either copy it from the section preceding these steps or download and copy it from \nhere\n.\n\n\n  \n\n\n\n\n\n\nWhen done, click \nCreate Policy\n.\n\n\n\n\n\n\nClick \nRefresh\n. Next, find the \"AssumeRole\" policy that you just created and select it by checking the box.\n\n\n \n\n\n\n\n\n\nWhen done, click \nNext: Review\n.\n\n\n\n\n\n\nIn the \nRoles name\n field, enter role name, for example \"CloudbreakRole\". \n\n\n \n\n\n\n\n\n\nWhen done, click \nCreate role\n to finish the role creation process.\n\n\n\n\n\n\nCreate CredentialRole\n\n\nUse these steps to create CredentialRole.\n\n\nUse the following \"cb-policy\" policy definition: \n\n\n{\n  \"Version\": \"2012-10-17\",\n  \"Statement\": [\n    {\n      \"Effect\": \"Allow\",\n      \"Action\": [\n        \"cloudformation:CreateStack\",\n        \"cloudformation:DeleteStack\",\n        \"cloudformation:DescribeStackEvents\",\n        \"cloudformation:DescribeStackResource\",\n        \"cloudformation:DescribeStacks\"\n      ],\n      \"Resource\": [\n        \"*\"\n      ]\n    },\n    {\n      \"Effect\": \"Allow\",\n      \"Action\": [\n        \"ec2:AllocateAddress\",\n        \"ec2:AssociateAddress\",\n        \"ec2:AssociateRouteTable\",\n        \"ec2:AuthorizeSecurityGroupIngress\",\n        \"ec2:DescribeRegions\",\n        \"ec2:DescribeAvailabilityZones\",\n        \"ec2:CreateRoute\",\n        \"ec2:CreateRouteTable\",\n        \"ec2:CreateSecurityGroup\",\n        \"ec2:CreateSubnet\",\n        \"ec2:CreateTags\",\n        \"ec2:CreateVpc\",\n        \"ec2:ModifyVpcAttribute\",\n        \"ec2:DeleteSubnet\",\n        \"ec2:CreateInternetGateway\",\n        \"ec2:CreateKeyPair\",\n        \"ec2:DeleteKeyPair\", \n        \"ec2:DisassociateAddress\",\n        \"ec2:DisassociateRouteTable\",\n        \"ec2:ModifySubnetAttribute\",\n        \"ec2:ReleaseAddress\",\n        \"ec2:DescribeAddresses\",\n        \"ec2:DescribeImages\",\n        \"ec2:DescribeInstanceStatus\",\n        \"ec2:DescribeInstances\",\n        \"ec2:DescribeInternetGateways\",\n        \"ec2:DescribeKeyPairs\",\n        \"ec2:DescribeRouteTables\",\n        \"ec2:DescribeSecurityGroups\",\n        \"ec2:DescribeSubnets\",\n        \"ec2:DescribeVpcs\",\n        \"ec2:DescribeSpotInstanceRequests\",\n        \"ec2:DescribeVpcAttribute\",\n        \"ec2:ImportKeyPair\",\n        \"ec2:AttachInternetGateway\",\n        \"ec2:DeleteVpc\",\n        \"ec2:DeleteSecurityGroup\",\n        \"ec2:DeleteRouteTable\",\n        \"ec2:DeleteInternetGateway\",\n        \"ec2:DeleteRouteTable\",\n        \"ec2:DeleteRoute\",\n        \"ec2:DetachInternetGateway\",\n        \"ec2:RunInstances\",\n        \"ec2:StartInstances\",\n        \"ec2:StopInstances\",\n        \"ec2:TerminateInstances\"\n      ],\n      \"Resource\": [\n        \"*\"\n      ]\n    },\n    {\n      \"Effect\": \"Allow\",\n      \"Action\": [\n        \"iam:ListRolePolicies\",\n        \"iam:GetRolePolicy\",\n        \"iam:ListAttachedRolePolicies\",\n        \"iam:ListInstanceProfiles\",\n        \"iam:PutRolePolicy\",\n        \"iam:PassRole\",\n        \"iam:GetRole\"\n      ],\n      \"Resource\": [\n        \"*\"\n      ]\n    },\n    {\n      \"Effect\": \"Allow\",\n      \"Action\": [\n        \"autoscaling:CreateAutoScalingGroup\",\n        \"autoscaling:CreateLaunchConfiguration\",\n        \"autoscaling:DeleteAutoScalingGroup\",\n        \"autoscaling:DeleteLaunchConfiguration\",\n        \"autoscaling:DescribeAutoScalingGroups\",\n        \"autoscaling:DescribeLaunchConfigurations\",\n        \"autoscaling:DescribeScalingActivities\",\n        \"autoscaling:DetachInstances\",\n        \"autoscaling:ResumeProcesses\",\n        \"autoscaling:SuspendProcesses\",\n        \"autoscaling:UpdateAutoScalingGroup\"\n      ],\n      \"Resource\": [\n        \"*\"\n      ]\n    }\n  ]\n}\n\n\n\nSteps\n\n\n\n\n\n\nNavigate to the \nIAM console\n \n \nRoles\n and click \nCreate Role\n.\n\n\n \n\n\n\n\n\n\nIn the \"Create Role\" wizard, select \nAnother AWS account\n role type. Next, provide the following:\n\n\n\n\nIn the \nAccount ID\n field, enter your AWS account ID.\n\n\nUnder \nOptions\n, check \nRequire external ID\n.\n\n\nIn the \nExternal ID\n, enter \"provision-ambari\".\n\n\n\n\n \n\n\n\n\n\n\nWhen done, click \nNext: Permissions\n to navigate to the next page in the wizard.\n\n\n\n\n\n\nClick \nCreate policy\n.\n\n\n\n\n\n\n\n\nClick \nSelect\n next to \"Create Your Own Policy\".\n\n\n \n\n\n\n\n\n\nIn the \nPolicy Name\n field, enter \"cb-policy\" and in the \nPolicy Document\n paste the policy definition.  You can either copy it from the section preceding these steps or download and copy it from \nhere\n.\n\n\n  \n\n\n\n\n\n\nWhen done, click \nCreate Policy\n.\n\n\n\n\n\n\nClick \nRefresh\n. Next, find the \"cb-policy\" that you just created and select it by checking the box.\n\n\n \n\n\n\n\n\n\nWhen done, click \nNext: Review\n.\n\n\n\n\n\n\nIn the \nRoles name\n field, enter role name, for example \"CredentialRole\". \n\n\n \n\n\n\n\n\n\nWhen done, click \nCreate role\n to finish the role creation process. \n\n\n\n\n\n\nOnce you are done, you can proceed to launch Cloudbreak.  \n\n\nRelated links\n\n\nUsing instance profiles\n (External)\n\n\nUsing an IAM role to grant permissions to applications\n (External)   \n\n\nLaunch Cloudbreak from a template\n\n\nFollow these steps to launch Cloudbreak by using an Amazon CloudFormation template:    \n\n\nSteps\n \n\n\n\n\n\n\nClick on the link to launch the CloudFormation template that will create the AWS resources, including an EC2 Instance running Cloudbreak:\n\n\n\n \n\n   \nRegion\n\n   \nLink\n\n \n\n \n\n   \nus-east-1 (N. Virginia)\n\n   \n\n   \n\n   \n Launch the CloudFormation Template in US East\n\n \n\n  \n\n   \nus-west-1 (N. California)\n\n   \n\n   \n\n   \n Launch the CloudFormation Template in US West (N. California)\n\n \n\n \n\n   \nus-west-2 (Oregon)\n\n   \n\n   \n\n   \n Launch the CloudFormation Template in US West (Oregon)\n\n \n\n \n\n   \neu-central-1 (Frankfurt)\n\n   \n\n   \n\n   \n Launch the CloudFormation Template in EU Central\n\n \n\n \n\n   \neu-west-1 (Dublin)\n\n   \n\n   \n\n   \n Launch the CloudFormation Template in EU West \n\n \n\n \n\n   \nsa-east-1 (S\u00e3o Paulo)\n\n   \n\n   \n\n   \n Launch the CloudFormation Template in South America\n\n \n\n \n\n   \nap-northeast-1 Asia Pacific (Tokyo)\n\n   \n\n   \n\n   \n Launch the CloudFormation Template in Asia Pacific (Tokyo)\n\n \n\n \n\n   \nap-southeast-1 (Singapore)\n\n   \n\n   \n\n   \n Launch the CloudFormation Template in Asia Pacific (Singapore)\n\n \n\n \n\n   \nap-southeast-2 (Sydney)\n\n   \n\n   \n\n   \n Launch the CloudFormation Template in Asia Pacific (Sydney)\n\n \n\n\n  \n\n\n\n\n\n\nThe \nCreate stack\n wizard is launched in the Amazon CloudFormation management console:  \n\n\n\n\n\n\nIn the top right corner, confirm the region in which you want to launch Cloudbreak:  \n\n\n  \n\n\n\n\nYou may change the region if needed by using the dropdown in the top right corner.\n\n\n\n\n\n\n\n\nYou do not need to change any template parameters on the \nSelect Template\n page. Click \nNext\n to display the \nSpecify Details\n page:\n\n\n \n\n\n\n\n\n\nOn the the \nSpecify Details\n page, enter the \nStack name\n:\n\n\n\n\n\n\n\n\nParameter\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nStack name\n\n\nEnter name for your stack. It must be unique in your AWS environment.\n\n\n\n\n\n\n\n\n\n\n\n\nOn the same page, enter the following in the \nParameters\n section:\n\n\n\n\nAll parameters are required.\n\n\n\n\nGeneral Configuration\n \n\n\n\n\n\n\n\n\nParameter\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nController Instance Type\n\n\nEC2 instance type to use for the cloud controller.\n\n\n\n\n\n\nEmail Address\n\n\nUsername for the Admin login. Must be a valid email address.\n\n\n\n\n\n\nAdmin Password\n\n\nPassword for Admin login. Must be at least 8 characters containing letters, numbers, and symbols.\n\n\n\n\n\n\n\n\nSecurity Configuration\n \n\n\n\n\n\n\n\n\nParameter\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nSSH Key Name\n\n\nName of an existing EC2 key pair to enable SSH to access the instances. Key pairs are region-specific, so only the key pairs that you created for a selected region will appear in the dropdown. See \nPrerequisites\n for more information.\n\n\n\n\n\n\nRemote Access\n\n\nAllow connections to the cloud controller ports from this address range. Must be a valid \nCIDR IP\n. For example: \n192.168.27.0/24 will allow access from 192.168.27.0 through 192.168.27.255.\n192.168.27.10/32 will allow access from 192.168.27.10.\n0.0.0.0/0 will allow access from all.\n Refer to \nNetwork security\n for more information on the inbound ports that are used with Cloudbreak.\n\n\n\n\n\n\n\n\n\n\n\n\nClick \nNext\n  to display the \nOptions\n page.    \n\n\n\n\n\n\nOn he \nOptions\n page, if you expand the \nAdvanced\n section, there is an option to \nRollback on failure\n. \n\n\n\n\nBy\ndefault, this option is set to \nYes\n, which means that if there are any event failures when creating\nthe stack, all the AWS resources created so far are deleted (i.e rolled back) to avoid unnecessary charges. \n\n\nIf you set this option to \nNo\n, if there are any event failures when creating\nthe stack, the\nresources are left intact (i.e. not rolled back). Select the \nNo\n option to aid in\ntroubleshooting. Note that in this case you are responsible for deleting the stack later.\n\n\n\n\n\n\n\n\nClick \nNext\n to display the \nReview\n page.\n\n\n\n\n\n\nOn the \nReview\n page, click the \nI acknowledge...\n checkbox.  \n\n\n\n\n\n\nClick \nCreate\n.\n\n\n\n\nThe \nStack Name\n is shown in the table with a \nCREATE_IN_PROGRESS\n status. You can click on the \nStack Name\n and see the specific events that are in progress. The create process takes about 10 minutes and once ready, you will see \nCREATE_COMPLETE\n. \n\n\n\n\n\n\n\n\nAccess Cloudbreak web UI\n\n\n\n\n\n\nOnce the stack creation is complete, the cloud controller is ready to use. You can obtain the URL to the cloud controller\nand the SSH access information from the \nOutputs\n tab:\n\n\n\n\n\n\nIf the Outputs tab is blank, refresh the page.\n\n\n\n\n\n\n\n\nOnce the stack creation is complete, browse instance created at the \nCloudURL\n provided in the \nOutputs\n tab and log in.\n\n\n\n\n\n\nLog in to the Cloudbreak web UI using the credential that you configured.\n\n\n\n\n\n\nUpon a successful login, you are redirected to the dashboard:\n\n\n  \n\n\n\n\n\n\nCreate Cloudbreak credential\n\n\nBefore you can start creating clusters, you must first create a \nCloudbreak credential\n. Without this credential, you will not be able to create clusters via Cloudbreak. \n\n\nAs part of the \nprerequisites\n, you had two options to allow Cloudbreak to authenticate with AWS and create resources on your behalf: key-based or role-based authentication. Depending on your choice, you must configure a key-based or role-based credential: \n\n\n\n\nCreate key-based credential\n  \n\n\nCreate role-based credential\n\n\n\n\nCreate key-based credential\n\n\nTo perform these steps, you must know your access and secret key. If needed, you or your AWS administrator can generate new access and secret keys from the \nIAM Console\n \n \nUsers\n \n select a user \n \nSecurity credentials\n. \n\n\nSteps\n\n\n\n\n\n\nIn the Cloudbreak web UI, select \nCredentials\n from the navigation pane. \n\n\n\n\n\n\nClick \nCreate Credential\n. \n\n\n\n\n\n\nUnder \nCloud provider\n, select \"Amazon Web Services\":\n\n\n  \n\n\n\n\n\n\nProvide the following information:\n\n\n\n\n\n\n\n\nParameter\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nSelect Credential Type\n\n\nSelect \nKey Based\n.\n\n\n\n\n\n\nName\n\n\nEnter a name for your credential.\n\n\n\n\n\n\nDescription\n\n\n(Optional) Enter a description.\n\n\n\n\n\n\nAccess Key\n\n\nPaste your access key.\n\n\n\n\n\n\nSecret Access Key\n\n\nPaste your secret key.\n\n\n\n\n\n\n\n\n\n\n\n\nClick \nCreate\n.\n\n\n\n\n\n\nYour credential should now be displayed in the \nCredentials\n pane.\n\n\nCongratulations! You've successfully launched Cloudbreak and create a Cloudbreak credential. Now it's time to \ncreate a cluster\n. \n\n\n\n\n\n\nCreate role-based credential\n\n\nTo perform these steps, you must know the \nIAM Role ARN\n corresponding to the \"CredentialRole\" (configured as a \nprerequisite\n).  \n\n\nSteps\n\n\n\n\n\n\nIn the Cloudbreak web UI, select \nCredentials\n from the navigation pane. \n\n\n\n\n\n\nClick \nCreate Credential\n. \n\n\n\n\n\n\nUnder \nCloud provider\n, select \"Amazon Web Services\":\n\n\n  \n\n\n\n\n\n\nProvide the following information:\n\n\n\n\n\n\n\n\nParameter\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nSelect Credential Type\n\n\nSelect \nRole Based\n (default value).\n\n\n\n\n\n\nName\n\n\nEnter a name for your credential.\n\n\n\n\n\n\nDescription\n\n\n(Optional) Enter a description.\n\n\n\n\n\n\nIAM Role ARN\n\n\nPaste the IAM Role ARN corresponding to the \"CredentialRole\" that you created earlier. For example \narn:aws:iam::315627065446:role/CredentialRole\n is a valid IAM Role ARN.\n\n\n\n\n\n\n\n\n\n\n\n\nClick \nCreate\n.\n\n\n\n\n\n\nYour credential should now be displayed in the \nCredentials\n pane.\n\n\nCongratulations! You have successfully launched Cloudbreak and created a Cloudbreak credential. Now you can use Cloubdreak to \ncreate clusters\n. \n\n\n\n\n\n\n\n\nNext: Create a Cluster", 
            "title": "Launch on AWS"
        }, 
        {
            "location": "/aws-launch/index.html#launching-cloudbreak-on-aws", 
            "text": "Before launching Cloudbreak on AWS, review and meet the prerequisites. Next, launch a VM using a Cloudbreak Amazon Machine Image, access the VM, and then start Cloudbreak. Once Cloudbreak is running, log in to the Cloudbreak UI and create a Cloudbreak credential.", 
            "title": "Launching Cloudbreak on AWS"
        }, 
        {
            "location": "/aws-launch/index.html#meet-the-prerequisites", 
            "text": "Before launching Cloudbreak on AWS, you must meet the following prerequisites.", 
            "title": "Meet the prerequisites"
        }, 
        {
            "location": "/aws-launch/index.html#aws-account", 
            "text": "In order to launch Cloudbreak on AWS, you must log in to your AWS account. If you don't have an account, you can create one at  https://aws.amazon.com/ .", 
            "title": "AWS account"
        }, 
        {
            "location": "/aws-launch/index.html#aws-region", 
            "text": "Decide in which AWS region you would like to launch Cloudbreak. The following AWS regions are supported:      Region name  Region      EU (Ireland)  eu-west-1    EU (Frankfurt)  eu-central-1    US East (N. Virginia)  us-east-1    US West (N. California)  us-west-1    US West (Oregon)  us-west-2    South America (S\u00e3o Paulo)  sa-east-1    Asia Pacific (Tokyo)  ap-northeast-1    Asia Pacific (Singapore)  ap-southeast-1    Asia Pacific (Sydney)  ap-southeast-2     Clusters created via Cloudbreak can be in the same or different region as Cloudbreak; when you launch a cluster, you select the region in which to launch it.  Related links  AWS regions and endpoints  (External)", 
            "title": "AWS region"
        }, 
        {
            "location": "/aws-launch/index.html#ssh-key-pair", 
            "text": "Import an existing key pair or generate a new key pair in the AWS region which you are planning to use for launching Cloudbreak and clusters. You can do this using the following steps.  Steps     Navigate to the Amazon EC2 console at https://console.aws.amazon.com/ec2/.    Check the region listed in the top right corner to make sure that you are in the correct region.    In the left pane, find  NETWORK AND SECURITY  and click  Key Pairs .     Do one of the following:  Click  Create Key Pair  to create a new key pair. Your private key file will be automatically downloaded onto your computer. Make sure to save it in a secure location. You will need it to SSH to the cluster nodes. You may want to change access settings for the file using  chmod 400 my-key-pair.pem .    Click  Import Key Pair  to upload an existing public key and then select it and click  Import . Make sure that you have access to its corresponding private key.         You need this SSH key pair to SSH to the Cloudbreak instance and start Cloudbreak.   Related links  Creating a key pair using Amazon EC2  (External)", 
            "title": "SSH key pair"
        }, 
        {
            "location": "/aws-launch/index.html#authentication", 
            "text": "Before you can start using Cloudbreak for provisioning clusters, you must select a way for Cloudbreak to authenticate with your AWS account and create resources on your behalf. There are two ways to do this:     Key-based : This is a simpler option which does not require additional configuration at this point. It requires that you provide your AWS access key and secret key pair in the Cloudbreak web UI later. All you need to do now is check your AWS account and ensure that you can access this key pair.    Role-based : This requires that you or your AWS admin create an IAM role to allow Cloudbreak to assume AWS roles (the \"AssumeRole\" policy).", 
            "title": "Authentication"
        }, 
        {
            "location": "/aws-launch/index.html#option-1-configure-key-based-authentication", 
            "text": "If you are using key-based authentication for Cloudbreak on AWS, you must be able to provide your AWS access key and secret key pair. Cloudbreak will use these keys to launch the resources. You must provide the access and secret keys later in the Cloudbreak web UI later when creating a credential.   If you choose this option, all you need to do at this point is check your AWS account and make sure that you can access this key pair. You can generate new access and secret keys from the  IAM Console     Users . Next, select a user and click on the  Security credentials  tab:     If you choose this option, you can proceed to  Launch Cloudbreak deployer from an image .", 
            "title": "(Option 1) Configure key-based authentication"
        }, 
        {
            "location": "/aws-launch/index.html#option-2-configure-role-based-authentication", 
            "text": "If you are using role-based authentication for Cloudbreak on AWS, you must create two IAM roles: one to grant Cloudbreak access to allow Cloudbreak to assume AWS roles (using the \"AssumeRole\" policy) and the second one to provide Cloudbreak with the capabilities required for cluster creation (using the \"cb-policy\" policy).  The following table provides contextual information about the two roles required:      Role  Purpose  Overview of steps  Configuration      CloudbreakRole  Allows Cloudbreak to assume other IAM roles - specifically the CredentialRole.  Create a role called \"CloudbreakRole\" and attach the \"AssumeRole\" policy. The \"AssumeRole\" policy definition and steps for creating the CloudbreakRole are provided below.  When launching Cloudbreak, you will attach the \"CloudbreakRole\" IAM role to the VM. If you are using hosted Cloudbreak, you do not need to perform this step.    CredentialRole  Allows Cloudbreak to create AWS resources required for clusters.  Create a new IAM role called \"CredentialRole\" and attach the \"cb-policy\" policy to it. The \"cb-policy\" policy definition and steps for creating the CredentialRole are provided below.  When creating this role using the AWS Console, make sure that that it is a role for cross-account access and that the trust-relation is set up as follows: 'Account ID' is your own 12-digit AWS account ID and 'External ID' is \u201cprovision-ambari\u201d. See steps below.  Once you log in to the Cloudbreak UI and are ready to create clusters, you will use this role to create the Cloudbreak credential.      These role and policy names are just examples. You may use different names when creating your resources.    CloudbreakRole: Alternatively, instead of attaching the \"CloudbreakRole\" role during the VM launch, you can assign the \"CloudbreakRole\" to an IAM user and then add the access and secret key of that user to your 'Profile'.  CredentialRole: Alternatively you can generate the \"CredentialRole\" role later once your Cloudbreak VM is running by SSHing to the Cloudbreak VM and running the  cbd aws generate-role  command. This command creates a role with the name \"cbreak-deployer\" (equivalent to the \"CredentialRole\"). To customize the name of the role, add  export AWS_ROLE_NAME=my-cloudbreak-role-name  (where \"my-cloudbreak-role-name\" is your custom role name) as a new line to your Profile. If you choose this option, you must make sure that the \"CloudbreakRole\" or the IAM user have a permission not only to assume a role but also to create a role.     You can create these roles in the  IAM console , on the  Roles  page via the  Create Role  option. Detailed steps are provided below.", 
            "title": "(Option 2) Configure role-based authentication"
        }, 
        {
            "location": "/aws-launch/index.html#create-cloudbreakrole", 
            "text": "Use these steps to create CloudbreakRole.   Use the following \"AssumeRole\" policy definition:   {\n  \"Version\": \"2012-10-17\",\n  \"Statement\": {\n    \"Sid\": \"Stmt1400068149000\",\n    \"Effect\": \"Allow\",\n    \"Action\": [\"sts:AssumeRole\"],\n    \"Resource\": \"*\"\n  }\n}  Steps    Navigate to the  IAM console     Roles  and click  Create Role .       In the \"Create Role\" wizard, select  AWS service  role type and then select any service.        When done, click  Next: Permissions  to navigate to the next page in the wizard.    Click  Create policy .     Click  Select  next to \"Create Your Own Policy\".        In the  Policy Name  field, enter \"AssumeRole\" and in the  Policy Document  paste the policy definition. You can either copy it from the section preceding these steps or download and copy it from  here .        When done, click  Create Policy .    Click  Refresh . Next, find the \"AssumeRole\" policy that you just created and select it by checking the box.       When done, click  Next: Review .    In the  Roles name  field, enter role name, for example \"CloudbreakRole\".        When done, click  Create role  to finish the role creation process.", 
            "title": "Create CloudbreakRole"
        }, 
        {
            "location": "/aws-launch/index.html#create-credentialrole", 
            "text": "Use these steps to create CredentialRole.  Use the following \"cb-policy\" policy definition:   {\n  \"Version\": \"2012-10-17\",\n  \"Statement\": [\n    {\n      \"Effect\": \"Allow\",\n      \"Action\": [\n        \"cloudformation:CreateStack\",\n        \"cloudformation:DeleteStack\",\n        \"cloudformation:DescribeStackEvents\",\n        \"cloudformation:DescribeStackResource\",\n        \"cloudformation:DescribeStacks\"\n      ],\n      \"Resource\": [\n        \"*\"\n      ]\n    },\n    {\n      \"Effect\": \"Allow\",\n      \"Action\": [\n        \"ec2:AllocateAddress\",\n        \"ec2:AssociateAddress\",\n        \"ec2:AssociateRouteTable\",\n        \"ec2:AuthorizeSecurityGroupIngress\",\n        \"ec2:DescribeRegions\",\n        \"ec2:DescribeAvailabilityZones\",\n        \"ec2:CreateRoute\",\n        \"ec2:CreateRouteTable\",\n        \"ec2:CreateSecurityGroup\",\n        \"ec2:CreateSubnet\",\n        \"ec2:CreateTags\",\n        \"ec2:CreateVpc\",\n        \"ec2:ModifyVpcAttribute\",\n        \"ec2:DeleteSubnet\",\n        \"ec2:CreateInternetGateway\",\n        \"ec2:CreateKeyPair\",\n        \"ec2:DeleteKeyPair\", \n        \"ec2:DisassociateAddress\",\n        \"ec2:DisassociateRouteTable\",\n        \"ec2:ModifySubnetAttribute\",\n        \"ec2:ReleaseAddress\",\n        \"ec2:DescribeAddresses\",\n        \"ec2:DescribeImages\",\n        \"ec2:DescribeInstanceStatus\",\n        \"ec2:DescribeInstances\",\n        \"ec2:DescribeInternetGateways\",\n        \"ec2:DescribeKeyPairs\",\n        \"ec2:DescribeRouteTables\",\n        \"ec2:DescribeSecurityGroups\",\n        \"ec2:DescribeSubnets\",\n        \"ec2:DescribeVpcs\",\n        \"ec2:DescribeSpotInstanceRequests\",\n        \"ec2:DescribeVpcAttribute\",\n        \"ec2:ImportKeyPair\",\n        \"ec2:AttachInternetGateway\",\n        \"ec2:DeleteVpc\",\n        \"ec2:DeleteSecurityGroup\",\n        \"ec2:DeleteRouteTable\",\n        \"ec2:DeleteInternetGateway\",\n        \"ec2:DeleteRouteTable\",\n        \"ec2:DeleteRoute\",\n        \"ec2:DetachInternetGateway\",\n        \"ec2:RunInstances\",\n        \"ec2:StartInstances\",\n        \"ec2:StopInstances\",\n        \"ec2:TerminateInstances\"\n      ],\n      \"Resource\": [\n        \"*\"\n      ]\n    },\n    {\n      \"Effect\": \"Allow\",\n      \"Action\": [\n        \"iam:ListRolePolicies\",\n        \"iam:GetRolePolicy\",\n        \"iam:ListAttachedRolePolicies\",\n        \"iam:ListInstanceProfiles\",\n        \"iam:PutRolePolicy\",\n        \"iam:PassRole\",\n        \"iam:GetRole\"\n      ],\n      \"Resource\": [\n        \"*\"\n      ]\n    },\n    {\n      \"Effect\": \"Allow\",\n      \"Action\": [\n        \"autoscaling:CreateAutoScalingGroup\",\n        \"autoscaling:CreateLaunchConfiguration\",\n        \"autoscaling:DeleteAutoScalingGroup\",\n        \"autoscaling:DeleteLaunchConfiguration\",\n        \"autoscaling:DescribeAutoScalingGroups\",\n        \"autoscaling:DescribeLaunchConfigurations\",\n        \"autoscaling:DescribeScalingActivities\",\n        \"autoscaling:DetachInstances\",\n        \"autoscaling:ResumeProcesses\",\n        \"autoscaling:SuspendProcesses\",\n        \"autoscaling:UpdateAutoScalingGroup\"\n      ],\n      \"Resource\": [\n        \"*\"\n      ]\n    }\n  ]\n}  Steps    Navigate to the  IAM console     Roles  and click  Create Role .       In the \"Create Role\" wizard, select  Another AWS account  role type. Next, provide the following:   In the  Account ID  field, enter your AWS account ID.  Under  Options , check  Require external ID .  In the  External ID , enter \"provision-ambari\".        When done, click  Next: Permissions  to navigate to the next page in the wizard.    Click  Create policy .     Click  Select  next to \"Create Your Own Policy\".       In the  Policy Name  field, enter \"cb-policy\" and in the  Policy Document  paste the policy definition.  You can either copy it from the section preceding these steps or download and copy it from  here .        When done, click  Create Policy .    Click  Refresh . Next, find the \"cb-policy\" that you just created and select it by checking the box.       When done, click  Next: Review .    In the  Roles name  field, enter role name, for example \"CredentialRole\".        When done, click  Create role  to finish the role creation process.     Once you are done, you can proceed to launch Cloudbreak.    Related links  Using instance profiles  (External)  Using an IAM role to grant permissions to applications  (External)", 
            "title": "Create CredentialRole"
        }, 
        {
            "location": "/aws-launch/index.html#launch-cloudbreak-from-a-template", 
            "text": "Follow these steps to launch Cloudbreak by using an Amazon CloudFormation template:      Steps      Click on the link to launch the CloudFormation template that will create the AWS resources, including an EC2 Instance running Cloudbreak:  \n  \n    Region \n    Link \n  \n  \n    us-east-1 (N. Virginia) \n    \n    \n     Launch the CloudFormation Template in US East \n  \n   \n    us-west-1 (N. California) \n    \n    \n     Launch the CloudFormation Template in US West (N. California) \n  \n  \n    us-west-2 (Oregon) \n    \n    \n     Launch the CloudFormation Template in US West (Oregon) \n  \n  \n    eu-central-1 (Frankfurt) \n    \n    \n     Launch the CloudFormation Template in EU Central \n  \n  \n    eu-west-1 (Dublin) \n    \n    \n     Launch the CloudFormation Template in EU West  \n  \n  \n    sa-east-1 (S\u00e3o Paulo) \n    \n    \n     Launch the CloudFormation Template in South America \n  \n  \n    ap-northeast-1 Asia Pacific (Tokyo) \n    \n    \n     Launch the CloudFormation Template in Asia Pacific (Tokyo) \n  \n  \n    ap-southeast-1 (Singapore) \n    \n    \n     Launch the CloudFormation Template in Asia Pacific (Singapore) \n  \n  \n    ap-southeast-2 (Sydney) \n    \n    \n     Launch the CloudFormation Template in Asia Pacific (Sydney) \n         The  Create stack  wizard is launched in the Amazon CloudFormation management console:      In the top right corner, confirm the region in which you want to launch Cloudbreak:         You may change the region if needed by using the dropdown in the top right corner.     You do not need to change any template parameters on the  Select Template  page. Click  Next  to display the  Specify Details  page:       On the the  Specify Details  page, enter the  Stack name :     Parameter  Description      Stack name  Enter name for your stack. It must be unique in your AWS environment.       On the same page, enter the following in the  Parameters  section:   All parameters are required.   General Configuration       Parameter  Description      Controller Instance Type  EC2 instance type to use for the cloud controller.    Email Address  Username for the Admin login. Must be a valid email address.    Admin Password  Password for Admin login. Must be at least 8 characters containing letters, numbers, and symbols.     Security Configuration       Parameter  Description      SSH Key Name  Name of an existing EC2 key pair to enable SSH to access the instances. Key pairs are region-specific, so only the key pairs that you created for a selected region will appear in the dropdown. See  Prerequisites  for more information.    Remote Access  Allow connections to the cloud controller ports from this address range. Must be a valid  CIDR IP . For example:  192.168.27.0/24 will allow access from 192.168.27.0 through 192.168.27.255. 192.168.27.10/32 will allow access from 192.168.27.10. 0.0.0.0/0 will allow access from all.  Refer to  Network security  for more information on the inbound ports that are used with Cloudbreak.       Click  Next   to display the  Options  page.        On he  Options  page, if you expand the  Advanced  section, there is an option to  Rollback on failure .    By\ndefault, this option is set to  Yes , which means that if there are any event failures when creating\nthe stack, all the AWS resources created so far are deleted (i.e rolled back) to avoid unnecessary charges.   If you set this option to  No , if there are any event failures when creating\nthe stack, the\nresources are left intact (i.e. not rolled back). Select the  No  option to aid in\ntroubleshooting. Note that in this case you are responsible for deleting the stack later.     Click  Next  to display the  Review  page.    On the  Review  page, click the  I acknowledge...  checkbox.      Click  Create .   The  Stack Name  is shown in the table with a  CREATE_IN_PROGRESS  status. You can click on the  Stack Name  and see the specific events that are in progress. The create process takes about 10 minutes and once ready, you will see  CREATE_COMPLETE .", 
            "title": "Launch Cloudbreak from a template"
        }, 
        {
            "location": "/aws-launch/index.html#access-cloudbreak-web-ui", 
            "text": "Once the stack creation is complete, the cloud controller is ready to use. You can obtain the URL to the cloud controller\nand the SSH access information from the  Outputs  tab:    If the Outputs tab is blank, refresh the page.     Once the stack creation is complete, browse instance created at the  CloudURL  provided in the  Outputs  tab and log in.    Log in to the Cloudbreak web UI using the credential that you configured.    Upon a successful login, you are redirected to the dashboard:", 
            "title": "Access Cloudbreak web UI"
        }, 
        {
            "location": "/aws-launch/index.html#create-cloudbreak-credential", 
            "text": "Before you can start creating clusters, you must first create a  Cloudbreak credential . Without this credential, you will not be able to create clusters via Cloudbreak.   As part of the  prerequisites , you had two options to allow Cloudbreak to authenticate with AWS and create resources on your behalf: key-based or role-based authentication. Depending on your choice, you must configure a key-based or role-based credential:    Create key-based credential     Create role-based credential", 
            "title": "Create Cloudbreak credential"
        }, 
        {
            "location": "/aws-launch/index.html#create-key-based-credential", 
            "text": "To perform these steps, you must know your access and secret key. If needed, you or your AWS administrator can generate new access and secret keys from the  IAM Console     Users    select a user    Security credentials .   Steps    In the Cloudbreak web UI, select  Credentials  from the navigation pane.     Click  Create Credential .     Under  Cloud provider , select \"Amazon Web Services\":        Provide the following information:     Parameter  Description      Select Credential Type  Select  Key Based .    Name  Enter a name for your credential.    Description  (Optional) Enter a description.    Access Key  Paste your access key.    Secret Access Key  Paste your secret key.       Click  Create .    Your credential should now be displayed in the  Credentials  pane.  Congratulations! You've successfully launched Cloudbreak and create a Cloudbreak credential. Now it's time to  create a cluster .", 
            "title": "Create key-based credential"
        }, 
        {
            "location": "/aws-launch/index.html#create-role-based-credential", 
            "text": "To perform these steps, you must know the  IAM Role ARN  corresponding to the \"CredentialRole\" (configured as a  prerequisite ).    Steps    In the Cloudbreak web UI, select  Credentials  from the navigation pane.     Click  Create Credential .     Under  Cloud provider , select \"Amazon Web Services\":        Provide the following information:     Parameter  Description      Select Credential Type  Select  Role Based  (default value).    Name  Enter a name for your credential.    Description  (Optional) Enter a description.    IAM Role ARN  Paste the IAM Role ARN corresponding to the \"CredentialRole\" that you created earlier. For example  arn:aws:iam::315627065446:role/CredentialRole  is a valid IAM Role ARN.       Click  Create .    Your credential should now be displayed in the  Credentials  pane.  Congratulations! You have successfully launched Cloudbreak and created a Cloudbreak credential. Now you can use Cloubdreak to  create clusters .      Next: Create a Cluster", 
            "title": "Create role-based credential"
        }, 
        {
            "location": "/aws-create/index.html", 
            "text": "Creating a cluster on AWS\n\n\nUse these steps to create a cluster.\n\n\n\n\nTroubleshooting cluster creation\n\n\nIf you experience problems during cluster creation, refer to \nTroubleshooting cluster creation\n.\n\n\n\nSteps\n\n\n\n\n\n\nLog in to the Cloudbreak UI.\n\n\n\n\n\n\nClick \nCreate Cluster\n and the \nCreate Cluster\n wizard is displayed.\n\n    By default, \nBasic\n view is displayed. To view advanced options, click \nAdvanced\n. To learn about advanced options, refer to \nAdvanced cluster options\n.\n\n\n \n\n\n\n\n\n\nOn the \nGeneral Configuration\n page, specify the following general parameters for your cluster:\n\n\n\n\n\n\n\n\nParameter\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nSelect Credential\n\n\nChoose a previously created credential.\n\n\n\n\n\n\nCluster Name\n\n\nEnter a name for your cluster. The name must be between 5 and 40 characters, must start with a letter, and must only include lowercase letters, numbers, and hyphens.\n\n\n\n\n\n\nRegion\n\n\nSelect the AWS region in which you would like to launch your cluster. For information on available AWS regions, refer to \nAWS documentation\n.\n\n\n\n\n\n\nPlatform Version\n\n\nChoose the HDP or HDF version to use for this cluster. Blueprints available for this platform version will be populated under \"Cluster Type\" below.\n\n\n\n\n\n\nCluster Type\n\n\nChoose one of default cluster configurations, or, if you have defined your own cluster configuration via Ambari blueprint, you can choose it here. For more information on default and custom blueprints, refer to \nUsing custom blueprints\n.\n\n\n\n\n\n\nFlex Subscription\n\n\nThis option will appear if you have configured your deployment for a \nflex support subscription\n.\n\n\n\n\n\n\n\n\n\n\n\n\nOn the \nHardware and Storage\n page, for each host group provide the following information to define your cluster nodes and attached storage:\n\n\n\n\n\n\n\n\nParameter\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nInstance Type\n\n\nSelect an instance type. For information about instance types on AWS refer to \nAWS documentation\n.\n\n\n\n\n\n\nInstance Count\n\n\nEnter the number of instances of a given type. Default is 1.\n\n\n\n\n\n\nAmbari Server\n\n\nYou must select one node for Ambari Server. The \"Group Size\" for that host group must be set to \"1\".\n\n\n\n\n\n\n\n\n\n\n\n\nOn the \nNetwork\n page, provide the following to specify the networking resources that will be used for your cluster:\n\n\n\n\n\n\n\n\nParameter\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nSelect Network\n\n\nSelect the virtual network in which you would like your cluster to be provisioned. You can select an existing network or create a new network.\n\n\n\n\n\n\nSelect Subnet\n\n\nSelect the subnet in which you would like your cluster to be provisioned. If you are using a new network, create a new subnet. If you are using an existing network, select an existing subnet.\n\n\n\n\n\n\nSubnet (CIDR)\n\n\nIf you selected to create a new subnet, you must define a valid \nCIDR\n for the subnet. Default is 10.0.0.0/16.\n\n\n\n\n\n\n\n\n\n\nCloudbreak uses public IP addresses when communicating with cluster nodes.\n\nOn AWS, you can configure it to use private IPs instead. For instructions, refer to \nConfigure communication via private IPs on AWS\n. \n\n\n\n\n\n\n\n\nDefine security groups for each host group. You can either create new security groups and define their rules or reuse existing security groups:\n\n\n\n\nExisting security groups are only available for an existing VPC. \n\n\n\n\n\n\n\n\n\n\nOption\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nNew Security Group\n\n\n(Default) Creates a new security group with the rules that you defined:\nA set of \ndefault rules\n is provided. You should review and adjust these default rules. If you do not make any modifications, default rules will be applied. \nYou may open ports by defining the CIDR, entering port range, selecting protocol and clicking \n+\n.\nYou may delete default or previously added rules using the delete icon.\nIf you don't want to use security group, remove the default rules.\n\n\n\n\n\n\nExisting Security Groups\n\n\nAllows you to select an existing security group that is already available in the selected provider region. This selection is disabled if no existing security groups are available in your chosen region.\n\n\n\n\n\n\n\n\n\n\nImportant\n\n\n\nBy default, ports 22, 443, and 9443 are set to 0.0.0.0/0 CIDR for inbound access on the Ambari node security group. We strongly recommend that you limit this CIDR, considering the following restrictions:\n\nPorts 22 and 9443 must be open to Cloudbreak's CIDR. You can set CB_DEFAULT_GATEWAY_CIDR in your Cloudbreak's Profile file in order to automatically open ports 22 and 9443 to your Cloudbreak IP. Refer to \nRestricting inbound access from Cloudbreak to cluster\n.\n\n\nPort 22 must be open to your CIDR if you would like to access the master node via SSH.\n\n\nPort 443 must be open to your CIDR if you would like to access Ambari web UI in a browser.\n\n\n\n\n\n\n\n\n\n\nImportant\n\n\n\nBy default, port 22 is set to 0.0.0.0/0 CIDR for inbound access on non-Ambari node security groups. We strongly recommend that you remove it.\n\n\n\n\n\n\nImportant\n\n\n\nDepending on what services you are including, you need to open additional ports as required by these services. For example, when using the Flow Management blueprint, you must open port 9091 for NiFi (on NiFI host group) and port 61443 for NiFI Registry (on the Services host group).\n\n\n\n\n\n\n\n\n\n\nOn the \nSecurity\n page, provide the following parameters:\n\n\n\n\n\n\n\n\nParameter\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nCluster User\n\n\nYou can log in to the Ambari UI using this username. By default, this is set to \nadmin\n.\n\n\n\n\n\n\nPassword\n\n\nYou can log in to the Ambari UI using this password.\n\n\n\n\n\n\nConfirm Password\n\n\nConfirm the password.\n\n\n\n\n\n\nNew SSH public key\n\n\nCheck this option to specify a new public key and then enter the public key. You will use the matching private key to access your cluster nodes via SSH.\n\n\n\n\n\n\nExisting SSH public key\n\n\nSelect an existing public key. You will use the matching private key to access your cluster nodes via SSH. This is a default option as long as an existing SSH public key is available.\n\n\n\n\n\n\n\n\n\n\n\n\nClick on \nCreate Cluster\n to create a cluster.\n\n\n\n\n\n\nYou will be redirected to the Cloudbreak dashboard, and a new tile representing your cluster will appear at the top of the page.\n\n\n\n\n\n\nRelated links\n \n\n\nFlex support subscription\n\n\nUsing custom blueprints\n \n\n\nDefault cluster security groups\n\n\nTroubleshooting cluster creation\n  \n\n\nAmazon EC2 instance types\n (External) \n\n\nAWS regions and endpoints\n (External)   \n\n\nCIDR\n (External)   \n\n\nAdvanced cluster options\n\n\nClick on \nAdvanced\n to view and enter additional configuration options\n\n\nAvailability zone\n\n\nChoose one of the availability zones within the selected region. \n\n\nChoose image catalog\n\n\nBy default, \nChoose Image Catalog\n is set to the default image catalog that is provided with Cloudbreak. If you would like to use a different image catalog, you must first create and register it. For complete instructions, refer to \nUsing custom images\n.\n\n\nRelated links\n   \n\n\nUsing custom images\n  \n\n\nPrewarmed and base images\n\n\nCloudbreak supports the following types of images for launching clusters:\n\n\n\n\n\n\n\n\nImage type\n\n\nDescription\n\n\nDefault images provided\n\n\nSupport for custom images\n\n\n\n\n\n\n\n\n\n\nBase Images\n\n\nBase images include default configuration and default tooling. These images include the operating system but do not include Ambari or HDP/HDF software.\n\n\nYes\n\n\nYes\n\n\n\n\n\n\nPrewarmed Images\n\n\nBy default, Cloudbreak launches clusters from prewarmed images. Prewarmed images include the operating system as well as Ambari and HDP. The Ambari and HDP version used by prewarmed images cannot be customized. No prewarmed HDF images are currently provided.\n\n\nYes (HDP only)\n\n\nNo\n\n\n\n\n\n\n\n\nBy default, Cloudbreak uses the included default \nprewarmed images\n, which include the operating system, as well as\nAmbari and HDP/HDF packages installed. You can optionally select the \nbase image\n option if you would like to:\n\n\n\n\nUse an Ambari and HDP/HDF versions different than what the prewarmed image includes and/or  \n\n\nChoose a previously created custom base image\n\n\n\n\nChoose image\n  \n\n\nIf under \nChoose image catalog\n, you selected a custom image catalog, under \nChoose Image\n you can select an image from that catalog. For complete instructions, refer to \nUsing custom images\n. \n\n\nIf you are trying to customize Ambari and HDP/HDF versions, you can ignore the \nChoose Image\n option; in this case default base image is used.\n\n\nAmbari repository specification\n\n\nIf you would like to use a custom Ambari version, provide the following information: \n\n\n\n\n Ambari 2.6.1\n\n\nIf you would like to use Ambari \n2.6.1\n, use the version provided by default in the Cloudbreak web UI, or newer.\n\n\n\n\n\n\n\n\n\nParameter\n\n\nDescription\n\n\nExample\n\n\n\n\n\n\n\n\n\n\nVersion\n\n\nEnter Ambari version.\n\n\n2.6.1.3\n\n\n\n\n\n\nRepo Url\n\n\nProvide a URL to the Ambari version repo that you would like to use.\n\n\nhttp://public-repo-1.hortonworks.com/ambari/centos6/2.x/updates/2.6.1.3\n\n\n\n\n\n\nRepo Gpg Key Url\n\n\nProvide a URL to the repo GPG key. Each stable RPM package that is published by CentOS Project is signed with a GPG signature. By default, yum and the graphical update tools will verify these signatures and refuse to install any packages that are not signed, or have an incorrect signature.\n\n\nhttp://public-repo-1.hortonworks.com/ambari/centos6/RPM-GPG-KEY/RPM-GPG-KEY-Jenkins\n\n\n\n\n\n\n\n\nHDP or HDF repository specification\n\n\nIf you would like to use a custom HDP or HDF version, provide the following information: \n\n\n\n\n\n\n\n\nParameter\n\n\nDescription\n\n\nExample\n\n\n\n\n\n\n\n\n\n\nStack\n\n\nThis is populated by default based on the \"Platform Version\" parameter.\n\n\nHDP\n\n\n\n\n\n\nVersion\n\n\nThis is populated by default based on the \"Platform Version\" parameter.\n\n\n2.6\n\n\n\n\n\n\nOS\n\n\nOperating system.\n\n\ncentos7 (Azure, GCP, OpenStack) or amazonlinux (AWS)\n\n\n\n\n\n\nRepository Version\n\n\nEnter repository version.\n\n\n2.6.4.0-91\n\n\n\n\n\n\nVersion Definition File\n\n\nEnter the URL of the VDF file.\n\n\nhttp://public-repo-1.hortonworks.com/HDP/centos6/2.x/updates/2.6.4.0/HDP-2.6.4.0-91.xml\n\n\n\n\n\n\n(HDF only) MPack Url\n\n\n(HDF only) Provide MPack URL.\n\n\nhttp://public-repo-1.hortonworks.com/HDF/centos7/3.x/updates/3.1.1.0/tars/hdf_ambari_mp/hdf-ambari-mpack-3.1.1.0-35.tar.gz\n\n\n\n\n\n\nEnable Ambari Server to download and install GPL Licensed LZO packages?\n\n\n(Optional, only available if using Ambari 2.6.1.0 or newer) Use this option to enable LZO compression in your HDP/HDF cluster. LZO is a lossless data compression library that favors speed over compression ratio. Ambari does not install nor enable LZO compression libraries by default, and must be explicitly configured to do so. For more information, refer to \nEnabling LZO\n.\n\n\n\n\n\n\n\n\n\n\n\n\nIf you choose to use a base image with custom Ambari and/or HDP/HDF version, Cloudbreak validates the information entered. When Cloudbreak detects that the information entered is incorrect, it displays a warning marked with the \n sign. You should review all the warnings before proceeding and make sure that the information that you entered is correct. If you choose to proceed in spite of the warnings, check \"Ignore repository warnings\".  \n\n\n\n\nRelated links\n    \n\n\nUsing custom images\n      \n\n\nEnable lifetime management\n\n\nCheck this option if you would like your cluster to be automatically terminated after a specific amount of time (defined as \"Time to Live\" in minutes). \n\n\nTags\n\n\nYou can optionally add tags, which will help you find your cluster-related resources, such as VMs, in your cloud provider account. \n\n\nBy default, the following tags are created:\n\n\n\n\n\n\n\n\nTag\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\ncb-version\n\n\nCloudbreak version\n\n\n\n\n\n\nOwner\n\n\nYour Cloudbreak admin email.\n\n\n\n\n\n\ncb-account-name\n\n\nYour automatically generated Cloudbreak account name stored in the identity server.\n\n\n\n\n\n\ncb-user-name\n\n\nYour Cloudbreak admin email.\n\n\n\n\n\n\n\n\nFor more information, refer to \nTagging resources\n.\n\n\nRelated links\n    \n\n\nTagging resources\n \n\n\nStorage\n\n\nYou can optionally specify the following storage options for your cluster:\n\n\n\n\n\n\n\n\nParameter\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nStorage Type\n\n\nSelect the volume type. The options are:\nMagnetic (default)\nGeneral Purpose (SSD)\nThroughput Optimized HDD\nFor more information about these options refer to \nAWS documentation\n.\n\n\n\n\n\n\nAttached Volumes Per Instance\n\n\nEnter the number of volumes attached per instance. Default is 1.\n\n\n\n\n\n\nVolume Size (GB)\n\n\nEnter the size in GBs for each volume. Default is 100.\n\n\n\n\n\n\n\n\nUse spot instances\n\n\nCheck this option to use EC2 spot instances as your cluster nodes. Next, enter your bid price. The price that is pre-loaded in the form is the current on-demand price for your chosen EC2 instance type.   \n\n\nNote that: \n\n\n\n\nWe recommend not using spot instances for any host group that includes Ambari server components.  \n\n\nIf you choose to use spot instances for a given host group when creating your cluster, any nodes that you add to that host group (during cluster creation or later) will be using spot instances. Any additional nodes will be requested at the same bid price that you entered when creating a cluster.  \n\n\nIf you decide not to use spot instances when creating your cluster, any nodes that you add to your host group (during cluster creation or later) will be using standard on-demand instances.     \n\n\nOnce someone outbids you, the spot instances are taken away, removing the nodes from the cluster. \n\n\nIf spot instances are not available right away, creating a cluster will take longer than usual. \n\n\n\n\nAfter creating a cluster, you can view your spot instance requests, including bid price, on the EC2 dashboard under \nINSTANCES\n \n \nSpot Requests\n. For more information about spot instances, refer to \nAWS documentation\n.  \n\n\nFile system\n\n\nIf you would like to access S3 from your cluster, you must configure access to S3 trough an instance profile. For instructions, refer to \nConfigure access to S3\n. \n\n\nRelated links\n\n\nConfigure access to S3\n  \n\n\nRecipes\n\n\nThis option allows you to select previously uploaded recipes (scripts that can be run pre or post cluster deployment) for each host group. For more information on recipes, refer to \nUsing custom scripts (recipes)\n. \n\n\nRelated links\n    \n\n\nUsing custom scripts (recipes)\n \n\n\nManagement Packs\n\n\nThis option allows you to select previously uploaded management packs. For more information on management packs, refer to \nUsing management packs\n. \n\n\nRelated links\n    \n\n\nUsing management packs\n  \n\n\nExternal sources\n\n\nYou can register external sources with Cloudbreak, and then select and attach them during cluster create. To register external sources with Cloudbreak, refer to:\n\n\n\n\nUsing an external authentication source\n    \n\n\nUsing an external database\n  \n\n\nRegister a proxy\n  \n\n\n\n\nAmbari server master key\n\n\nThe Ambari server master key is used to configure Ambari to encrypt database and Kerberos credentials that are retained by Ambari as part of the Ambari setup.  \n\n\nEnable Kerberos security\n\n\nSelect this option to enable Kerberos for your cluster. For information about available Kerberos options, refer to \nEnabling Kerberos security\n. \n\n\nRelated links\n    \n\n\nEnabling Kerberos security\n \n\n\nAmazon EC2 instance store\n (External)\n\n\nAmazon EC2 spot instances\n (External)   \n\n\n\n\nNext: Access Cluster", 
            "title": "Create a cluster"
        }, 
        {
            "location": "/aws-create/index.html#creating-a-cluster-on-aws", 
            "text": "Use these steps to create a cluster.   Troubleshooting cluster creation  If you experience problems during cluster creation, refer to  Troubleshooting cluster creation .  Steps    Log in to the Cloudbreak UI.    Click  Create Cluster  and the  Create Cluster  wizard is displayed. \n    By default,  Basic  view is displayed. To view advanced options, click  Advanced . To learn about advanced options, refer to  Advanced cluster options .       On the  General Configuration  page, specify the following general parameters for your cluster:     Parameter  Description      Select Credential  Choose a previously created credential.    Cluster Name  Enter a name for your cluster. The name must be between 5 and 40 characters, must start with a letter, and must only include lowercase letters, numbers, and hyphens.    Region  Select the AWS region in which you would like to launch your cluster. For information on available AWS regions, refer to  AWS documentation .    Platform Version  Choose the HDP or HDF version to use for this cluster. Blueprints available for this platform version will be populated under \"Cluster Type\" below.    Cluster Type  Choose one of default cluster configurations, or, if you have defined your own cluster configuration via Ambari blueprint, you can choose it here. For more information on default and custom blueprints, refer to  Using custom blueprints .    Flex Subscription  This option will appear if you have configured your deployment for a  flex support subscription .       On the  Hardware and Storage  page, for each host group provide the following information to define your cluster nodes and attached storage:     Parameter  Description      Instance Type  Select an instance type. For information about instance types on AWS refer to  AWS documentation .    Instance Count  Enter the number of instances of a given type. Default is 1.    Ambari Server  You must select one node for Ambari Server. The \"Group Size\" for that host group must be set to \"1\".       On the  Network  page, provide the following to specify the networking resources that will be used for your cluster:     Parameter  Description      Select Network  Select the virtual network in which you would like your cluster to be provisioned. You can select an existing network or create a new network.    Select Subnet  Select the subnet in which you would like your cluster to be provisioned. If you are using a new network, create a new subnet. If you are using an existing network, select an existing subnet.    Subnet (CIDR)  If you selected to create a new subnet, you must define a valid  CIDR  for the subnet. Default is 10.0.0.0/16.      Cloudbreak uses public IP addresses when communicating with cluster nodes. \nOn AWS, you can configure it to use private IPs instead. For instructions, refer to  Configure communication via private IPs on AWS .      Define security groups for each host group. You can either create new security groups and define their rules or reuse existing security groups:   Existing security groups are only available for an existing VPC.       Option  Description      New Security Group  (Default) Creates a new security group with the rules that you defined: A set of  default rules  is provided. You should review and adjust these default rules. If you do not make any modifications, default rules will be applied.  You may open ports by defining the CIDR, entering port range, selecting protocol and clicking  + . You may delete default or previously added rules using the delete icon. If you don't want to use security group, remove the default rules.    Existing Security Groups  Allows you to select an existing security group that is already available in the selected provider region. This selection is disabled if no existing security groups are available in your chosen region.      Important  \nBy default, ports 22, 443, and 9443 are set to 0.0.0.0/0 CIDR for inbound access on the Ambari node security group. We strongly recommend that you limit this CIDR, considering the following restrictions: Ports 22 and 9443 must be open to Cloudbreak's CIDR. You can set CB_DEFAULT_GATEWAY_CIDR in your Cloudbreak's Profile file in order to automatically open ports 22 and 9443 to your Cloudbreak IP. Refer to  Restricting inbound access from Cloudbreak to cluster .  Port 22 must be open to your CIDR if you would like to access the master node via SSH.  Port 443 must be open to your CIDR if you would like to access Ambari web UI in a browser.      Important  \nBy default, port 22 is set to 0.0.0.0/0 CIDR for inbound access on non-Ambari node security groups. We strongly recommend that you remove it.    Important  \nDepending on what services you are including, you need to open additional ports as required by these services. For example, when using the Flow Management blueprint, you must open port 9091 for NiFi (on NiFI host group) and port 61443 for NiFI Registry (on the Services host group).      On the  Security  page, provide the following parameters:     Parameter  Description      Cluster User  You can log in to the Ambari UI using this username. By default, this is set to  admin .    Password  You can log in to the Ambari UI using this password.    Confirm Password  Confirm the password.    New SSH public key  Check this option to specify a new public key and then enter the public key. You will use the matching private key to access your cluster nodes via SSH.    Existing SSH public key  Select an existing public key. You will use the matching private key to access your cluster nodes via SSH. This is a default option as long as an existing SSH public key is available.       Click on  Create Cluster  to create a cluster.    You will be redirected to the Cloudbreak dashboard, and a new tile representing your cluster will appear at the top of the page.    Related links    Flex support subscription  Using custom blueprints    Default cluster security groups  Troubleshooting cluster creation     Amazon EC2 instance types  (External)   AWS regions and endpoints  (External)     CIDR  (External)", 
            "title": "Creating a cluster on AWS"
        }, 
        {
            "location": "/aws-create/index.html#advanced-cluster-options", 
            "text": "Click on  Advanced  to view and enter additional configuration options", 
            "title": "Advanced cluster options"
        }, 
        {
            "location": "/aws-create/index.html#availability-zone", 
            "text": "Choose one of the availability zones within the selected region.", 
            "title": "Availability zone"
        }, 
        {
            "location": "/aws-create/index.html#choose-image-catalog", 
            "text": "By default,  Choose Image Catalog  is set to the default image catalog that is provided with Cloudbreak. If you would like to use a different image catalog, you must first create and register it. For complete instructions, refer to  Using custom images .  Related links      Using custom images", 
            "title": "Choose image catalog"
        }, 
        {
            "location": "/aws-create/index.html#prewarmed-and-base-images", 
            "text": "Cloudbreak supports the following types of images for launching clusters:     Image type  Description  Default images provided  Support for custom images      Base Images  Base images include default configuration and default tooling. These images include the operating system but do not include Ambari or HDP/HDF software.  Yes  Yes    Prewarmed Images  By default, Cloudbreak launches clusters from prewarmed images. Prewarmed images include the operating system as well as Ambari and HDP. The Ambari and HDP version used by prewarmed images cannot be customized. No prewarmed HDF images are currently provided.  Yes (HDP only)  No     By default, Cloudbreak uses the included default  prewarmed images , which include the operating system, as well as\nAmbari and HDP/HDF packages installed. You can optionally select the  base image  option if you would like to:   Use an Ambari and HDP/HDF versions different than what the prewarmed image includes and/or    Choose a previously created custom base image   Choose image     If under  Choose image catalog , you selected a custom image catalog, under  Choose Image  you can select an image from that catalog. For complete instructions, refer to  Using custom images .   If you are trying to customize Ambari and HDP/HDF versions, you can ignore the  Choose Image  option; in this case default base image is used.  Ambari repository specification  If you would like to use a custom Ambari version, provide the following information:     Ambari 2.6.1  If you would like to use Ambari  2.6.1 , use the version provided by default in the Cloudbreak web UI, or newer.     Parameter  Description  Example      Version  Enter Ambari version.  2.6.1.3    Repo Url  Provide a URL to the Ambari version repo that you would like to use.  http://public-repo-1.hortonworks.com/ambari/centos6/2.x/updates/2.6.1.3    Repo Gpg Key Url  Provide a URL to the repo GPG key. Each stable RPM package that is published by CentOS Project is signed with a GPG signature. By default, yum and the graphical update tools will verify these signatures and refuse to install any packages that are not signed, or have an incorrect signature.  http://public-repo-1.hortonworks.com/ambari/centos6/RPM-GPG-KEY/RPM-GPG-KEY-Jenkins     HDP or HDF repository specification  If you would like to use a custom HDP or HDF version, provide the following information:      Parameter  Description  Example      Stack  This is populated by default based on the \"Platform Version\" parameter.  HDP    Version  This is populated by default based on the \"Platform Version\" parameter.  2.6    OS  Operating system.  centos7 (Azure, GCP, OpenStack) or amazonlinux (AWS)    Repository Version  Enter repository version.  2.6.4.0-91    Version Definition File  Enter the URL of the VDF file.  http://public-repo-1.hortonworks.com/HDP/centos6/2.x/updates/2.6.4.0/HDP-2.6.4.0-91.xml    (HDF only) MPack Url  (HDF only) Provide MPack URL.  http://public-repo-1.hortonworks.com/HDF/centos7/3.x/updates/3.1.1.0/tars/hdf_ambari_mp/hdf-ambari-mpack-3.1.1.0-35.tar.gz    Enable Ambari Server to download and install GPL Licensed LZO packages?  (Optional, only available if using Ambari 2.6.1.0 or newer) Use this option to enable LZO compression in your HDP/HDF cluster. LZO is a lossless data compression library that favors speed over compression ratio. Ambari does not install nor enable LZO compression libraries by default, and must be explicitly configured to do so. For more information, refer to  Enabling LZO .       If you choose to use a base image with custom Ambari and/or HDP/HDF version, Cloudbreak validates the information entered. When Cloudbreak detects that the information entered is incorrect, it displays a warning marked with the   sign. You should review all the warnings before proceeding and make sure that the information that you entered is correct. If you choose to proceed in spite of the warnings, check \"Ignore repository warnings\".     Related links       Using custom images", 
            "title": "Prewarmed and base images"
        }, 
        {
            "location": "/aws-create/index.html#enable-lifetime-management", 
            "text": "Check this option if you would like your cluster to be automatically terminated after a specific amount of time (defined as \"Time to Live\" in minutes).", 
            "title": "Enable lifetime management"
        }, 
        {
            "location": "/aws-create/index.html#tags", 
            "text": "You can optionally add tags, which will help you find your cluster-related resources, such as VMs, in your cloud provider account.   By default, the following tags are created:     Tag  Description      cb-version  Cloudbreak version    Owner  Your Cloudbreak admin email.    cb-account-name  Your automatically generated Cloudbreak account name stored in the identity server.    cb-user-name  Your Cloudbreak admin email.     For more information, refer to  Tagging resources .  Related links       Tagging resources", 
            "title": "Tags"
        }, 
        {
            "location": "/aws-create/index.html#storage", 
            "text": "You can optionally specify the following storage options for your cluster:     Parameter  Description      Storage Type  Select the volume type. The options are: Magnetic (default) General Purpose (SSD) Throughput Optimized HDD For more information about these options refer to  AWS documentation .    Attached Volumes Per Instance  Enter the number of volumes attached per instance. Default is 1.    Volume Size (GB)  Enter the size in GBs for each volume. Default is 100.", 
            "title": "Storage"
        }, 
        {
            "location": "/aws-create/index.html#use-spot-instances", 
            "text": "Check this option to use EC2 spot instances as your cluster nodes. Next, enter your bid price. The price that is pre-loaded in the form is the current on-demand price for your chosen EC2 instance type.     Note that:    We recommend not using spot instances for any host group that includes Ambari server components.    If you choose to use spot instances for a given host group when creating your cluster, any nodes that you add to that host group (during cluster creation or later) will be using spot instances. Any additional nodes will be requested at the same bid price that you entered when creating a cluster.    If you decide not to use spot instances when creating your cluster, any nodes that you add to your host group (during cluster creation or later) will be using standard on-demand instances.       Once someone outbids you, the spot instances are taken away, removing the nodes from the cluster.   If spot instances are not available right away, creating a cluster will take longer than usual.    After creating a cluster, you can view your spot instance requests, including bid price, on the EC2 dashboard under  INSTANCES     Spot Requests . For more information about spot instances, refer to  AWS documentation .", 
            "title": "Use spot instances"
        }, 
        {
            "location": "/aws-create/index.html#file-system", 
            "text": "If you would like to access S3 from your cluster, you must configure access to S3 trough an instance profile. For instructions, refer to  Configure access to S3 .   Related links  Configure access to S3", 
            "title": "File system"
        }, 
        {
            "location": "/aws-create/index.html#recipes", 
            "text": "This option allows you to select previously uploaded recipes (scripts that can be run pre or post cluster deployment) for each host group. For more information on recipes, refer to  Using custom scripts (recipes) .   Related links       Using custom scripts (recipes)", 
            "title": "Recipes"
        }, 
        {
            "location": "/aws-create/index.html#management-packs", 
            "text": "This option allows you to select previously uploaded management packs. For more information on management packs, refer to  Using management packs .   Related links       Using management packs", 
            "title": "Management Packs"
        }, 
        {
            "location": "/aws-create/index.html#external-sources", 
            "text": "You can register external sources with Cloudbreak, and then select and attach them during cluster create. To register external sources with Cloudbreak, refer to:   Using an external authentication source       Using an external database     Register a proxy", 
            "title": "External sources"
        }, 
        {
            "location": "/aws-create/index.html#ambari-server-master-key", 
            "text": "The Ambari server master key is used to configure Ambari to encrypt database and Kerberos credentials that are retained by Ambari as part of the Ambari setup.", 
            "title": "Ambari server master key"
        }, 
        {
            "location": "/aws-create/index.html#enable-kerberos-security", 
            "text": "Select this option to enable Kerberos for your cluster. For information about available Kerberos options, refer to  Enabling Kerberos security .   Related links       Enabling Kerberos security    Amazon EC2 instance store  (External)  Amazon EC2 spot instances  (External)      Next: Access Cluster", 
            "title": "Enable Kerberos security"
        }, 
        {
            "location": "/aws-clusters-access/index.html", 
            "text": "Accessing a cluster\n\n\nThe following section describes how to access the various services in the cluster.\n\n\nCloudbreak user accounts\n\n\nThe following table describes what credentials to use to access Cloudbreak and Cloudbreak-managed clusters:\n\n\n\n\n\n\n\n\nComponent\n\n\nMethod\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nCloudbreak\n\n\nWeb UI, CLI\n\n\nAccess with the username and password provided when launching Cloudbreak on the cloud provider.\n\n\n\n\n\n\nCloudbreak\n\n\nSSH to VM\n\n\nAccess as the \"cloudbreak\" user with the SSH key provided when launching Cloudbreak on the cloud provider.\n\n\n\n\n\n\nCluster\n\n\nSSH to VMs\n\n\nAccess as the \"cloudbreak\" user with the SSH key provided during cluster creation.\n\n\n\n\n\n\nCluster\n\n\nAmbari web UI\n\n\nAccess with the credentials provided in the \u201cCluster User\u201d parameter during cluster creation.\n\n\n\n\n\n\nCluster\n\n\nWeb UIs for specific cluster services\n\n\nAccess with the credentials provided in the \u201cCluster User\u201d parameter during cluster creation.\n\n\n\n\n\n\n\n\nFinding cluster information in the web UI\n\n\nOnce your cluster is up and running, click on the tile representing your cluster in the Cloudbreak UI to access information related the cluster and access cluster actions. \n\n\n \n\n\nThe information presented includes:\n\n\n\n\nCluster summary\n  \n\n\nCluster information\n     \n\n\nEvent history\n  \n\n\n\n\n\n  \nTips\n\n  \n\n  \n Access cluster actions such as resize and sync by clicking on \nACTIONS\n.\n\n  \n Access Ambari web UI by clicking on the link in the \nCLUSTER INFORMATION\n section.\n\n\n View public IP addresses for all cluster instances in the \nHARDWARE\n section. Click on the links to view the instances in the cloud console.\n\n\n The SSH user that you must use when accessing cluster VMs is \"cloudbreak\".\n \n\n\n\n\n\n\n\n\nCluster summary\n\n\nThe summary bar includes the following information about your cluster:\n\n\n\n\n\n\n\n\nItem\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nCluster Name\n\n\nThe name that you selected for your cluster is displayed at the top of the page. Below it is the name of the cluster blueprint.\n\n\n\n\n\n\nTime Remaining\n\n\nIf you enabled lifetime management for your cluster, the clock next to the cluster name indicates the amount of time that your cluster will run before it gets terminated. Note that the time remaining counter does not stop when you stop the cluster.\n\n\n\n\n\n\nCloud Provider\n\n\nThe logo of the cloud provider on which the cluster is running.\n\n\n\n\n\n\nCredential\n\n\nThe name of the credential used to create the cluster.\n\n\n\n\n\n\nStatus\n\n\nCurrent status. When a cluster is healthy, the status is \nRunning\n.\n\n\n\n\n\n\nNodes\n\n\nThe current number of cluster nodes, including the master node.\n\n\n\n\n\n\nUptime\n\n\nThe amount of time (HH:MM) that the cluster has been in the running state since it was started. Each time you stop and restart the cluster, the running time is reset to 0.\n\n\n\n\n\n\nCreated\n\n\nThe date when the cluster was created. The date format is Mon DD, YYYY. For example: Oct 27, 2017.\n\n\n\n\n\n\n\n\nCluster information\n\n\nThe following information is available on the cluster details page: \n\n\n\n\n\n\n\n\nItem\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nCluster User\n\n\nThe name of the cluster user that you created when creating the cluster.\n\n\n\n\n\n\nSSH Username\n\n\nThe SSH user which you must use when accessing cluster VMs via SSH. The SSH user is always \"cloudbreak\".\n\n\n\n\n\n\nAmbari URL\n\n\nLink to the Ambari web UI.\n\n\n\n\n\n\nRegion\n\n\nThe region in which the cluster is running in the cloud provider infrastructure.\n\n\n\n\n\n\nAvailability Zone\n\n\nThe availability zone within the region in which the cluster is running.\n\n\n\n\n\n\nBlueprint\n\n\nThe name of the blueprint selected under \"Cluster Type\" to create this cluster.\n\n\n\n\n\n\nCreated With\n\n\nThe version of Cloudbreak used to create this cluster.\n\n\n\n\n\n\nAmbari Version\n\n\nThe Ambari version which this cluster is currently running.\n\n\n\n\n\n\nHDP/HDF Version\n\n\nThe HDP or HDF version which this cluster is currently running.\n\n\n\n\n\n\nAuthentication Source\n\n\nIf you are using an external authentication source (LDAP/AD) for your cluster, you can see it here. Refer to \nUsing an external authentication source\n.\n\n\n\n\n\n\n\n\nBelow this, you will see additional tabs that you can click on in order to see their content:\n\n\n\n\n\n\n\n\nItem\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nHardware\n\n\nThis section includes information about your cluster instances: instance names, instance IDs, instance types, their status, fully qualified domain names (FQDNs), and private and public IPs.\n\n\n\n\n\n\nTags\n\n\nThis section lists keys and values of the user-defined tags, in the same order as you added them.\n\n\n\n\n\n\nRecipes\n\n\nThis section includes recipe-related information. For each recipe, you can see the host group on which a recipe was executed, recipe name, and recipe type. Refer to \nUsing custom scripts (recipes)\n.\n\n\n\n\n\n\nExternal Databases\n\n\nIf you are using an external database for your cluster, you can see it here. Refer to \nusing an external database\n.\n\n\n\n\n\n\nRepository Details\n\n\nThis section includes Ambari and HDP/HDF repository information, as you provided it in the \"Base Images\" section when creating a cluster.\n\n\n\n\n\n\nImage Details\n\n\nThis section includes information about the base image that was used for the Cloudbreak instance.\n\n\n\n\n\n\nNetwork\n\n\nThis section includes information about the names of the network and subnet in which the cluster is running and the links to related cloud provider console.\n\n\n\n\n\n\nSecurity\n\n\nThis section is only available if you have enabled Kerberos security. It provides you with the details of your Kerberos configuration.\n\n\n\n\n\n\nAutoscaling\n\n\nThis section includes configuration options related to autoscaling. Refer to \nConfiguring autoscaling\n.\n\n\n\n\n\n\n\n\nEvent history\n\n\nThe Event History section shows you events logged for the cluster, with the most recent event at the top. For example, after your cluster has been created, the following messages will be written to the log:\n\n\n\nAmbari cluster built; Ambari ip:34.215.103.66\n10/26/2017, 9:41:58 AM\nBuilding Ambari cluster; Ambari ip:34.215.103.66\n10/26/2017, 9:30:20 AM\nStarting Ambari cluster services\n10/26/2017, 9:27:12 AM\nSetting up infrastructure metadata\n10/26/2017, 9:27:11 AM\nBootstrapping infrastructure cluster\n10/26/2017, 9:26:38 AM\nInfrastructure successfully provisioned\n10/26/2017, 9:26:37 AM\nBilling started, Infrastructure successfully provisioned\n10/26/2017, 9:26:37 AM\nInfrastructure metadata collection finished\n10/26/2017, 9:25:39 AM\nInfrastructure creation took 194 seconds\n10/26/2017, 9:25:37 AM\nCreating infrastructure\n10/26/2017, 9:22:22 AM\nSetting up HDP image\n10/26/2017, 9:22:21 AM\n\n\n\nAccessing cluster via SSH\n\n\nIf you plan to access the cluster via the command line clients, SSH into the master node instance in the cluster. \n\n\n\n\nIn order to use SSH, you must generate an SSH key pair or use an existing SSH key pair.  \n\n\nYou can find the cluster instance public IP addresses on the cluster details page.  \n\n\nWhen accessing instances via SSH use the \ncloudbreak\n user. \n\n\n\n\nOn Mac OS, you can use the following syntax to SSH to the VM:\n\nssh -i \"privatekey.pem\" cloudbreak@publicIP\n\nFor example:\n\nssh -i \"dominika-kp.pem\" cloudbreak@p52.25.169.132\n\n\nOn Windows, you can SSH using an SSH client such as PuTTY.\n\n\nAccess Ambari\n\n\nYou can access Ambari web UI by clicking on the links provided in the \nCluster Information\n \n \nAmbari URL\n.\n\n\nSteps\n\n\n\n\n\n\nFrom the cluster dashboard, click on the tile representing your cluster to navigate to cluster details.\n\n\n\n\n\n\nFind the Ambari URL in the \nCluster Information\n section. This URL is available once the Ambari cluster creation process has completed.  \n\n\n\n\n\n\nClick on the \nAmbari URL\n link.\n\n\n\n\n\n\nThe first time you access the server, your browser will attempt to confirm that the SSL Certificate is valid. Since Cloudbreak automatically generates a self-signed certificate, your browser will warn you about an Untrusted Connection and ask you to confirm a Security Exception. Depending on your browser, perform the steps below to proceed.\n\n\n\n\n\n\n\n\nBrowser\n\n\nSteps\n\n\n\n\n\n\n\n\n\n\nFirefox\n\n\nClick \nAdvanced\n \n Click \nAdd Exception...\n \n Click \nConfirm Security Exception\n\n\n\n\n\n\nSafari\n\n\nClick \nContinue\n\n\n\n\n\n\nChrome\n\n\nClick \nAdvanced\n \n Click \nProceed...\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNext: Manage and Monitor Clusters", 
            "title": "Access cluster"
        }, 
        {
            "location": "/aws-clusters-access/index.html#accessing-a-cluster", 
            "text": "The following section describes how to access the various services in the cluster.", 
            "title": "Accessing a cluster"
        }, 
        {
            "location": "/aws-clusters-access/index.html#cloudbreak-user-accounts", 
            "text": "The following table describes what credentials to use to access Cloudbreak and Cloudbreak-managed clusters:     Component  Method  Description      Cloudbreak  Web UI, CLI  Access with the username and password provided when launching Cloudbreak on the cloud provider.    Cloudbreak  SSH to VM  Access as the \"cloudbreak\" user with the SSH key provided when launching Cloudbreak on the cloud provider.    Cluster  SSH to VMs  Access as the \"cloudbreak\" user with the SSH key provided during cluster creation.    Cluster  Ambari web UI  Access with the credentials provided in the \u201cCluster User\u201d parameter during cluster creation.    Cluster  Web UIs for specific cluster services  Access with the credentials provided in the \u201cCluster User\u201d parameter during cluster creation.", 
            "title": "Cloudbreak user accounts"
        }, 
        {
            "location": "/aws-clusters-access/index.html#finding-cluster-information-in-the-web-ui", 
            "text": "Once your cluster is up and running, click on the tile representing your cluster in the Cloudbreak UI to access information related the cluster and access cluster actions.      The information presented includes:   Cluster summary     Cluster information        Event history      \n   Tips \n   \n    Access cluster actions such as resize and sync by clicking on  ACTIONS . \n    Access Ambari web UI by clicking on the link in the  CLUSTER INFORMATION  section.   View public IP addresses for all cluster instances in the  HARDWARE  section. Click on the links to view the instances in the cloud console.   The SSH user that you must use when accessing cluster VMs is \"cloudbreak\".", 
            "title": "Finding cluster information in the web UI"
        }, 
        {
            "location": "/aws-clusters-access/index.html#cluster-summary", 
            "text": "The summary bar includes the following information about your cluster:     Item  Description      Cluster Name  The name that you selected for your cluster is displayed at the top of the page. Below it is the name of the cluster blueprint.    Time Remaining  If you enabled lifetime management for your cluster, the clock next to the cluster name indicates the amount of time that your cluster will run before it gets terminated. Note that the time remaining counter does not stop when you stop the cluster.    Cloud Provider  The logo of the cloud provider on which the cluster is running.    Credential  The name of the credential used to create the cluster.    Status  Current status. When a cluster is healthy, the status is  Running .    Nodes  The current number of cluster nodes, including the master node.    Uptime  The amount of time (HH:MM) that the cluster has been in the running state since it was started. Each time you stop and restart the cluster, the running time is reset to 0.    Created  The date when the cluster was created. The date format is Mon DD, YYYY. For example: Oct 27, 2017.", 
            "title": "Cluster summary"
        }, 
        {
            "location": "/aws-clusters-access/index.html#cluster-information", 
            "text": "The following information is available on the cluster details page:      Item  Description      Cluster User  The name of the cluster user that you created when creating the cluster.    SSH Username  The SSH user which you must use when accessing cluster VMs via SSH. The SSH user is always \"cloudbreak\".    Ambari URL  Link to the Ambari web UI.    Region  The region in which the cluster is running in the cloud provider infrastructure.    Availability Zone  The availability zone within the region in which the cluster is running.    Blueprint  The name of the blueprint selected under \"Cluster Type\" to create this cluster.    Created With  The version of Cloudbreak used to create this cluster.    Ambari Version  The Ambari version which this cluster is currently running.    HDP/HDF Version  The HDP or HDF version which this cluster is currently running.    Authentication Source  If you are using an external authentication source (LDAP/AD) for your cluster, you can see it here. Refer to  Using an external authentication source .     Below this, you will see additional tabs that you can click on in order to see their content:     Item  Description      Hardware  This section includes information about your cluster instances: instance names, instance IDs, instance types, their status, fully qualified domain names (FQDNs), and private and public IPs.    Tags  This section lists keys and values of the user-defined tags, in the same order as you added them.    Recipes  This section includes recipe-related information. For each recipe, you can see the host group on which a recipe was executed, recipe name, and recipe type. Refer to  Using custom scripts (recipes) .    External Databases  If you are using an external database for your cluster, you can see it here. Refer to  using an external database .    Repository Details  This section includes Ambari and HDP/HDF repository information, as you provided it in the \"Base Images\" section when creating a cluster.    Image Details  This section includes information about the base image that was used for the Cloudbreak instance.    Network  This section includes information about the names of the network and subnet in which the cluster is running and the links to related cloud provider console.    Security  This section is only available if you have enabled Kerberos security. It provides you with the details of your Kerberos configuration.    Autoscaling  This section includes configuration options related to autoscaling. Refer to  Configuring autoscaling .", 
            "title": "Cluster information"
        }, 
        {
            "location": "/aws-clusters-access/index.html#event-history", 
            "text": "The Event History section shows you events logged for the cluster, with the most recent event at the top. For example, after your cluster has been created, the following messages will be written to the log:  \nAmbari cluster built; Ambari ip:34.215.103.66\n10/26/2017, 9:41:58 AM\nBuilding Ambari cluster; Ambari ip:34.215.103.66\n10/26/2017, 9:30:20 AM\nStarting Ambari cluster services\n10/26/2017, 9:27:12 AM\nSetting up infrastructure metadata\n10/26/2017, 9:27:11 AM\nBootstrapping infrastructure cluster\n10/26/2017, 9:26:38 AM\nInfrastructure successfully provisioned\n10/26/2017, 9:26:37 AM\nBilling started, Infrastructure successfully provisioned\n10/26/2017, 9:26:37 AM\nInfrastructure metadata collection finished\n10/26/2017, 9:25:39 AM\nInfrastructure creation took 194 seconds\n10/26/2017, 9:25:37 AM\nCreating infrastructure\n10/26/2017, 9:22:22 AM\nSetting up HDP image\n10/26/2017, 9:22:21 AM", 
            "title": "Event history"
        }, 
        {
            "location": "/aws-clusters-access/index.html#accessing-cluster-via-ssh", 
            "text": "If you plan to access the cluster via the command line clients, SSH into the master node instance in the cluster.    In order to use SSH, you must generate an SSH key pair or use an existing SSH key pair.    You can find the cluster instance public IP addresses on the cluster details page.    When accessing instances via SSH use the  cloudbreak  user.    On Mac OS, you can use the following syntax to SSH to the VM: ssh -i \"privatekey.pem\" cloudbreak@publicIP \nFor example: ssh -i \"dominika-kp.pem\" cloudbreak@p52.25.169.132  On Windows, you can SSH using an SSH client such as PuTTY.", 
            "title": "Accessing cluster via SSH"
        }, 
        {
            "location": "/aws-clusters-access/index.html#access-ambari", 
            "text": "You can access Ambari web UI by clicking on the links provided in the  Cluster Information     Ambari URL .  Steps    From the cluster dashboard, click on the tile representing your cluster to navigate to cluster details.    Find the Ambari URL in the  Cluster Information  section. This URL is available once the Ambari cluster creation process has completed.      Click on the  Ambari URL  link.    The first time you access the server, your browser will attempt to confirm that the SSL Certificate is valid. Since Cloudbreak automatically generates a self-signed certificate, your browser will warn you about an Untrusted Connection and ask you to confirm a Security Exception. Depending on your browser, perform the steps below to proceed.     Browser  Steps      Firefox  Click  Advanced    Click  Add Exception...    Click  Confirm Security Exception    Safari  Click  Continue    Chrome  Click  Advanced    Click  Proceed...        Next: Manage and Monitor Clusters", 
            "title": "Access Ambari"
        }, 
        {
            "location": "/aws-clusters-manage/index.html", 
            "text": "Managing and monitoring clusters\n\n\nYou can manage monitor your clusters from the Cloudbreak UI. To do that, click on the tile representing the cluster that you want to access. The actions available for your cluster are listed in the top right corner: \n\n\n \n\n\n\n  \nTips\n\n  \n\n  \nTo add or remove nodes from your cluster click \nACTIONS>Resize\n.\n\n  \nTo synchronize your cluster with the cloud provider account click \nACTIONS>Sync\n.\n\n  \nTo temporarily stop your cluster click \nSTOP\n.\n\n  \nTo terminate your cluster click \nTERMINATE\n.\n\n\n\n\n\n\n\n\n\nRetry a cluster\n\n\nWhen a stack provisioning or cluster creation failure occurs, the \"retry\" option allows you to resume the process from the last failed step. \n\n\nIn some cases the cause of a failed stack provisioning or cluster creation can be eliminated by simply retrying the process. For example, in case of a temporary network outage, a retry should be successful. In other cases, a manual modification is required before a retry can succeed. For example, if you are using a custom image but some configuration is missing, causing the process to fail, you must log in to the machine and fix the issue; Only after that you can retry the the process.\n\n\nOnly failed stack or cluster creations can be retried. A retry can be initiated any number of times on a failed creation process. \n\n\nTo retry provisioning a failed stack or cluster, follow these steps.  \n\n\nSteps\n\n\n\n\n\n\nBrowse to the cluster details.\n\n\n\n\n\n\nClick \nActions\n and select \nRetry\n. \n\n\nOnly failed stack or cluster creations can be retried, so the option is only available in these two cases.  \n\n\n\n\n\n\nClick \nYes\n to confirm. \n\n\nThe operation continues from the last failed step. \n\n\n\n\n\n\nResize a cluster\n\n\nTo resize a cluster, follow these steps.\n\n\n\n\nCluster resizing is not supported for HDF clusters.\n\nTo configure automated cluster scaling, refer to \nConfigure autoscaling\n.   \n\n\n\n\nSteps\n\n\n\n\n\n\nBrowse to the cluster details.\n\n\n\n\n\n\nClick \nActions\n and select \nResize\n. The cluster resize dialog is displayed.\n\n\n\n\n\n\nUsing the +/- controls, adjust the number of nodes for a chosen host group. \n\n\n\n\nYou can only modify one host group at a time. \n\nIt is not possible to resize the Ambari server host group.     \n\n\n\n\n\n\n\n\nClick \nYes\n to confirm the scale-up/scale-down.\n\n\nWhile nodes are being added or removed, cluster status changes to \"Update In Progress\". Once the operation has completed, cluster status changes back to \"Running\". \n\n\n\n\n\n\nSynchronize a cluster\n\n\nUse the \nsync\n option if you:  \n\n\n\n\nMade changes on your cloud provider side (for example, deleted cluster VMs) and you would like to synchronize Cloudbreak with the cloud provider.  \n\n\nManually changed service status in Ambari (for example, restarted services).   \n\n\n\n\nTo synchronize your cluster with the cloud provider, follow these steps. \n\n\nSteps\n\n\n\n\n\n\nBrowse to the cluster details.\n\n\n\n\n\n\nClick \nActions\n and select \nSync\n.\n\n\n\n\n\n\nClick \nYes\n to confirm.\n\n\nYour cluster infrastructure is synchronized based on changes on the cloud provider. The updates are written to the \"Event History\". \n\n\n\n\n\n\nStop a cluster\n\n\nCloudbreak supports stopping and restarting clusters. To stop and restart a cluster managed by Cloudbreak, use the options available from the Cloudbreak UI. \n\n\nSteps\n\n\n\n\n\n\nBrowse to the cluster details.\n\n\n\n\n\n\nClick \nStop\n to stop a currently running cluster.  \n\n\n\n\n\n\nClick \nYes\n to confirm. \n\n\n\n\n\n\nYour cluster status changes to \"Stopping in progress\" and then to \"Stopped\". Once stopping the infrastructure has completed, you will see a \nStart\n option to restart your cluster. \n\n\n\n\n\n\nWhen a cluster is in the \"stopped\" state, you are not charged for the VMs, but you are charged for external storage.  \n\n\nRestart a cluster\n\n\nIf your cluster is in the \"Stopped\" state, you can restart the cluster by follow these steps.\n\n\nSteps\n\n\n\n\n\n\nclick \nStart\n. This option is only available when the cluster has been stopped. \n\n\n\n\n\n\nClick \nYes\n to confirm.\n\n\nYour cluster status changes to \"Start in progress\" and then to \"Running\". \n\n\n\n\n\n\nTerminate a cluster\n\n\nTo terminate a cluster managed by Cloudbreak, use the option available from the Cloudbreak UI. \n\n\nSteps\n\n\n\n\n\n\nBrowse to the cluster details.\n\n\n\n\n\n\nClick \nTerminate\n. \n\n\n\n\n\n\nClick \nYes\n to confirm.\n\n\nAll cluster-related resources will be deleted, unless the resources (such as networks and subnets) existed prior to cluster creation or are used by other VMs in which case they will be preserved. \n\n\n\n\n\n\nForce terminate a cluster\n\n\nCluster deletion may fail if Cloudbreak is unable to delete one or more of the cloud resources that were part of your cluster infrastructure. In such as case, you can use the \nTerminate\n \n \nForce terminate\n option to remove the cluster entry from the Cloudbreak web UI, but you must also check your cloud provider account to see if there are any resources that must be deleted manually.\n\n\nSteps\n\n\n\n\n\n\nBrowse to the cluster details.\n\n\n\n\n\n\nClick \nTerminate\n. \n\n\n\n\n\n\nCheck  \nForce terminate\n.\n\n\n\n\n\n\nClick \nYes\n to confirm. \n\n\nWhen terminating a cluster with Kerberos enabled, you have an option to disable Kerberos prior to cluster termination. This option removes any cluster-related principals from the KDC.\n\n\n\n\n\n\nThis deletes the cluster tile from the UI.  \n\n\n\n\n\n\nLog in to your cloud provider account and \nmanually delete\n any resources that failed to be deleted.\n\n\n\n\n\n\nView cluster history\n\n\nFrom the navigation menu in the Cloudbreak UI, you can access the History page that allows you to generate a report showing basic information related to the clusters that were running within the specified range of dates.\n\n\nTo generate a report, follow these steps.\n\n\nSteps\n\n\n\n\n\n\nFrom the Cloudbreak UI navigation menu, select \nHistory\n.\n\n\n\n\n\n\nOn the History page, select the range of dates and click \nShow History\n to generate a tabular report for the selected period.\n\n\n\n\n\n\nHistory report content\n\n\nEach entry in the report represents one cluster instance group. For each entry, the report includes the following information:\n\n\n\n\nCreated\n - The date when your cluster was created (YYYY-MM-DD).\n\n\nProvider\n - The name of the cloud provider (AWS, Azure, Google, or OpenStack) on which the cluster instances are/were running.\n\n\nCluster Name\n - The name that you selected for the cluster.  \n\n\nInstance Group\n - The name of the host group.   \n\n\nInstance Count\n - The number of nodes in the host group. This number may be a decimal if a cluster has been resized.\n\n\nInstance Type\n - Provider-specific VM type of the cluster instances. \n\n\nRegion\n - The AWS region in which your cluster is/was running.\n\n\nAvailability Zone\n - The availability zone in which your cluster is/was running.      \n\n\nRunning Time (hours)\n - The sum of the running times for all the nodes in the instance group.\n\n\n\n\nThe \nAGGREGATE RUNNING TIME\n is the sum of the Running Times, adjusted for the selected time range.\n\n\nTo learn about how your cloud provider bills you for the VMs, refer to their documentation:\n\n\n\n\nAWS\n      \n\n\nAzure\n     \n\n\nGCP\n   \n\n\n\n\n\n\nNext: Access Cloud Data", 
            "title": "Manage and monitor clusters"
        }, 
        {
            "location": "/aws-clusters-manage/index.html#managing-and-monitoring-clusters", 
            "text": "You can manage monitor your clusters from the Cloudbreak UI. To do that, click on the tile representing the cluster that you want to access. The actions available for your cluster are listed in the top right corner:      \n   Tips \n   \n   To add or remove nodes from your cluster click  ACTIONS>Resize . \n   To synchronize your cluster with the cloud provider account click  ACTIONS>Sync . \n   To temporarily stop your cluster click  STOP . \n   To terminate your cluster click  TERMINATE .", 
            "title": "Managing and monitoring clusters"
        }, 
        {
            "location": "/aws-clusters-manage/index.html#retry-a-cluster", 
            "text": "When a stack provisioning or cluster creation failure occurs, the \"retry\" option allows you to resume the process from the last failed step.   In some cases the cause of a failed stack provisioning or cluster creation can be eliminated by simply retrying the process. For example, in case of a temporary network outage, a retry should be successful. In other cases, a manual modification is required before a retry can succeed. For example, if you are using a custom image but some configuration is missing, causing the process to fail, you must log in to the machine and fix the issue; Only after that you can retry the the process.  Only failed stack or cluster creations can be retried. A retry can be initiated any number of times on a failed creation process.   To retry provisioning a failed stack or cluster, follow these steps.    Steps    Browse to the cluster details.    Click  Actions  and select  Retry .   Only failed stack or cluster creations can be retried, so the option is only available in these two cases.      Click  Yes  to confirm.   The operation continues from the last failed step.", 
            "title": "Retry a cluster"
        }, 
        {
            "location": "/aws-clusters-manage/index.html#resize-a-cluster", 
            "text": "To resize a cluster, follow these steps.   Cluster resizing is not supported for HDF clusters. \nTo configure automated cluster scaling, refer to  Configure autoscaling .      Steps    Browse to the cluster details.    Click  Actions  and select  Resize . The cluster resize dialog is displayed.    Using the +/- controls, adjust the number of nodes for a chosen host group.    You can only modify one host group at a time.  \nIt is not possible to resize the Ambari server host group.          Click  Yes  to confirm the scale-up/scale-down.  While nodes are being added or removed, cluster status changes to \"Update In Progress\". Once the operation has completed, cluster status changes back to \"Running\".", 
            "title": "Resize a cluster"
        }, 
        {
            "location": "/aws-clusters-manage/index.html#synchronize-a-cluster", 
            "text": "Use the  sync  option if you:     Made changes on your cloud provider side (for example, deleted cluster VMs) and you would like to synchronize Cloudbreak with the cloud provider.    Manually changed service status in Ambari (for example, restarted services).      To synchronize your cluster with the cloud provider, follow these steps.   Steps    Browse to the cluster details.    Click  Actions  and select  Sync .    Click  Yes  to confirm.  Your cluster infrastructure is synchronized based on changes on the cloud provider. The updates are written to the \"Event History\".", 
            "title": "Synchronize a cluster"
        }, 
        {
            "location": "/aws-clusters-manage/index.html#stop-a-cluster", 
            "text": "Cloudbreak supports stopping and restarting clusters. To stop and restart a cluster managed by Cloudbreak, use the options available from the Cloudbreak UI.   Steps    Browse to the cluster details.    Click  Stop  to stop a currently running cluster.      Click  Yes  to confirm.     Your cluster status changes to \"Stopping in progress\" and then to \"Stopped\". Once stopping the infrastructure has completed, you will see a  Start  option to restart your cluster.     When a cluster is in the \"stopped\" state, you are not charged for the VMs, but you are charged for external storage.", 
            "title": "Stop a cluster"
        }, 
        {
            "location": "/aws-clusters-manage/index.html#restart-a-cluster", 
            "text": "If your cluster is in the \"Stopped\" state, you can restart the cluster by follow these steps.  Steps    click  Start . This option is only available when the cluster has been stopped.     Click  Yes  to confirm.  Your cluster status changes to \"Start in progress\" and then to \"Running\".", 
            "title": "Restart a cluster"
        }, 
        {
            "location": "/aws-clusters-manage/index.html#terminate-a-cluster", 
            "text": "To terminate a cluster managed by Cloudbreak, use the option available from the Cloudbreak UI.   Steps    Browse to the cluster details.    Click  Terminate .     Click  Yes  to confirm.  All cluster-related resources will be deleted, unless the resources (such as networks and subnets) existed prior to cluster creation or are used by other VMs in which case they will be preserved.", 
            "title": "Terminate a cluster"
        }, 
        {
            "location": "/aws-clusters-manage/index.html#force-terminate-a-cluster", 
            "text": "Cluster deletion may fail if Cloudbreak is unable to delete one or more of the cloud resources that were part of your cluster infrastructure. In such as case, you can use the  Terminate     Force terminate  option to remove the cluster entry from the Cloudbreak web UI, but you must also check your cloud provider account to see if there are any resources that must be deleted manually.  Steps    Browse to the cluster details.    Click  Terminate .     Check   Force terminate .    Click  Yes  to confirm.   When terminating a cluster with Kerberos enabled, you have an option to disable Kerberos prior to cluster termination. This option removes any cluster-related principals from the KDC.    This deletes the cluster tile from the UI.      Log in to your cloud provider account and  manually delete  any resources that failed to be deleted.", 
            "title": "Force terminate a cluster"
        }, 
        {
            "location": "/aws-clusters-manage/index.html#view-cluster-history", 
            "text": "From the navigation menu in the Cloudbreak UI, you can access the History page that allows you to generate a report showing basic information related to the clusters that were running within the specified range of dates.  To generate a report, follow these steps.  Steps    From the Cloudbreak UI navigation menu, select  History .    On the History page, select the range of dates and click  Show History  to generate a tabular report for the selected period.", 
            "title": "View cluster history"
        }, 
        {
            "location": "/aws-clusters-manage/index.html#history-report-content", 
            "text": "Each entry in the report represents one cluster instance group. For each entry, the report includes the following information:   Created  - The date when your cluster was created (YYYY-MM-DD).  Provider  - The name of the cloud provider (AWS, Azure, Google, or OpenStack) on which the cluster instances are/were running.  Cluster Name  - The name that you selected for the cluster.    Instance Group  - The name of the host group.     Instance Count  - The number of nodes in the host group. This number may be a decimal if a cluster has been resized.  Instance Type  - Provider-specific VM type of the cluster instances.   Region  - The AWS region in which your cluster is/was running.  Availability Zone  - The availability zone in which your cluster is/was running.        Running Time (hours)  - The sum of the running times for all the nodes in the instance group.   The  AGGREGATE RUNNING TIME  is the sum of the Running Times, adjusted for the selected time range.  To learn about how your cloud provider bills you for the VMs, refer to their documentation:   AWS         Azure        GCP        Next: Access Cloud Data", 
            "title": "History report content"
        }, 
        {
            "location": "/aws-data/index.html", 
            "text": "Accessing data on S3\n\n\nUse these steps to configure access from your cluster to Amazon S3. \n\n\nPrerequisites\n\n\nTo use S3 storage, you must have one or more S3 buckets on your AWS account. For instructions on how to create a bucket on S3, refer to \nAWS documentation\n.\n\n\nRelated links\n\n\nCreate a bucket\n (External)    \n\n\nCreating an IAM role for S3 access\n\n\nIn order to configure access from your cluster to Amazon S3, you must have an existing IAM role which determines what actions can be performed on which S3 buckets. If you already have an IAM role, skip to the next step. If you do not have an existing IAM role, use the following instructions to create one. \n\n\nSteps\n\n\n\n\n\n\nNavigate to the \nIAM console\n \n \nRoles\n and click \nCreate Role\n.\n\n\n \n\n\n\n\n\n\nIn the \"Create Role\" wizard, select \nAWS service\n role type and then select \nEC2\n service and \nEC2\n use case. \n\n\n \n\n\n\n\n\n\nWhen done, click \nNext: Permissions\n to navigate to the next page in the wizard.\n\n\n\n\n\n\nSelect an existing S3 access policy or click \nCreate policy\n to define a new policy. If you are just getting started, you can select a built-in policy called \"AmazonS3FullAccess\", which provides full access to S3 buckets that are part of your account:\n\n\n\n\n\n\n\n\nWhen done attaching the policy, click \nNext: Review\n.\n\n\n\n\n\n\nIn the \nRoles name\n field, enter a name for the role that you are creating:  \n\n\n \n\n\n\n\n\n\nClick \nCreate role\n to finish the role creation process.\n\n\n\n\n\n\nConfigure access to S3\n\n\nAmazon S3 is not supported as a default file system, but access to data in S3 from your cluster VMs can be automatically configured by attaching an \ninstance profile\n allowing access to S3. You can optionally create or attach an existing instance profile during cluster creation on the \nFile System\n page.\n\n\nTo configure access to S3 with an instance profile, follow these steps. \n\n\nSteps\n\n\n\n\nYou or your AWS admin must create an IAM role with an S3 access policy which can be used by cluster instances to access one or more S3 buckets. Refer to \nCreating an IAM role for S3 access\n.  \n\n\nOn the \nFile System\n page in the advanced cluster wizard view, select \nUse existing instance profile\n. \n\n\nSelect an existing IAM role created in step 1:\n\n\n\n\nDuring the cluster creation process, Cloudbreak assigns the IAM role and its associated permissions to the EC2 instances that are part of the cluster so that applications running on these instances can use the role to access S3.   \n\n\nTesting access from HDP to S3\n\n\nAmazon S3 is not supported in HDP as a default file system, but access to data in Amazon S3 is possible via the s3a connector. \n\n\nTo tests access to S3 from HDP, SSH to a cluster node and run a few hadoop fs shell commands against your existing S3 bucket.\n\n\nAmazon S3 access path syntax is:\n\n\ns3a://bucket/dir/file\n\n\n\nFor example, to access a file called \"mytestfile\" in a directory called \"mytestdir\", which is stored in a bucket called \"mytestbucket\", the URL is:\n\n\ns3a://mytestbucket/mytestdir/mytestfile\n\n\n\nThe following FileSystem shell commands demonstrate access to a bucket named \"mytestbucket\": \n\n\nhadoop fs -ls s3a://mytestbucket/\n\nhadoop fs -mkdir s3a://mytestbucket/testDir\n\nhadoop fs -put testFile s3a://mytestbucket/testFile\n\nhadoop fs -cat s3a://mytestbucket/testFile\ntest file content\n\n\n\nFor more information about configuring the S3 connector for HDP and working with data stored on S3, refer to \nCloud Data Access\n documentation.\n\n\nRelated links\n\n\nCloud Data Access\n (Hortonworks)", 
            "title": "Access data on S3"
        }, 
        {
            "location": "/aws-data/index.html#accessing-data-on-s3", 
            "text": "Use these steps to configure access from your cluster to Amazon S3.", 
            "title": "Accessing data on S3"
        }, 
        {
            "location": "/aws-data/index.html#prerequisites", 
            "text": "To use S3 storage, you must have one or more S3 buckets on your AWS account. For instructions on how to create a bucket on S3, refer to  AWS documentation .  Related links  Create a bucket  (External)", 
            "title": "Prerequisites"
        }, 
        {
            "location": "/aws-data/index.html#creating-an-iam-role-for-s3-access", 
            "text": "In order to configure access from your cluster to Amazon S3, you must have an existing IAM role which determines what actions can be performed on which S3 buckets. If you already have an IAM role, skip to the next step. If you do not have an existing IAM role, use the following instructions to create one.   Steps    Navigate to the  IAM console     Roles  and click  Create Role .       In the \"Create Role\" wizard, select  AWS service  role type and then select  EC2  service and  EC2  use case.        When done, click  Next: Permissions  to navigate to the next page in the wizard.    Select an existing S3 access policy or click  Create policy  to define a new policy. If you are just getting started, you can select a built-in policy called \"AmazonS3FullAccess\", which provides full access to S3 buckets that are part of your account:     When done attaching the policy, click  Next: Review .    In the  Roles name  field, enter a name for the role that you are creating:         Click  Create role  to finish the role creation process.", 
            "title": "Creating an IAM role for S3 access"
        }, 
        {
            "location": "/aws-data/index.html#configure-access-to-s3", 
            "text": "Amazon S3 is not supported as a default file system, but access to data in S3 from your cluster VMs can be automatically configured by attaching an  instance profile  allowing access to S3. You can optionally create or attach an existing instance profile during cluster creation on the  File System  page.  To configure access to S3 with an instance profile, follow these steps.   Steps   You or your AWS admin must create an IAM role with an S3 access policy which can be used by cluster instances to access one or more S3 buckets. Refer to  Creating an IAM role for S3 access .    On the  File System  page in the advanced cluster wizard view, select  Use existing instance profile .   Select an existing IAM role created in step 1:   During the cluster creation process, Cloudbreak assigns the IAM role and its associated permissions to the EC2 instances that are part of the cluster so that applications running on these instances can use the role to access S3.", 
            "title": "Configure access to S3"
        }, 
        {
            "location": "/aws-data/index.html#testing-access-from-hdp-to-s3", 
            "text": "Amazon S3 is not supported in HDP as a default file system, but access to data in Amazon S3 is possible via the s3a connector.   To tests access to S3 from HDP, SSH to a cluster node and run a few hadoop fs shell commands against your existing S3 bucket.  Amazon S3 access path syntax is:  s3a://bucket/dir/file  For example, to access a file called \"mytestfile\" in a directory called \"mytestdir\", which is stored in a bucket called \"mytestbucket\", the URL is:  s3a://mytestbucket/mytestdir/mytestfile  The following FileSystem shell commands demonstrate access to a bucket named \"mytestbucket\":   hadoop fs -ls s3a://mytestbucket/\n\nhadoop fs -mkdir s3a://mytestbucket/testDir\n\nhadoop fs -put testFile s3a://mytestbucket/testFile\n\nhadoop fs -cat s3a://mytestbucket/testFile\ntest file content  For more information about configuring the S3 connector for HDP and working with data stored on S3, refer to  Cloud Data Access  documentation.  Related links  Cloud Data Access  (Hortonworks)", 
            "title": "Testing access from HDP to S3"
        }, 
        {
            "location": "/azure-launch/index.html", 
            "text": "Launching Cloudbreak on Azure\n\n\nBefore launching Cloudbreak on Azure, review and meet the prerequisites. Next, launch Cloudbreak using one of the two available methods. Once Cloudbreak is running, log in to the Cloudbreak UI and create a Cloudbreak credential.\n\n\nMeet the prerequisites\n\n\nBefore launching Cloudbreak on Azure, you must meet the following prerequisites.\n\n\nAzure account\n\n\nIn order to launch Cloudbreak on the Azure, log in to your existing Microsoft Azure account. If you don't have an account, you can set it up at \nhttps://azure.microsoft.com\n.\n\n\nAzure region\n\n\nDecide in which Azure region you would like to launch Cloudbreak. You can launch Cloudbreak and provision your clusters in all regions \nsupported by Microsoft Azure\n.\n\n\nClusters created via Cloudbreak can be in the same or different region as Cloudbreak; when you launch a cluster, you select the region in which to launch it.\n\n\nRelated links\n\n\nAzure regions\n (External)\n\n\nAzure roles\n\n\nIn order to provision clusters on Azure, Cloudbreak must be able to assume a sufficient Azure role (\"Owner\" or \"Contributor\") via Cloudbreak credential:\n\n\n\n\n\n\nYour account must have the \"\nOwner\n\" role (or a role with equivalent permissions) in the subscription in order to \ncreate a Cloudbreak credential\n using the interactive credential method.\n\n\n\n\n\n\nYour account must have the \"\nContributor\n\" role (or a role with equivalent permissions) in the subscription in order to \ncreate a Cloudbreak credential\n using the app-based credential method. The role must be assigned to the app that you register in Cloudbreak.\n\n\n\n\n\n\nTo check the roles in your subscription, log in to your Azure account and navigate to \nSubscriptions\n.\n\n\nRelated links\n\n\nBuilt-in roles: Owner\n (External)\n\n\nBuilt-in roles: Contributor\n (External)\n\n\nSSH key pair\n\n\nWhen launching Cloudbreak, you will be required to provide your public SSH key. If needed, you can generate a new SSH key pair:\n\n\n\n\nOn MacOS X and Linux using \nssh-keygen -t rsa -b 4096 -C \"your_email@example.com\"\n  \n\n\nOn Windows using \nPuTTygen\n\n\n\n\nLaunch Cloudbreak from a template\n\n\nLaunch Cloudbreak deployer by using the following steps.\n\n\nSteps\n\n\n\n\n\n\nLog in to your \nAzure Portal\n.\n\n\n\n\n\n\nClick here to get started with Cloudbreak installation using the Azure Resource Manager template:\n\n\n \n\n\n\n\n\n\nThe template for installing Cloudbreak will appear. On the \nBasics\n page, provide the following basic parameters:   \n\n\n\n\nAll parameters except \"SmartSense Id\" are required.\n\n\n\n\nBASICS\n\n\n\n\n\n\n\n\nParameter\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nSubscription\n\n\nSelect which existing subscription you want to use.\n\n\n\n\n\n\nResource group\n\n\nSelect an existing resource group or create a new one by selecting \nCreate new\n and entering a name for your new resource group. Cloudbreak resources will later be accessible in that chosen resource group.\n\n\n\n\n\n\nLocation\n\n\nSelect an Azure region in which you want to deploy Cloudbreak.\n\n\n\n\n\n\n\n\nSETTINGS\n\n\n\n\n\n\n\n\nParameter\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nVm Size\n\n\nSelect virtual machine instance type to use for the Cloudbreak controller. The minimum instance type suitable for Cloudbreak is \nStandard_DS3\n. The minimum requirements are 16GB RAM, 40GB disk, 4 cores.\n\n\n\n\n\n\nAdmin Username\n\n\nCreate an admin login that you will use to log in to the Cloudbreak UI. Must be a valid email address. By default, admin@example.com is used but you should change it to your email address.\n\n\n\n\n\n\nAdmin User Password\n\n\nPassword for the admin login. Must be at least 8 characters containing letters, numbers, and symbols.\n\n\n\n\n\n\nUsername\n\n\nEnter an admin username for the virtual machine. You will use it to SSH to the VM. By default, \"cloudbreak\" is used.\n\n\n\n\n\n\nRemote Location\n\n\nEnter a valid \nCIDR IP\n or use one of the default tags. Default value is \nInternet\n which allows access from all IP addresses. Examples: \n10.0.0.0/24 will allow access from 10.0.0.0 through 10.0.0.255\n'Internet' will allow access from all. This is not a secure option but you can use it it you are just getting started and are not planning to have the instance on for a longer period. \n(Advanced) 'VirtualNetwork' will allow access from the address space of the Virtual Network.\n (Advanced) 'AzureLoadBalancer' will allow access from the address space of the load balancer.\nFor more information, refer to the \nAzure documentation\n.\n\n\n\n\n\n\nSsh Key\n\n\nPaste your SSH public key.\nYou can use \npbcopy\n to quickly copy it. For example: \npbcopy \n /Users/homedir/.ssh/id_rsa.pub\n\n\n\n\n\n\nVnet New Or Existing\n\n\nBy default, Cloudbreak is launched in a new VNet called \ncbdeployerVnet\n and a new subnet called \ncbdeployerSubnet\n; if needed, you can customize the settings for the new VNet using available VNet and Subnet parameters.\n\n\n\n\n\n\nVnet Name\n\n\nProvide the name for a new Vnet. Default is \n`cbdeployerVnet\n.\n\n\n\n\n\n\nVnet Subnet Name\n\n\nProvide a name for a new subnet. Default is \ncbdeployerSubnet\n.\n\n\n\n\n\n\nVnet Address Prefix\n\n\nProvide a CIDR for the virtual network. Default is \n10.0.0.0/16\n.\n\n\n\n\n\n\nVnet Subnet Address Prefix\n\n\nProvide a CIDR for the subnet. Default is \n10.0.0.0/24\n.\n\n\n\n\n\n\nVnet RG Name\n\n\nThe name of the resource group in which the Vnet is located. If creating a new Vnet, enter the same resource group name as provided in the \nResource group\n field in the \nBASICS\n section.\n\n\n\n\n\n\n\n\n\n\n\n\nReview terms of use and check \"I agree to the terms and conditions stated above\".\n\n\n\n\n\n\nClick \nPurchase\n.\n\n\n\n\n\n\nYour deployment should be initiated.  \n\n\n\n\nIf you encounter errors, refer to \nTroubleshooting Cloudbreak on Azure\n.   \n\n\n\n\n\n\n\n\nRelated links\n\n\nCIDR IP\n (External) \n\n\nFilter network traffic with network security groups\n (External)  \n\n\nAccess Cloudbreak web UI\n\n\nLog in to the Cloudbreak UI using the following steps.\n\n\nSteps\n\n\n\n\n\n\nWhen your deployment succeeds, you will receive a notification in the top-right corner. You can click on the link provided to navigate to the resource group created earlier.\n\n\n\n\nThis only works right after deployment. At other times, you can find your resource group by selecting \nResource Groups\n from the service menu and then finding your resource group by name.\n\n\n\n\n\n\n\n\nOnce you've navigated to your resource group, click on \nDeployments\n and then click on \nMicrosoft.Template\n:\n\n\n\n\n\n\n\n\nFrom \nOutputs\n, you can copy the link by clicking on the copy icon:\n\n\n   \n\n\n\n\n\n\nPaste the link in your browser's address bar.\n\n\n\n\n\n\nConfirm the security exception to proceed to the Cloudbreak web UI.\n\n\nThe first time you access Cloudbreak UI, Cloudbreak will automatically generate a self-signed certificate, due to which your browser will warn you about an untrusted connection and will ask you to confirm a security exception.\n\n\n\n\n\n\n\n\nBrowser\n\n\nSteps\n\n\n\n\n\n\n\n\n\n\nFirefox\n\n\nClick \nAdvanced\n \n Click \nAdd Exception...\n \n Click \nConfirm Security Exception\n\n\n\n\n\n\nSafari\n\n\nClick \nContinue\n\n\n\n\n\n\nChrome\n\n\nClick \nAdvanced\n \n Click \nProceed...\n\n\n\n\n\n\n\n\n\n\n\n\nThe login page is displayed:\n\n\n  \n\n\n\n\n\n\nNow you should be able to access Cloudbreak UI and log in with the \nAdmin email address\n and \nAdmin password\n that you created when launching Cloudbreak. \n\n\n\n\n\n\nUpon a successful login, you are redirected to the dashboard:\n\n\n  \n\n\n\n\n\n\nThe last task that you need to perform before you can use Cloudbreak is to \ncreate a cloudbreak credential\n.         \n\n\nCreate Cloudbreak credential\n\n\nBefore you can start creating clusters, you must first create a \nCloudbreak credential\n. Without this credential, you will not be able to create clusters via Cloudbreak. Cloudbreak works by connecting your Azure account through this credential, and then uses it to create resources on your behalf.\n\n\nThere are two methods for creating a Cloudbreak credential:\n\n\n\n\n\n\n\n\nMethod\n\n\nDescription\n\n\nPrerequisite\n\n\nSteps\n\n\n\n\n\n\n\n\n\n\nInteractive\n\n\nThe advantage of using this method is that the app and service principal creation and role assignment are fully automated, so the only input that you need to provide is the Subscription ID and Directory ID. During the interactive credential creation, you are required to log in to your Azure account.\n\n\n(1) Your account must have the \"Owner\" role (or its equivalent) in the subscription. (2) You must be able log in to your Azure account.\n\n\nTo configure an interactive credential, refer to \nCreate an interactive credential\n.\n\n\n\n\n\n\nApp-based\n\n\nThe advantage of the app-based credential creation is that it allows you to create a credential without logging in to the Azure account, as long as you have been given all the information. In addition to providing your Subscription ID and  Directory ID, you must provide information for your previously created Azure AD application (its ID and key which allows access to it).\n\n\n(1) Your account must have the \"Contributor\" role (or equivalent) in the subscription. (2) You or your Azure administrator must perform prerequisite steps of registering an Azure application and assigning the  \"Contributor\" role to it. This step typically requires admin permissions so you may have to contact your Azure administrator.\n\n\nTo configure an app based credential, refer to \nCreate an app-based credential\n.\n\n\n\n\n\n\n\n\nCreate an interactive credential\n\n\nFollow these steps to create an interactive Cloudbreak credential.\n\n\nSteps\n\n\n\n\n\n\nIn the Cloudbreak web UI, select \nCredentials\n from the navigation pane.\n\n\n\n\n\n\nClick \nCreate Credential\n.\n\n\n\n\n\n\nUnder \nCloud provider\n, select \"Microsoft Azure\".\n\n\n\n\n\n\nSelect \nInteractive Login\n:\n\n\n     \n\n\n\n\n\n\nProvide the following information:\n\n\n\n\n\n\n\n\nParameter\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nName\n\n\nEnter a name for your credential.\n\n\n\n\n\n\nDescription\n\n\n(Optional) Enter a description.\n\n\n\n\n\n\nSubscription Id\n\n\nCopy and paste the Subscription ID from your \nSubscriptions\n.\n\n\n\n\n\n\nTenant Id\n\n\nCopy and paste your Directory ID from your \nActive Directory\n \n \nProperties\n.\n\n\n\n\n\n\nAzure role type\n\n\nYou have the following options:\n\"Use existing Contributor role\" (default): If you select this option, Cloudbreak will use the \"\nContributor\n\" role to create resources. This requires no further input.\n\"Reuse existing custom role\": If you select this option and enter the name of an existing role, Cloudbreak will use this role to create resources.\n\"Let Cloudbreak create a custom role\": If you select this option and enter a name for the new role, the role will be created. When choosing role name, make sure that there is no existing role with the name chosen. For information on creating custom roles, refer to \nAzure\n documentation. \nIf using a custom role, make sure that it includes the necessary Action set for Cloudbreak to be able to manage clusters: \nMicrosoft.Compute/*\n, \nMicrosoft.Network/*\n, \nMicrosoft.Storage/*\n, \nMicrosoft.Resources/*\n.\n\n\n\n\n\n\n\n\nTo obtain the \nSubscription Id\n:\n\n\n   \n\n\nTo obtain the \nTenant ID\n (actually \nDirectory Id\n):\n\n\n    \n\n\n\n\n\n\nAfter providing the parameters, click \nInteractive Login\n.\n\n\n\n\n\n\nCopy the code provided in the UI:\n\n\n     \n\n\n\n\n\n\nClick \nAzure login\n and a new \nDevice login\n page will open in a new browser tab:\n\n\n  \n\n\n\n\n\n\nNext, paste the code in field on the  \nDevice login\n page and click \nContinue\n.\n\n\n\n\n\n\nConfirm your account by selecting it:\n\n\n\n\n\n\n\n\nA confirmation page will appear, confirming that you have signed in to the Microsoft Azure Cross-platform Command Line Interface application on your device. You may now close this window.\n\n\n\n\nIf you encounter errors, refer to \nTroubleshooting Azure\n.\n\n\n\n\nCongratulations! You've successfully launched and configured Cloudbreak. Now you can use Cloudbreak to \ncreate clusters\n.\n\n\n\n\n\n\nCreate an app-based credential\n\n\nFollow these steps to create an app based Cloudbreak credential.\n\n\nPrerequisites\n\n\n\n\n\n\nOn Azure Portal, navigate to the \nActive Directory\n \n \nApp Registrations\n and register a new application. For more information, refer to \nCreate an Azure AD application\n.\n\n\n\n\nAa an alternative to the steps listed below for creating an application registration, you use a utility called \nazure-cli-tools\n. The utility supports app creation and role assignment. It is available at \nhttps://github.com/sequenceiq/azure-cli-tools/blob/master/cli_tools\n.\n\n\n\n\n  \n\n\n\n\n\n\nNavigate to the \nSubscriptions\n, choose \nAccess control (IAM)\n. Click \nAdd\n and then assign the \"Contributor\" role to your newly created application by selecting \"Contributor\" under \nRole\n and your app name under \nSelect\n:\n\n\n\n\nThis step typically requires admin permissions so you may have to contact your Azure administrator.\n\n\n\n\n   \n\n\n\n\n\n\nSteps\n\n\n\n\n\n\nIn the Cloudbreak web UI, select \nCredentials\n from the navigation pane.\n\n\n\n\n\n\nClick \nCreate Credential\n.\n\n\n\n\n\n\nUnder \nCloud provider\n, select \"Microsoft Azure\".\n\n\n\n\n\n\nSelect \nApp based Login\n:\n\n\n\n\n\n\n\n\nOn the \nConfigure credential\n page, provide the following parameters:\n\n\n\n\n\n\n\n\nParameter\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nSelect Credential Type\n\n\nSelect \nApp based\n.\n\n\n\n\n\n\nName\n\n\nEnter a name for your credential.\n\n\n\n\n\n\nDescription\n\n\n(Optional) Enter a description.\n\n\n\n\n\n\nSubscription Id\n\n\nCopy and paste the Subscription ID from your \nSubscriptions\n.\n\n\n\n\n\n\nTenant Id\n\n\nCopy and paste your Directory ID from your \nActive Directory\n \n \nProperties\n.\n\n\n\n\n\n\nApp Id\n\n\nCopy and paste the Application ID from your \nAzure Active Directory\n \n \nApp Registrations\n \n your app registration's \nSettings\n \n \nProperties\n.\n\n\n\n\n\n\nPassword\n\n\nThis is your application key. You can generate it from your \nAzure Active Directory\n app registration's \nSettings\n \n \nKeys\n.\n\n\n\n\n\n\n\n\nTo obtain the \nSubscription Id\n from Subscriptions:\n\n\n   \n\n\nTo obtain the \nApp ID\n (actually \nApplication ID\n) and an application key from Azure Active Directory:\n\n\n  \n\n\n      \n\n\nTo obtain the \nTenant ID\n (actually \nDirectory Id\n) from Azure Active Directory:\n\n\n  \n\n\n\n\n\n\nClick \nCreate\n.\n\n\n\n\nIf you encounter errors, refer to \nTroubleshooting Cloudbreak on Azure\n.  \n\n\n\n\nCongratulations! You have successfully launched Cloudbreak and created a Cloudbreak credential. Now you can use Cloudbreak to \ncreate clusters\n.\n\n\n\n\n\n\nRelated links\n\n\nCLI tools\n (Hortonworks)  \n\n\nUse portal to create an Azure Active Directory application\n (External)     \n\n\n\n\nNext: Create a Cluster", 
            "title": "Launch on Azure"
        }, 
        {
            "location": "/azure-launch/index.html#launching-cloudbreak-on-azure", 
            "text": "Before launching Cloudbreak on Azure, review and meet the prerequisites. Next, launch Cloudbreak using one of the two available methods. Once Cloudbreak is running, log in to the Cloudbreak UI and create a Cloudbreak credential.", 
            "title": "Launching Cloudbreak on Azure"
        }, 
        {
            "location": "/azure-launch/index.html#meet-the-prerequisites", 
            "text": "Before launching Cloudbreak on Azure, you must meet the following prerequisites.", 
            "title": "Meet the prerequisites"
        }, 
        {
            "location": "/azure-launch/index.html#azure-account", 
            "text": "In order to launch Cloudbreak on the Azure, log in to your existing Microsoft Azure account. If you don't have an account, you can set it up at  https://azure.microsoft.com .", 
            "title": "Azure account"
        }, 
        {
            "location": "/azure-launch/index.html#azure-region", 
            "text": "Decide in which Azure region you would like to launch Cloudbreak. You can launch Cloudbreak and provision your clusters in all regions  supported by Microsoft Azure .  Clusters created via Cloudbreak can be in the same or different region as Cloudbreak; when you launch a cluster, you select the region in which to launch it.  Related links  Azure regions  (External)", 
            "title": "Azure region"
        }, 
        {
            "location": "/azure-launch/index.html#azure-roles", 
            "text": "In order to provision clusters on Azure, Cloudbreak must be able to assume a sufficient Azure role (\"Owner\" or \"Contributor\") via Cloudbreak credential:    Your account must have the \" Owner \" role (or a role with equivalent permissions) in the subscription in order to  create a Cloudbreak credential  using the interactive credential method.    Your account must have the \" Contributor \" role (or a role with equivalent permissions) in the subscription in order to  create a Cloudbreak credential  using the app-based credential method. The role must be assigned to the app that you register in Cloudbreak.    To check the roles in your subscription, log in to your Azure account and navigate to  Subscriptions .  Related links  Built-in roles: Owner  (External)  Built-in roles: Contributor  (External)", 
            "title": "Azure roles"
        }, 
        {
            "location": "/azure-launch/index.html#ssh-key-pair", 
            "text": "When launching Cloudbreak, you will be required to provide your public SSH key. If needed, you can generate a new SSH key pair:   On MacOS X and Linux using  ssh-keygen -t rsa -b 4096 -C \"your_email@example.com\"     On Windows using  PuTTygen", 
            "title": "SSH key pair"
        }, 
        {
            "location": "/azure-launch/index.html#launch-cloudbreak-from-a-template", 
            "text": "Launch Cloudbreak deployer by using the following steps.  Steps    Log in to your  Azure Portal .    Click here to get started with Cloudbreak installation using the Azure Resource Manager template:       The template for installing Cloudbreak will appear. On the  Basics  page, provide the following basic parameters:      All parameters except \"SmartSense Id\" are required.   BASICS     Parameter  Description      Subscription  Select which existing subscription you want to use.    Resource group  Select an existing resource group or create a new one by selecting  Create new  and entering a name for your new resource group. Cloudbreak resources will later be accessible in that chosen resource group.    Location  Select an Azure region in which you want to deploy Cloudbreak.     SETTINGS     Parameter  Description      Vm Size  Select virtual machine instance type to use for the Cloudbreak controller. The minimum instance type suitable for Cloudbreak is  Standard_DS3 . The minimum requirements are 16GB RAM, 40GB disk, 4 cores.    Admin Username  Create an admin login that you will use to log in to the Cloudbreak UI. Must be a valid email address. By default, admin@example.com is used but you should change it to your email address.    Admin User Password  Password for the admin login. Must be at least 8 characters containing letters, numbers, and symbols.    Username  Enter an admin username for the virtual machine. You will use it to SSH to the VM. By default, \"cloudbreak\" is used.    Remote Location  Enter a valid  CIDR IP  or use one of the default tags. Default value is  Internet  which allows access from all IP addresses. Examples:  10.0.0.0/24 will allow access from 10.0.0.0 through 10.0.0.255 'Internet' will allow access from all. This is not a secure option but you can use it it you are just getting started and are not planning to have the instance on for a longer period.  (Advanced) 'VirtualNetwork' will allow access from the address space of the Virtual Network.  (Advanced) 'AzureLoadBalancer' will allow access from the address space of the load balancer. For more information, refer to the  Azure documentation .    Ssh Key  Paste your SSH public key. You can use  pbcopy  to quickly copy it. For example:  pbcopy   /Users/homedir/.ssh/id_rsa.pub    Vnet New Or Existing  By default, Cloudbreak is launched in a new VNet called  cbdeployerVnet  and a new subnet called  cbdeployerSubnet ; if needed, you can customize the settings for the new VNet using available VNet and Subnet parameters.    Vnet Name  Provide the name for a new Vnet. Default is  `cbdeployerVnet .    Vnet Subnet Name  Provide a name for a new subnet. Default is  cbdeployerSubnet .    Vnet Address Prefix  Provide a CIDR for the virtual network. Default is  10.0.0.0/16 .    Vnet Subnet Address Prefix  Provide a CIDR for the subnet. Default is  10.0.0.0/24 .    Vnet RG Name  The name of the resource group in which the Vnet is located. If creating a new Vnet, enter the same resource group name as provided in the  Resource group  field in the  BASICS  section.       Review terms of use and check \"I agree to the terms and conditions stated above\".    Click  Purchase .    Your deployment should be initiated.     If you encounter errors, refer to  Troubleshooting Cloudbreak on Azure .        Related links  CIDR IP  (External)   Filter network traffic with network security groups  (External)", 
            "title": "Launch Cloudbreak from a template"
        }, 
        {
            "location": "/azure-launch/index.html#access-cloudbreak-web-ui", 
            "text": "Log in to the Cloudbreak UI using the following steps.  Steps    When your deployment succeeds, you will receive a notification in the top-right corner. You can click on the link provided to navigate to the resource group created earlier.   This only works right after deployment. At other times, you can find your resource group by selecting  Resource Groups  from the service menu and then finding your resource group by name.     Once you've navigated to your resource group, click on  Deployments  and then click on  Microsoft.Template :     From  Outputs , you can copy the link by clicking on the copy icon:         Paste the link in your browser's address bar.    Confirm the security exception to proceed to the Cloudbreak web UI.  The first time you access Cloudbreak UI, Cloudbreak will automatically generate a self-signed certificate, due to which your browser will warn you about an untrusted connection and will ask you to confirm a security exception.     Browser  Steps      Firefox  Click  Advanced    Click  Add Exception...    Click  Confirm Security Exception    Safari  Click  Continue    Chrome  Click  Advanced    Click  Proceed...       The login page is displayed:        Now you should be able to access Cloudbreak UI and log in with the  Admin email address  and  Admin password  that you created when launching Cloudbreak.     Upon a successful login, you are redirected to the dashboard:        The last task that you need to perform before you can use Cloudbreak is to  create a cloudbreak credential .", 
            "title": "Access Cloudbreak web UI"
        }, 
        {
            "location": "/azure-launch/index.html#create-cloudbreak-credential", 
            "text": "Before you can start creating clusters, you must first create a  Cloudbreak credential . Without this credential, you will not be able to create clusters via Cloudbreak. Cloudbreak works by connecting your Azure account through this credential, and then uses it to create resources on your behalf.  There are two methods for creating a Cloudbreak credential:     Method  Description  Prerequisite  Steps      Interactive  The advantage of using this method is that the app and service principal creation and role assignment are fully automated, so the only input that you need to provide is the Subscription ID and Directory ID. During the interactive credential creation, you are required to log in to your Azure account.  (1) Your account must have the \"Owner\" role (or its equivalent) in the subscription. (2) You must be able log in to your Azure account.  To configure an interactive credential, refer to  Create an interactive credential .    App-based  The advantage of the app-based credential creation is that it allows you to create a credential without logging in to the Azure account, as long as you have been given all the information. In addition to providing your Subscription ID and  Directory ID, you must provide information for your previously created Azure AD application (its ID and key which allows access to it).  (1) Your account must have the \"Contributor\" role (or equivalent) in the subscription. (2) You or your Azure administrator must perform prerequisite steps of registering an Azure application and assigning the  \"Contributor\" role to it. This step typically requires admin permissions so you may have to contact your Azure administrator.  To configure an app based credential, refer to  Create an app-based credential .", 
            "title": "Create Cloudbreak credential"
        }, 
        {
            "location": "/azure-launch/index.html#create-an-interactive-credential", 
            "text": "Follow these steps to create an interactive Cloudbreak credential.  Steps    In the Cloudbreak web UI, select  Credentials  from the navigation pane.    Click  Create Credential .    Under  Cloud provider , select \"Microsoft Azure\".    Select  Interactive Login :           Provide the following information:     Parameter  Description      Name  Enter a name for your credential.    Description  (Optional) Enter a description.    Subscription Id  Copy and paste the Subscription ID from your  Subscriptions .    Tenant Id  Copy and paste your Directory ID from your  Active Directory     Properties .    Azure role type  You have the following options: \"Use existing Contributor role\" (default): If you select this option, Cloudbreak will use the \" Contributor \" role to create resources. This requires no further input. \"Reuse existing custom role\": If you select this option and enter the name of an existing role, Cloudbreak will use this role to create resources. \"Let Cloudbreak create a custom role\": If you select this option and enter a name for the new role, the role will be created. When choosing role name, make sure that there is no existing role with the name chosen. For information on creating custom roles, refer to  Azure  documentation.  If using a custom role, make sure that it includes the necessary Action set for Cloudbreak to be able to manage clusters:  Microsoft.Compute/* ,  Microsoft.Network/* ,  Microsoft.Storage/* ,  Microsoft.Resources/* .     To obtain the  Subscription Id :       To obtain the  Tenant ID  (actually  Directory Id ):          After providing the parameters, click  Interactive Login .    Copy the code provided in the UI:           Click  Azure login  and a new  Device login  page will open in a new browser tab:        Next, paste the code in field on the   Device login  page and click  Continue .    Confirm your account by selecting it:     A confirmation page will appear, confirming that you have signed in to the Microsoft Azure Cross-platform Command Line Interface application on your device. You may now close this window.   If you encounter errors, refer to  Troubleshooting Azure .   Congratulations! You've successfully launched and configured Cloudbreak. Now you can use Cloudbreak to  create clusters .", 
            "title": "Create an interactive credential"
        }, 
        {
            "location": "/azure-launch/index.html#create-an-app-based-credential", 
            "text": "Follow these steps to create an app based Cloudbreak credential.  Prerequisites    On Azure Portal, navigate to the  Active Directory     App Registrations  and register a new application. For more information, refer to  Create an Azure AD application .   Aa an alternative to the steps listed below for creating an application registration, you use a utility called  azure-cli-tools . The utility supports app creation and role assignment. It is available at  https://github.com/sequenceiq/azure-cli-tools/blob/master/cli_tools .         Navigate to the  Subscriptions , choose  Access control (IAM) . Click  Add  and then assign the \"Contributor\" role to your newly created application by selecting \"Contributor\" under  Role  and your app name under  Select :   This step typically requires admin permissions so you may have to contact your Azure administrator.          Steps    In the Cloudbreak web UI, select  Credentials  from the navigation pane.    Click  Create Credential .    Under  Cloud provider , select \"Microsoft Azure\".    Select  App based Login :     On the  Configure credential  page, provide the following parameters:     Parameter  Description      Select Credential Type  Select  App based .    Name  Enter a name for your credential.    Description  (Optional) Enter a description.    Subscription Id  Copy and paste the Subscription ID from your  Subscriptions .    Tenant Id  Copy and paste your Directory ID from your  Active Directory     Properties .    App Id  Copy and paste the Application ID from your  Azure Active Directory     App Registrations    your app registration's  Settings     Properties .    Password  This is your application key. You can generate it from your  Azure Active Directory  app registration's  Settings     Keys .     To obtain the  Subscription Id  from Subscriptions:       To obtain the  App ID  (actually  Application ID ) and an application key from Azure Active Directory:              To obtain the  Tenant ID  (actually  Directory Id ) from Azure Active Directory:        Click  Create .   If you encounter errors, refer to  Troubleshooting Cloudbreak on Azure .     Congratulations! You have successfully launched Cloudbreak and created a Cloudbreak credential. Now you can use Cloudbreak to  create clusters .    Related links  CLI tools  (Hortonworks)    Use portal to create an Azure Active Directory application  (External)        Next: Create a Cluster", 
            "title": "Create an app-based credential"
        }, 
        {
            "location": "/azure-create/index.html", 
            "text": "Creating a cluster on Azure\n\n\nUse these steps to create a cluster.\n\n\n\n\nTroubleshooting cluster creation\n\n\nIf you experience problems during cluster creation, refer to \nTroubleshooting cluster creation\n.\n\n\n\nSteps\n\n\n\n\n\n\nLog in to the Cloudbreak UI.\n\n\n\n\n\n\nClick \nCreate Cluster\n and the \nCreate Cluster\n wizard is displayed.\n\n    By default, \nBasic\n view is displayed. To view advanced options, click \nAdvanced\n. To learn about advanced options, refer to \nAdvanced cluster options\n.\n\n\n \n\n\n\n\n\n\nOn the \nGeneral Configuration\n page, specify the following general parameters for your cluster:\n\n\n\n\n\n\n\n\nParameter\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nSelect Credential\n\n\nChoose a previously created credential.\n\n\n\n\n\n\nCluster Name\n\n\nEnter a name for your cluster. The name must be between 5 and 40 characters, must start with a letter, and must only include lowercase letters, numbers, and hyphens.\n\n\n\n\n\n\nRegion\n\n\nSelect the Azure region in which you would like to launch your cluster. For information on available Azure regions, refer to \nAzure documentation\n.\n\n\n\n\n\n\nPlatform Version\n\n\nChoose the HDP or HDF version to use for this cluster. Blueprints available for this platform version will be populated under \"Cluster Type\" below.\n\n\n\n\n\n\nCluster Type\n\n\nChoose one of default cluster configurations, or, if you have defined your own cluster configuration via Ambari blueprint, you can choose it here. For more information on default and custom blueprints, refer to \nUsing custom blueprints\n.\n\n\n\n\n\n\nFlex Subscription\n\n\nThis option will appear if you have configured your deployment for a \nflex support subscription\n.\n\n\n\n\n\n\n\n\n\n\n\n\nOn the \nHardware and Storage\n page, for each host group provide the following information to define your cluster nodes and attached storage:\n\n\n\n\n\n\n\n\nParameter\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nInstance Type\n\n\nSelect an instance type. For information about instance types on Azure refer to \nAzure documentation\n.\n\n\n\n\n\n\nInstance Count\n\n\nEnter the number of instances of a given type. Default is 1.\n\n\n\n\n\n\nAmbari Server\n\n\nYou must select one node for Ambari Server. The \"Group Size\" for that host group must be set to \"1\".\n\n\n\n\n\n\n\n\n\n\n\n\nOn the \nNetwork\n page, provide the following to specify the networking resources that will be used for your cluster:\n\n\n\n\n\n\n\n\nParameter\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nSelect Network\n\n\nSelect the virtual network in which you would like your cluster to be provisioned. You can select an existing network or create a new network.\n\n\n\n\n\n\nSelect Subnet\n\n\nSelect the subnet in which you would like your cluster to be provisioned. If you are using a new network, create a new subnet. If you are using an existing network, select an existing subnet.\n\n\n\n\n\n\nSubnet (CIDR)\n\n\nIf you selected to create a new subnet, you must define a valid \nCIDR\n for the subnet. Default is 10.0.0.0/16.\n\n\n\n\n\n\n\n\n\n\nCloudbreak uses public IP addresses when communicating with cluster nodes.  \n\n\n\n\n\n\n\n\nDefine security groups for each host group. You can either create new security groups and define their rules or reuse existing security groups:\n\n\n\n\n\n\n\n\nOption\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nNew Security Group\n\n\n(Default) Creates a new security group with the rules that you defined:\nA set of \ndefault rules\n is provided. You should review and adjust these default rules. If you do not make any modifications, default rules will be applied. \nYou may open ports by defining the CIDR, entering port range, selecting protocol and clicking \n+\n.\nYou may delete default or previously added rules using the delete icon.\nIf you don't want to use security group, remove the default rules.\n\n\n\n\n\n\nExisting Security Groups\n\n\nAllows you to select an existing security group that is already available in the selected provider region. This selection is disabled if no existing security groups are available in your chosen region.\n\n\n\n\n\n\n\n\n\n\nImportant\n\n\n\nBy default, ports 22, 443, and 9443 are set to 0.0.0.0/0 CIDR for inbound access on the Ambari node security group. We strongly recommend that you limit this CIDR, considering the following restrictions:\n\nPorts 22 and 9443 must be open to Cloudbreak's CIDR. You can set CB_DEFAULT_GATEWAY_CIDR in your Cloudbreak's Profile file in order to automatically open ports 22 and 9443 to your Cloudbreak IP. Refer to \nRestricting inbound access from Cloudbreak to cluster\n.\n\n\nPort 22 must be open to your CIDR if you would like to access the master node via SSH.\n\n\nPort 443 must be open to your CIDR if you would like to access Ambari web UI in a browser.\n\n\n\n\n\n\n\n\n\n\nImportant\n\n\n\nBy default, port 22 is set to 0.0.0.0/0 CIDR for inbound access on non-Ambari node security groups. We strongly recommend that you remove it.\n\n\n\n\n\n\nImportant\n\n\n\nDepending on what services you are including, you need to open additional ports as required by these services. For example, when using the Flow Management blueprint, you must open port 9091 for NiFi (on NiFI host group) and port 61443 for NiFI Registry (on the Services host group).\n\n\n\n\n\n\n\n\n\n\nOn the \nSecurity\n page, provide the following parameters:\n\n\n\n\n\n\n\n\nParameter\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nCluster User\n\n\nYou can log in to the Ambari UI using this username. By default, this is set to \nadmin\n.\n\n\n\n\n\n\nPassword\n\n\nYou can log in to the Ambari UI using this password.\n\n\n\n\n\n\nConfirm Password\n\n\nConfirm the password.\n\n\n\n\n\n\nNew SSH public key\n\n\nCheck this option to specify a new public key and then enter the public key. You will use the matching private key to access your cluster nodes via SSH.\n\n\n\n\n\n\nExisting SSH public key\n\n\nSelect an existing public key. You will use the matching private key to access your cluster nodes via SSH. This is a default option as long as an existing SSH public key is available.\n\n\n\n\n\n\n\n\n\n\n\n\nClick on \nCreate Cluster\n to create a cluster.\n\n\n\n\n\n\nYou will be redirected to the Cloudbreak dashboard, and a new tile representing your cluster will appear at the top of the page.\n\n\n\n\n\n\nRelated links\n\n\nFlex support subscription\n\n\nUsing custom blueprints\n \n\n\nDefault cluster security groups\n\n\nTroubleshooting cluster creation\n \n\n\nAzure regions\n (External)   \n\n\nCIDR\n (External)\n\n\nGeneral purpose Linux VM sizes\n (External)  \n\n\nAdvanced cluster options\n\n\nClick on \nAdvanced\n to view and enter additional configuration options\n\n\nChoose image catalog\n\n\nBy default, \nChoose Image Catalog\n is set to the default image catalog that is provided with Cloudbreak. If you would like to use a different image catalog, you must first create and register it. For complete instructions, refer to \nUsing custom images\n.\n\n\nRelated links\n   \n\n\nUsing custom images\n  \n\n\nPrewarmed and base images\n\n\nCloudbreak supports the following types of images for launching clusters:\n\n\n\n\n\n\n\n\nImage type\n\n\nDescription\n\n\nDefault images provided\n\n\nSupport for custom images\n\n\n\n\n\n\n\n\n\n\nBase Images\n\n\nBase images include default configuration and default tooling. These images include the operating system but do not include Ambari or HDP/HDF software.\n\n\nYes\n\n\nYes\n\n\n\n\n\n\nPrewarmed Images\n\n\nBy default, Cloudbreak launches clusters from prewarmed images. Prewarmed images include the operating system as well as Ambari and HDP. The Ambari and HDP version used by prewarmed images cannot be customized. No prewarmed HDF images are currently provided.\n\n\nYes (HDP only)\n\n\nNo\n\n\n\n\n\n\n\n\nBy default, Cloudbreak uses the included default \nprewarmed images\n, which include the operating system, as well as\nAmbari and HDP/HDF packages installed. You can optionally select the \nbase image\n option if you would like to:\n\n\n\n\nUse an Ambari and HDP/HDF versions different than what the prewarmed image includes and/or  \n\n\nChoose a previously created custom base image\n\n\n\n\nChoose image\n  \n\n\nIf under \nChoose image catalog\n, you selected a custom image catalog, under \nChoose Image\n you can select an image from that catalog. For complete instructions, refer to \nUsing custom images\n. \n\n\nIf you are trying to customize Ambari and HDP/HDF versions, you can ignore the \nChoose Image\n option; in this case default base image is used.\n\n\nAmbari repository specification\n\n\nIf you would like to use a custom Ambari version, provide the following information: \n\n\n\n\n Ambari 2.6.1\n\n\nIf you would like to use Ambari \n2.6.1\n, use the version provided by default in the Cloudbreak web UI, or newer.\n\n\n\n\n\n\n\n\n\nParameter\n\n\nDescription\n\n\nExample\n\n\n\n\n\n\n\n\n\n\nVersion\n\n\nEnter Ambari version.\n\n\n2.6.1.3\n\n\n\n\n\n\nRepo Url\n\n\nProvide a URL to the Ambari version repo that you would like to use.\n\n\nhttp://public-repo-1.hortonworks.com/ambari/centos6/2.x/updates/2.6.1.3\n\n\n\n\n\n\nRepo Gpg Key Url\n\n\nProvide a URL to the repo GPG key. Each stable RPM package that is published by CentOS Project is signed with a GPG signature. By default, yum and the graphical update tools will verify these signatures and refuse to install any packages that are not signed, or have an incorrect signature.\n\n\nhttp://public-repo-1.hortonworks.com/ambari/centos6/RPM-GPG-KEY/RPM-GPG-KEY-Jenkins\n\n\n\n\n\n\n\n\nHDP or HDF repository specification\n\n\nIf you would like to use a custom HDP or HDF version, provide the following information: \n\n\n\n\n\n\n\n\nParameter\n\n\nDescription\n\n\nExample\n\n\n\n\n\n\n\n\n\n\nStack\n\n\nThis is populated by default based on the \"Platform Version\" parameter.\n\n\nHDP\n\n\n\n\n\n\nVersion\n\n\nThis is populated by default based on the \"Platform Version\" parameter.\n\n\n2.6\n\n\n\n\n\n\nOS\n\n\nOperating system.\n\n\ncentos7 (Azure, GCP, OpenStack) or amazonlinux (AWS)\n\n\n\n\n\n\nRepository Version\n\n\nEnter repository version.\n\n\n2.6.4.0-91\n\n\n\n\n\n\nVersion Definition File\n\n\nEnter the URL of the VDF file.\n\n\nhttp://public-repo-1.hortonworks.com/HDP/centos6/2.x/updates/2.6.4.0/HDP-2.6.4.0-91.xml\n\n\n\n\n\n\n(HDF only) MPack Url\n\n\n(HDF only) Provide MPack URL.\n\n\nhttp://public-repo-1.hortonworks.com/HDF/centos7/3.x/updates/3.1.1.0/tars/hdf_ambari_mp/hdf-ambari-mpack-3.1.1.0-35.tar.gz\n\n\n\n\n\n\nEnable Ambari Server to download and install GPL Licensed LZO packages?\n\n\n(Optional, only available if using Ambari 2.6.1.0 or newer) Use this option to enable LZO compression in your HDP/HDF cluster. LZO is a lossless data compression library that favors speed over compression ratio. Ambari does not install nor enable LZO compression libraries by default, and must be explicitly configured to do so. For more information, refer to \nEnabling LZO\n.\n\n\n\n\n\n\n\n\n\n\n\n\nIf you choose to use a base image with custom Ambari and/or HDP/HDF version, Cloudbreak validates the information entered. When Cloudbreak detects that the information entered is incorrect, it displays a warning marked with the \n sign. You should review all the warnings before proceeding and make sure that the information that you entered is correct. If you choose to proceed in spite of the warnings, check \"Ignore repository warnings\".  \n\n\n\n\nRelated links\n    \n\n\nUsing custom images\n      \n\n\nEnable lifetime management\n\n\nCheck this option if you would like your cluster to be automatically terminated after a specific amount of time (defined as \"Time to Live\" in minutes). \n\n\nTags\n\n\nYou can optionally add tags, which will help you find your cluster-related resources, such as VMs, in your cloud provider account. \n\n\nBy default, the following tags are created:\n\n\n\n\n\n\n\n\nTag\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\ncb-version\n\n\nCloudbreak version\n\n\n\n\n\n\nOwner\n\n\nYour Cloudbreak admin email.\n\n\n\n\n\n\ncb-account-name\n\n\nYour automatically generated Cloudbreak account name stored in the identity server.\n\n\n\n\n\n\ncb-user-name\n\n\nYour Cloudbreak admin email.\n\n\n\n\n\n\n\n\nFor more information, refer to \nTagging resources\n.\n\n\nRelated links\n    \n\n\nTagging resources\n \n\n\nStorage\n\n\nYou can optionally specify the following storage options for your cluster:\n\n\n\n\n\n\n\n\nParameter\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nStorage Type\n\n\nSelect the volume type. The options are:\nLocally-redundant storage\nGeo-redundant storage\nPremium locally-redundant storage\n For more information about these options refer to \nAzure documentation\n.\n\n\n\n\n\n\nAttached Volumes Per Instance\n\n\nEnter the number of volumes attached per instance. Default is 1.\n\n\n\n\n\n\nVolume Size (GB)\n\n\nEnter the size in GBs for each volume. Default is 100.\n\n\n\n\n\n\n\n\nAvailability sets\n\n\nTo support fault tolerance for VMs, Azure uses the concept of \navailability sets\n. This allows two or more VMs to be mapped to multiple fault domains, each of which defines a group of virtual machines that share a common power source and a network switch. When adding VMs to an availability set, Azure automatically assigns each VM a fault domain. The SLA includes guarantees that during OS Patching in Azure or during maintenance operations, at least one VM belonging to a given fault domain will be available.\n\n\nIn Cloudbreak, an availability set is automatically configured during cluster creation for each non-Ambari host group with \"Instance Count\" that is set to 2 or larger. The assignment of fault domains is automated by Azure, so there is no option for this in Cloudbreak UI. \n\n\nCloudbreak allows you to configure the availability set on the advanced \nHardware and Storage\n page of the create cluster wizard by providing the following options for each host group:  \n\n\n\n\n\n\n\n\nParameter\n\n\nDescription\n\n\nDefault\n\n\n\n\n\n\n\n\n\n\nAvailability Set Name\n\n\nChoose a name for the availability set that will be created for the selected host group\n\n\nThe following convention is used: \"clustername-hostgroupname-as\"\n\n\n\n\n\n\nFault Domain Count\n\n\nThe number of fault domains.\n\n\n2 or 3, depending on the setting supported by Azure\n\n\n\n\n\n\nUpdate Domain Count\n\n\nThis number of update domains. This can be set to a number in range of 2-20.\n\n\n20\n\n\n\n\n\n\n\n\nAfter the deployment is finished, you can check the layout of the VMs inside an availability set on Azure Portal. You will find the \"Availability set\" resources corresponding to the host groups inside the deployment's resource group.\n\n\nRecipes\n\n\nThis option allows you to select previously uploaded recipes (scripts that can be run pre or post cluster deployment) for each host group. For more information on recipes, refer to \nUsing custom scripts (recipes)\n. \n\n\nRelated links\n    \n\n\nUsing custom scripts (recipes)\n \n\n\nManagement Packs\n\n\nThis option allows you to select previously uploaded management packs. For more information on management packs, refer to \nUsing management packs\n. \n\n\nRelated links\n    \n\n\nUsing management packs\n  \n\n\nExternal sources\n\n\nYou can register external sources with Cloudbreak, and then select and attach them during cluster create. To register external sources with Cloudbreak, refer to:\n\n\n\n\nUsing an external authentication source\n    \n\n\nUsing an external database\n  \n\n\nRegister a proxy\n  \n\n\n\n\nDon't create public IP\n\n\nThis option is available if you are creating a cluster in an existing network and subnet. Select this option if you don't want to use public IPs for the network. \n\n\nDon't create new firewall rules\n\n\nThis option is available if you are creating a cluster in an existing network and subnet. Select this option if you don't want to create new firewall rules for the network. \n\n\nAmbari server master key\n\n\nThe Ambari server master key is used to configure Ambari to encrypt database and Kerberos credentials that are retained by Ambari as part of the Ambari setup.  \n\n\nEnable Azure disk encryption\n\n\nCheck this option if you would like to have your virtual machine disks encrypted using the Azure Disk Encryption capability provided by Azure. For more information, refer to \nAzure documentation\n.  \n\n\nEnable Kerberos security\n\n\nSelect this option to enable Kerberos for your cluster. For information about available Kerberos options, refer to \nEnabling Kerberos security\n. \n\n\nRelated links\n    \n\n\nEnabling Kerberos security\n \n\n\nIntroduction to Microsoft Azure storage\n (External)  \n\n\n\n\nNext: Access Cluster", 
            "title": "Create a cluster"
        }, 
        {
            "location": "/azure-create/index.html#creating-a-cluster-on-azure", 
            "text": "Use these steps to create a cluster.   Troubleshooting cluster creation  If you experience problems during cluster creation, refer to  Troubleshooting cluster creation .  Steps    Log in to the Cloudbreak UI.    Click  Create Cluster  and the  Create Cluster  wizard is displayed. \n    By default,  Basic  view is displayed. To view advanced options, click  Advanced . To learn about advanced options, refer to  Advanced cluster options .       On the  General Configuration  page, specify the following general parameters for your cluster:     Parameter  Description      Select Credential  Choose a previously created credential.    Cluster Name  Enter a name for your cluster. The name must be between 5 and 40 characters, must start with a letter, and must only include lowercase letters, numbers, and hyphens.    Region  Select the Azure region in which you would like to launch your cluster. For information on available Azure regions, refer to  Azure documentation .    Platform Version  Choose the HDP or HDF version to use for this cluster. Blueprints available for this platform version will be populated under \"Cluster Type\" below.    Cluster Type  Choose one of default cluster configurations, or, if you have defined your own cluster configuration via Ambari blueprint, you can choose it here. For more information on default and custom blueprints, refer to  Using custom blueprints .    Flex Subscription  This option will appear if you have configured your deployment for a  flex support subscription .       On the  Hardware and Storage  page, for each host group provide the following information to define your cluster nodes and attached storage:     Parameter  Description      Instance Type  Select an instance type. For information about instance types on Azure refer to  Azure documentation .    Instance Count  Enter the number of instances of a given type. Default is 1.    Ambari Server  You must select one node for Ambari Server. The \"Group Size\" for that host group must be set to \"1\".       On the  Network  page, provide the following to specify the networking resources that will be used for your cluster:     Parameter  Description      Select Network  Select the virtual network in which you would like your cluster to be provisioned. You can select an existing network or create a new network.    Select Subnet  Select the subnet in which you would like your cluster to be provisioned. If you are using a new network, create a new subnet. If you are using an existing network, select an existing subnet.    Subnet (CIDR)  If you selected to create a new subnet, you must define a valid  CIDR  for the subnet. Default is 10.0.0.0/16.      Cloudbreak uses public IP addresses when communicating with cluster nodes.       Define security groups for each host group. You can either create new security groups and define their rules or reuse existing security groups:     Option  Description      New Security Group  (Default) Creates a new security group with the rules that you defined: A set of  default rules  is provided. You should review and adjust these default rules. If you do not make any modifications, default rules will be applied.  You may open ports by defining the CIDR, entering port range, selecting protocol and clicking  + . You may delete default or previously added rules using the delete icon. If you don't want to use security group, remove the default rules.    Existing Security Groups  Allows you to select an existing security group that is already available in the selected provider region. This selection is disabled if no existing security groups are available in your chosen region.      Important  \nBy default, ports 22, 443, and 9443 are set to 0.0.0.0/0 CIDR for inbound access on the Ambari node security group. We strongly recommend that you limit this CIDR, considering the following restrictions: Ports 22 and 9443 must be open to Cloudbreak's CIDR. You can set CB_DEFAULT_GATEWAY_CIDR in your Cloudbreak's Profile file in order to automatically open ports 22 and 9443 to your Cloudbreak IP. Refer to  Restricting inbound access from Cloudbreak to cluster .  Port 22 must be open to your CIDR if you would like to access the master node via SSH.  Port 443 must be open to your CIDR if you would like to access Ambari web UI in a browser.      Important  \nBy default, port 22 is set to 0.0.0.0/0 CIDR for inbound access on non-Ambari node security groups. We strongly recommend that you remove it.    Important  \nDepending on what services you are including, you need to open additional ports as required by these services. For example, when using the Flow Management blueprint, you must open port 9091 for NiFi (on NiFI host group) and port 61443 for NiFI Registry (on the Services host group).      On the  Security  page, provide the following parameters:     Parameter  Description      Cluster User  You can log in to the Ambari UI using this username. By default, this is set to  admin .    Password  You can log in to the Ambari UI using this password.    Confirm Password  Confirm the password.    New SSH public key  Check this option to specify a new public key and then enter the public key. You will use the matching private key to access your cluster nodes via SSH.    Existing SSH public key  Select an existing public key. You will use the matching private key to access your cluster nodes via SSH. This is a default option as long as an existing SSH public key is available.       Click on  Create Cluster  to create a cluster.    You will be redirected to the Cloudbreak dashboard, and a new tile representing your cluster will appear at the top of the page.    Related links  Flex support subscription  Using custom blueprints    Default cluster security groups  Troubleshooting cluster creation    Azure regions  (External)     CIDR  (External)  General purpose Linux VM sizes  (External)", 
            "title": "Creating a cluster on Azure"
        }, 
        {
            "location": "/azure-create/index.html#advanced-cluster-options", 
            "text": "Click on  Advanced  to view and enter additional configuration options", 
            "title": "Advanced cluster options"
        }, 
        {
            "location": "/azure-create/index.html#choose-image-catalog", 
            "text": "By default,  Choose Image Catalog  is set to the default image catalog that is provided with Cloudbreak. If you would like to use a different image catalog, you must first create and register it. For complete instructions, refer to  Using custom images .  Related links      Using custom images", 
            "title": "Choose image catalog"
        }, 
        {
            "location": "/azure-create/index.html#prewarmed-and-base-images", 
            "text": "Cloudbreak supports the following types of images for launching clusters:     Image type  Description  Default images provided  Support for custom images      Base Images  Base images include default configuration and default tooling. These images include the operating system but do not include Ambari or HDP/HDF software.  Yes  Yes    Prewarmed Images  By default, Cloudbreak launches clusters from prewarmed images. Prewarmed images include the operating system as well as Ambari and HDP. The Ambari and HDP version used by prewarmed images cannot be customized. No prewarmed HDF images are currently provided.  Yes (HDP only)  No     By default, Cloudbreak uses the included default  prewarmed images , which include the operating system, as well as\nAmbari and HDP/HDF packages installed. You can optionally select the  base image  option if you would like to:   Use an Ambari and HDP/HDF versions different than what the prewarmed image includes and/or    Choose a previously created custom base image   Choose image     If under  Choose image catalog , you selected a custom image catalog, under  Choose Image  you can select an image from that catalog. For complete instructions, refer to  Using custom images .   If you are trying to customize Ambari and HDP/HDF versions, you can ignore the  Choose Image  option; in this case default base image is used.  Ambari repository specification  If you would like to use a custom Ambari version, provide the following information:     Ambari 2.6.1  If you would like to use Ambari  2.6.1 , use the version provided by default in the Cloudbreak web UI, or newer.     Parameter  Description  Example      Version  Enter Ambari version.  2.6.1.3    Repo Url  Provide a URL to the Ambari version repo that you would like to use.  http://public-repo-1.hortonworks.com/ambari/centos6/2.x/updates/2.6.1.3    Repo Gpg Key Url  Provide a URL to the repo GPG key. Each stable RPM package that is published by CentOS Project is signed with a GPG signature. By default, yum and the graphical update tools will verify these signatures and refuse to install any packages that are not signed, or have an incorrect signature.  http://public-repo-1.hortonworks.com/ambari/centos6/RPM-GPG-KEY/RPM-GPG-KEY-Jenkins     HDP or HDF repository specification  If you would like to use a custom HDP or HDF version, provide the following information:      Parameter  Description  Example      Stack  This is populated by default based on the \"Platform Version\" parameter.  HDP    Version  This is populated by default based on the \"Platform Version\" parameter.  2.6    OS  Operating system.  centos7 (Azure, GCP, OpenStack) or amazonlinux (AWS)    Repository Version  Enter repository version.  2.6.4.0-91    Version Definition File  Enter the URL of the VDF file.  http://public-repo-1.hortonworks.com/HDP/centos6/2.x/updates/2.6.4.0/HDP-2.6.4.0-91.xml    (HDF only) MPack Url  (HDF only) Provide MPack URL.  http://public-repo-1.hortonworks.com/HDF/centos7/3.x/updates/3.1.1.0/tars/hdf_ambari_mp/hdf-ambari-mpack-3.1.1.0-35.tar.gz    Enable Ambari Server to download and install GPL Licensed LZO packages?  (Optional, only available if using Ambari 2.6.1.0 or newer) Use this option to enable LZO compression in your HDP/HDF cluster. LZO is a lossless data compression library that favors speed over compression ratio. Ambari does not install nor enable LZO compression libraries by default, and must be explicitly configured to do so. For more information, refer to  Enabling LZO .       If you choose to use a base image with custom Ambari and/or HDP/HDF version, Cloudbreak validates the information entered. When Cloudbreak detects that the information entered is incorrect, it displays a warning marked with the   sign. You should review all the warnings before proceeding and make sure that the information that you entered is correct. If you choose to proceed in spite of the warnings, check \"Ignore repository warnings\".     Related links       Using custom images", 
            "title": "Prewarmed and base images"
        }, 
        {
            "location": "/azure-create/index.html#enable-lifetime-management", 
            "text": "Check this option if you would like your cluster to be automatically terminated after a specific amount of time (defined as \"Time to Live\" in minutes).", 
            "title": "Enable lifetime management"
        }, 
        {
            "location": "/azure-create/index.html#tags", 
            "text": "You can optionally add tags, which will help you find your cluster-related resources, such as VMs, in your cloud provider account.   By default, the following tags are created:     Tag  Description      cb-version  Cloudbreak version    Owner  Your Cloudbreak admin email.    cb-account-name  Your automatically generated Cloudbreak account name stored in the identity server.    cb-user-name  Your Cloudbreak admin email.     For more information, refer to  Tagging resources .  Related links       Tagging resources", 
            "title": "Tags"
        }, 
        {
            "location": "/azure-create/index.html#storage", 
            "text": "You can optionally specify the following storage options for your cluster:     Parameter  Description      Storage Type  Select the volume type. The options are: Locally-redundant storage Geo-redundant storage Premium locally-redundant storage  For more information about these options refer to  Azure documentation .    Attached Volumes Per Instance  Enter the number of volumes attached per instance. Default is 1.    Volume Size (GB)  Enter the size in GBs for each volume. Default is 100.", 
            "title": "Storage"
        }, 
        {
            "location": "/azure-create/index.html#availability-sets", 
            "text": "To support fault tolerance for VMs, Azure uses the concept of  availability sets . This allows two or more VMs to be mapped to multiple fault domains, each of which defines a group of virtual machines that share a common power source and a network switch. When adding VMs to an availability set, Azure automatically assigns each VM a fault domain. The SLA includes guarantees that during OS Patching in Azure or during maintenance operations, at least one VM belonging to a given fault domain will be available.  In Cloudbreak, an availability set is automatically configured during cluster creation for each non-Ambari host group with \"Instance Count\" that is set to 2 or larger. The assignment of fault domains is automated by Azure, so there is no option for this in Cloudbreak UI.   Cloudbreak allows you to configure the availability set on the advanced  Hardware and Storage  page of the create cluster wizard by providing the following options for each host group:       Parameter  Description  Default      Availability Set Name  Choose a name for the availability set that will be created for the selected host group  The following convention is used: \"clustername-hostgroupname-as\"    Fault Domain Count  The number of fault domains.  2 or 3, depending on the setting supported by Azure    Update Domain Count  This number of update domains. This can be set to a number in range of 2-20.  20     After the deployment is finished, you can check the layout of the VMs inside an availability set on Azure Portal. You will find the \"Availability set\" resources corresponding to the host groups inside the deployment's resource group.", 
            "title": "Availability sets"
        }, 
        {
            "location": "/azure-create/index.html#recipes", 
            "text": "This option allows you to select previously uploaded recipes (scripts that can be run pre or post cluster deployment) for each host group. For more information on recipes, refer to  Using custom scripts (recipes) .   Related links       Using custom scripts (recipes)", 
            "title": "Recipes"
        }, 
        {
            "location": "/azure-create/index.html#management-packs", 
            "text": "This option allows you to select previously uploaded management packs. For more information on management packs, refer to  Using management packs .   Related links       Using management packs", 
            "title": "Management Packs"
        }, 
        {
            "location": "/azure-create/index.html#external-sources", 
            "text": "You can register external sources with Cloudbreak, and then select and attach them during cluster create. To register external sources with Cloudbreak, refer to:   Using an external authentication source       Using an external database     Register a proxy", 
            "title": "External sources"
        }, 
        {
            "location": "/azure-create/index.html#dont-create-public-ip", 
            "text": "This option is available if you are creating a cluster in an existing network and subnet. Select this option if you don't want to use public IPs for the network.", 
            "title": "Don't create public IP"
        }, 
        {
            "location": "/azure-create/index.html#dont-create-new-firewall-rules", 
            "text": "This option is available if you are creating a cluster in an existing network and subnet. Select this option if you don't want to create new firewall rules for the network.", 
            "title": "Don't create new firewall rules"
        }, 
        {
            "location": "/azure-create/index.html#ambari-server-master-key", 
            "text": "The Ambari server master key is used to configure Ambari to encrypt database and Kerberos credentials that are retained by Ambari as part of the Ambari setup.", 
            "title": "Ambari server master key"
        }, 
        {
            "location": "/azure-create/index.html#enable-azure-disk-encryption", 
            "text": "Check this option if you would like to have your virtual machine disks encrypted using the Azure Disk Encryption capability provided by Azure. For more information, refer to  Azure documentation .", 
            "title": "Enable Azure disk encryption"
        }, 
        {
            "location": "/azure-create/index.html#enable-kerberos-security", 
            "text": "Select this option to enable Kerberos for your cluster. For information about available Kerberos options, refer to  Enabling Kerberos security .   Related links       Enabling Kerberos security    Introduction to Microsoft Azure storage  (External)     Next: Access Cluster", 
            "title": "Enable Kerberos security"
        }, 
        {
            "location": "/azure-clusters-access/index.html", 
            "text": "Accessing a cluster\n\n\nThe following section describes how to access the various services in the cluster.\n\n\nCloudbreak user accounts\n\n\nThe following table describes what credentials to use to access Cloudbreak and Cloudbreak-managed clusters:\n\n\n\n\n\n\n\n\nComponent\n\n\nMethod\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nCloudbreak\n\n\nWeb UI, CLI\n\n\nAccess with the username and password provided when launching Cloudbreak on the cloud provider.\n\n\n\n\n\n\nCloudbreak\n\n\nSSH to VM\n\n\nAccess as the \"cloudbreak\" user with the SSH key provided when launching Cloudbreak on the cloud provider.\n\n\n\n\n\n\nCluster\n\n\nSSH to VMs\n\n\nAccess as the \"cloudbreak\" user with the SSH key provided during cluster creation.\n\n\n\n\n\n\nCluster\n\n\nAmbari web UI\n\n\nAccess with the credentials provided in the \u201cCluster User\u201d parameter during cluster creation.\n\n\n\n\n\n\nCluster\n\n\nWeb UIs for specific cluster services\n\n\nAccess with the credentials provided in the \u201cCluster User\u201d parameter during cluster creation.\n\n\n\n\n\n\n\n\nFinding cluster information in the web UI\n\n\nOnce your cluster is up and running, click on the tile representing your cluster in the Cloudbreak UI to access information related the cluster and access cluster actions. \n\n\n \n\n\nThe information presented includes:\n\n\n\n\nCluster summary\n  \n\n\nCluster information\n     \n\n\nEvent history\n  \n\n\n\n\n\n  \nTips\n\n  \n\n  \n Access cluster actions such as resize and sync by clicking on \nACTIONS\n.\n\n  \n Access Ambari web UI by clicking on the link in the \nCLUSTER INFORMATION\n section.\n\n\n View public IP addresses for all cluster instances in the \nHARDWARE\n section. Click on the links to view the instances in the cloud console.\n\n\n The SSH user that you must use when accessing cluster VMs is \"cloudbreak\".\n \n\n\n\n\n\n\n\n\nCluster summary\n\n\nThe summary bar includes the following information about your cluster:\n\n\n\n\n\n\n\n\nItem\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nCluster Name\n\n\nThe name that you selected for your cluster is displayed at the top of the page. Below it is the name of the cluster blueprint.\n\n\n\n\n\n\nTime Remaining\n\n\nIf you enabled lifetime management for your cluster, the clock next to the cluster name indicates the amount of time that your cluster will run before it gets terminated. Note that the time remaining counter does not stop when you stop the cluster.\n\n\n\n\n\n\nCloud Provider\n\n\nThe logo of the cloud provider on which the cluster is running.\n\n\n\n\n\n\nCredential\n\n\nThe name of the credential used to create the cluster.\n\n\n\n\n\n\nStatus\n\n\nCurrent status. When a cluster is healthy, the status is \nRunning\n.\n\n\n\n\n\n\nNodes\n\n\nThe current number of cluster nodes, including the master node.\n\n\n\n\n\n\nUptime\n\n\nThe amount of time (HH:MM) that the cluster has been in the running state since it was started. Each time you stop and restart the cluster, the running time is reset to 0.\n\n\n\n\n\n\nCreated\n\n\nThe date when the cluster was created. The date format is Mon DD, YYYY. For example: Oct 27, 2017.\n\n\n\n\n\n\n\n\nCluster information\n\n\nThe following information is available on the cluster details page: \n\n\n\n\n\n\n\n\nItem\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nCluster User\n\n\nThe name of the cluster user that you created when creating the cluster.\n\n\n\n\n\n\nSSH Username\n\n\nThe SSH user which you must use when accessing cluster VMs via SSH. The SSH user is always \"cloudbreak\".\n\n\n\n\n\n\nAmbari URL\n\n\nLink to the Ambari web UI.\n\n\n\n\n\n\nRegion\n\n\nThe region in which the cluster is running in the cloud provider infrastructure.\n\n\n\n\n\n\nAvailability Zone\n\n\nThe availability zone within the region in which the cluster is running.\n\n\n\n\n\n\nBlueprint\n\n\nThe name of the blueprint selected under \"Cluster Type\" to create this cluster.\n\n\n\n\n\n\nCreated With\n\n\nThe version of Cloudbreak used to create this cluster.\n\n\n\n\n\n\nAmbari Version\n\n\nThe Ambari version which this cluster is currently running.\n\n\n\n\n\n\nHDP/HDF Version\n\n\nThe HDP or HDF version which this cluster is currently running.\n\n\n\n\n\n\nAuthentication Source\n\n\nIf you are using an external authentication source (LDAP/AD) for your cluster, you can see it here. Refer to \nUsing an external authentication source\n.\n\n\n\n\n\n\n\n\nBelow this, you will see additional tabs that you can click on in order to see their content:\n\n\n\n\n\n\n\n\nItem\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nHardware\n\n\nThis section includes information about your cluster instances: instance names, instance IDs, instance types, their status, fully qualified domain names (FQDNs), and private and public IPs.\n\n\n\n\n\n\nTags\n\n\nThis section lists keys and values of the user-defined tags, in the same order as you added them.\n\n\n\n\n\n\nRecipes\n\n\nThis section includes recipe-related information. For each recipe, you can see the host group on which a recipe was executed, recipe name, and recipe type. Refer to \nUsing custom scripts (recipes)\n.\n\n\n\n\n\n\nExternal Databases\n\n\nIf you are using an external database for your cluster, you can see it here. Refer to \nusing an external database\n.\n\n\n\n\n\n\nRepository Details\n\n\nThis section includes Ambari and HDP/HDF repository information, as you provided it in the \"Base Images\" section when creating a cluster.\n\n\n\n\n\n\nImage Details\n\n\nThis section includes information about the base image that was used for the Cloudbreak instance.\n\n\n\n\n\n\nNetwork\n\n\nThis section includes information about the names of the network and subnet in which the cluster is running and the links to related cloud provider console.\n\n\n\n\n\n\nSecurity\n\n\nThis section is only available if you have enabled Kerberos security. It provides you with the details of your Kerberos configuration.\n\n\n\n\n\n\nAutoscaling\n\n\nThis section includes configuration options related to autoscaling. Refer to \nConfiguring autoscaling\n.\n\n\n\n\n\n\n\n\nEvent history\n\n\nThe Event History section shows you events logged for the cluster, with the most recent event at the top. For example, after your cluster has been created, the following messages will be written to the log:\n\n\n\nAmbari cluster built; Ambari ip:34.215.103.66\n10/26/2017, 9:41:58 AM\nBuilding Ambari cluster; Ambari ip:34.215.103.66\n10/26/2017, 9:30:20 AM\nStarting Ambari cluster services\n10/26/2017, 9:27:12 AM\nSetting up infrastructure metadata\n10/26/2017, 9:27:11 AM\nBootstrapping infrastructure cluster\n10/26/2017, 9:26:38 AM\nInfrastructure successfully provisioned\n10/26/2017, 9:26:37 AM\nBilling started, Infrastructure successfully provisioned\n10/26/2017, 9:26:37 AM\nInfrastructure metadata collection finished\n10/26/2017, 9:25:39 AM\nInfrastructure creation took 194 seconds\n10/26/2017, 9:25:37 AM\nCreating infrastructure\n10/26/2017, 9:22:22 AM\nSetting up HDP image\n10/26/2017, 9:22:21 AM\n\n\n\nAccessing cluster via SSH\n\n\nIf you plan to access the cluster via the command line clients, SSH into the master node instance in the cluster. \n\n\n\n\nIn order to use SSH, you must generate an SSH key pair or use an existing SSH key pair.  \n\n\nYou can find the cluster instance public IP addresses on the cluster details page.  \n\n\nWhen accessing instances via SSH use the \ncloudbreak\n user. \n\n\n\n\nOn Mac OS, you can use the following syntax to SSH to the VM:\n\nssh -i \"privatekey.pem\" cloudbreak@publicIP\n\nFor example:\n\nssh -i \"dominika-kp.pem\" cloudbreak@p52.25.169.132\n\n\nOn Windows, you can SSH using an SSH client such as PuTTY.\n\n\nAccess Ambari\n\n\nYou can access Ambari web UI by clicking on the links provided in the \nCluster Information\n \n \nAmbari URL\n.\n\n\nSteps\n\n\n\n\n\n\nFrom the cluster dashboard, click on the tile representing your cluster to navigate to cluster details.\n\n\n\n\n\n\nFind the Ambari URL in the \nCluster Information\n section. This URL is available once the Ambari cluster creation process has completed.  \n\n\n\n\n\n\nClick on the \nAmbari URL\n link.\n\n\n\n\n\n\nThe first time you access the server, your browser will attempt to confirm that the SSL Certificate is valid. Since Cloudbreak automatically generates a self-signed certificate, your browser will warn you about an Untrusted Connection and ask you to confirm a Security Exception. Depending on your browser, perform the steps below to proceed.\n\n\n\n\n\n\n\n\nBrowser\n\n\nSteps\n\n\n\n\n\n\n\n\n\n\nFirefox\n\n\nClick \nAdvanced\n \n Click \nAdd Exception...\n \n Click \nConfirm Security Exception\n\n\n\n\n\n\nSafari\n\n\nClick \nContinue\n\n\n\n\n\n\nChrome\n\n\nClick \nAdvanced\n \n Click \nProceed...\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNext: Manage and Monitor Clusters", 
            "title": "Access cluster"
        }, 
        {
            "location": "/azure-clusters-access/index.html#accessing-a-cluster", 
            "text": "The following section describes how to access the various services in the cluster.", 
            "title": "Accessing a cluster"
        }, 
        {
            "location": "/azure-clusters-access/index.html#cloudbreak-user-accounts", 
            "text": "The following table describes what credentials to use to access Cloudbreak and Cloudbreak-managed clusters:     Component  Method  Description      Cloudbreak  Web UI, CLI  Access with the username and password provided when launching Cloudbreak on the cloud provider.    Cloudbreak  SSH to VM  Access as the \"cloudbreak\" user with the SSH key provided when launching Cloudbreak on the cloud provider.    Cluster  SSH to VMs  Access as the \"cloudbreak\" user with the SSH key provided during cluster creation.    Cluster  Ambari web UI  Access with the credentials provided in the \u201cCluster User\u201d parameter during cluster creation.    Cluster  Web UIs for specific cluster services  Access with the credentials provided in the \u201cCluster User\u201d parameter during cluster creation.", 
            "title": "Cloudbreak user accounts"
        }, 
        {
            "location": "/azure-clusters-access/index.html#finding-cluster-information-in-the-web-ui", 
            "text": "Once your cluster is up and running, click on the tile representing your cluster in the Cloudbreak UI to access information related the cluster and access cluster actions.      The information presented includes:   Cluster summary     Cluster information        Event history      \n   Tips \n   \n    Access cluster actions such as resize and sync by clicking on  ACTIONS . \n    Access Ambari web UI by clicking on the link in the  CLUSTER INFORMATION  section.   View public IP addresses for all cluster instances in the  HARDWARE  section. Click on the links to view the instances in the cloud console.   The SSH user that you must use when accessing cluster VMs is \"cloudbreak\".", 
            "title": "Finding cluster information in the web UI"
        }, 
        {
            "location": "/azure-clusters-access/index.html#cluster-summary", 
            "text": "The summary bar includes the following information about your cluster:     Item  Description      Cluster Name  The name that you selected for your cluster is displayed at the top of the page. Below it is the name of the cluster blueprint.    Time Remaining  If you enabled lifetime management for your cluster, the clock next to the cluster name indicates the amount of time that your cluster will run before it gets terminated. Note that the time remaining counter does not stop when you stop the cluster.    Cloud Provider  The logo of the cloud provider on which the cluster is running.    Credential  The name of the credential used to create the cluster.    Status  Current status. When a cluster is healthy, the status is  Running .    Nodes  The current number of cluster nodes, including the master node.    Uptime  The amount of time (HH:MM) that the cluster has been in the running state since it was started. Each time you stop and restart the cluster, the running time is reset to 0.    Created  The date when the cluster was created. The date format is Mon DD, YYYY. For example: Oct 27, 2017.", 
            "title": "Cluster summary"
        }, 
        {
            "location": "/azure-clusters-access/index.html#cluster-information", 
            "text": "The following information is available on the cluster details page:      Item  Description      Cluster User  The name of the cluster user that you created when creating the cluster.    SSH Username  The SSH user which you must use when accessing cluster VMs via SSH. The SSH user is always \"cloudbreak\".    Ambari URL  Link to the Ambari web UI.    Region  The region in which the cluster is running in the cloud provider infrastructure.    Availability Zone  The availability zone within the region in which the cluster is running.    Blueprint  The name of the blueprint selected under \"Cluster Type\" to create this cluster.    Created With  The version of Cloudbreak used to create this cluster.    Ambari Version  The Ambari version which this cluster is currently running.    HDP/HDF Version  The HDP or HDF version which this cluster is currently running.    Authentication Source  If you are using an external authentication source (LDAP/AD) for your cluster, you can see it here. Refer to  Using an external authentication source .     Below this, you will see additional tabs that you can click on in order to see their content:     Item  Description      Hardware  This section includes information about your cluster instances: instance names, instance IDs, instance types, their status, fully qualified domain names (FQDNs), and private and public IPs.    Tags  This section lists keys and values of the user-defined tags, in the same order as you added them.    Recipes  This section includes recipe-related information. For each recipe, you can see the host group on which a recipe was executed, recipe name, and recipe type. Refer to  Using custom scripts (recipes) .    External Databases  If you are using an external database for your cluster, you can see it here. Refer to  using an external database .    Repository Details  This section includes Ambari and HDP/HDF repository information, as you provided it in the \"Base Images\" section when creating a cluster.    Image Details  This section includes information about the base image that was used for the Cloudbreak instance.    Network  This section includes information about the names of the network and subnet in which the cluster is running and the links to related cloud provider console.    Security  This section is only available if you have enabled Kerberos security. It provides you with the details of your Kerberos configuration.    Autoscaling  This section includes configuration options related to autoscaling. Refer to  Configuring autoscaling .", 
            "title": "Cluster information"
        }, 
        {
            "location": "/azure-clusters-access/index.html#event-history", 
            "text": "The Event History section shows you events logged for the cluster, with the most recent event at the top. For example, after your cluster has been created, the following messages will be written to the log:  \nAmbari cluster built; Ambari ip:34.215.103.66\n10/26/2017, 9:41:58 AM\nBuilding Ambari cluster; Ambari ip:34.215.103.66\n10/26/2017, 9:30:20 AM\nStarting Ambari cluster services\n10/26/2017, 9:27:12 AM\nSetting up infrastructure metadata\n10/26/2017, 9:27:11 AM\nBootstrapping infrastructure cluster\n10/26/2017, 9:26:38 AM\nInfrastructure successfully provisioned\n10/26/2017, 9:26:37 AM\nBilling started, Infrastructure successfully provisioned\n10/26/2017, 9:26:37 AM\nInfrastructure metadata collection finished\n10/26/2017, 9:25:39 AM\nInfrastructure creation took 194 seconds\n10/26/2017, 9:25:37 AM\nCreating infrastructure\n10/26/2017, 9:22:22 AM\nSetting up HDP image\n10/26/2017, 9:22:21 AM", 
            "title": "Event history"
        }, 
        {
            "location": "/azure-clusters-access/index.html#accessing-cluster-via-ssh", 
            "text": "If you plan to access the cluster via the command line clients, SSH into the master node instance in the cluster.    In order to use SSH, you must generate an SSH key pair or use an existing SSH key pair.    You can find the cluster instance public IP addresses on the cluster details page.    When accessing instances via SSH use the  cloudbreak  user.    On Mac OS, you can use the following syntax to SSH to the VM: ssh -i \"privatekey.pem\" cloudbreak@publicIP \nFor example: ssh -i \"dominika-kp.pem\" cloudbreak@p52.25.169.132  On Windows, you can SSH using an SSH client such as PuTTY.", 
            "title": "Accessing cluster via SSH"
        }, 
        {
            "location": "/azure-clusters-access/index.html#access-ambari", 
            "text": "You can access Ambari web UI by clicking on the links provided in the  Cluster Information     Ambari URL .  Steps    From the cluster dashboard, click on the tile representing your cluster to navigate to cluster details.    Find the Ambari URL in the  Cluster Information  section. This URL is available once the Ambari cluster creation process has completed.      Click on the  Ambari URL  link.    The first time you access the server, your browser will attempt to confirm that the SSL Certificate is valid. Since Cloudbreak automatically generates a self-signed certificate, your browser will warn you about an Untrusted Connection and ask you to confirm a Security Exception. Depending on your browser, perform the steps below to proceed.     Browser  Steps      Firefox  Click  Advanced    Click  Add Exception...    Click  Confirm Security Exception    Safari  Click  Continue    Chrome  Click  Advanced    Click  Proceed...        Next: Manage and Monitor Clusters", 
            "title": "Access Ambari"
        }, 
        {
            "location": "/azure-clusters-manage/index.html", 
            "text": "Managing and monitoring clusters\n\n\nYou can manage monitor your clusters from the Cloudbreak UI. To do that, click on the tile representing the cluster that you want to access. The actions available for your cluster are listed in the top right corner: \n\n\n \n\n\n\n  \nTips\n\n  \n\n  \nTo add or remove nodes from your cluster click \nACTIONS>Resize\n.\n\n  \nTo synchronize your cluster with the cloud provider account click \nACTIONS>Sync\n.\n\n  \nTo temporarily stop your cluster click \nSTOP\n.\n\n  \nTo terminate your cluster click \nTERMINATE\n.\n\n\n\n\n\n\n\n\n\nRetry a cluster\n\n\nWhen a stack provisioning or cluster creation failure occurs, the \"retry\" option allows you to resume the process from the last failed step. \n\n\nIn some cases the cause of a failed stack provisioning or cluster creation can be eliminated by simply retrying the process. For example, in case of a temporary network outage, a retry should be successful. In other cases, a manual modification is required before a retry can succeed. For example, if you are using a custom image but some configuration is missing, causing the process to fail, you must log in to the machine and fix the issue; Only after that you can retry the the process.\n\n\nOnly failed stack or cluster creations can be retried. A retry can be initiated any number of times on a failed creation process. \n\n\nTo retry provisioning a failed stack or cluster, follow these steps.  \n\n\nSteps\n\n\n\n\n\n\nBrowse to the cluster details.\n\n\n\n\n\n\nClick \nActions\n and select \nRetry\n. \n\n\nOnly failed stack or cluster creations can be retried, so the option is only available in these two cases.  \n\n\n\n\n\n\nClick \nYes\n to confirm. \n\n\nThe operation continues from the last failed step. \n\n\n\n\n\n\nResize a cluster\n\n\nTo resize a cluster, follow these steps.\n\n\n\n\nCluster resizing is not supported for HDF clusters.\n\nTo configure automated cluster scaling, refer to \nConfigure autoscaling\n.   \n\n\n\n\nSteps\n\n\n\n\n\n\nBrowse to the cluster details.\n\n\n\n\n\n\nClick \nActions\n and select \nResize\n. The cluster resize dialog is displayed.\n\n\n\n\n\n\nUsing the +/- controls, adjust the number of nodes for a chosen host group. \n\n\n\n\nYou can only modify one host group at a time. \n\nIt is not possible to resize the Ambari server host group.     \n\n\n\n\n\n\n\n\nClick \nYes\n to confirm the scale-up/scale-down.\n\n\nWhile nodes are being added or removed, cluster status changes to \"Update In Progress\". Once the operation has completed, cluster status changes back to \"Running\". \n\n\n\n\n\n\nSynchronize a cluster\n\n\nUse the \nsync\n option if you:  \n\n\n\n\nMade changes on your cloud provider side (for example, deleted cluster VMs) and you would like to synchronize Cloudbreak with the cloud provider.  \n\n\nManually changed service status in Ambari (for example, restarted services).   \n\n\n\n\nTo synchronize your cluster with the cloud provider, follow these steps. \n\n\nSteps\n\n\n\n\n\n\nBrowse to the cluster details.\n\n\n\n\n\n\nClick \nActions\n and select \nSync\n.\n\n\n\n\n\n\nClick \nYes\n to confirm.\n\n\nYour cluster infrastructure is synchronized based on changes on the cloud provider. The updates are written to the \"Event History\". \n\n\n\n\n\n\nStop a cluster\n\n\nCloudbreak supports stopping and restarting clusters. To stop and restart a cluster managed by Cloudbreak, use the options available from the Cloudbreak UI. \n\n\nSteps\n\n\n\n\n\n\nBrowse to the cluster details.\n\n\n\n\n\n\nClick \nStop\n to stop a currently running cluster.  \n\n\n\n\n\n\nClick \nYes\n to confirm. \n\n\n\n\n\n\nYour cluster status changes to \"Stopping in progress\" and then to \"Stopped\". Once stopping the infrastructure has completed, you will see a \nStart\n option to restart your cluster. \n\n\n\n\n\n\nWhen a cluster is in the \"stopped\" state, you are not charged for the VMs, but you are charged for external storage.  \n\n\nRestart a cluster\n\n\nIf your cluster is in the \"Stopped\" state, you can restart the cluster by follow these steps.\n\n\nSteps\n\n\n\n\n\n\nclick \nStart\n. This option is only available when the cluster has been stopped. \n\n\n\n\n\n\nClick \nYes\n to confirm.\n\n\nYour cluster status changes to \"Start in progress\" and then to \"Running\". \n\n\n\n\n\n\nTerminate a cluster\n\n\nTo terminate a cluster managed by Cloudbreak, use the option available from the Cloudbreak UI. \n\n\nSteps\n\n\n\n\n\n\nBrowse to the cluster details.\n\n\n\n\n\n\nClick \nTerminate\n. \n\n\n\n\n\n\nClick \nYes\n to confirm.\n\n\nAll cluster-related resources will be deleted, unless the resources (such as networks and subnets) existed prior to cluster creation or are used by other VMs in which case they will be preserved. \n\n\n\n\n\n\nForce terminate a cluster\n\n\nCluster deletion may fail if Cloudbreak is unable to delete one or more of the cloud resources that were part of your cluster infrastructure. In such as case, you can use the \nTerminate\n \n \nForce terminate\n option to remove the cluster entry from the Cloudbreak web UI, but you must also check your cloud provider account to see if there are any resources that must be deleted manually.\n\n\nSteps\n\n\n\n\n\n\nBrowse to the cluster details.\n\n\n\n\n\n\nClick \nTerminate\n. \n\n\n\n\n\n\nCheck  \nForce terminate\n.\n\n\n\n\n\n\nClick \nYes\n to confirm. \n\n\nWhen terminating a cluster with Kerberos enabled, you have an option to disable Kerberos prior to cluster termination. This option removes any cluster-related principals from the KDC.\n\n\n\n\n\n\nThis deletes the cluster tile from the UI.  \n\n\n\n\n\n\nLog in to your cloud provider account and \nmanually delete\n any resources that failed to be deleted.\n\n\n\n\n\n\nView cluster history\n\n\nFrom the navigation menu in the Cloudbreak UI, you can access the History page that allows you to generate a report showing basic information related to the clusters that were running within the specified range of dates.\n\n\nTo generate a report, follow these steps.\n\n\nSteps\n\n\n\n\n\n\nFrom the Cloudbreak UI navigation menu, select \nHistory\n.\n\n\n\n\n\n\nOn the History page, select the range of dates and click \nShow History\n to generate a tabular report for the selected period.\n\n\n\n\n\n\nHistory report content\n\n\nEach entry in the report represents one cluster instance group. For each entry, the report includes the following information:\n\n\n\n\nCreated\n - The date when your cluster was created (YYYY-MM-DD).\n\n\nProvider\n - The name of the cloud provider (AWS, Azure, Google, or OpenStack) on which the cluster instances are/were running.\n\n\nCluster Name\n - The name that you selected for the cluster.  \n\n\nInstance Group\n - The name of the host group.   \n\n\nInstance Count\n - The number of nodes in the host group. This number may be a decimal if a cluster has been resized.\n\n\nInstance Type\n - Provider-specific VM type of the cluster instances. \n\n\nRegion\n - The AWS region in which your cluster is/was running.\n\n\nAvailability Zone\n - The availability zone in which your cluster is/was running.      \n\n\nRunning Time (hours)\n - The sum of the running times for all the nodes in the instance group.\n\n\n\n\nThe \nAGGREGATE RUNNING TIME\n is the sum of the Running Times, adjusted for the selected time range.\n\n\nTo learn about how your cloud provider bills you for the VMs, refer to their documentation:\n\n\n\n\nAWS\n      \n\n\nAzure\n     \n\n\nGCP\n   \n\n\n\n\n\n\nNext: Access Cloud Data", 
            "title": "Manage and monitor clusters"
        }, 
        {
            "location": "/azure-clusters-manage/index.html#managing-and-monitoring-clusters", 
            "text": "You can manage monitor your clusters from the Cloudbreak UI. To do that, click on the tile representing the cluster that you want to access. The actions available for your cluster are listed in the top right corner:      \n   Tips \n   \n   To add or remove nodes from your cluster click  ACTIONS>Resize . \n   To synchronize your cluster with the cloud provider account click  ACTIONS>Sync . \n   To temporarily stop your cluster click  STOP . \n   To terminate your cluster click  TERMINATE .", 
            "title": "Managing and monitoring clusters"
        }, 
        {
            "location": "/azure-clusters-manage/index.html#retry-a-cluster", 
            "text": "When a stack provisioning or cluster creation failure occurs, the \"retry\" option allows you to resume the process from the last failed step.   In some cases the cause of a failed stack provisioning or cluster creation can be eliminated by simply retrying the process. For example, in case of a temporary network outage, a retry should be successful. In other cases, a manual modification is required before a retry can succeed. For example, if you are using a custom image but some configuration is missing, causing the process to fail, you must log in to the machine and fix the issue; Only after that you can retry the the process.  Only failed stack or cluster creations can be retried. A retry can be initiated any number of times on a failed creation process.   To retry provisioning a failed stack or cluster, follow these steps.    Steps    Browse to the cluster details.    Click  Actions  and select  Retry .   Only failed stack or cluster creations can be retried, so the option is only available in these two cases.      Click  Yes  to confirm.   The operation continues from the last failed step.", 
            "title": "Retry a cluster"
        }, 
        {
            "location": "/azure-clusters-manage/index.html#resize-a-cluster", 
            "text": "To resize a cluster, follow these steps.   Cluster resizing is not supported for HDF clusters. \nTo configure automated cluster scaling, refer to  Configure autoscaling .      Steps    Browse to the cluster details.    Click  Actions  and select  Resize . The cluster resize dialog is displayed.    Using the +/- controls, adjust the number of nodes for a chosen host group.    You can only modify one host group at a time.  \nIt is not possible to resize the Ambari server host group.          Click  Yes  to confirm the scale-up/scale-down.  While nodes are being added or removed, cluster status changes to \"Update In Progress\". Once the operation has completed, cluster status changes back to \"Running\".", 
            "title": "Resize a cluster"
        }, 
        {
            "location": "/azure-clusters-manage/index.html#synchronize-a-cluster", 
            "text": "Use the  sync  option if you:     Made changes on your cloud provider side (for example, deleted cluster VMs) and you would like to synchronize Cloudbreak with the cloud provider.    Manually changed service status in Ambari (for example, restarted services).      To synchronize your cluster with the cloud provider, follow these steps.   Steps    Browse to the cluster details.    Click  Actions  and select  Sync .    Click  Yes  to confirm.  Your cluster infrastructure is synchronized based on changes on the cloud provider. The updates are written to the \"Event History\".", 
            "title": "Synchronize a cluster"
        }, 
        {
            "location": "/azure-clusters-manage/index.html#stop-a-cluster", 
            "text": "Cloudbreak supports stopping and restarting clusters. To stop and restart a cluster managed by Cloudbreak, use the options available from the Cloudbreak UI.   Steps    Browse to the cluster details.    Click  Stop  to stop a currently running cluster.      Click  Yes  to confirm.     Your cluster status changes to \"Stopping in progress\" and then to \"Stopped\". Once stopping the infrastructure has completed, you will see a  Start  option to restart your cluster.     When a cluster is in the \"stopped\" state, you are not charged for the VMs, but you are charged for external storage.", 
            "title": "Stop a cluster"
        }, 
        {
            "location": "/azure-clusters-manage/index.html#restart-a-cluster", 
            "text": "If your cluster is in the \"Stopped\" state, you can restart the cluster by follow these steps.  Steps    click  Start . This option is only available when the cluster has been stopped.     Click  Yes  to confirm.  Your cluster status changes to \"Start in progress\" and then to \"Running\".", 
            "title": "Restart a cluster"
        }, 
        {
            "location": "/azure-clusters-manage/index.html#terminate-a-cluster", 
            "text": "To terminate a cluster managed by Cloudbreak, use the option available from the Cloudbreak UI.   Steps    Browse to the cluster details.    Click  Terminate .     Click  Yes  to confirm.  All cluster-related resources will be deleted, unless the resources (such as networks and subnets) existed prior to cluster creation or are used by other VMs in which case they will be preserved.", 
            "title": "Terminate a cluster"
        }, 
        {
            "location": "/azure-clusters-manage/index.html#force-terminate-a-cluster", 
            "text": "Cluster deletion may fail if Cloudbreak is unable to delete one or more of the cloud resources that were part of your cluster infrastructure. In such as case, you can use the  Terminate     Force terminate  option to remove the cluster entry from the Cloudbreak web UI, but you must also check your cloud provider account to see if there are any resources that must be deleted manually.  Steps    Browse to the cluster details.    Click  Terminate .     Check   Force terminate .    Click  Yes  to confirm.   When terminating a cluster with Kerberos enabled, you have an option to disable Kerberos prior to cluster termination. This option removes any cluster-related principals from the KDC.    This deletes the cluster tile from the UI.      Log in to your cloud provider account and  manually delete  any resources that failed to be deleted.", 
            "title": "Force terminate a cluster"
        }, 
        {
            "location": "/azure-clusters-manage/index.html#view-cluster-history", 
            "text": "From the navigation menu in the Cloudbreak UI, you can access the History page that allows you to generate a report showing basic information related to the clusters that were running within the specified range of dates.  To generate a report, follow these steps.  Steps    From the Cloudbreak UI navigation menu, select  History .    On the History page, select the range of dates and click  Show History  to generate a tabular report for the selected period.", 
            "title": "View cluster history"
        }, 
        {
            "location": "/azure-clusters-manage/index.html#history-report-content", 
            "text": "Each entry in the report represents one cluster instance group. For each entry, the report includes the following information:   Created  - The date when your cluster was created (YYYY-MM-DD).  Provider  - The name of the cloud provider (AWS, Azure, Google, or OpenStack) on which the cluster instances are/were running.  Cluster Name  - The name that you selected for the cluster.    Instance Group  - The name of the host group.     Instance Count  - The number of nodes in the host group. This number may be a decimal if a cluster has been resized.  Instance Type  - Provider-specific VM type of the cluster instances.   Region  - The AWS region in which your cluster is/was running.  Availability Zone  - The availability zone in which your cluster is/was running.        Running Time (hours)  - The sum of the running times for all the nodes in the instance group.   The  AGGREGATE RUNNING TIME  is the sum of the Running Times, adjusted for the selected time range.  To learn about how your cloud provider bills you for the VMs, refer to their documentation:   AWS         Azure        GCP        Next: Access Cloud Data", 
            "title": "History report content"
        }, 
        {
            "location": "/azure-data/index.html", 
            "text": "Accessing data on Azure\n\n\nHortonworks Data Platform (HDP) supports reading and writing both block blobs and page blobs\nfrom/to \nWindows Azure Storage Blob (WASB)\n object store, as well as reading and writing files stored in an\n\nAzure Data Lake Storage (ADLS)\n account. \n\n\nAccessing data in ADLS from HDP\n\n\nAzure Data Lake Store (ADLS)\n is an enterprise-wide hyper-scale repository for big data analytic workloads.\n\n\nPrerequisites\n\n\nIf you want to use ADLS to store your data, you must enable Azure subscription for Data Lake Store, and then create an Azure Data Lake Store \nstorage account\n.\n\n\nConfigure access to ADLS\n\n\nADLS is not supported as a default file system, but access to data in ADLS via the adl connector. To configure access to ADLS from a cluster managed via Cloudbreak use the steps described in \nHow to configure authentication with ADLS\n.\n\n\nTest access to ADLS\n\n\nTo tests access to ADLS, SSH to a cluster node and run a few hadoop fs shell commands against your existing ADLS account.\n\n\nADLS access path syntax is:\n\n\nadl://\naccount_name\n.azuredatalakestore.net/\ndir/file\n\n\n\nFor example, the following Hadoop FileSystem shell commands demonstrate access to a storage account named \"myaccount\":\n\n\nhadoop fs -mkdir adl://myaccount.azuredatalakestore.net/testdir\n\n\n\nhadoop fs -put testfile adl://myaccount.azuredatalakestore.net/testdir/testfile\n\n\n\nTo use DistCp against ADLS, use the following syntax:\n\nhadoop distcp\n    [-D hadoop.security.credential.provider.path=localjceks://file/home/user/adls.jceks]\n    hdfs://\nnamenode_hostname\n:9001/user/foo/007020615\n    adl://\nmyaccount\n.azuredatalakestore.net/testDir/\n\n\nWorking with ADLS\n\n\nFor more information about configuring the ADLS connector and working with data stored in ADLS, refer to \nCloud Data Access\n documentation.\n\n\nRelated links\n \n\n\nCloud Data Access\n (Hortonworks)\n\n\nHow to configure authentication with ADLS\n (Hortonworks)\n\n\nAzure Data Lake Store\n (External)   \n\n\nCreate a storage account\n (External)   \n\n\nGet started with Azure Data Lake Store\n (External)  \n\n\nAccessing data in WASB from HDP\n\n\nWindows Azure Storage Blob (WASB) is an object store service available on Azure.\n\n\nPrerequisites\n\n\nIf you want to use Windows Azure Storage Blob to store your data, you must enable Azure subscription for Blob Storage, and then create a \nstorage account\n.  \n\n\nConfigure access to WASB\n\n\nIn order to access data stored in your Azure blob storage account, you must configure your storage account access key in \ncore-site.xml\n. The configuration property that you must use is \nfs.azure.account.key.\naccount name\n.blob.core.windows.net\n and the value is the access key. \n\n\nFor example the following property should be used for a storage account called \"testaccount\": \n\n\nproperty\n\n  \nname\nfs.azure.account.key.testaccount.blob.core.windows.net\n/name\n\n  \nvalue\nTESTACCOUNT-ACCESS-KEY\n/value\n\n\n/property\n\n\n\n\n\nYou can obtain your access key from the Access keys in your storage account settings.\n\n\nTest access to WASB\n\n\nTo tests access to WASB, SSH to a cluster node and run a few hadoop fs shell commands against your existing WASB account.\n\n\nWASB access path syntax is:\n\n\nwasb://\ncontainer_name\n@\nstorage_account_name\n.blob.core.windows.net/\ndir/file\n\n\n\nFor example, to access a file called \"testfile\" located in a directory called \"testdir\", stored in the container called \"testcontainer\" on the account called \"hortonworks\", the URL is:\n\n\nwasb://testcontainer@hortonworks.blob.core.windows.net/testdir/testfile\n\n\n\nYou can also use \"wasbs\" prefix to utilize SSL-encrypted HTTPS access:\n\n\nwasbs://\n@\n.blob.core.windows.net/dir/file\n\n\n\nThe following Hadoop FileSystem shell commands demonstrate access to a storage account named \"myaccount\" and a container named \"mycontainer\":\n\n\nhadoop fs -ls wasb://mycontainer@myaccount.blob.core.windows.net/\n\nhadoop fs -mkdir wasb://mycontainer@myaccount.blob.core.windows.net/testDir\n\nhadoop fs -put testFile wasb://mycontainer@myaccount.blob.core.windows.net/testDir/testFile\n\nhadoop fs -cat wasb://mycontainer@myaccount.blob.core.windows.net/testDir/testFile\ntest file content\n\n\n\nWorking with WASB\n\n\nFor more information about configuring the WASB connector and working with data stored in WASB, refer to \nCloud Data Access\n documentation.\n\n\nRelated links\n \n\n\nCloud Data Access\n (Hortonworks) \n\n\nCreate a storage account\n (External)", 
            "title": "Access data on Azure"
        }, 
        {
            "location": "/azure-data/index.html#accessing-data-on-azure", 
            "text": "Hortonworks Data Platform (HDP) supports reading and writing both block blobs and page blobs\nfrom/to  Windows Azure Storage Blob (WASB)  object store, as well as reading and writing files stored in an Azure Data Lake Storage (ADLS)  account.", 
            "title": "Accessing data on Azure"
        }, 
        {
            "location": "/azure-data/index.html#accessing-data-in-adls-from-hdp", 
            "text": "Azure Data Lake Store (ADLS)  is an enterprise-wide hyper-scale repository for big data analytic workloads.", 
            "title": "Accessing data in ADLS from HDP"
        }, 
        {
            "location": "/azure-data/index.html#prerequisites", 
            "text": "If you want to use ADLS to store your data, you must enable Azure subscription for Data Lake Store, and then create an Azure Data Lake Store  storage account .", 
            "title": "Prerequisites"
        }, 
        {
            "location": "/azure-data/index.html#configure-access-to-adls", 
            "text": "ADLS is not supported as a default file system, but access to data in ADLS via the adl connector. To configure access to ADLS from a cluster managed via Cloudbreak use the steps described in  How to configure authentication with ADLS .", 
            "title": "Configure access to ADLS"
        }, 
        {
            "location": "/azure-data/index.html#test-access-to-adls", 
            "text": "To tests access to ADLS, SSH to a cluster node and run a few hadoop fs shell commands against your existing ADLS account.  ADLS access path syntax is:  adl:// account_name .azuredatalakestore.net/ dir/file  For example, the following Hadoop FileSystem shell commands demonstrate access to a storage account named \"myaccount\":  hadoop fs -mkdir adl://myaccount.azuredatalakestore.net/testdir  hadoop fs -put testfile adl://myaccount.azuredatalakestore.net/testdir/testfile  To use DistCp against ADLS, use the following syntax: hadoop distcp\n    [-D hadoop.security.credential.provider.path=localjceks://file/home/user/adls.jceks]\n    hdfs:// namenode_hostname :9001/user/foo/007020615\n    adl:// myaccount .azuredatalakestore.net/testDir/", 
            "title": "Test access to ADLS"
        }, 
        {
            "location": "/azure-data/index.html#working-with-adls", 
            "text": "For more information about configuring the ADLS connector and working with data stored in ADLS, refer to  Cloud Data Access  documentation.  Related links    Cloud Data Access  (Hortonworks)  How to configure authentication with ADLS  (Hortonworks)  Azure Data Lake Store  (External)     Create a storage account  (External)     Get started with Azure Data Lake Store  (External)", 
            "title": "Working with ADLS"
        }, 
        {
            "location": "/azure-data/index.html#accessing-data-in-wasb-from-hdp", 
            "text": "Windows Azure Storage Blob (WASB) is an object store service available on Azure.", 
            "title": "Accessing data in WASB from HDP"
        }, 
        {
            "location": "/azure-data/index.html#prerequisites_1", 
            "text": "If you want to use Windows Azure Storage Blob to store your data, you must enable Azure subscription for Blob Storage, and then create a  storage account .", 
            "title": "Prerequisites"
        }, 
        {
            "location": "/azure-data/index.html#configure-access-to-wasb", 
            "text": "In order to access data stored in your Azure blob storage account, you must configure your storage account access key in  core-site.xml . The configuration property that you must use is  fs.azure.account.key. account name .blob.core.windows.net  and the value is the access key.   For example the following property should be used for a storage account called \"testaccount\":   property \n   name fs.azure.account.key.testaccount.blob.core.windows.net /name \n   value TESTACCOUNT-ACCESS-KEY /value  /property   You can obtain your access key from the Access keys in your storage account settings.", 
            "title": "Configure access to WASB"
        }, 
        {
            "location": "/azure-data/index.html#test-access-to-wasb", 
            "text": "To tests access to WASB, SSH to a cluster node and run a few hadoop fs shell commands against your existing WASB account.  WASB access path syntax is:  wasb:// container_name @ storage_account_name .blob.core.windows.net/ dir/file  For example, to access a file called \"testfile\" located in a directory called \"testdir\", stored in the container called \"testcontainer\" on the account called \"hortonworks\", the URL is:  wasb://testcontainer@hortonworks.blob.core.windows.net/testdir/testfile  You can also use \"wasbs\" prefix to utilize SSL-encrypted HTTPS access:  wasbs:// @ .blob.core.windows.net/dir/file  The following Hadoop FileSystem shell commands demonstrate access to a storage account named \"myaccount\" and a container named \"mycontainer\":  hadoop fs -ls wasb://mycontainer@myaccount.blob.core.windows.net/\n\nhadoop fs -mkdir wasb://mycontainer@myaccount.blob.core.windows.net/testDir\n\nhadoop fs -put testFile wasb://mycontainer@myaccount.blob.core.windows.net/testDir/testFile\n\nhadoop fs -cat wasb://mycontainer@myaccount.blob.core.windows.net/testDir/testFile\ntest file content", 
            "title": "Test access to WASB"
        }, 
        {
            "location": "/azure-data/index.html#working-with-wasb", 
            "text": "For more information about configuring the WASB connector and working with data stored in WASB, refer to  Cloud Data Access  documentation.  Related links    Cloud Data Access  (Hortonworks)   Create a storage account  (External)", 
            "title": "Working with WASB"
        }, 
        {
            "location": "/gcp-launch/index.html", 
            "text": "Launching Cloudbreak on GCP\n\n\nBefore launching Cloudbreak on Google Cloud, review and meet the prerequisites. Next, import Cloudbreak image, launch a VM, SSH to the VM, and start Cloudbreak. Once Cloudbreak is running, log in to the Cloudbreak UI and create a Cloudbreak credential. \n\n\nMeet the prerequisites\n\n\nBefore launching Cloudbreak on GCP, you must meet the following prerequisites.\n\n\nGCP account\n\n\nIn order to launch Cloudbreak on GCP, you must log in to your GCP account. If you don't have an account, you can create one at \nhttps://console.cloud.google.com\n.\n\n\nOnce you log in to your GCP account, you must either create a project or use an existing project. \n\n\nService account\n\n\nIn order to launch clusters on GCP via Cloudbreak, you must have a Service Account that Cloudbreak can use to create resources. In addition, you must also have a P12 key associated with the account. The service account must have the following roles are enabled:\n\n\n\n\nCompute Engine \n Compute Image User   \n\n\nCompute Engine \n Compute Instance Admin (v1)  \n\n\nCompute Engine \n Compute Network Admin  \n\n\nCompute Engine \n Compute Security Admin  \n\n\nStorage \n Storage Admin \n\n\n\n\nA user with an \"Owner\" role can assign roles to new and existing service accounts from \nIAM \n admin\n \n \nService accounts\n, as presented in the following screenshots: \n\n\n \n\n\n \n\n\nFor more information on creating a Service Account and generating a P12 key, refer to \nGCP documentation\n. \n\n\nRelated links\n\n\nService account credentials\n (External)  \n\n\nSSH key pair\n\n\nGenerate a new SSH key pair\n or use an existing SSH key pair. You will be required to provide it when launching the VM. \n\n\nRegion and zone\n\n\nDecide in which region and zone you would like to launch Cloudbreak. You can launch Cloudbreak and provision your clusters in all regions \nsupported by GCP\n.  \n\n\nClusters created via Cloudbreak can be in the same or different region as Cloudbreak; when you launch a cluster, you select the region in which to launch it. \n\n\nRelated links\n\n\nRegions and zones\n (External)  \n\n\nVPC network\n\n\nWhen launching Cloudbreak, you will be required to select an existing network in which Cloudbreak can be placed. The following ports must be open on the security group: 22 (for access via SSH), 80 (for access via HTTP), and 443 (for access via HTTPS). You may use the \ndefault\n network as long as the aforementioned ports are open. \n\n\nYou can manage networks under \nNetworking\n \n \nVPC Networks\n. To edit ports, click on the network name and then click on \nAdd firewall rules\n. \n\n\nLaunch Cloudbreak from a template\n\n\nFollow these steps to launch Cloudbreak on Google Cloud by using the Cloud Deployer Manager. \n\n\nSteps\n\n\n\n\n\n\nIn order to use the Cloud Deployment Manager, you must install the Google cloud SDK on your machine. The SDK contains the gcloud CLI tool, which is used to deploy Cloudbreak. \n\n\nFor instructions, refer to \nInstalling Google Cloud SDK\n in Google Cloud documentation.  \n\n\n\n\n\n\nOpen a text editor and paste the following into the file:  \n\n\nregion: us-central1\nzone: us-central1-a\ninstance_type: n1-standard-4\nssh_pub_key: ....\nos_user: cloudbreak\nuser_email: admin@example.com\nuser_password: cloudbreak\n\n\n\n\n\n\nEdit the file by updating the parameters: \n\n\n\n\n\n\n\n\nParameter\n\n\nDescription\n\n\nDefault\n\n\n\n\n\n\n\n\n\n\nregion\n\n\nThe GCP region in which you would like to launch Cloudbreak. You can launch Cloudbreak and provision your clusters in all regions \nsupported by GCP\n.\n\n\nus-central-1\n\n\n\n\n\n\nzone\n\n\nThe GCP region's zone in which you would like to launch Cloudbreak. You can launch Cloudbreak and provision your clusters in all regions \nsupported by GCP\n.\n\n\nus-central1-a\n\n\n\n\n\n\ninstance_type\n\n\nSelect the VM instance type. For available instance types, reefer to \nMachine Types\n in GCP docuemntation.\n\n\nn1-standard-4\n\n\n\n\n\n\nssh_pub_key\n\n\nPaste your SSH public key.\n\n\n...\n\n\n\n\n\n\nos_user\n\n\nEnter the name of the user that you would like to use to SSH to the VM.\n\n\ncloudbreak\n\n\n\n\n\n\nuser_email\n\n\nEnter the email address that you would like to use to log in to Cloudbreak.\n\n\nadmin@cloudbreak.com\n\n\n\n\n\n\nuser_password\n\n\nEnter the password that you would like to use to log in to Cloudbreak.\n\n\ncloudbreak\n\n\n\n\n\n\n\n\n\n\n\n\nSave the file on your local machine in the yaml format. \n\n\n\n\n\n\nRun the following command to create a new deployment:\n\n\ngcloud deployment-manager deployments create cbd-deployment --config=/path/to/your/file.yaml\n\n\nFor example:\n\n\ngcloud deployment-manager deployments create cbd-deployment --config=vm_template_config.yaml\n\n\n\n\n\n\nTo delete the previously created deployment via gcloud command line interface, use:\n\n\ngcloud deployment-manager deployments delete cbd-deployment -q\n\n\n\nRelated Links\n\n\nInstalling Google Cloud SDK\n (External)\n\n\nRegions and Zones\n (External)  \n\n\nMachine Types\n (External)    \n\n\nAccess Cloudbreak web UI\n\n\nLog in to the Cloudbreak UI using the following steps.\n\n\nSteps\n\n\n\n\n\n\nYou can log into the Cloudbreak application at \nhttps://IP_Address\n. For example \nhttps://34.212.141.253\n. You can obtain the VM's IP address from \nCompute Engine\n \n \nVM Instances\n, the \nExternal IP\n column.\n\n\n\n\n\n\nConfirm the security exception to proceed to the Cloudbreak web UI.\n\n\nThe first time you access Cloudbreak UI, Cloudbreak will automatically generate a self-signed certificate, due to which your browser will warn you about an untrusted connection and will ask you to confirm a security exception.\n\n\n\n\n\n\n\n\nBrowser\n\n\nSteps\n\n\n\n\n\n\n\n\n\n\nFirefox\n\n\nClick \nAdvanced\n \n Click \nAdd Exception...\n \n Click \nConfirm Security Exception\n\n\n\n\n\n\nSafari\n\n\nClick \nContinue\n\n\n\n\n\n\nChrome\n\n\nClick \nAdvanced\n \n Click \nProceed...\n\n\n\n\n\n\n\n\n\n\n\n\nThe login page is displayed:\n\n\n  \n\n\n\n\n\n\nThe login page is displayed:\n\n\n  \n\n\n\n\n\n\nLog in to the Cloudbreak web UI using the credential that you configured in your \nProfile\n file when \nlaunching Cloudbreak deployer from an image\n:\n\n\n\n\nThe username is the \nUAA_DEFAULT_USER_EMAIL\n     \n\n\nThe password is the \nUAA_DEFAULT_USER_PW\n \n\n\n\n\n\n\n\n\nUpon a successful login, you are redirected to the dashboard:\n\n\n  \n\n\n\n\n\n\nCreate Cloudbreak credential\n\n\nCloudbreak works by connecting your GCP account through this credential, and then uses it to create resources on your behalf. Before you can start provisioning cluster using Cloudbreak, you must create a Cloudbreak credential.\n\n\nSteps\n\n\n\n\n\n\nIn the Cloudbreak web UI, select \nCredentials\n from the navigation pane. \n\n\n\n\n\n\nClick \nCreate Credential\n. \n\n\n\n\n\n\nUnder \nCloud provider\n, select \"Google Cloud Platform\":\n\n\n  \n\n\n\n\n\n\nProvide the following information:\n\n\n\n\n\n\n\n\nParameter\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nName\n\n\nEnter a name for your credential.\n\n\n\n\n\n\nDescription\n\n\n(Optional) Enter a description.\n\n\n\n\n\n\nProject Id\n\n\nEnter the project ID. You can obtain it from your GCP account by clicking on the name of your project at the top of the page and copying the \nID\n.\n\n\n\n\n\n\nService Account Email Address\n\n\n\"Service account ID\" value for your service account created in prerequisites. You can find it on GCP at \nIAM \n Admin\n \n \nService accounts\n.\n\n\n\n\n\n\nService Account Private (p12) Key\n\n\nUpload the P12 key that you created in the prerequisites when creating a service account.\n\n\n\n\n\n\n\n\n\n\n\n\nClick \nCreate\n.\n\n\n\n\n\n\nYour credential should now be displayed in the \nCredentials\n pane.\n\n\nCongratulations! You have successfully launched Cloudbreak and created a Cloudbreak credential. Now you can use Cloudbreak to \ncreate clusters\n. \n\n\n\n\n\n\n\n\nNext: Create a Cluster", 
            "title": "Launch on GCP"
        }, 
        {
            "location": "/gcp-launch/index.html#launching-cloudbreak-on-gcp", 
            "text": "Before launching Cloudbreak on Google Cloud, review and meet the prerequisites. Next, import Cloudbreak image, launch a VM, SSH to the VM, and start Cloudbreak. Once Cloudbreak is running, log in to the Cloudbreak UI and create a Cloudbreak credential.", 
            "title": "Launching Cloudbreak on GCP"
        }, 
        {
            "location": "/gcp-launch/index.html#meet-the-prerequisites", 
            "text": "Before launching Cloudbreak on GCP, you must meet the following prerequisites.", 
            "title": "Meet the prerequisites"
        }, 
        {
            "location": "/gcp-launch/index.html#gcp-account", 
            "text": "In order to launch Cloudbreak on GCP, you must log in to your GCP account. If you don't have an account, you can create one at  https://console.cloud.google.com .  Once you log in to your GCP account, you must either create a project or use an existing project.", 
            "title": "GCP account"
        }, 
        {
            "location": "/gcp-launch/index.html#service-account", 
            "text": "In order to launch clusters on GCP via Cloudbreak, you must have a Service Account that Cloudbreak can use to create resources. In addition, you must also have a P12 key associated with the account. The service account must have the following roles are enabled:   Compute Engine   Compute Image User     Compute Engine   Compute Instance Admin (v1)    Compute Engine   Compute Network Admin    Compute Engine   Compute Security Admin    Storage   Storage Admin    A user with an \"Owner\" role can assign roles to new and existing service accounts from  IAM   admin     Service accounts , as presented in the following screenshots:         For more information on creating a Service Account and generating a P12 key, refer to  GCP documentation .   Related links  Service account credentials  (External)", 
            "title": "Service account"
        }, 
        {
            "location": "/gcp-launch/index.html#ssh-key-pair", 
            "text": "Generate a new SSH key pair  or use an existing SSH key pair. You will be required to provide it when launching the VM.", 
            "title": "SSH key pair"
        }, 
        {
            "location": "/gcp-launch/index.html#region-and-zone", 
            "text": "Decide in which region and zone you would like to launch Cloudbreak. You can launch Cloudbreak and provision your clusters in all regions  supported by GCP .    Clusters created via Cloudbreak can be in the same or different region as Cloudbreak; when you launch a cluster, you select the region in which to launch it.   Related links  Regions and zones  (External)", 
            "title": "Region and zone"
        }, 
        {
            "location": "/gcp-launch/index.html#vpc-network", 
            "text": "When launching Cloudbreak, you will be required to select an existing network in which Cloudbreak can be placed. The following ports must be open on the security group: 22 (for access via SSH), 80 (for access via HTTP), and 443 (for access via HTTPS). You may use the  default  network as long as the aforementioned ports are open.   You can manage networks under  Networking     VPC Networks . To edit ports, click on the network name and then click on  Add firewall rules .", 
            "title": "VPC network"
        }, 
        {
            "location": "/gcp-launch/index.html#launch-cloudbreak-from-a-template", 
            "text": "Follow these steps to launch Cloudbreak on Google Cloud by using the Cloud Deployer Manager.   Steps    In order to use the Cloud Deployment Manager, you must install the Google cloud SDK on your machine. The SDK contains the gcloud CLI tool, which is used to deploy Cloudbreak.   For instructions, refer to  Installing Google Cloud SDK  in Google Cloud documentation.      Open a text editor and paste the following into the file:    region: us-central1\nzone: us-central1-a\ninstance_type: n1-standard-4\nssh_pub_key: ....\nos_user: cloudbreak\nuser_email: admin@example.com\nuser_password: cloudbreak    Edit the file by updating the parameters:      Parameter  Description  Default      region  The GCP region in which you would like to launch Cloudbreak. You can launch Cloudbreak and provision your clusters in all regions  supported by GCP .  us-central-1    zone  The GCP region's zone in which you would like to launch Cloudbreak. You can launch Cloudbreak and provision your clusters in all regions  supported by GCP .  us-central1-a    instance_type  Select the VM instance type. For available instance types, reefer to  Machine Types  in GCP docuemntation.  n1-standard-4    ssh_pub_key  Paste your SSH public key.  ...    os_user  Enter the name of the user that you would like to use to SSH to the VM.  cloudbreak    user_email  Enter the email address that you would like to use to log in to Cloudbreak.  admin@cloudbreak.com    user_password  Enter the password that you would like to use to log in to Cloudbreak.  cloudbreak       Save the file on your local machine in the yaml format.     Run the following command to create a new deployment:  gcloud deployment-manager deployments create cbd-deployment --config=/path/to/your/file.yaml  For example:  gcloud deployment-manager deployments create cbd-deployment --config=vm_template_config.yaml    To delete the previously created deployment via gcloud command line interface, use:  gcloud deployment-manager deployments delete cbd-deployment -q  Related Links  Installing Google Cloud SDK  (External)  Regions and Zones  (External)    Machine Types  (External)", 
            "title": "Launch Cloudbreak from a template"
        }, 
        {
            "location": "/gcp-launch/index.html#access-cloudbreak-web-ui", 
            "text": "Log in to the Cloudbreak UI using the following steps.  Steps    You can log into the Cloudbreak application at  https://IP_Address . For example  https://34.212.141.253 . You can obtain the VM's IP address from  Compute Engine     VM Instances , the  External IP  column.    Confirm the security exception to proceed to the Cloudbreak web UI.  The first time you access Cloudbreak UI, Cloudbreak will automatically generate a self-signed certificate, due to which your browser will warn you about an untrusted connection and will ask you to confirm a security exception.     Browser  Steps      Firefox  Click  Advanced    Click  Add Exception...    Click  Confirm Security Exception    Safari  Click  Continue    Chrome  Click  Advanced    Click  Proceed...       The login page is displayed:        The login page is displayed:        Log in to the Cloudbreak web UI using the credential that you configured in your  Profile  file when  launching Cloudbreak deployer from an image :   The username is the  UAA_DEFAULT_USER_EMAIL        The password is the  UAA_DEFAULT_USER_PW       Upon a successful login, you are redirected to the dashboard:", 
            "title": "Access Cloudbreak web UI"
        }, 
        {
            "location": "/gcp-launch/index.html#create-cloudbreak-credential", 
            "text": "Cloudbreak works by connecting your GCP account through this credential, and then uses it to create resources on your behalf. Before you can start provisioning cluster using Cloudbreak, you must create a Cloudbreak credential.  Steps    In the Cloudbreak web UI, select  Credentials  from the navigation pane.     Click  Create Credential .     Under  Cloud provider , select \"Google Cloud Platform\":        Provide the following information:     Parameter  Description      Name  Enter a name for your credential.    Description  (Optional) Enter a description.    Project Id  Enter the project ID. You can obtain it from your GCP account by clicking on the name of your project at the top of the page and copying the  ID .    Service Account Email Address  \"Service account ID\" value for your service account created in prerequisites. You can find it on GCP at  IAM   Admin     Service accounts .    Service Account Private (p12) Key  Upload the P12 key that you created in the prerequisites when creating a service account.       Click  Create .    Your credential should now be displayed in the  Credentials  pane.  Congratulations! You have successfully launched Cloudbreak and created a Cloudbreak credential. Now you can use Cloudbreak to  create clusters .      Next: Create a Cluster", 
            "title": "Create Cloudbreak credential"
        }, 
        {
            "location": "/gcp-create/index.html", 
            "text": "Creating a cluster on GCP\n\n\nUse these steps to create a cluster.\n\n\n\n\nTroubleshooting cluster creation\n\n\nIf you experience problems during cluster creation, refer to \nTroubleshooting cluster creation\n.\n\n\n\nSteps\n\n\n\n\n\n\nLog in to the Cloudbreak UI.\n\n\n\n\n\n\nClick \nCreate Cluster\n and the \nCreate Cluster\n wizard is displayed.\n\n    By default, \nBasic\n view is displayed. To view advanced options, click \nAdvanced\n. To learn about advanced options, refer to \nAdvanced cluster options\n.\n\n\n \n\n\n\n\n\n\nOn the \nGeneral Configuration\n page, specify the following general parameters for your cluster:\n\n\n\n\n\n\n\n\nParameter\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nSelect Credential\n\n\nChoose a previously created credential.\n\n\n\n\n\n\nCluster Name\n\n\nEnter a name for your cluster. The name must be between 5 and 40 characters, must start with a letter, and must only include lowercase letters, numbers, and hyphens.\n\n\n\n\n\n\nRegion\n\n\nSelect the GCP region in which you would like to launch your cluster. For information on available GCP regions, refer to \nGCP documentation\n.\n\n\n\n\n\n\nPlatform Version\n\n\nChoose the HDP or HDF version to use for this cluster. Blueprints available for this platform version will be populated under \"Cluster Type\" below.\n\n\n\n\n\n\nCluster Type\n\n\nChoose one of default cluster configurations, or, if you have defined your own cluster configuration via Ambari blueprint, you can choose it here. For more information on default and custom blueprints, refer to \nUsing custom blueprints\n.\n\n\n\n\n\n\nFlex Subscription\n\n\nThis option will appear if you have configured your deployment for a \nflex support subscription\n.\n\n\n\n\n\n\n\n\n\n\n\n\nOn the \nHardware and Storage\n page, for each host group provide the following information to define your cluster nodes and attached storage:\n\n\n\n\n\n\n\n\nParameter\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nInstance Type\n\n\nSelect a VM instance type. For information about instance types on GCP refer to \nGCP documentation\n.\n\n\n\n\n\n\nInstance Count\n\n\nEnter the number of instances of a given type. Default is 1.\n\n\n\n\n\n\nAmbari Server\n\n\nYou must select one node for Ambari Server. The \"Group Size\" for that host group must be set to \"1\".\n\n\n\n\n\n\n\n\n\n\n\n\nOn the \nNetwork\n page, provide the following to specify the networking resources that will be used for your cluster:\n\n\n\n\n\n\n\n\nParameter\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nSelect Network\n\n\nSelect the virtual network in which you would like your cluster to be provisioned. You can select an existing network or create a new network.\n\n\n\n\n\n\nSelect Subnet\n\n\nSelect the subnet in which you would like your cluster to be provisioned. If you are using a new network, create a new subnet. If you are using an existing network, select an existing subnet.\n\n\n\n\n\n\nSubnet (CIDR)\n\n\nIf you selected to create a new subnet, you must define a valid \nCIDR\n for the subnet. Default is 10.0.0.0/16.\n\n\n\n\n\n\n\n\n\n\nCloudbreak uses public IP addresses when communicating with cluster nodes.  \n\n\n\n\n\n\n\n\nDefine security groups for each host group. You can either create new security groups and define their rules or reuse existing security groups:\n\n\n\n\nExisting security groups are only available for an existing network. \n\n\n\n\n\n\n\n\n\n\nOption\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nNew Security Group\n\n\n(Default) Creates a new security group with the rules that you defined:\nA set of \ndefault rules\n is provided. You should review and adjust these default rules. If you do not make any modifications, default rules will be applied. \nYou may open ports by defining the CIDR, entering port range, selecting protocol and clicking \n+\n.\nYou may delete default or previously added rules using the delete icon.\nIf you don't want to use security group, remove the default rules.\n\n\n\n\n\n\nExisting Security Groups\n\n\nAllows you to select an existing security group that is already available in the selected provider region. This selection is disabled if no existing security groups are available in your chosen region.\n\n\n\n\n\n\n\n\n\n\nImportant\n\n\n\nBy default, ports 22, 443, and 9443 are set to 0.0.0.0/0 CIDR for inbound access on the Ambari node security group. We strongly recommend that you limit this CIDR, considering the following restrictions:\n\nPorts 22 and 9443 must be open to Cloudbreak's CIDR. You can set CB_DEFAULT_GATEWAY_CIDR in your Cloudbreak's Profile file in order to automatically open ports 22 and 9443 to your Cloudbreak IP. Refer to \nRestricting inbound access from Cloudbreak to cluster\n.\n\n\nPort 22 must be open to your CIDR if you would like to access the master node via SSH.\n\n\nPort 443 must be open to your CIDR if you would like to access Ambari web UI in a browser.\n\n\n\n\n\n\n\n\n\n\nImportant\n\n\n\nBy default, port 22 is set to 0.0.0.0/0 CIDR for inbound access on non-Ambari node security groups. We strongly recommend that you remove it.\n\n\n\n\n\n\nImportant\n\n\n\nDepending on what services you are including, you need to open additional ports as required by these services. For example, when using the Flow Management blueprint, you must open port 9091 for NiFi (on NiFI host group) and port 61443 for NiFI Registry (on the Services host group).\n\n\n\n\n\n\n\n\n\n\nOn the \nSecurity\n page, provide the following parameters:\n\n\n\n\n\n\n\n\nParameter\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nCluster User\n\n\nYou can log in to the Ambari UI using this username. By default, this is set to \nadmin\n.\n\n\n\n\n\n\nPassword\n\n\nYou can log in to the Ambari UI using this password.\n\n\n\n\n\n\nConfirm Password\n\n\nConfirm the password.\n\n\n\n\n\n\nNew SSH public key\n\n\nCheck this option to specify a new public key and then enter the public key. You will use the matching private key to access your cluster nodes via SSH.\n\n\n\n\n\n\nExisting SSH public key\n\n\nSelect an existing public key. You will use the matching private key to access your cluster nodes via SSH. This is a default option as long as an existing SSH public key is available.\n\n\n\n\n\n\n\n\n\n\n\n\nClick on \nCreate Cluster\n to create a cluster.\n\n\n\n\n\n\nYou will be redirected to the Cloudbreak dashboard, and a new tile representing your cluster will appear at the top of the page.\n\n\n\n\n\n\nRelated Links\n\n\nFlex support subscription\n\n\nUsing custom blueprints\n \n\n\nDefault cluster security groups\n\n\nTroubleshooting cluster creation\n    \n\n\nCIDR\n (External) \n\n\nCloud locations\n (External)\n\n\nMachine types\n (External)     \n\n\nAdvanced options\n\n\nClick on \nAdvanced\n to view and enter additional configuration options\n\n\nAvailability zone\n\n\nChoose one of the availability zones within the selected region. \n\n\nChoose image catalog\n\n\nBy default, \nChoose Image Catalog\n is set to the default image catalog that is provided with Cloudbreak. If you would like to use a different image catalog, you must first create and register it. For complete instructions, refer to \nUsing custom images\n.\n\n\nRelated links\n   \n\n\nUsing custom images\n  \n\n\nPrewarmed and base images\n\n\nCloudbreak supports the following types of images for launching clusters:\n\n\n\n\n\n\n\n\nImage type\n\n\nDescription\n\n\nDefault images provided\n\n\nSupport for custom images\n\n\n\n\n\n\n\n\n\n\nBase Images\n\n\nBase images include default configuration and default tooling. These images include the operating system but do not include Ambari or HDP/HDF software.\n\n\nYes\n\n\nYes\n\n\n\n\n\n\nPrewarmed Images\n\n\nBy default, Cloudbreak launches clusters from prewarmed images. Prewarmed images include the operating system as well as Ambari and HDP. The Ambari and HDP version used by prewarmed images cannot be customized. No prewarmed HDF images are currently provided.\n\n\nYes (HDP only)\n\n\nNo\n\n\n\n\n\n\n\n\nBy default, Cloudbreak uses the included default \nprewarmed images\n, which include the operating system, as well as\nAmbari and HDP/HDF packages installed. You can optionally select the \nbase image\n option if you would like to:\n\n\n\n\nUse an Ambari and HDP/HDF versions different than what the prewarmed image includes and/or  \n\n\nChoose a previously created custom base image\n\n\n\n\nChoose image\n  \n\n\nIf under \nChoose image catalog\n, you selected a custom image catalog, under \nChoose Image\n you can select an image from that catalog. For complete instructions, refer to \nUsing custom images\n. \n\n\nIf you are trying to customize Ambari and HDP/HDF versions, you can ignore the \nChoose Image\n option; in this case default base image is used.\n\n\nAmbari repository specification\n\n\nIf you would like to use a custom Ambari version, provide the following information: \n\n\n\n\n Ambari 2.6.1\n\n\nIf you would like to use Ambari \n2.6.1\n, use the version provided by default in the Cloudbreak web UI, or newer.\n\n\n\n\n\n\n\n\n\nParameter\n\n\nDescription\n\n\nExample\n\n\n\n\n\n\n\n\n\n\nVersion\n\n\nEnter Ambari version.\n\n\n2.6.1.3\n\n\n\n\n\n\nRepo Url\n\n\nProvide a URL to the Ambari version repo that you would like to use.\n\n\nhttp://public-repo-1.hortonworks.com/ambari/centos6/2.x/updates/2.6.1.3\n\n\n\n\n\n\nRepo Gpg Key Url\n\n\nProvide a URL to the repo GPG key. Each stable RPM package that is published by CentOS Project is signed with a GPG signature. By default, yum and the graphical update tools will verify these signatures and refuse to install any packages that are not signed, or have an incorrect signature.\n\n\nhttp://public-repo-1.hortonworks.com/ambari/centos6/RPM-GPG-KEY/RPM-GPG-KEY-Jenkins\n\n\n\n\n\n\n\n\nHDP or HDF repository specification\n\n\nIf you would like to use a custom HDP or HDF version, provide the following information: \n\n\n\n\n\n\n\n\nParameter\n\n\nDescription\n\n\nExample\n\n\n\n\n\n\n\n\n\n\nStack\n\n\nThis is populated by default based on the \"Platform Version\" parameter.\n\n\nHDP\n\n\n\n\n\n\nVersion\n\n\nThis is populated by default based on the \"Platform Version\" parameter.\n\n\n2.6\n\n\n\n\n\n\nOS\n\n\nOperating system.\n\n\ncentos7 (Azure, GCP, OpenStack) or amazonlinux (AWS)\n\n\n\n\n\n\nRepository Version\n\n\nEnter repository version.\n\n\n2.6.4.0-91\n\n\n\n\n\n\nVersion Definition File\n\n\nEnter the URL of the VDF file.\n\n\nhttp://public-repo-1.hortonworks.com/HDP/centos6/2.x/updates/2.6.4.0/HDP-2.6.4.0-91.xml\n\n\n\n\n\n\n(HDF only) MPack Url\n\n\n(HDF only) Provide MPack URL.\n\n\nhttp://public-repo-1.hortonworks.com/HDF/centos7/3.x/updates/3.1.1.0/tars/hdf_ambari_mp/hdf-ambari-mpack-3.1.1.0-35.tar.gz\n\n\n\n\n\n\nEnable Ambari Server to download and install GPL Licensed LZO packages?\n\n\n(Optional, only available if using Ambari 2.6.1.0 or newer) Use this option to enable LZO compression in your HDP/HDF cluster. LZO is a lossless data compression library that favors speed over compression ratio. Ambari does not install nor enable LZO compression libraries by default, and must be explicitly configured to do so. For more information, refer to \nEnabling LZO\n.\n\n\n\n\n\n\n\n\n\n\n\n\nIf you choose to use a base image with custom Ambari and/or HDP/HDF version, Cloudbreak validates the information entered. When Cloudbreak detects that the information entered is incorrect, it displays a warning marked with the \n sign. You should review all the warnings before proceeding and make sure that the information that you entered is correct. If you choose to proceed in spite of the warnings, check \"Ignore repository warnings\".  \n\n\n\n\nRelated links\n    \n\n\nUsing custom images\n      \n\n\nEnable lifetime management\n\n\nCheck this option if you would like your cluster to be automatically terminated after a specific amount of time (defined as \"Time to Live\" in minutes). \n\n\nTags\n\n\nYou can optionally add tags, which will help you find your cluster-related resources, such as VMs, in your cloud provider account. \n\n\nBy default, the following tags are created:\n\n\n\n\n\n\n\n\nTag\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\ncb-version\n\n\nCloudbreak version\n\n\n\n\n\n\nOwner\n\n\nYour Cloudbreak admin email.\n\n\n\n\n\n\ncb-account-name\n\n\nYour automatically generated Cloudbreak account name stored in the identity server.\n\n\n\n\n\n\ncb-user-name\n\n\nYour Cloudbreak admin email.\n\n\n\n\n\n\n\n\nFor more information, refer to \nTagging resources\n.\n\n\nRelated links\n    \n\n\nTagging resources\n \n\n\nStorage\n\n\nYou can optionally specify the following storage options for your cluster:\n\n\n\n\n\n\n\n\nParameter\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nStorage Type\n\n\nSelect the volume type. The options are:\nStandard persistent disks (HDD)\nSolid-state persistent disks (SSD)\n For more information about these options refer to \nGCP documentation\n.\n\n\n\n\n\n\nAttached Volumes Per Instance\n\n\nEnter the number of volumes attached per instance. Default is 1.\n\n\n\n\n\n\nVolume Size (GB)\n\n\nEnter the size in GBs for each volume. Default is 100.\n\n\n\n\n\n\n\n\nUse preemptible instances\n\n\nCheck this option to use Google Cloud preemptive VM instances as your cluster nodes. To learn more, refer to \nGoogle Cloud documentation\n.    \n\n\nNote that: \n\n\n\n\nWe recommend not using preemptible instances for any host group that includes Ambari server components.  \n\n\nIf you choose to use preemptible instances for a given host group when creating your cluster, any nodes that you add to that host group (during cluster creation or later) will be using preemptible instances.   \n\n\nIf you decide not to use preemptible instances when creating your cluster, any nodes that you add to your host group (during cluster creation or later) will be using standard on-demand instances.     \n\n\nOnce someone outbids you, the preemptible instances are taken away, removing the nodes from the cluster. \n\n\nIf the preemptible instances are not available right away, creating a cluster will take longer than usual. \n\n\n\n\nRecipes\n\n\nThis option allows you to select previously uploaded recipes (scripts that can be run pre or post cluster deployment) for each host group. For more information on recipes, refer to \nUsing custom scripts (recipes)\n. \n\n\nRelated links\n    \n\n\nUsing custom scripts (recipes)\n \n\n\nManagement Packs\n\n\nThis option allows you to select previously uploaded management packs. For more information on management packs, refer to \nUsing management packs\n. \n\n\nRelated links\n    \n\n\nUsing management packs\n  \n\n\nExternal sources\n\n\nYou can register external sources with Cloudbreak, and then select and attach them during cluster create. To register external sources with Cloudbreak, refer to:\n\n\n\n\nUsing an external authentication source\n    \n\n\nUsing an external database\n  \n\n\nRegister a proxy\n  \n\n\n\n\nAmbari server master key\n\n\nThe Ambari server master key is used to configure Ambari to encrypt database and Kerberos credentials that are retained by Ambari as part of the Ambari setup.  \n\n\nEnable Kerberos security\n\n\nSelect this option to enable Kerberos for your cluster. For information about available Kerberos options, refer to \nEnabling Kerberos security\n. \n\n\nRelated links\n    \n\n\nEnabling Kerberos security\n \n\n\nPreemptible VM instances\n (External) \n\n\nStorage options\n (External)  \n\n\n\n\nNext: Access Cluster", 
            "title": "Create a cluster"
        }, 
        {
            "location": "/gcp-create/index.html#creating-a-cluster-on-gcp", 
            "text": "Use these steps to create a cluster.   Troubleshooting cluster creation  If you experience problems during cluster creation, refer to  Troubleshooting cluster creation .  Steps    Log in to the Cloudbreak UI.    Click  Create Cluster  and the  Create Cluster  wizard is displayed. \n    By default,  Basic  view is displayed. To view advanced options, click  Advanced . To learn about advanced options, refer to  Advanced cluster options .       On the  General Configuration  page, specify the following general parameters for your cluster:     Parameter  Description      Select Credential  Choose a previously created credential.    Cluster Name  Enter a name for your cluster. The name must be between 5 and 40 characters, must start with a letter, and must only include lowercase letters, numbers, and hyphens.    Region  Select the GCP region in which you would like to launch your cluster. For information on available GCP regions, refer to  GCP documentation .    Platform Version  Choose the HDP or HDF version to use for this cluster. Blueprints available for this platform version will be populated under \"Cluster Type\" below.    Cluster Type  Choose one of default cluster configurations, or, if you have defined your own cluster configuration via Ambari blueprint, you can choose it here. For more information on default and custom blueprints, refer to  Using custom blueprints .    Flex Subscription  This option will appear if you have configured your deployment for a  flex support subscription .       On the  Hardware and Storage  page, for each host group provide the following information to define your cluster nodes and attached storage:     Parameter  Description      Instance Type  Select a VM instance type. For information about instance types on GCP refer to  GCP documentation .    Instance Count  Enter the number of instances of a given type. Default is 1.    Ambari Server  You must select one node for Ambari Server. The \"Group Size\" for that host group must be set to \"1\".       On the  Network  page, provide the following to specify the networking resources that will be used for your cluster:     Parameter  Description      Select Network  Select the virtual network in which you would like your cluster to be provisioned. You can select an existing network or create a new network.    Select Subnet  Select the subnet in which you would like your cluster to be provisioned. If you are using a new network, create a new subnet. If you are using an existing network, select an existing subnet.    Subnet (CIDR)  If you selected to create a new subnet, you must define a valid  CIDR  for the subnet. Default is 10.0.0.0/16.      Cloudbreak uses public IP addresses when communicating with cluster nodes.       Define security groups for each host group. You can either create new security groups and define their rules or reuse existing security groups:   Existing security groups are only available for an existing network.       Option  Description      New Security Group  (Default) Creates a new security group with the rules that you defined: A set of  default rules  is provided. You should review and adjust these default rules. If you do not make any modifications, default rules will be applied.  You may open ports by defining the CIDR, entering port range, selecting protocol and clicking  + . You may delete default or previously added rules using the delete icon. If you don't want to use security group, remove the default rules.    Existing Security Groups  Allows you to select an existing security group that is already available in the selected provider region. This selection is disabled if no existing security groups are available in your chosen region.      Important  \nBy default, ports 22, 443, and 9443 are set to 0.0.0.0/0 CIDR for inbound access on the Ambari node security group. We strongly recommend that you limit this CIDR, considering the following restrictions: Ports 22 and 9443 must be open to Cloudbreak's CIDR. You can set CB_DEFAULT_GATEWAY_CIDR in your Cloudbreak's Profile file in order to automatically open ports 22 and 9443 to your Cloudbreak IP. Refer to  Restricting inbound access from Cloudbreak to cluster .  Port 22 must be open to your CIDR if you would like to access the master node via SSH.  Port 443 must be open to your CIDR if you would like to access Ambari web UI in a browser.      Important  \nBy default, port 22 is set to 0.0.0.0/0 CIDR for inbound access on non-Ambari node security groups. We strongly recommend that you remove it.    Important  \nDepending on what services you are including, you need to open additional ports as required by these services. For example, when using the Flow Management blueprint, you must open port 9091 for NiFi (on NiFI host group) and port 61443 for NiFI Registry (on the Services host group).      On the  Security  page, provide the following parameters:     Parameter  Description      Cluster User  You can log in to the Ambari UI using this username. By default, this is set to  admin .    Password  You can log in to the Ambari UI using this password.    Confirm Password  Confirm the password.    New SSH public key  Check this option to specify a new public key and then enter the public key. You will use the matching private key to access your cluster nodes via SSH.    Existing SSH public key  Select an existing public key. You will use the matching private key to access your cluster nodes via SSH. This is a default option as long as an existing SSH public key is available.       Click on  Create Cluster  to create a cluster.    You will be redirected to the Cloudbreak dashboard, and a new tile representing your cluster will appear at the top of the page.    Related Links  Flex support subscription  Using custom blueprints    Default cluster security groups  Troubleshooting cluster creation       CIDR  (External)   Cloud locations  (External)  Machine types  (External)", 
            "title": "Creating a cluster on GCP"
        }, 
        {
            "location": "/gcp-create/index.html#advanced-options", 
            "text": "Click on  Advanced  to view and enter additional configuration options", 
            "title": "Advanced options"
        }, 
        {
            "location": "/gcp-create/index.html#availability-zone", 
            "text": "Choose one of the availability zones within the selected region.", 
            "title": "Availability zone"
        }, 
        {
            "location": "/gcp-create/index.html#choose-image-catalog", 
            "text": "By default,  Choose Image Catalog  is set to the default image catalog that is provided with Cloudbreak. If you would like to use a different image catalog, you must first create and register it. For complete instructions, refer to  Using custom images .  Related links      Using custom images", 
            "title": "Choose image catalog"
        }, 
        {
            "location": "/gcp-create/index.html#prewarmed-and-base-images", 
            "text": "Cloudbreak supports the following types of images for launching clusters:     Image type  Description  Default images provided  Support for custom images      Base Images  Base images include default configuration and default tooling. These images include the operating system but do not include Ambari or HDP/HDF software.  Yes  Yes    Prewarmed Images  By default, Cloudbreak launches clusters from prewarmed images. Prewarmed images include the operating system as well as Ambari and HDP. The Ambari and HDP version used by prewarmed images cannot be customized. No prewarmed HDF images are currently provided.  Yes (HDP only)  No     By default, Cloudbreak uses the included default  prewarmed images , which include the operating system, as well as\nAmbari and HDP/HDF packages installed. You can optionally select the  base image  option if you would like to:   Use an Ambari and HDP/HDF versions different than what the prewarmed image includes and/or    Choose a previously created custom base image   Choose image     If under  Choose image catalog , you selected a custom image catalog, under  Choose Image  you can select an image from that catalog. For complete instructions, refer to  Using custom images .   If you are trying to customize Ambari and HDP/HDF versions, you can ignore the  Choose Image  option; in this case default base image is used.  Ambari repository specification  If you would like to use a custom Ambari version, provide the following information:     Ambari 2.6.1  If you would like to use Ambari  2.6.1 , use the version provided by default in the Cloudbreak web UI, or newer.     Parameter  Description  Example      Version  Enter Ambari version.  2.6.1.3    Repo Url  Provide a URL to the Ambari version repo that you would like to use.  http://public-repo-1.hortonworks.com/ambari/centos6/2.x/updates/2.6.1.3    Repo Gpg Key Url  Provide a URL to the repo GPG key. Each stable RPM package that is published by CentOS Project is signed with a GPG signature. By default, yum and the graphical update tools will verify these signatures and refuse to install any packages that are not signed, or have an incorrect signature.  http://public-repo-1.hortonworks.com/ambari/centos6/RPM-GPG-KEY/RPM-GPG-KEY-Jenkins     HDP or HDF repository specification  If you would like to use a custom HDP or HDF version, provide the following information:      Parameter  Description  Example      Stack  This is populated by default based on the \"Platform Version\" parameter.  HDP    Version  This is populated by default based on the \"Platform Version\" parameter.  2.6    OS  Operating system.  centos7 (Azure, GCP, OpenStack) or amazonlinux (AWS)    Repository Version  Enter repository version.  2.6.4.0-91    Version Definition File  Enter the URL of the VDF file.  http://public-repo-1.hortonworks.com/HDP/centos6/2.x/updates/2.6.4.0/HDP-2.6.4.0-91.xml    (HDF only) MPack Url  (HDF only) Provide MPack URL.  http://public-repo-1.hortonworks.com/HDF/centos7/3.x/updates/3.1.1.0/tars/hdf_ambari_mp/hdf-ambari-mpack-3.1.1.0-35.tar.gz    Enable Ambari Server to download and install GPL Licensed LZO packages?  (Optional, only available if using Ambari 2.6.1.0 or newer) Use this option to enable LZO compression in your HDP/HDF cluster. LZO is a lossless data compression library that favors speed over compression ratio. Ambari does not install nor enable LZO compression libraries by default, and must be explicitly configured to do so. For more information, refer to  Enabling LZO .       If you choose to use a base image with custom Ambari and/or HDP/HDF version, Cloudbreak validates the information entered. When Cloudbreak detects that the information entered is incorrect, it displays a warning marked with the   sign. You should review all the warnings before proceeding and make sure that the information that you entered is correct. If you choose to proceed in spite of the warnings, check \"Ignore repository warnings\".     Related links       Using custom images", 
            "title": "Prewarmed and base images"
        }, 
        {
            "location": "/gcp-create/index.html#enable-lifetime-management", 
            "text": "Check this option if you would like your cluster to be automatically terminated after a specific amount of time (defined as \"Time to Live\" in minutes).", 
            "title": "Enable lifetime management"
        }, 
        {
            "location": "/gcp-create/index.html#tags", 
            "text": "You can optionally add tags, which will help you find your cluster-related resources, such as VMs, in your cloud provider account.   By default, the following tags are created:     Tag  Description      cb-version  Cloudbreak version    Owner  Your Cloudbreak admin email.    cb-account-name  Your automatically generated Cloudbreak account name stored in the identity server.    cb-user-name  Your Cloudbreak admin email.     For more information, refer to  Tagging resources .  Related links       Tagging resources", 
            "title": "Tags"
        }, 
        {
            "location": "/gcp-create/index.html#storage", 
            "text": "You can optionally specify the following storage options for your cluster:     Parameter  Description      Storage Type  Select the volume type. The options are: Standard persistent disks (HDD) Solid-state persistent disks (SSD)  For more information about these options refer to  GCP documentation .    Attached Volumes Per Instance  Enter the number of volumes attached per instance. Default is 1.    Volume Size (GB)  Enter the size in GBs for each volume. Default is 100.", 
            "title": "Storage"
        }, 
        {
            "location": "/gcp-create/index.html#use-preemptible-instances", 
            "text": "Check this option to use Google Cloud preemptive VM instances as your cluster nodes. To learn more, refer to  Google Cloud documentation .      Note that:    We recommend not using preemptible instances for any host group that includes Ambari server components.    If you choose to use preemptible instances for a given host group when creating your cluster, any nodes that you add to that host group (during cluster creation or later) will be using preemptible instances.     If you decide not to use preemptible instances when creating your cluster, any nodes that you add to your host group (during cluster creation or later) will be using standard on-demand instances.       Once someone outbids you, the preemptible instances are taken away, removing the nodes from the cluster.   If the preemptible instances are not available right away, creating a cluster will take longer than usual.", 
            "title": "Use preemptible instances"
        }, 
        {
            "location": "/gcp-create/index.html#recipes", 
            "text": "This option allows you to select previously uploaded recipes (scripts that can be run pre or post cluster deployment) for each host group. For more information on recipes, refer to  Using custom scripts (recipes) .   Related links       Using custom scripts (recipes)", 
            "title": "Recipes"
        }, 
        {
            "location": "/gcp-create/index.html#management-packs", 
            "text": "This option allows you to select previously uploaded management packs. For more information on management packs, refer to  Using management packs .   Related links       Using management packs", 
            "title": "Management Packs"
        }, 
        {
            "location": "/gcp-create/index.html#external-sources", 
            "text": "You can register external sources with Cloudbreak, and then select and attach them during cluster create. To register external sources with Cloudbreak, refer to:   Using an external authentication source       Using an external database     Register a proxy", 
            "title": "External sources"
        }, 
        {
            "location": "/gcp-create/index.html#ambari-server-master-key", 
            "text": "The Ambari server master key is used to configure Ambari to encrypt database and Kerberos credentials that are retained by Ambari as part of the Ambari setup.", 
            "title": "Ambari server master key"
        }, 
        {
            "location": "/gcp-create/index.html#enable-kerberos-security", 
            "text": "Select this option to enable Kerberos for your cluster. For information about available Kerberos options, refer to  Enabling Kerberos security .   Related links       Enabling Kerberos security    Preemptible VM instances  (External)   Storage options  (External)     Next: Access Cluster", 
            "title": "Enable Kerberos security"
        }, 
        {
            "location": "/gcp-clusters-access/index.html", 
            "text": "Accessing a cluster\n\n\nThe following section describes how to access the various services in the cluster.\n\n\nCloudbreak user accounts\n\n\nThe following table describes what credentials to use to access Cloudbreak and Cloudbreak-managed clusters:\n\n\n\n\n\n\n\n\nComponent\n\n\nMethod\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nCloudbreak\n\n\nWeb UI, CLI\n\n\nAccess with the username and password provided when launching Cloudbreak on the cloud provider.\n\n\n\n\n\n\nCloudbreak\n\n\nSSH to VM\n\n\nAccess as the \"cloudbreak\" user with the SSH key provided when launching Cloudbreak on the cloud provider.\n\n\n\n\n\n\nCluster\n\n\nSSH to VMs\n\n\nAccess as the \"cloudbreak\" user with the SSH key provided during cluster creation.\n\n\n\n\n\n\nCluster\n\n\nAmbari web UI\n\n\nAccess with the credentials provided in the \u201cCluster User\u201d parameter during cluster creation.\n\n\n\n\n\n\nCluster\n\n\nWeb UIs for specific cluster services\n\n\nAccess with the credentials provided in the \u201cCluster User\u201d parameter during cluster creation.\n\n\n\n\n\n\n\n\nFinding cluster information in the web UI\n\n\nOnce your cluster is up and running, click on the tile representing your cluster in the Cloudbreak UI to access information related the cluster and access cluster actions. \n\n\n \n\n\nThe information presented includes:\n\n\n\n\nCluster summary\n  \n\n\nCluster information\n     \n\n\nEvent history\n  \n\n\n\n\n\n  \nTips\n\n  \n\n  \n Access cluster actions such as resize and sync by clicking on \nACTIONS\n.\n\n  \n Access Ambari web UI by clicking on the link in the \nCLUSTER INFORMATION\n section.\n\n\n View public IP addresses for all cluster instances in the \nHARDWARE\n section. Click on the links to view the instances in the cloud console.\n\n\n The SSH user that you must use when accessing cluster VMs is \"cloudbreak\".\n \n\n\n\n\n\n\n\n\nCluster summary\n\n\nThe summary bar includes the following information about your cluster:\n\n\n\n\n\n\n\n\nItem\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nCluster Name\n\n\nThe name that you selected for your cluster is displayed at the top of the page. Below it is the name of the cluster blueprint.\n\n\n\n\n\n\nTime Remaining\n\n\nIf you enabled lifetime management for your cluster, the clock next to the cluster name indicates the amount of time that your cluster will run before it gets terminated. Note that the time remaining counter does not stop when you stop the cluster.\n\n\n\n\n\n\nCloud Provider\n\n\nThe logo of the cloud provider on which the cluster is running.\n\n\n\n\n\n\nCredential\n\n\nThe name of the credential used to create the cluster.\n\n\n\n\n\n\nStatus\n\n\nCurrent status. When a cluster is healthy, the status is \nRunning\n.\n\n\n\n\n\n\nNodes\n\n\nThe current number of cluster nodes, including the master node.\n\n\n\n\n\n\nUptime\n\n\nThe amount of time (HH:MM) that the cluster has been in the running state since it was started. Each time you stop and restart the cluster, the running time is reset to 0.\n\n\n\n\n\n\nCreated\n\n\nThe date when the cluster was created. The date format is Mon DD, YYYY. For example: Oct 27, 2017.\n\n\n\n\n\n\n\n\nCluster information\n\n\nThe following information is available on the cluster details page: \n\n\n\n\n\n\n\n\nItem\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nCluster User\n\n\nThe name of the cluster user that you created when creating the cluster.\n\n\n\n\n\n\nSSH Username\n\n\nThe SSH user which you must use when accessing cluster VMs via SSH. The SSH user is always \"cloudbreak\".\n\n\n\n\n\n\nAmbari URL\n\n\nLink to the Ambari web UI.\n\n\n\n\n\n\nRegion\n\n\nThe region in which the cluster is running in the cloud provider infrastructure.\n\n\n\n\n\n\nAvailability Zone\n\n\nThe availability zone within the region in which the cluster is running.\n\n\n\n\n\n\nBlueprint\n\n\nThe name of the blueprint selected under \"Cluster Type\" to create this cluster.\n\n\n\n\n\n\nCreated With\n\n\nThe version of Cloudbreak used to create this cluster.\n\n\n\n\n\n\nAmbari Version\n\n\nThe Ambari version which this cluster is currently running.\n\n\n\n\n\n\nHDP/HDF Version\n\n\nThe HDP or HDF version which this cluster is currently running.\n\n\n\n\n\n\nAuthentication Source\n\n\nIf you are using an external authentication source (LDAP/AD) for your cluster, you can see it here. Refer to \nUsing an external authentication source\n.\n\n\n\n\n\n\n\n\nBelow this, you will see additional tabs that you can click on in order to see their content:\n\n\n\n\n\n\n\n\nItem\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nHardware\n\n\nThis section includes information about your cluster instances: instance names, instance IDs, instance types, their status, fully qualified domain names (FQDNs), and private and public IPs.\n\n\n\n\n\n\nTags\n\n\nThis section lists keys and values of the user-defined tags, in the same order as you added them.\n\n\n\n\n\n\nRecipes\n\n\nThis section includes recipe-related information. For each recipe, you can see the host group on which a recipe was executed, recipe name, and recipe type. Refer to \nUsing custom scripts (recipes)\n.\n\n\n\n\n\n\nExternal Databases\n\n\nIf you are using an external database for your cluster, you can see it here. Refer to \nusing an external database\n.\n\n\n\n\n\n\nRepository Details\n\n\nThis section includes Ambari and HDP/HDF repository information, as you provided it in the \"Base Images\" section when creating a cluster.\n\n\n\n\n\n\nImage Details\n\n\nThis section includes information about the base image that was used for the Cloudbreak instance.\n\n\n\n\n\n\nNetwork\n\n\nThis section includes information about the names of the network and subnet in which the cluster is running and the links to related cloud provider console.\n\n\n\n\n\n\nSecurity\n\n\nThis section is only available if you have enabled Kerberos security. It provides you with the details of your Kerberos configuration.\n\n\n\n\n\n\nAutoscaling\n\n\nThis section includes configuration options related to autoscaling. Refer to \nConfiguring autoscaling\n.\n\n\n\n\n\n\n\n\nEvent history\n\n\nThe Event History section shows you events logged for the cluster, with the most recent event at the top. For example, after your cluster has been created, the following messages will be written to the log:\n\n\n\nAmbari cluster built; Ambari ip:34.215.103.66\n10/26/2017, 9:41:58 AM\nBuilding Ambari cluster; Ambari ip:34.215.103.66\n10/26/2017, 9:30:20 AM\nStarting Ambari cluster services\n10/26/2017, 9:27:12 AM\nSetting up infrastructure metadata\n10/26/2017, 9:27:11 AM\nBootstrapping infrastructure cluster\n10/26/2017, 9:26:38 AM\nInfrastructure successfully provisioned\n10/26/2017, 9:26:37 AM\nBilling started, Infrastructure successfully provisioned\n10/26/2017, 9:26:37 AM\nInfrastructure metadata collection finished\n10/26/2017, 9:25:39 AM\nInfrastructure creation took 194 seconds\n10/26/2017, 9:25:37 AM\nCreating infrastructure\n10/26/2017, 9:22:22 AM\nSetting up HDP image\n10/26/2017, 9:22:21 AM\n\n\n\nAccessing cluster via SSH\n\n\nIf you plan to access the cluster via the command line clients, SSH into the master node instance in the cluster. \n\n\n\n\nIn order to use SSH, you must generate an SSH key pair or use an existing SSH key pair.  \n\n\nYou can find the cluster instance public IP addresses on the cluster details page.  \n\n\nWhen accessing instances via SSH use the \ncloudbreak\n user. \n\n\n\n\nOn Mac OS, you can use the following syntax to SSH to the VM:\n\nssh -i \"privatekey.pem\" cloudbreak@publicIP\n\nFor example:\n\nssh -i \"dominika-kp.pem\" cloudbreak@p52.25.169.132\n\n\nOn Windows, you can SSH using an SSH client such as PuTTY.\n\n\nAccess Ambari\n\n\nYou can access Ambari web UI by clicking on the links provided in the \nCluster Information\n \n \nAmbari URL\n.\n\n\nSteps\n\n\n\n\n\n\nFrom the cluster dashboard, click on the tile representing your cluster to navigate to cluster details.\n\n\n\n\n\n\nFind the Ambari URL in the \nCluster Information\n section. This URL is available once the Ambari cluster creation process has completed.  \n\n\n\n\n\n\nClick on the \nAmbari URL\n link.\n\n\n\n\n\n\nThe first time you access the server, your browser will attempt to confirm that the SSL Certificate is valid. Since Cloudbreak automatically generates a self-signed certificate, your browser will warn you about an Untrusted Connection and ask you to confirm a Security Exception. Depending on your browser, perform the steps below to proceed.\n\n\n\n\n\n\n\n\nBrowser\n\n\nSteps\n\n\n\n\n\n\n\n\n\n\nFirefox\n\n\nClick \nAdvanced\n \n Click \nAdd Exception...\n \n Click \nConfirm Security Exception\n\n\n\n\n\n\nSafari\n\n\nClick \nContinue\n\n\n\n\n\n\nChrome\n\n\nClick \nAdvanced\n \n Click \nProceed...\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNext: Manage and Monitor Clusters", 
            "title": "Access cluster"
        }, 
        {
            "location": "/gcp-clusters-access/index.html#accessing-a-cluster", 
            "text": "The following section describes how to access the various services in the cluster.", 
            "title": "Accessing a cluster"
        }, 
        {
            "location": "/gcp-clusters-access/index.html#cloudbreak-user-accounts", 
            "text": "The following table describes what credentials to use to access Cloudbreak and Cloudbreak-managed clusters:     Component  Method  Description      Cloudbreak  Web UI, CLI  Access with the username and password provided when launching Cloudbreak on the cloud provider.    Cloudbreak  SSH to VM  Access as the \"cloudbreak\" user with the SSH key provided when launching Cloudbreak on the cloud provider.    Cluster  SSH to VMs  Access as the \"cloudbreak\" user with the SSH key provided during cluster creation.    Cluster  Ambari web UI  Access with the credentials provided in the \u201cCluster User\u201d parameter during cluster creation.    Cluster  Web UIs for specific cluster services  Access with the credentials provided in the \u201cCluster User\u201d parameter during cluster creation.", 
            "title": "Cloudbreak user accounts"
        }, 
        {
            "location": "/gcp-clusters-access/index.html#finding-cluster-information-in-the-web-ui", 
            "text": "Once your cluster is up and running, click on the tile representing your cluster in the Cloudbreak UI to access information related the cluster and access cluster actions.      The information presented includes:   Cluster summary     Cluster information        Event history      \n   Tips \n   \n    Access cluster actions such as resize and sync by clicking on  ACTIONS . \n    Access Ambari web UI by clicking on the link in the  CLUSTER INFORMATION  section.   View public IP addresses for all cluster instances in the  HARDWARE  section. Click on the links to view the instances in the cloud console.   The SSH user that you must use when accessing cluster VMs is \"cloudbreak\".", 
            "title": "Finding cluster information in the web UI"
        }, 
        {
            "location": "/gcp-clusters-access/index.html#cluster-summary", 
            "text": "The summary bar includes the following information about your cluster:     Item  Description      Cluster Name  The name that you selected for your cluster is displayed at the top of the page. Below it is the name of the cluster blueprint.    Time Remaining  If you enabled lifetime management for your cluster, the clock next to the cluster name indicates the amount of time that your cluster will run before it gets terminated. Note that the time remaining counter does not stop when you stop the cluster.    Cloud Provider  The logo of the cloud provider on which the cluster is running.    Credential  The name of the credential used to create the cluster.    Status  Current status. When a cluster is healthy, the status is  Running .    Nodes  The current number of cluster nodes, including the master node.    Uptime  The amount of time (HH:MM) that the cluster has been in the running state since it was started. Each time you stop and restart the cluster, the running time is reset to 0.    Created  The date when the cluster was created. The date format is Mon DD, YYYY. For example: Oct 27, 2017.", 
            "title": "Cluster summary"
        }, 
        {
            "location": "/gcp-clusters-access/index.html#cluster-information", 
            "text": "The following information is available on the cluster details page:      Item  Description      Cluster User  The name of the cluster user that you created when creating the cluster.    SSH Username  The SSH user which you must use when accessing cluster VMs via SSH. The SSH user is always \"cloudbreak\".    Ambari URL  Link to the Ambari web UI.    Region  The region in which the cluster is running in the cloud provider infrastructure.    Availability Zone  The availability zone within the region in which the cluster is running.    Blueprint  The name of the blueprint selected under \"Cluster Type\" to create this cluster.    Created With  The version of Cloudbreak used to create this cluster.    Ambari Version  The Ambari version which this cluster is currently running.    HDP/HDF Version  The HDP or HDF version which this cluster is currently running.    Authentication Source  If you are using an external authentication source (LDAP/AD) for your cluster, you can see it here. Refer to  Using an external authentication source .     Below this, you will see additional tabs that you can click on in order to see their content:     Item  Description      Hardware  This section includes information about your cluster instances: instance names, instance IDs, instance types, their status, fully qualified domain names (FQDNs), and private and public IPs.    Tags  This section lists keys and values of the user-defined tags, in the same order as you added them.    Recipes  This section includes recipe-related information. For each recipe, you can see the host group on which a recipe was executed, recipe name, and recipe type. Refer to  Using custom scripts (recipes) .    External Databases  If you are using an external database for your cluster, you can see it here. Refer to  using an external database .    Repository Details  This section includes Ambari and HDP/HDF repository information, as you provided it in the \"Base Images\" section when creating a cluster.    Image Details  This section includes information about the base image that was used for the Cloudbreak instance.    Network  This section includes information about the names of the network and subnet in which the cluster is running and the links to related cloud provider console.    Security  This section is only available if you have enabled Kerberos security. It provides you with the details of your Kerberos configuration.    Autoscaling  This section includes configuration options related to autoscaling. Refer to  Configuring autoscaling .", 
            "title": "Cluster information"
        }, 
        {
            "location": "/gcp-clusters-access/index.html#event-history", 
            "text": "The Event History section shows you events logged for the cluster, with the most recent event at the top. For example, after your cluster has been created, the following messages will be written to the log:  \nAmbari cluster built; Ambari ip:34.215.103.66\n10/26/2017, 9:41:58 AM\nBuilding Ambari cluster; Ambari ip:34.215.103.66\n10/26/2017, 9:30:20 AM\nStarting Ambari cluster services\n10/26/2017, 9:27:12 AM\nSetting up infrastructure metadata\n10/26/2017, 9:27:11 AM\nBootstrapping infrastructure cluster\n10/26/2017, 9:26:38 AM\nInfrastructure successfully provisioned\n10/26/2017, 9:26:37 AM\nBilling started, Infrastructure successfully provisioned\n10/26/2017, 9:26:37 AM\nInfrastructure metadata collection finished\n10/26/2017, 9:25:39 AM\nInfrastructure creation took 194 seconds\n10/26/2017, 9:25:37 AM\nCreating infrastructure\n10/26/2017, 9:22:22 AM\nSetting up HDP image\n10/26/2017, 9:22:21 AM", 
            "title": "Event history"
        }, 
        {
            "location": "/gcp-clusters-access/index.html#accessing-cluster-via-ssh", 
            "text": "If you plan to access the cluster via the command line clients, SSH into the master node instance in the cluster.    In order to use SSH, you must generate an SSH key pair or use an existing SSH key pair.    You can find the cluster instance public IP addresses on the cluster details page.    When accessing instances via SSH use the  cloudbreak  user.    On Mac OS, you can use the following syntax to SSH to the VM: ssh -i \"privatekey.pem\" cloudbreak@publicIP \nFor example: ssh -i \"dominika-kp.pem\" cloudbreak@p52.25.169.132  On Windows, you can SSH using an SSH client such as PuTTY.", 
            "title": "Accessing cluster via SSH"
        }, 
        {
            "location": "/gcp-clusters-access/index.html#access-ambari", 
            "text": "You can access Ambari web UI by clicking on the links provided in the  Cluster Information     Ambari URL .  Steps    From the cluster dashboard, click on the tile representing your cluster to navigate to cluster details.    Find the Ambari URL in the  Cluster Information  section. This URL is available once the Ambari cluster creation process has completed.      Click on the  Ambari URL  link.    The first time you access the server, your browser will attempt to confirm that the SSL Certificate is valid. Since Cloudbreak automatically generates a self-signed certificate, your browser will warn you about an Untrusted Connection and ask you to confirm a Security Exception. Depending on your browser, perform the steps below to proceed.     Browser  Steps      Firefox  Click  Advanced    Click  Add Exception...    Click  Confirm Security Exception    Safari  Click  Continue    Chrome  Click  Advanced    Click  Proceed...        Next: Manage and Monitor Clusters", 
            "title": "Access Ambari"
        }, 
        {
            "location": "/gcp-clusters-manage/index.html", 
            "text": "Managing and monitoring clusters\n\n\nYou can manage monitor your clusters from the Cloudbreak UI. To do that, click on the tile representing the cluster that you want to access. The actions available for your cluster are listed in the top right corner: \n\n\n \n\n\n\n  \nTips\n\n  \n\n  \nTo add or remove nodes from your cluster click \nACTIONS>Resize\n.\n\n  \nTo synchronize your cluster with the cloud provider account click \nACTIONS>Sync\n.\n\n  \nTo temporarily stop your cluster click \nSTOP\n.\n\n  \nTo terminate your cluster click \nTERMINATE\n.\n\n\n\n\n\n\n\n\n\nRetry a cluster\n\n\nWhen a stack provisioning or cluster creation failure occurs, the \"retry\" option allows you to resume the process from the last failed step. \n\n\nIn some cases the cause of a failed stack provisioning or cluster creation can be eliminated by simply retrying the process. For example, in case of a temporary network outage, a retry should be successful. In other cases, a manual modification is required before a retry can succeed. For example, if you are using a custom image but some configuration is missing, causing the process to fail, you must log in to the machine and fix the issue; Only after that you can retry the the process.\n\n\nOnly failed stack or cluster creations can be retried. A retry can be initiated any number of times on a failed creation process. \n\n\nTo retry provisioning a failed stack or cluster, follow these steps.  \n\n\nSteps\n\n\n\n\n\n\nBrowse to the cluster details.\n\n\n\n\n\n\nClick \nActions\n and select \nRetry\n. \n\n\nOnly failed stack or cluster creations can be retried, so the option is only available in these two cases.  \n\n\n\n\n\n\nClick \nYes\n to confirm. \n\n\nThe operation continues from the last failed step. \n\n\n\n\n\n\nResize a cluster\n\n\nTo resize a cluster, follow these steps.\n\n\n\n\nCluster resizing is not supported for HDF clusters.\n\nTo configure automated cluster scaling, refer to \nConfigure autoscaling\n.   \n\n\n\n\nSteps\n\n\n\n\n\n\nBrowse to the cluster details.\n\n\n\n\n\n\nClick \nActions\n and select \nResize\n. The cluster resize dialog is displayed.\n\n\n\n\n\n\nUsing the +/- controls, adjust the number of nodes for a chosen host group. \n\n\n\n\nYou can only modify one host group at a time. \n\nIt is not possible to resize the Ambari server host group.     \n\n\n\n\n\n\n\n\nClick \nYes\n to confirm the scale-up/scale-down.\n\n\nWhile nodes are being added or removed, cluster status changes to \"Update In Progress\". Once the operation has completed, cluster status changes back to \"Running\". \n\n\n\n\n\n\nSynchronize a cluster\n\n\nUse the \nsync\n option if you:  \n\n\n\n\nMade changes on your cloud provider side (for example, deleted cluster VMs) and you would like to synchronize Cloudbreak with the cloud provider.  \n\n\nManually changed service status in Ambari (for example, restarted services).   \n\n\n\n\nTo synchronize your cluster with the cloud provider, follow these steps. \n\n\nSteps\n\n\n\n\n\n\nBrowse to the cluster details.\n\n\n\n\n\n\nClick \nActions\n and select \nSync\n.\n\n\n\n\n\n\nClick \nYes\n to confirm.\n\n\nYour cluster infrastructure is synchronized based on changes on the cloud provider. The updates are written to the \"Event History\". \n\n\n\n\n\n\nStop a cluster\n\n\nCloudbreak supports stopping and restarting clusters. To stop and restart a cluster managed by Cloudbreak, use the options available from the Cloudbreak UI. \n\n\nSteps\n\n\n\n\n\n\nBrowse to the cluster details.\n\n\n\n\n\n\nClick \nStop\n to stop a currently running cluster.  \n\n\n\n\n\n\nClick \nYes\n to confirm. \n\n\n\n\n\n\nYour cluster status changes to \"Stopping in progress\" and then to \"Stopped\". Once stopping the infrastructure has completed, you will see a \nStart\n option to restart your cluster. \n\n\n\n\n\n\nWhen a cluster is in the \"stopped\" state, you are not charged for the VMs, but you are charged for external storage.  \n\n\nRestart a cluster\n\n\nIf your cluster is in the \"Stopped\" state, you can restart the cluster by follow these steps.\n\n\nSteps\n\n\n\n\n\n\nclick \nStart\n. This option is only available when the cluster has been stopped. \n\n\n\n\n\n\nClick \nYes\n to confirm.\n\n\nYour cluster status changes to \"Start in progress\" and then to \"Running\". \n\n\n\n\n\n\nTerminate a cluster\n\n\nTo terminate a cluster managed by Cloudbreak, use the option available from the Cloudbreak UI. \n\n\nSteps\n\n\n\n\n\n\nBrowse to the cluster details.\n\n\n\n\n\n\nClick \nTerminate\n. \n\n\n\n\n\n\nClick \nYes\n to confirm.\n\n\nAll cluster-related resources will be deleted, unless the resources (such as networks and subnets) existed prior to cluster creation or are used by other VMs in which case they will be preserved. \n\n\n\n\n\n\nForce terminate a cluster\n\n\nCluster deletion may fail if Cloudbreak is unable to delete one or more of the cloud resources that were part of your cluster infrastructure. In such as case, you can use the \nTerminate\n \n \nForce terminate\n option to remove the cluster entry from the Cloudbreak web UI, but you must also check your cloud provider account to see if there are any resources that must be deleted manually.\n\n\nSteps\n\n\n\n\n\n\nBrowse to the cluster details.\n\n\n\n\n\n\nClick \nTerminate\n. \n\n\n\n\n\n\nCheck  \nForce terminate\n.\n\n\n\n\n\n\nClick \nYes\n to confirm. \n\n\nWhen terminating a cluster with Kerberos enabled, you have an option to disable Kerberos prior to cluster termination. This option removes any cluster-related principals from the KDC.\n\n\n\n\n\n\nThis deletes the cluster tile from the UI.  \n\n\n\n\n\n\nLog in to your cloud provider account and \nmanually delete\n any resources that failed to be deleted.\n\n\n\n\n\n\nView cluster history\n\n\nFrom the navigation menu in the Cloudbreak UI, you can access the History page that allows you to generate a report showing basic information related to the clusters that were running within the specified range of dates.\n\n\nTo generate a report, follow these steps.\n\n\nSteps\n\n\n\n\n\n\nFrom the Cloudbreak UI navigation menu, select \nHistory\n.\n\n\n\n\n\n\nOn the History page, select the range of dates and click \nShow History\n to generate a tabular report for the selected period.\n\n\n\n\n\n\nHistory report content\n\n\nEach entry in the report represents one cluster instance group. For each entry, the report includes the following information:\n\n\n\n\nCreated\n - The date when your cluster was created (YYYY-MM-DD).\n\n\nProvider\n - The name of the cloud provider (AWS, Azure, Google, or OpenStack) on which the cluster instances are/were running.\n\n\nCluster Name\n - The name that you selected for the cluster.  \n\n\nInstance Group\n - The name of the host group.   \n\n\nInstance Count\n - The number of nodes in the host group. This number may be a decimal if a cluster has been resized.\n\n\nInstance Type\n - Provider-specific VM type of the cluster instances. \n\n\nRegion\n - The AWS region in which your cluster is/was running.\n\n\nAvailability Zone\n - The availability zone in which your cluster is/was running.      \n\n\nRunning Time (hours)\n - The sum of the running times for all the nodes in the instance group.\n\n\n\n\nThe \nAGGREGATE RUNNING TIME\n is the sum of the Running Times, adjusted for the selected time range.\n\n\nTo learn about how your cloud provider bills you for the VMs, refer to their documentation:\n\n\n\n\nAWS\n      \n\n\nAzure\n     \n\n\nGCP", 
            "title": "Manage and monitor clusters"
        }, 
        {
            "location": "/gcp-clusters-manage/index.html#managing-and-monitoring-clusters", 
            "text": "You can manage monitor your clusters from the Cloudbreak UI. To do that, click on the tile representing the cluster that you want to access. The actions available for your cluster are listed in the top right corner:      \n   Tips \n   \n   To add or remove nodes from your cluster click  ACTIONS>Resize . \n   To synchronize your cluster with the cloud provider account click  ACTIONS>Sync . \n   To temporarily stop your cluster click  STOP . \n   To terminate your cluster click  TERMINATE .", 
            "title": "Managing and monitoring clusters"
        }, 
        {
            "location": "/gcp-clusters-manage/index.html#retry-a-cluster", 
            "text": "When a stack provisioning or cluster creation failure occurs, the \"retry\" option allows you to resume the process from the last failed step.   In some cases the cause of a failed stack provisioning or cluster creation can be eliminated by simply retrying the process. For example, in case of a temporary network outage, a retry should be successful. In other cases, a manual modification is required before a retry can succeed. For example, if you are using a custom image but some configuration is missing, causing the process to fail, you must log in to the machine and fix the issue; Only after that you can retry the the process.  Only failed stack or cluster creations can be retried. A retry can be initiated any number of times on a failed creation process.   To retry provisioning a failed stack or cluster, follow these steps.    Steps    Browse to the cluster details.    Click  Actions  and select  Retry .   Only failed stack or cluster creations can be retried, so the option is only available in these two cases.      Click  Yes  to confirm.   The operation continues from the last failed step.", 
            "title": "Retry a cluster"
        }, 
        {
            "location": "/gcp-clusters-manage/index.html#resize-a-cluster", 
            "text": "To resize a cluster, follow these steps.   Cluster resizing is not supported for HDF clusters. \nTo configure automated cluster scaling, refer to  Configure autoscaling .      Steps    Browse to the cluster details.    Click  Actions  and select  Resize . The cluster resize dialog is displayed.    Using the +/- controls, adjust the number of nodes for a chosen host group.    You can only modify one host group at a time.  \nIt is not possible to resize the Ambari server host group.          Click  Yes  to confirm the scale-up/scale-down.  While nodes are being added or removed, cluster status changes to \"Update In Progress\". Once the operation has completed, cluster status changes back to \"Running\".", 
            "title": "Resize a cluster"
        }, 
        {
            "location": "/gcp-clusters-manage/index.html#synchronize-a-cluster", 
            "text": "Use the  sync  option if you:     Made changes on your cloud provider side (for example, deleted cluster VMs) and you would like to synchronize Cloudbreak with the cloud provider.    Manually changed service status in Ambari (for example, restarted services).      To synchronize your cluster with the cloud provider, follow these steps.   Steps    Browse to the cluster details.    Click  Actions  and select  Sync .    Click  Yes  to confirm.  Your cluster infrastructure is synchronized based on changes on the cloud provider. The updates are written to the \"Event History\".", 
            "title": "Synchronize a cluster"
        }, 
        {
            "location": "/gcp-clusters-manage/index.html#stop-a-cluster", 
            "text": "Cloudbreak supports stopping and restarting clusters. To stop and restart a cluster managed by Cloudbreak, use the options available from the Cloudbreak UI.   Steps    Browse to the cluster details.    Click  Stop  to stop a currently running cluster.      Click  Yes  to confirm.     Your cluster status changes to \"Stopping in progress\" and then to \"Stopped\". Once stopping the infrastructure has completed, you will see a  Start  option to restart your cluster.     When a cluster is in the \"stopped\" state, you are not charged for the VMs, but you are charged for external storage.", 
            "title": "Stop a cluster"
        }, 
        {
            "location": "/gcp-clusters-manage/index.html#restart-a-cluster", 
            "text": "If your cluster is in the \"Stopped\" state, you can restart the cluster by follow these steps.  Steps    click  Start . This option is only available when the cluster has been stopped.     Click  Yes  to confirm.  Your cluster status changes to \"Start in progress\" and then to \"Running\".", 
            "title": "Restart a cluster"
        }, 
        {
            "location": "/gcp-clusters-manage/index.html#terminate-a-cluster", 
            "text": "To terminate a cluster managed by Cloudbreak, use the option available from the Cloudbreak UI.   Steps    Browse to the cluster details.    Click  Terminate .     Click  Yes  to confirm.  All cluster-related resources will be deleted, unless the resources (such as networks and subnets) existed prior to cluster creation or are used by other VMs in which case they will be preserved.", 
            "title": "Terminate a cluster"
        }, 
        {
            "location": "/gcp-clusters-manage/index.html#force-terminate-a-cluster", 
            "text": "Cluster deletion may fail if Cloudbreak is unable to delete one or more of the cloud resources that were part of your cluster infrastructure. In such as case, you can use the  Terminate     Force terminate  option to remove the cluster entry from the Cloudbreak web UI, but you must also check your cloud provider account to see if there are any resources that must be deleted manually.  Steps    Browse to the cluster details.    Click  Terminate .     Check   Force terminate .    Click  Yes  to confirm.   When terminating a cluster with Kerberos enabled, you have an option to disable Kerberos prior to cluster termination. This option removes any cluster-related principals from the KDC.    This deletes the cluster tile from the UI.      Log in to your cloud provider account and  manually delete  any resources that failed to be deleted.", 
            "title": "Force terminate a cluster"
        }, 
        {
            "location": "/gcp-clusters-manage/index.html#view-cluster-history", 
            "text": "From the navigation menu in the Cloudbreak UI, you can access the History page that allows you to generate a report showing basic information related to the clusters that were running within the specified range of dates.  To generate a report, follow these steps.  Steps    From the Cloudbreak UI navigation menu, select  History .    On the History page, select the range of dates and click  Show History  to generate a tabular report for the selected period.", 
            "title": "View cluster history"
        }, 
        {
            "location": "/gcp-clusters-manage/index.html#history-report-content", 
            "text": "Each entry in the report represents one cluster instance group. For each entry, the report includes the following information:   Created  - The date when your cluster was created (YYYY-MM-DD).  Provider  - The name of the cloud provider (AWS, Azure, Google, or OpenStack) on which the cluster instances are/were running.  Cluster Name  - The name that you selected for the cluster.    Instance Group  - The name of the host group.     Instance Count  - The number of nodes in the host group. This number may be a decimal if a cluster has been resized.  Instance Type  - Provider-specific VM type of the cluster instances.   Region  - The AWS region in which your cluster is/was running.  Availability Zone  - The availability zone in which your cluster is/was running.        Running Time (hours)  - The sum of the running times for all the nodes in the instance group.   The  AGGREGATE RUNNING TIME  is the sum of the Running Times, adjusted for the selected time range.  To learn about how your cloud provider bills you for the VMs, refer to their documentation:   AWS         Azure        GCP", 
            "title": "History report content"
        }, 
        {
            "location": "/os-launch/index.html", 
            "text": "Launching Cloudbreak on OpenStack\n\n\nBefore launching Cloudbreak on OpenStack, review and meet the prerequisites. Next, import Cloudbreak image, launch a VM, SSH to the VM, and start Cloudbreak. Once Cloudbreak is running, log in to the Cloudbreak UI and create a Cloudbreak credential.\n\n\nMeet minimum system requirements\n\n\nBefore launching Cloudbreak on your OpenStack, make sure that your OpenStack deployment fulfills the following requirements.\n\n\nSupported Linux distributions\n\n\nThe following versions of the \nRed Hat distribution of OpenStack\n (RDO) are supported:\n\n\n\n\nJuno\n\n\nKilo\n\n\nLiberty\n\n\nMitaka\n\n\n\n\nStandard modules\n\n\nCloudbreak requires that the following standard modules are installed and configured on OpenStack:\n\n\n\n\nKeystone V2 or Keystone V3\n\n\nNeutron (Self-service and provider networking)\n\n\nNova (KVM or Xen hypervisor)\n\n\nGlance\n\n\nCinder (Optional)\n\n\nHeat (Optional but highly recommended, since provisioning through native API calls will be deprecated in the future)\n\n\n\n\nRelated links\n\n\nRed Hat distribution of OpenStack\n (External)\n\n\nMeet the prerequisites\n\n\nBefore launching Cloudbreak on OpenStack, you must meet the following prerequisites.\n\n\nSSH key pair\n\n\nGenerate a new SSH key pair\n or use an existing SSH key pair to your OpenStack account. You will be required to select it when launching the VM.\n\n\nSecurity group\n\n\nIn order to launch Cloudbreak, you must have an existing security group with the following ports open: 22 (for access via SSH), 80 (for access via HTTP), and 443 (for access via HTTPS).\n\n\nFor information about OpenStack security groups, refer to the \nOpenStack Administrator Guide\n.\n\n\nRelated links\n\n\nOpenStack Administrator Guide\n (External)\n\n\nLaunch the VM\n\n\nIn your OpenStack, launch and instance providing the following parameters:\n\n\n\n\nSelect a VM flavor which meets the following minimum requirements: 4GB RAM, 10GB disk, 2 cores.\n\n\nSelect the Cloudbreak deployer image that you imported earlier and launch an instance using that image.\n\n\nSelect your SSH key pair.\n\n\nSelect the security group which has the following ports open: 22 (SSH) and 443 (HTTPS).\n\n\nSelect your preconfigured network.\n\n\n\n\nSSH to the VM\n\n\nNow that your VM is ready, access it via SSH:\n\n\n\n\nUse a private key matching the public key that you added to your OpenStack project.\n\n\nThe SSH user is called \"cloudbreak\".\n\n\nYou can obtain the VM's IP address from the details of your instance.\n\n\n\n\nOn Mac OS X, you can SSH to the VM by running the following from the Terminal app: \nssh -i \"your-private-key.pem\" cloudbreak@instance_IP\n where \"your-private-key.pem\" points to the location of your private key and \"instance_IP\" is the public IP address of the VM.\n\n\nOn Windows, you can use \nPuTTy\n.\n\n\nInitialize the Profile\n\n\nAfter accessing the VM via SSH, you must initialize your Profile.\n\n\nSteps\n\n\n\n\n\n\nNavigate to the cloudbreak-deployment directory:\n\n\ncd /var/lib/cloudbreak-deployment/\n\n\nThis directory contains configuration files and the supporting binaries for Cloudbreak deployer.\n\n\n\n\n\n\nInitialize your profile by creating a new file called \nProfile\n and adding the following content:\n\n\nexport UAA_DEFAULT_SECRET=MY-SECRET\nexport UAA_DEFAULT_USER_PW=MY-PASSWORD\nexport UAA_DEFAULT_USER_EMAIL=MY-EMAIL\nexport PUBLIC_IP=VM-PUBLIC-IP\n\n\nFor example:\n\n\nexport UAA_DEFAULT_SECRET=MySecret123\nexport UAA_DEFAULT_USER_PW=MySecurePassword123\nexport UAA_DEFAULT_USER_EMAIL=dbialek@hortonworks.com\nexport PUBLIC_IP=34.212.141.253\n\n\n\n\nYou will need to provide the email and password when logging in to the Cloudbreak web UI and when using the Cloudbreak CLI. The secret will be used by Cloudbreak for authentication.\n\n\n\n\n\n\n\n\nPerform optional configurations\n\n\n\n\nThese configurations are optional.\n\n\n\n\nConfiguring a self-signed certificate\n\n\nIf your OpenStack is secured with a self-signed certificate, you need to import that certificate into Cloudbreak, or else Cloudbreak won't be able to communicate with your OpenStack.\n\n\nTo import the certificate, place the certificate file in the \n/certs/trusted/\n directory, follow these steps.\n\n\nSteps\n\n\n\n\nNavigate to the \ncerts\n directory (automatically generated).\n\n\nCreate the \ntrusted\n directory.\n\n\nCopy the certificate to the \ntrusted\n directory.\n\n\n\n\nCloudbreak will automatically pick up the certificate and import it into its trust store upon start.\n\n\nStart Cloudbreak deployer\n\n\nLaunch Cloudbreak deployer using the following steps.\n\n\nStart the Cloudbreak application by using the following command:\n\n\ncbd start\n\n\n\nThis will start the Docker containers and initialize the application. The first time you start the Cloudbreak app, this also downloads of all the necessary docker images.\n\n\nOnce the \ncbd start\n has finished, it returns the \"Uluwatu (Cloudbreak UI) url\" which you can later paste in your browser and log in to Cloudbreak web UI.\n\n\nTo check Cloudbreak deployer version and health, use:\n\n\ncbd doctor\n\n\n\nIf you would like to check Cloudbreak Application logs, use:\n\n\ncbd logs cloudbreak\n\n\n\nYou should see a message like this in the log: \nStarted CloudbreakApplication in 36.823 seconds.\n Cloudbreak takes less than a minute to start. If you try to access the Cloudbreak UI before Cloudbreak started, you will get a \"Bad Gateway\" error or \"Cannot connect to Cloudbreak\" error.\n\n\nAccess Cloudbreak web UI\n\n\nLog in to the Cloudbreak UI using the following steps.\n\n\nSteps\n\n\n\n\n\n\nYou can log into the Cloudbreak application at \nhttps://IP_Address\n where \"IP_Address\" if the public IP of your OpenStack VM. For example \nhttps://34.212.141.253\n.\n\n\n\n\n\n\nConfirm the security exception to proceed to the Cloudbreak web UI.\n\n\nThe first time you access Cloudbreak UI, Cloudbreak will automatically generate a self-signed certificate, due to which your browser will warn you about an untrusted connection and will ask you to confirm a security exception. You can safely proceed to the website.\n\n\n\n\n\n\n\n\nBrowser\n\n\nSteps\n\n\n\n\n\n\n\n\n\n\nFirefox\n\n\nClick \nAdvanced\n \n Click \nAdd Exception...\n \n Click \nConfirm Security Exception\n\n\n\n\n\n\nSafari\n\n\nClick \nContinue\n\n\n\n\n\n\nChrome\n\n\nClick \nAdvanced\n \n Click \nProceed...\n\n\n\n\n\n\n\n\n\n\n\n\nThe login page is displayed:\n\n\n\n\n\n\n\n\nLog in to the Cloudbreak web UI using the credential that you configured in your \nProfile\n file when \nstarting Cloudbreak deployer\n:\n\n\n\n\nThe username is the \nUAA_DEFAULT_USER_EMAIL\n\n\nThe password is the \nUAA_DEFAULT_USER_PW\n\n\n\n\n\n\n\n\nUpon a successful login, you are redirected to the dashboard:\n\n\n\n\n\n\n\n\nCreate Cloudbreak credential\n\n\nCloudbreak works by connecting your OpenStack account through this credential, and then uses it to create resources on your behalf. Before you can start provisioning cluster using Cloudbreak, you must create a Cloudbreak credential.\n\n\nSteps\n\n\n\n\n\n\nIn the Cloudbreak web UI, select \nCredentials\n from the navigation pane.\n\n\n\n\n\n\nClick \nCreate Credential\n.\n\n\n\n\n\n\nUnder \nCloud provider\n, select \"Google Cloud Platform\".\n\n\n\n\n\n\n\n\nSelect the keystone version.\n\n\n\n\n\n\nProvide the  following information:\n\n\nFor Keystone v2:\n\n\n\n\n\n\n\n\nParameter\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nName\n\n\nEnter a name for your credential.\n\n\n\n\n\n\nDescription\n\n\n(Optional) Enter a description.\n\n\n\n\n\n\nUser\n\n\nEnter your OpenStack user name.\n\n\n\n\n\n\nPassword\n\n\nEnter your OpenStack password.\n\n\n\n\n\n\nTenant Name\n\n\nEnter the OpenStack tenant name.\n\n\n\n\n\n\nEndpoint\n\n\nEnter the OpenStack endpoint.\n\n\n\n\n\n\nAPI Facing\n\n\n(Optional) Select \npublic\n, \nprivate\n, or \ninternal\n.\n\n\n\n\n\n\n\n\nFor Keystone v3:\n\n\n\n\n\n\n\n\nParameter\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nKeystone scope\n\n\nSelect the scope: default, domain, or project.\n\n\n\n\n\n\nName\n\n\nEnter a name for your credential.\n\n\n\n\n\n\nDescription\n\n\n(Optional) Enter a description.\n\n\n\n\n\n\nUser\n\n\nEnter your OpenStack user name.\n\n\n\n\n\n\nPassword\n\n\nEnter your OpenStack password.\n\n\n\n\n\n\nUser Domain\n\n\nEnter your OpenStack user domain.\n\n\n\n\n\n\nEndpoint\n\n\nEnter the OpenStack endpoint.\n\n\n\n\n\n\nAPI Facing\n\n\n(Optional) Select \npublic\n, \nprivate\n, or \ninternal\n.\n\n\n\n\n\n\n\n\n\n\n\n\nClick \nCreate\n.\n\n\n\n\n\n\nYour credential should now be displayed in the \nCredentials\n pane.\n\n\nCongratulations! You have successfully launched Cloudbreak and created a Cloudbreak credential. Now you can use Cloudbreak to \ncreate clusters\n.\n\n\n\n\n\n\n\n\nNext: Create a Cluster\n\n\n\n\n\nOptional Steps\n\n\nImport images to OpenStack\n\n\nAn OpenStack administrator must perform these steps to add the Cloudbreak deployer image to your OpenStack deployment. \n\n\n\n\nImporting prewarmed and base HDP and HDF images is no longer required, because if these images are not imported manually, Cloudbreak will import them once you attempt to create an HDP cluster.\n\n\n\n\nImport HDP prewarmed image\n\n\nThese steps are no longer required, because if these images are not imported manually, Cloudbreak will import them once you attempt to create an HDP cluster. If you would like to import HDP prewarmed images manually (instead of having them imported), you can do this by using the following steps.\n\n\nSteps\n\n\n\n\n\n\nDownload the latest HDP image to your local machine:\n\n\ncurl -O https://public-repo-1.hortonworks.com/HDP/cloudbreak/cb-hdp-26-1803301748.img\n\n\n\n\n\n\nSet the following environment variables for the OpenStack image import:\n\n\nexport CB_LATEST_IMAGE=cb-hdp-26-1803301748.img\nexport CB_LATEST_IMAGE_NAME=cb-hdp-26-1803301748.img\nexport OS_USERNAME=your_os_user_name\nexport OS_AUTH_URL=your_authentication_url\nexport OS_TENANT_NAME=your_os_tenant_name\n\n\n\n\n\n\nImport the new image into your OpenStack:\n\n\nglance image-create --name \"$CB_LATEST_IMAGE_NAME\" --file \"$CB_LATEST_IMAGE\" --disk-format qcow2 --container-format bare --progress\n\n\n\n\n\n\nAfter performing the import, you should be able to see the Cloudbreak image among your OpenStack images.\n\n\nImport HDF prewarmed image\n\n\nNo prewarmed image is currently available for HDF. \n\n\nImport HDP/HDF base image\n\n\nThese steps are no longer required, because if these images are not imported manually, Cloudbreak will import them once you attempt to create an HDP or HDF cluster. If you would like to import HDP/HDF base images manually (instead of having them imported), you can do this by using the following steps.\n\n\nSteps\n\n\n\n\n\n\nDownload the latest HDP image to your local machine:\n\n\ncurl -O https://public-repo-1.hortonworks.com/HDP/cloudbreak/cb-hdp--1803211203.img\n\n\n\n\n\n\nSet the following environment variables for the OpenStack image import:\n\n\nexport CB_LATEST_IMAGE=cb-hdp--1803211203.img\nexport CB_LATEST_IMAGE_NAME=cb-hdp--1803211203.img\nexport OS_USERNAME=your_os_user_name\nexport OS_AUTH_URL=your_authentication_url\nexport OS_TENANT_NAME=your_os_tenant_name\n\n\n\n\n\n\nImport the new image into your OpenStack:\n\n\nglance image-create --name \"$CB_LATEST_IMAGE_NAME\" --file \"$CB_LATEST_IMAGE\" --disk-format qcow2 --container-format bare --progress\n\n\n\n\n\n\nAfter performing the import, you should be able to see the Cloudbreak image among your OpenStack images.", 
            "title": "Launch on OpenStack"
        }, 
        {
            "location": "/os-launch/index.html#launching-cloudbreak-on-openstack", 
            "text": "Before launching Cloudbreak on OpenStack, review and meet the prerequisites. Next, import Cloudbreak image, launch a VM, SSH to the VM, and start Cloudbreak. Once Cloudbreak is running, log in to the Cloudbreak UI and create a Cloudbreak credential.", 
            "title": "Launching Cloudbreak on OpenStack"
        }, 
        {
            "location": "/os-launch/index.html#meet-minimum-system-requirements", 
            "text": "Before launching Cloudbreak on your OpenStack, make sure that your OpenStack deployment fulfills the following requirements.", 
            "title": "Meet minimum system requirements"
        }, 
        {
            "location": "/os-launch/index.html#supported-linux-distributions", 
            "text": "The following versions of the  Red Hat distribution of OpenStack  (RDO) are supported:   Juno  Kilo  Liberty  Mitaka", 
            "title": "Supported Linux distributions"
        }, 
        {
            "location": "/os-launch/index.html#standard-modules", 
            "text": "Cloudbreak requires that the following standard modules are installed and configured on OpenStack:   Keystone V2 or Keystone V3  Neutron (Self-service and provider networking)  Nova (KVM or Xen hypervisor)  Glance  Cinder (Optional)  Heat (Optional but highly recommended, since provisioning through native API calls will be deprecated in the future)   Related links  Red Hat distribution of OpenStack  (External)", 
            "title": "Standard modules"
        }, 
        {
            "location": "/os-launch/index.html#meet-the-prerequisites", 
            "text": "Before launching Cloudbreak on OpenStack, you must meet the following prerequisites.", 
            "title": "Meet the prerequisites"
        }, 
        {
            "location": "/os-launch/index.html#ssh-key-pair", 
            "text": "Generate a new SSH key pair  or use an existing SSH key pair to your OpenStack account. You will be required to select it when launching the VM.", 
            "title": "SSH key pair"
        }, 
        {
            "location": "/os-launch/index.html#security-group", 
            "text": "In order to launch Cloudbreak, you must have an existing security group with the following ports open: 22 (for access via SSH), 80 (for access via HTTP), and 443 (for access via HTTPS).  For information about OpenStack security groups, refer to the  OpenStack Administrator Guide .  Related links  OpenStack Administrator Guide  (External)", 
            "title": "Security group"
        }, 
        {
            "location": "/os-launch/index.html#launch-the-vm", 
            "text": "In your OpenStack, launch and instance providing the following parameters:   Select a VM flavor which meets the following minimum requirements: 4GB RAM, 10GB disk, 2 cores.  Select the Cloudbreak deployer image that you imported earlier and launch an instance using that image.  Select your SSH key pair.  Select the security group which has the following ports open: 22 (SSH) and 443 (HTTPS).  Select your preconfigured network.", 
            "title": "Launch the VM"
        }, 
        {
            "location": "/os-launch/index.html#ssh-to-the-vm", 
            "text": "Now that your VM is ready, access it via SSH:   Use a private key matching the public key that you added to your OpenStack project.  The SSH user is called \"cloudbreak\".  You can obtain the VM's IP address from the details of your instance.   On Mac OS X, you can SSH to the VM by running the following from the Terminal app:  ssh -i \"your-private-key.pem\" cloudbreak@instance_IP  where \"your-private-key.pem\" points to the location of your private key and \"instance_IP\" is the public IP address of the VM.  On Windows, you can use  PuTTy .", 
            "title": "SSH to the VM"
        }, 
        {
            "location": "/os-launch/index.html#initialize-the-profile", 
            "text": "After accessing the VM via SSH, you must initialize your Profile.  Steps    Navigate to the cloudbreak-deployment directory:  cd /var/lib/cloudbreak-deployment/  This directory contains configuration files and the supporting binaries for Cloudbreak deployer.    Initialize your profile by creating a new file called  Profile  and adding the following content:  export UAA_DEFAULT_SECRET=MY-SECRET\nexport UAA_DEFAULT_USER_PW=MY-PASSWORD\nexport UAA_DEFAULT_USER_EMAIL=MY-EMAIL\nexport PUBLIC_IP=VM-PUBLIC-IP  For example:  export UAA_DEFAULT_SECRET=MySecret123\nexport UAA_DEFAULT_USER_PW=MySecurePassword123\nexport UAA_DEFAULT_USER_EMAIL=dbialek@hortonworks.com\nexport PUBLIC_IP=34.212.141.253   You will need to provide the email and password when logging in to the Cloudbreak web UI and when using the Cloudbreak CLI. The secret will be used by Cloudbreak for authentication.", 
            "title": "Initialize the Profile"
        }, 
        {
            "location": "/os-launch/index.html#perform-optional-configurations", 
            "text": "These configurations are optional.", 
            "title": "Perform optional configurations"
        }, 
        {
            "location": "/os-launch/index.html#configuring-a-self-signed-certificate", 
            "text": "If your OpenStack is secured with a self-signed certificate, you need to import that certificate into Cloudbreak, or else Cloudbreak won't be able to communicate with your OpenStack.  To import the certificate, place the certificate file in the  /certs/trusted/  directory, follow these steps.  Steps   Navigate to the  certs  directory (automatically generated).  Create the  trusted  directory.  Copy the certificate to the  trusted  directory.   Cloudbreak will automatically pick up the certificate and import it into its trust store upon start.", 
            "title": "Configuring a self-signed certificate"
        }, 
        {
            "location": "/os-launch/index.html#start-cloudbreak-deployer", 
            "text": "Launch Cloudbreak deployer using the following steps.  Start the Cloudbreak application by using the following command:  cbd start  This will start the Docker containers and initialize the application. The first time you start the Cloudbreak app, this also downloads of all the necessary docker images.  Once the  cbd start  has finished, it returns the \"Uluwatu (Cloudbreak UI) url\" which you can later paste in your browser and log in to Cloudbreak web UI.  To check Cloudbreak deployer version and health, use:  cbd doctor  If you would like to check Cloudbreak Application logs, use:  cbd logs cloudbreak  You should see a message like this in the log:  Started CloudbreakApplication in 36.823 seconds.  Cloudbreak takes less than a minute to start. If you try to access the Cloudbreak UI before Cloudbreak started, you will get a \"Bad Gateway\" error or \"Cannot connect to Cloudbreak\" error.", 
            "title": "Start Cloudbreak deployer"
        }, 
        {
            "location": "/os-launch/index.html#access-cloudbreak-web-ui", 
            "text": "Log in to the Cloudbreak UI using the following steps.  Steps    You can log into the Cloudbreak application at  https://IP_Address  where \"IP_Address\" if the public IP of your OpenStack VM. For example  https://34.212.141.253 .    Confirm the security exception to proceed to the Cloudbreak web UI.  The first time you access Cloudbreak UI, Cloudbreak will automatically generate a self-signed certificate, due to which your browser will warn you about an untrusted connection and will ask you to confirm a security exception. You can safely proceed to the website.     Browser  Steps      Firefox  Click  Advanced    Click  Add Exception...    Click  Confirm Security Exception    Safari  Click  Continue    Chrome  Click  Advanced    Click  Proceed...       The login page is displayed:     Log in to the Cloudbreak web UI using the credential that you configured in your  Profile  file when  starting Cloudbreak deployer :   The username is the  UAA_DEFAULT_USER_EMAIL  The password is the  UAA_DEFAULT_USER_PW     Upon a successful login, you are redirected to the dashboard:", 
            "title": "Access Cloudbreak web UI"
        }, 
        {
            "location": "/os-launch/index.html#create-cloudbreak-credential", 
            "text": "Cloudbreak works by connecting your OpenStack account through this credential, and then uses it to create resources on your behalf. Before you can start provisioning cluster using Cloudbreak, you must create a Cloudbreak credential.  Steps    In the Cloudbreak web UI, select  Credentials  from the navigation pane.    Click  Create Credential .    Under  Cloud provider , select \"Google Cloud Platform\".     Select the keystone version.    Provide the  following information:  For Keystone v2:     Parameter  Description      Name  Enter a name for your credential.    Description  (Optional) Enter a description.    User  Enter your OpenStack user name.    Password  Enter your OpenStack password.    Tenant Name  Enter the OpenStack tenant name.    Endpoint  Enter the OpenStack endpoint.    API Facing  (Optional) Select  public ,  private , or  internal .     For Keystone v3:     Parameter  Description      Keystone scope  Select the scope: default, domain, or project.    Name  Enter a name for your credential.    Description  (Optional) Enter a description.    User  Enter your OpenStack user name.    Password  Enter your OpenStack password.    User Domain  Enter your OpenStack user domain.    Endpoint  Enter the OpenStack endpoint.    API Facing  (Optional) Select  public ,  private , or  internal .       Click  Create .    Your credential should now be displayed in the  Credentials  pane.  Congratulations! You have successfully launched Cloudbreak and created a Cloudbreak credential. Now you can use Cloudbreak to  create clusters .     Next: Create a Cluster", 
            "title": "Create Cloudbreak credential"
        }, 
        {
            "location": "/os-launch/index.html#optional-steps", 
            "text": "", 
            "title": "Optional Steps"
        }, 
        {
            "location": "/os-launch/index.html#import-images-to-openstack", 
            "text": "An OpenStack administrator must perform these steps to add the Cloudbreak deployer image to your OpenStack deployment.    Importing prewarmed and base HDP and HDF images is no longer required, because if these images are not imported manually, Cloudbreak will import them once you attempt to create an HDP cluster.", 
            "title": "Import images to OpenStack"
        }, 
        {
            "location": "/os-launch/index.html#import-hdp-prewarmed-image", 
            "text": "These steps are no longer required, because if these images are not imported manually, Cloudbreak will import them once you attempt to create an HDP cluster. If you would like to import HDP prewarmed images manually (instead of having them imported), you can do this by using the following steps.  Steps    Download the latest HDP image to your local machine:  curl -O https://public-repo-1.hortonworks.com/HDP/cloudbreak/cb-hdp-26-1803301748.img    Set the following environment variables for the OpenStack image import:  export CB_LATEST_IMAGE=cb-hdp-26-1803301748.img\nexport CB_LATEST_IMAGE_NAME=cb-hdp-26-1803301748.img\nexport OS_USERNAME=your_os_user_name\nexport OS_AUTH_URL=your_authentication_url\nexport OS_TENANT_NAME=your_os_tenant_name    Import the new image into your OpenStack:  glance image-create --name \"$CB_LATEST_IMAGE_NAME\" --file \"$CB_LATEST_IMAGE\" --disk-format qcow2 --container-format bare --progress    After performing the import, you should be able to see the Cloudbreak image among your OpenStack images.", 
            "title": "Import HDP prewarmed image"
        }, 
        {
            "location": "/os-launch/index.html#import-hdf-prewarmed-image", 
            "text": "No prewarmed image is currently available for HDF.", 
            "title": "Import HDF prewarmed image"
        }, 
        {
            "location": "/os-launch/index.html#import-hdphdf-base-image", 
            "text": "These steps are no longer required, because if these images are not imported manually, Cloudbreak will import them once you attempt to create an HDP or HDF cluster. If you would like to import HDP/HDF base images manually (instead of having them imported), you can do this by using the following steps.  Steps    Download the latest HDP image to your local machine:  curl -O https://public-repo-1.hortonworks.com/HDP/cloudbreak/cb-hdp--1803211203.img    Set the following environment variables for the OpenStack image import:  export CB_LATEST_IMAGE=cb-hdp--1803211203.img\nexport CB_LATEST_IMAGE_NAME=cb-hdp--1803211203.img\nexport OS_USERNAME=your_os_user_name\nexport OS_AUTH_URL=your_authentication_url\nexport OS_TENANT_NAME=your_os_tenant_name    Import the new image into your OpenStack:  glance image-create --name \"$CB_LATEST_IMAGE_NAME\" --file \"$CB_LATEST_IMAGE\" --disk-format qcow2 --container-format bare --progress    After performing the import, you should be able to see the Cloudbreak image among your OpenStack images.", 
            "title": "Import HDP/HDF base image"
        }, 
        {
            "location": "/os-create/index.html", 
            "text": "Creating a cluster on OpenStack\n\n\nUse these steps to create a cluster.\n\n\n\n\nTroubleshooting cluster creation\n\n\nIf you experience problems during cluster creation, refer to \nTroubleshooting cluster creation\n.\n\n\n\nSteps\n\n\n\n\n\n\nLog in to the Cloudbreak UI.\n\n\n\n\n\n\nClick \nCreate Cluster\n and the \nCreate Cluster\n wizard is displayed.\n\n    By default, \nBasic\n view is displayed. To view advanced options, click \nAdvanced\n. To learn about advanced options, refer to \nAdvanced cluster options\n.\n\n\n \n\n\n\n\n\n\nOn the \nGeneral Configuration\n page, specify the following general parameters for your cluster:\n\n\n\n\n\n\n\n\nParameter\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nSelect Credential\n\n\nChoose a previously created credential.\n\n\n\n\n\n\nCluster Name\n\n\nEnter a name for your cluster. The name must be between 5 and 40 characters, must start with a letter, and must only include lowercase letters, numbers, and hyphens.\n\n\n\n\n\n\nRegion\n\n\nSelect the region in which you would like to launch your cluster.\n\n\n\n\n\n\nPlatform Version\n\n\nChoose the HDP or HDF version to use for this cluster. Blueprints available for this platform version will be populated under \"Cluster Type\" below.\n\n\n\n\n\n\nCluster Type\n\n\nChoose one of default cluster configurations, or, if you have defined your own cluster configuration via Ambari blueprint, you can choose it here. For more information on default and custom blueprints, refer to \nUsing custom blueprints\n.\n\n\n\n\n\n\nFlex Subscription\n\n\nThis option will appear if you have configured your deployment for a \nflex support subscription\n.\n\n\n\n\n\n\n\n\n\n\n\n\nOn the \nHardware and Storage\n page, for each host group provide the following information to define your cluster nodes and attached storage:\n\n\n\n\n\n\n\n\nParameter\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nInstance Type\n\n\nSelect an instance type.\n\n\n\n\n\n\nInstance Count\n\n\nEnter the number of instances of a given type. Default is 1.\n\n\n\n\n\n\nAmbari Server\n\n\nYou must select one node for Ambari Server. The \"Group Size\" for that host group must be set to \"1\".\n\n\n\n\n\n\n\n\n\n\n\n\nOn the \nNetwork\n page, provide the following to specify the networking resources that will be used for your cluster:\n\n\n\n\n\n\n\n\nParameter\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nSelect Network\n\n\nSelect the virtual network in which you would like your cluster to be provisioned. You can select an existing network or create a new network.\n\n\n\n\n\n\nSelect Subnet\n\n\nSelect the subnet in which you would like your cluster to be provisioned. If you are using a new network, create a new subnet. If you are using an existing network, select an existing subnet.\n\n\n\n\n\n\nSubnet (CIDR)\n\n\nIf you selected to create a new subnet, you must define a valid \nCIDR\n for the subnet. Default is 10.0.0.0/16.\n\n\n\n\n\n\n\n\n\n\nCloudbreak uses public IP addresses when communicating with cluster nodes.  \n\n\n\n\n\n\n\n\nDefine security groups for each host group. You can either create new security groups and define their rules or reuse existing security groups:\n\n\n\n\n\n\n\n\nOption\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nNew Security Group\n\n\n(Default) Creates a new security group with the rules that you defined:\nA set of \ndefault rules\n is provided. You should review and adjust these default rules. If you do not make any modifications, default rules will be applied. \nYou may open ports by defining the CIDR, entering port range, selecting protocol and clicking \n+\n.\nYou may delete default or previously added rules using the delete icon.\nIf you don't want to use security group, remove the default rules.\n\n\n\n\n\n\nExisting Security Groups\n\n\nAllows you to select an existing security group that is already available in the selected provider region. This selection is disabled if no existing security groups are available in your chosen region.\n\n\n\n\n\n\n\n\n\n\nImportant\n\n\n\nBy default, ports 22, 443, and 9443 are set to 0.0.0.0/0 CIDR for inbound access on the Ambari node security group. We strongly recommend that you limit this CIDR, considering the following restrictions:\n\nPorts 22 and 9443 must be open to Cloudbreak's CIDR. You can set CB_DEFAULT_GATEWAY_CIDR in your Cloudbreak's Profile file in order to automatically open ports 22 and 9443 to your Cloudbreak IP. Refer to \nRestricting inbound access from Cloudbreak to cluster\n.\n\n\nPort 22 must be open to your CIDR if you would like to access the master node via SSH.\n\n\nPort 443 must be open to your CIDR if you would like to access Ambari web UI in a browser.\n\n\n\n\n\n\n\n\n\n\nImportant\n\n\n\nBy default, port 22 is set to 0.0.0.0/0 CIDR for inbound access on non-Ambari node security groups. We strongly recommend that you remove it.\n\n\n\n\n\n\nImportant\n\n\n\nDepending on what services you are including, you need to open additional ports as required by these services. For example, when using the Flow Management blueprint, you must open port 9091 for NiFi (on NiFI host group) and port 61443 for NiFI Registry (on the Services host group).\n\n\n\n\n\n\n\n\n\n\nOn the \nSecurity\n page, provide the following parameters:\n\n\n\n\n\n\n\n\nParameter\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nCluster User\n\n\nYou can log in to the Ambari UI using this username. By default, this is set to \nadmin\n.\n\n\n\n\n\n\nPassword\n\n\nYou can log in to the Ambari UI using this password.\n\n\n\n\n\n\nConfirm Password\n\n\nConfirm the password.\n\n\n\n\n\n\nNew SSH public key\n\n\nCheck this option to specify a new public key and then enter the public key. You will use the matching private key to access your cluster nodes via SSH.\n\n\n\n\n\n\nExisting SSH public key\n\n\nSelect an existing public key. You will use the matching private key to access your cluster nodes via SSH. This is a default option as long as an existing SSH public key is available.\n\n\n\n\n\n\n\n\n\n\n\n\nClick on \nCreate Cluster\n to create a cluster.\n\n\n\n\n\n\nYou will be redirected to the Cloudbreak dashboard, and a new tile representing your cluster will appear at the top of the page.\n\n\n\n\n\n\nRelated links\n\n\nFlex support subscription\n\n\nUsing custom blueprints\n \n\n\nDefault cluster security groups\n\n\nTroubleshooting cluster creation\n  \n\n\nCIDR\n (External)    \n\n\nAdvanced cluster options\n\n\nClick on \nAdvanced\n to view and enter additional configuration options\n\n\nAvailability zone\n\n\nChoose one of the availability zones within the selected region. \n\n\nChoose image catalog\n\n\nBy default, \nChoose Image Catalog\n is set to the default image catalog that is provided with Cloudbreak. If you would like to use a different image catalog, you must first create and register it. For complete instructions, refer to \nUsing custom images\n.\n\n\nRelated links\n   \n\n\nUsing custom images\n  \n\n\nPrewarmed and base images\n\n\nCloudbreak supports the following types of images for launching clusters:\n\n\n\n\n\n\n\n\nImage type\n\n\nDescription\n\n\nDefault images provided\n\n\nSupport for custom images\n\n\n\n\n\n\n\n\n\n\nBase Images\n\n\nBase images include default configuration and default tooling. These images include the operating system but do not include Ambari or HDP/HDF software.\n\n\nYes\n\n\nYes\n\n\n\n\n\n\nPrewarmed Images\n\n\nBy default, Cloudbreak launches clusters from prewarmed images. Prewarmed images include the operating system as well as Ambari and HDP. The Ambari and HDP version used by prewarmed images cannot be customized. No prewarmed HDF images are currently provided.\n\n\nYes (HDP only)\n\n\nNo\n\n\n\n\n\n\n\n\nBy default, Cloudbreak uses the included default \nprewarmed images\n, which include the operating system, as well as\nAmbari and HDP/HDF packages installed. You can optionally select the \nbase image\n option if you would like to:\n\n\n\n\nUse an Ambari and HDP/HDF versions different than what the prewarmed image includes and/or  \n\n\nChoose a previously created custom base image\n\n\n\n\nChoose image\n  \n\n\nIf under \nChoose image catalog\n, you selected a custom image catalog, under \nChoose Image\n you can select an image from that catalog. For complete instructions, refer to \nUsing custom images\n. \n\n\nIf you are trying to customize Ambari and HDP/HDF versions, you can ignore the \nChoose Image\n option; in this case default base image is used.\n\n\nAmbari repository specification\n\n\nIf you would like to use a custom Ambari version, provide the following information: \n\n\n\n\n Ambari 2.6.1\n\n\nIf you would like to use Ambari \n2.6.1\n, use the version provided by default in the Cloudbreak web UI, or newer.\n\n\n\n\n\n\n\n\n\nParameter\n\n\nDescription\n\n\nExample\n\n\n\n\n\n\n\n\n\n\nVersion\n\n\nEnter Ambari version.\n\n\n2.6.1.3\n\n\n\n\n\n\nRepo Url\n\n\nProvide a URL to the Ambari version repo that you would like to use.\n\n\nhttp://public-repo-1.hortonworks.com/ambari/centos6/2.x/updates/2.6.1.3\n\n\n\n\n\n\nRepo Gpg Key Url\n\n\nProvide a URL to the repo GPG key. Each stable RPM package that is published by CentOS Project is signed with a GPG signature. By default, yum and the graphical update tools will verify these signatures and refuse to install any packages that are not signed, or have an incorrect signature.\n\n\nhttp://public-repo-1.hortonworks.com/ambari/centos6/RPM-GPG-KEY/RPM-GPG-KEY-Jenkins\n\n\n\n\n\n\n\n\nHDP or HDF repository specification\n\n\nIf you would like to use a custom HDP or HDF version, provide the following information: \n\n\n\n\n\n\n\n\nParameter\n\n\nDescription\n\n\nExample\n\n\n\n\n\n\n\n\n\n\nStack\n\n\nThis is populated by default based on the \"Platform Version\" parameter.\n\n\nHDP\n\n\n\n\n\n\nVersion\n\n\nThis is populated by default based on the \"Platform Version\" parameter.\n\n\n2.6\n\n\n\n\n\n\nOS\n\n\nOperating system.\n\n\ncentos7 (Azure, GCP, OpenStack) or amazonlinux (AWS)\n\n\n\n\n\n\nRepository Version\n\n\nEnter repository version.\n\n\n2.6.4.0-91\n\n\n\n\n\n\nVersion Definition File\n\n\nEnter the URL of the VDF file.\n\n\nhttp://public-repo-1.hortonworks.com/HDP/centos6/2.x/updates/2.6.4.0/HDP-2.6.4.0-91.xml\n\n\n\n\n\n\n(HDF only) MPack Url\n\n\n(HDF only) Provide MPack URL.\n\n\nhttp://public-repo-1.hortonworks.com/HDF/centos7/3.x/updates/3.1.1.0/tars/hdf_ambari_mp/hdf-ambari-mpack-3.1.1.0-35.tar.gz\n\n\n\n\n\n\nEnable Ambari Server to download and install GPL Licensed LZO packages?\n\n\n(Optional, only available if using Ambari 2.6.1.0 or newer) Use this option to enable LZO compression in your HDP/HDF cluster. LZO is a lossless data compression library that favors speed over compression ratio. Ambari does not install nor enable LZO compression libraries by default, and must be explicitly configured to do so. For more information, refer to \nEnabling LZO\n.\n\n\n\n\n\n\n\n\n\n\n\n\nIf you choose to use a base image with custom Ambari and/or HDP/HDF version, Cloudbreak validates the information entered. When Cloudbreak detects that the information entered is incorrect, it displays a warning marked with the \n sign. You should review all the warnings before proceeding and make sure that the information that you entered is correct. If you choose to proceed in spite of the warnings, check \"Ignore repository warnings\".  \n\n\n\n\nRelated links\n    \n\n\nUsing custom images\n      \n\n\nEnable lifetime management\n\n\nCheck this option if you would like your cluster to be automatically terminated after a specific amount of time (defined as \"Time to Live\" in minutes). \n\n\nTags\n\n\nYou can optionally add tags, which will help you find your cluster-related resources, such as VMs, in your cloud provider account. \n\n\nBy default, the following tags are created:\n\n\n\n\n\n\n\n\nTag\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\ncb-version\n\n\nCloudbreak version\n\n\n\n\n\n\nOwner\n\n\nYour Cloudbreak admin email.\n\n\n\n\n\n\ncb-account-name\n\n\nYour automatically generated Cloudbreak account name stored in the identity server.\n\n\n\n\n\n\ncb-user-name\n\n\nYour Cloudbreak admin email.\n\n\n\n\n\n\n\n\nFor more information, refer to \nTagging resources\n.\n\n\nRelated links\n    \n\n\nTagging resources\n \n\n\nStorage\n\n\nYou can optionally specify the following storage options for your cluster:\n\n\n\n\n\n\n\n\nParameter\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nStorage Type\n\n\nSelect the volume type. The options are:\nMagnetic\nEphemeral\nGeneral Purpose (SSD)\nThroughput Optimized HDD\nFor more information about these options refer to \nAWS documentation\n.\n\n\n\n\n\n\nAttached Volumes Per Instance\n\n\nEnter the number of volumes attached per instance. Default is 1.\n\n\n\n\n\n\nVolume Size (GB)\n\n\nEnter the size in GBs for each volume. Default is 100.\n\n\n\n\n\n\n\n\nRecipes\n\n\nThis option allows you to select previously uploaded recipes (scripts that can be run pre or post cluster deployment) for each host group. For more information on recipes, refer to \nUsing custom scripts (recipes)\n. \n\n\nRelated links\n    \n\n\nUsing custom scripts (recipes)\n \n\n\nManagement Packs\n\n\nThis option allows you to select previously uploaded management packs. For more information on management packs, refer to \nUsing management packs\n. \n\n\nRelated links\n    \n\n\nUsing management packs\n  \n\n\nExternal sources\n\n\nYou can register external sources with Cloudbreak, and then select and attach them during cluster create. To register external sources with Cloudbreak, refer to:\n\n\n\n\nUsing an external authentication source\n    \n\n\nUsing an external database\n  \n\n\nRegister a proxy\n  \n\n\n\n\nAmbari server master key\n\n\nThe Ambari server master key is used to configure Ambari to encrypt database and Kerberos credentials that are retained by Ambari as part of the Ambari setup.  \n\n\nEnable Kerberos security\n\n\nSelect this option to enable Kerberos for your cluster. For information about available Kerberos options, refer to \nEnabling Kerberos security\n. \n\n\nRelated links\n    \n\n\nEnabling Kerberos security\n \n\n\n\n\nNext: Access Cluster", 
            "title": "Create a cluster"
        }, 
        {
            "location": "/os-create/index.html#creating-a-cluster-on-openstack", 
            "text": "Use these steps to create a cluster.   Troubleshooting cluster creation  If you experience problems during cluster creation, refer to  Troubleshooting cluster creation .  Steps    Log in to the Cloudbreak UI.    Click  Create Cluster  and the  Create Cluster  wizard is displayed. \n    By default,  Basic  view is displayed. To view advanced options, click  Advanced . To learn about advanced options, refer to  Advanced cluster options .       On the  General Configuration  page, specify the following general parameters for your cluster:     Parameter  Description      Select Credential  Choose a previously created credential.    Cluster Name  Enter a name for your cluster. The name must be between 5 and 40 characters, must start with a letter, and must only include lowercase letters, numbers, and hyphens.    Region  Select the region in which you would like to launch your cluster.    Platform Version  Choose the HDP or HDF version to use for this cluster. Blueprints available for this platform version will be populated under \"Cluster Type\" below.    Cluster Type  Choose one of default cluster configurations, or, if you have defined your own cluster configuration via Ambari blueprint, you can choose it here. For more information on default and custom blueprints, refer to  Using custom blueprints .    Flex Subscription  This option will appear if you have configured your deployment for a  flex support subscription .       On the  Hardware and Storage  page, for each host group provide the following information to define your cluster nodes and attached storage:     Parameter  Description      Instance Type  Select an instance type.    Instance Count  Enter the number of instances of a given type. Default is 1.    Ambari Server  You must select one node for Ambari Server. The \"Group Size\" for that host group must be set to \"1\".       On the  Network  page, provide the following to specify the networking resources that will be used for your cluster:     Parameter  Description      Select Network  Select the virtual network in which you would like your cluster to be provisioned. You can select an existing network or create a new network.    Select Subnet  Select the subnet in which you would like your cluster to be provisioned. If you are using a new network, create a new subnet. If you are using an existing network, select an existing subnet.    Subnet (CIDR)  If you selected to create a new subnet, you must define a valid  CIDR  for the subnet. Default is 10.0.0.0/16.      Cloudbreak uses public IP addresses when communicating with cluster nodes.       Define security groups for each host group. You can either create new security groups and define their rules or reuse existing security groups:     Option  Description      New Security Group  (Default) Creates a new security group with the rules that you defined: A set of  default rules  is provided. You should review and adjust these default rules. If you do not make any modifications, default rules will be applied.  You may open ports by defining the CIDR, entering port range, selecting protocol and clicking  + . You may delete default or previously added rules using the delete icon. If you don't want to use security group, remove the default rules.    Existing Security Groups  Allows you to select an existing security group that is already available in the selected provider region. This selection is disabled if no existing security groups are available in your chosen region.      Important  \nBy default, ports 22, 443, and 9443 are set to 0.0.0.0/0 CIDR for inbound access on the Ambari node security group. We strongly recommend that you limit this CIDR, considering the following restrictions: Ports 22 and 9443 must be open to Cloudbreak's CIDR. You can set CB_DEFAULT_GATEWAY_CIDR in your Cloudbreak's Profile file in order to automatically open ports 22 and 9443 to your Cloudbreak IP. Refer to  Restricting inbound access from Cloudbreak to cluster .  Port 22 must be open to your CIDR if you would like to access the master node via SSH.  Port 443 must be open to your CIDR if you would like to access Ambari web UI in a browser.      Important  \nBy default, port 22 is set to 0.0.0.0/0 CIDR for inbound access on non-Ambari node security groups. We strongly recommend that you remove it.    Important  \nDepending on what services you are including, you need to open additional ports as required by these services. For example, when using the Flow Management blueprint, you must open port 9091 for NiFi (on NiFI host group) and port 61443 for NiFI Registry (on the Services host group).      On the  Security  page, provide the following parameters:     Parameter  Description      Cluster User  You can log in to the Ambari UI using this username. By default, this is set to  admin .    Password  You can log in to the Ambari UI using this password.    Confirm Password  Confirm the password.    New SSH public key  Check this option to specify a new public key and then enter the public key. You will use the matching private key to access your cluster nodes via SSH.    Existing SSH public key  Select an existing public key. You will use the matching private key to access your cluster nodes via SSH. This is a default option as long as an existing SSH public key is available.       Click on  Create Cluster  to create a cluster.    You will be redirected to the Cloudbreak dashboard, and a new tile representing your cluster will appear at the top of the page.    Related links  Flex support subscription  Using custom blueprints    Default cluster security groups  Troubleshooting cluster creation     CIDR  (External)", 
            "title": "Creating a cluster on OpenStack"
        }, 
        {
            "location": "/os-create/index.html#advanced-cluster-options", 
            "text": "Click on  Advanced  to view and enter additional configuration options", 
            "title": "Advanced cluster options"
        }, 
        {
            "location": "/os-create/index.html#availability-zone", 
            "text": "Choose one of the availability zones within the selected region.", 
            "title": "Availability zone"
        }, 
        {
            "location": "/os-create/index.html#choose-image-catalog", 
            "text": "By default,  Choose Image Catalog  is set to the default image catalog that is provided with Cloudbreak. If you would like to use a different image catalog, you must first create and register it. For complete instructions, refer to  Using custom images .  Related links      Using custom images", 
            "title": "Choose image catalog"
        }, 
        {
            "location": "/os-create/index.html#prewarmed-and-base-images", 
            "text": "Cloudbreak supports the following types of images for launching clusters:     Image type  Description  Default images provided  Support for custom images      Base Images  Base images include default configuration and default tooling. These images include the operating system but do not include Ambari or HDP/HDF software.  Yes  Yes    Prewarmed Images  By default, Cloudbreak launches clusters from prewarmed images. Prewarmed images include the operating system as well as Ambari and HDP. The Ambari and HDP version used by prewarmed images cannot be customized. No prewarmed HDF images are currently provided.  Yes (HDP only)  No     By default, Cloudbreak uses the included default  prewarmed images , which include the operating system, as well as\nAmbari and HDP/HDF packages installed. You can optionally select the  base image  option if you would like to:   Use an Ambari and HDP/HDF versions different than what the prewarmed image includes and/or    Choose a previously created custom base image   Choose image     If under  Choose image catalog , you selected a custom image catalog, under  Choose Image  you can select an image from that catalog. For complete instructions, refer to  Using custom images .   If you are trying to customize Ambari and HDP/HDF versions, you can ignore the  Choose Image  option; in this case default base image is used.  Ambari repository specification  If you would like to use a custom Ambari version, provide the following information:     Ambari 2.6.1  If you would like to use Ambari  2.6.1 , use the version provided by default in the Cloudbreak web UI, or newer.     Parameter  Description  Example      Version  Enter Ambari version.  2.6.1.3    Repo Url  Provide a URL to the Ambari version repo that you would like to use.  http://public-repo-1.hortonworks.com/ambari/centos6/2.x/updates/2.6.1.3    Repo Gpg Key Url  Provide a URL to the repo GPG key. Each stable RPM package that is published by CentOS Project is signed with a GPG signature. By default, yum and the graphical update tools will verify these signatures and refuse to install any packages that are not signed, or have an incorrect signature.  http://public-repo-1.hortonworks.com/ambari/centos6/RPM-GPG-KEY/RPM-GPG-KEY-Jenkins     HDP or HDF repository specification  If you would like to use a custom HDP or HDF version, provide the following information:      Parameter  Description  Example      Stack  This is populated by default based on the \"Platform Version\" parameter.  HDP    Version  This is populated by default based on the \"Platform Version\" parameter.  2.6    OS  Operating system.  centos7 (Azure, GCP, OpenStack) or amazonlinux (AWS)    Repository Version  Enter repository version.  2.6.4.0-91    Version Definition File  Enter the URL of the VDF file.  http://public-repo-1.hortonworks.com/HDP/centos6/2.x/updates/2.6.4.0/HDP-2.6.4.0-91.xml    (HDF only) MPack Url  (HDF only) Provide MPack URL.  http://public-repo-1.hortonworks.com/HDF/centos7/3.x/updates/3.1.1.0/tars/hdf_ambari_mp/hdf-ambari-mpack-3.1.1.0-35.tar.gz    Enable Ambari Server to download and install GPL Licensed LZO packages?  (Optional, only available if using Ambari 2.6.1.0 or newer) Use this option to enable LZO compression in your HDP/HDF cluster. LZO is a lossless data compression library that favors speed over compression ratio. Ambari does not install nor enable LZO compression libraries by default, and must be explicitly configured to do so. For more information, refer to  Enabling LZO .       If you choose to use a base image with custom Ambari and/or HDP/HDF version, Cloudbreak validates the information entered. When Cloudbreak detects that the information entered is incorrect, it displays a warning marked with the   sign. You should review all the warnings before proceeding and make sure that the information that you entered is correct. If you choose to proceed in spite of the warnings, check \"Ignore repository warnings\".     Related links       Using custom images", 
            "title": "Prewarmed and base images"
        }, 
        {
            "location": "/os-create/index.html#enable-lifetime-management", 
            "text": "Check this option if you would like your cluster to be automatically terminated after a specific amount of time (defined as \"Time to Live\" in minutes).", 
            "title": "Enable lifetime management"
        }, 
        {
            "location": "/os-create/index.html#tags", 
            "text": "You can optionally add tags, which will help you find your cluster-related resources, such as VMs, in your cloud provider account.   By default, the following tags are created:     Tag  Description      cb-version  Cloudbreak version    Owner  Your Cloudbreak admin email.    cb-account-name  Your automatically generated Cloudbreak account name stored in the identity server.    cb-user-name  Your Cloudbreak admin email.     For more information, refer to  Tagging resources .  Related links       Tagging resources", 
            "title": "Tags"
        }, 
        {
            "location": "/os-create/index.html#storage", 
            "text": "You can optionally specify the following storage options for your cluster:     Parameter  Description      Storage Type  Select the volume type. The options are: Magnetic Ephemeral General Purpose (SSD) Throughput Optimized HDD For more information about these options refer to  AWS documentation .    Attached Volumes Per Instance  Enter the number of volumes attached per instance. Default is 1.    Volume Size (GB)  Enter the size in GBs for each volume. Default is 100.", 
            "title": "Storage"
        }, 
        {
            "location": "/os-create/index.html#recipes", 
            "text": "This option allows you to select previously uploaded recipes (scripts that can be run pre or post cluster deployment) for each host group. For more information on recipes, refer to  Using custom scripts (recipes) .   Related links       Using custom scripts (recipes)", 
            "title": "Recipes"
        }, 
        {
            "location": "/os-create/index.html#management-packs", 
            "text": "This option allows you to select previously uploaded management packs. For more information on management packs, refer to  Using management packs .   Related links       Using management packs", 
            "title": "Management Packs"
        }, 
        {
            "location": "/os-create/index.html#external-sources", 
            "text": "You can register external sources with Cloudbreak, and then select and attach them during cluster create. To register external sources with Cloudbreak, refer to:   Using an external authentication source       Using an external database     Register a proxy", 
            "title": "External sources"
        }, 
        {
            "location": "/os-create/index.html#ambari-server-master-key", 
            "text": "The Ambari server master key is used to configure Ambari to encrypt database and Kerberos credentials that are retained by Ambari as part of the Ambari setup.", 
            "title": "Ambari server master key"
        }, 
        {
            "location": "/os-create/index.html#enable-kerberos-security", 
            "text": "Select this option to enable Kerberos for your cluster. For information about available Kerberos options, refer to  Enabling Kerberos security .   Related links       Enabling Kerberos security     Next: Access Cluster", 
            "title": "Enable Kerberos security"
        }, 
        {
            "location": "/os-clusters-access/index.html", 
            "text": "Accessing a cluster\n\n\nThe following section describes how to access the various services in the cluster.\n\n\nCloudbreak user accounts\n\n\nThe following table describes what credentials to use to access Cloudbreak and Cloudbreak-managed clusters:\n\n\n\n\n\n\n\n\nComponent\n\n\nMethod\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nCloudbreak\n\n\nWeb UI, CLI\n\n\nAccess with the username and password provided when launching Cloudbreak on the cloud provider.\n\n\n\n\n\n\nCloudbreak\n\n\nSSH to VM\n\n\nAccess as the \"cloudbreak\" user with the SSH key provided when launching Cloudbreak on the cloud provider.\n\n\n\n\n\n\nCluster\n\n\nSSH to VMs\n\n\nAccess as the \"cloudbreak\" user with the SSH key provided during cluster creation.\n\n\n\n\n\n\nCluster\n\n\nAmbari web UI\n\n\nAccess with the credentials provided in the \u201cCluster User\u201d parameter during cluster creation.\n\n\n\n\n\n\nCluster\n\n\nWeb UIs for specific cluster services\n\n\nAccess with the credentials provided in the \u201cCluster User\u201d parameter during cluster creation.\n\n\n\n\n\n\n\n\nFinding cluster information in the web UI\n\n\nOnce your cluster is up and running, click on the tile representing your cluster in the Cloudbreak UI to access information related the cluster and access cluster actions. \n\n\n \n\n\nThe information presented includes:\n\n\n\n\nCluster summary\n  \n\n\nCluster information\n     \n\n\nEvent history\n  \n\n\n\n\n\n  \nTips\n\n  \n\n  \n Access cluster actions such as resize and sync by clicking on \nACTIONS\n.\n\n  \n Access Ambari web UI by clicking on the link in the \nCLUSTER INFORMATION\n section.\n\n\n View public IP addresses for all cluster instances in the \nHARDWARE\n section. Click on the links to view the instances in the cloud console.\n\n\n The SSH user that you must use when accessing cluster VMs is \"cloudbreak\".\n \n\n\n\n\n\n\n\n\nCluster summary\n\n\nThe summary bar includes the following information about your cluster:\n\n\n\n\n\n\n\n\nItem\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nCluster Name\n\n\nThe name that you selected for your cluster is displayed at the top of the page. Below it is the name of the cluster blueprint.\n\n\n\n\n\n\nTime Remaining\n\n\nIf you enabled lifetime management for your cluster, the clock next to the cluster name indicates the amount of time that your cluster will run before it gets terminated. Note that the time remaining counter does not stop when you stop the cluster.\n\n\n\n\n\n\nCloud Provider\n\n\nThe logo of the cloud provider on which the cluster is running.\n\n\n\n\n\n\nCredential\n\n\nThe name of the credential used to create the cluster.\n\n\n\n\n\n\nStatus\n\n\nCurrent status. When a cluster is healthy, the status is \nRunning\n.\n\n\n\n\n\n\nNodes\n\n\nThe current number of cluster nodes, including the master node.\n\n\n\n\n\n\nUptime\n\n\nThe amount of time (HH:MM) that the cluster has been in the running state since it was started. Each time you stop and restart the cluster, the running time is reset to 0.\n\n\n\n\n\n\nCreated\n\n\nThe date when the cluster was created. The date format is Mon DD, YYYY. For example: Oct 27, 2017.\n\n\n\n\n\n\n\n\nCluster information\n\n\nThe following information is available on the cluster details page: \n\n\n\n\n\n\n\n\nItem\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nCluster User\n\n\nThe name of the cluster user that you created when creating the cluster.\n\n\n\n\n\n\nSSH Username\n\n\nThe SSH user which you must use when accessing cluster VMs via SSH. The SSH user is always \"cloudbreak\".\n\n\n\n\n\n\nAmbari URL\n\n\nLink to the Ambari web UI.\n\n\n\n\n\n\nRegion\n\n\nThe region in which the cluster is running in the cloud provider infrastructure.\n\n\n\n\n\n\nAvailability Zone\n\n\nThe availability zone within the region in which the cluster is running.\n\n\n\n\n\n\nBlueprint\n\n\nThe name of the blueprint selected under \"Cluster Type\" to create this cluster.\n\n\n\n\n\n\nCreated With\n\n\nThe version of Cloudbreak used to create this cluster.\n\n\n\n\n\n\nAmbari Version\n\n\nThe Ambari version which this cluster is currently running.\n\n\n\n\n\n\nHDP/HDF Version\n\n\nThe HDP or HDF version which this cluster is currently running.\n\n\n\n\n\n\nAuthentication Source\n\n\nIf you are using an external authentication source (LDAP/AD) for your cluster, you can see it here. Refer to \nUsing an external authentication source\n.\n\n\n\n\n\n\n\n\nBelow this, you will see additional tabs that you can click on in order to see their content:\n\n\n\n\n\n\n\n\nItem\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nHardware\n\n\nThis section includes information about your cluster instances: instance names, instance IDs, instance types, their status, fully qualified domain names (FQDNs), and private and public IPs.\n\n\n\n\n\n\nTags\n\n\nThis section lists keys and values of the user-defined tags, in the same order as you added them.\n\n\n\n\n\n\nRecipes\n\n\nThis section includes recipe-related information. For each recipe, you can see the host group on which a recipe was executed, recipe name, and recipe type. Refer to \nUsing custom scripts (recipes)\n.\n\n\n\n\n\n\nExternal Databases\n\n\nIf you are using an external database for your cluster, you can see it here. Refer to \nusing an external database\n.\n\n\n\n\n\n\nRepository Details\n\n\nThis section includes Ambari and HDP/HDF repository information, as you provided it in the \"Base Images\" section when creating a cluster.\n\n\n\n\n\n\nImage Details\n\n\nThis section includes information about the base image that was used for the Cloudbreak instance.\n\n\n\n\n\n\nNetwork\n\n\nThis section includes information about the names of the network and subnet in which the cluster is running and the links to related cloud provider console.\n\n\n\n\n\n\nSecurity\n\n\nThis section is only available if you have enabled Kerberos security. It provides you with the details of your Kerberos configuration.\n\n\n\n\n\n\nAutoscaling\n\n\nThis section includes configuration options related to autoscaling. Refer to \nConfiguring autoscaling\n.\n\n\n\n\n\n\n\n\nEvent history\n\n\nThe Event History section shows you events logged for the cluster, with the most recent event at the top. For example, after your cluster has been created, the following messages will be written to the log:\n\n\n\nAmbari cluster built; Ambari ip:34.215.103.66\n10/26/2017, 9:41:58 AM\nBuilding Ambari cluster; Ambari ip:34.215.103.66\n10/26/2017, 9:30:20 AM\nStarting Ambari cluster services\n10/26/2017, 9:27:12 AM\nSetting up infrastructure metadata\n10/26/2017, 9:27:11 AM\nBootstrapping infrastructure cluster\n10/26/2017, 9:26:38 AM\nInfrastructure successfully provisioned\n10/26/2017, 9:26:37 AM\nBilling started, Infrastructure successfully provisioned\n10/26/2017, 9:26:37 AM\nInfrastructure metadata collection finished\n10/26/2017, 9:25:39 AM\nInfrastructure creation took 194 seconds\n10/26/2017, 9:25:37 AM\nCreating infrastructure\n10/26/2017, 9:22:22 AM\nSetting up HDP image\n10/26/2017, 9:22:21 AM\n\n\n\nAccessing cluster via SSH\n\n\nIf you plan to access the cluster via the command line clients, SSH into the master node instance in the cluster. \n\n\n\n\nIn order to use SSH, you must generate an SSH key pair or use an existing SSH key pair.  \n\n\nYou can find the cluster instance public IP addresses on the cluster details page.  \n\n\nWhen accessing instances via SSH use the \ncloudbreak\n user. \n\n\n\n\nOn Mac OS, you can use the following syntax to SSH to the VM:\n\nssh -i \"privatekey.pem\" cloudbreak@publicIP\n\nFor example:\n\nssh -i \"dominika-kp.pem\" cloudbreak@p52.25.169.132\n\n\nOn Windows, you can SSH using an SSH client such as PuTTY.\n\n\nAccess Ambari\n\n\nYou can access Ambari web UI by clicking on the links provided in the \nCluster Information\n \n \nAmbari URL\n.\n\n\nSteps\n\n\n\n\n\n\nFrom the cluster dashboard, click on the tile representing your cluster to navigate to cluster details.\n\n\n\n\n\n\nFind the Ambari URL in the \nCluster Information\n section. This URL is available once the Ambari cluster creation process has completed.  \n\n\n\n\n\n\nClick on the \nAmbari URL\n link.\n\n\n\n\n\n\nThe first time you access the server, your browser will attempt to confirm that the SSL Certificate is valid. Since Cloudbreak automatically generates a self-signed certificate, your browser will warn you about an Untrusted Connection and ask you to confirm a Security Exception. Depending on your browser, perform the steps below to proceed.\n\n\n\n\n\n\n\n\nBrowser\n\n\nSteps\n\n\n\n\n\n\n\n\n\n\nFirefox\n\n\nClick \nAdvanced\n \n Click \nAdd Exception...\n \n Click \nConfirm Security Exception\n\n\n\n\n\n\nSafari\n\n\nClick \nContinue\n\n\n\n\n\n\nChrome\n\n\nClick \nAdvanced\n \n Click \nProceed...\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNext: Manage and Monitor Clusters", 
            "title": "Access cluster"
        }, 
        {
            "location": "/os-clusters-access/index.html#accessing-a-cluster", 
            "text": "The following section describes how to access the various services in the cluster.", 
            "title": "Accessing a cluster"
        }, 
        {
            "location": "/os-clusters-access/index.html#cloudbreak-user-accounts", 
            "text": "The following table describes what credentials to use to access Cloudbreak and Cloudbreak-managed clusters:     Component  Method  Description      Cloudbreak  Web UI, CLI  Access with the username and password provided when launching Cloudbreak on the cloud provider.    Cloudbreak  SSH to VM  Access as the \"cloudbreak\" user with the SSH key provided when launching Cloudbreak on the cloud provider.    Cluster  SSH to VMs  Access as the \"cloudbreak\" user with the SSH key provided during cluster creation.    Cluster  Ambari web UI  Access with the credentials provided in the \u201cCluster User\u201d parameter during cluster creation.    Cluster  Web UIs for specific cluster services  Access with the credentials provided in the \u201cCluster User\u201d parameter during cluster creation.", 
            "title": "Cloudbreak user accounts"
        }, 
        {
            "location": "/os-clusters-access/index.html#finding-cluster-information-in-the-web-ui", 
            "text": "Once your cluster is up and running, click on the tile representing your cluster in the Cloudbreak UI to access information related the cluster and access cluster actions.      The information presented includes:   Cluster summary     Cluster information        Event history      \n   Tips \n   \n    Access cluster actions such as resize and sync by clicking on  ACTIONS . \n    Access Ambari web UI by clicking on the link in the  CLUSTER INFORMATION  section.   View public IP addresses for all cluster instances in the  HARDWARE  section. Click on the links to view the instances in the cloud console.   The SSH user that you must use when accessing cluster VMs is \"cloudbreak\".", 
            "title": "Finding cluster information in the web UI"
        }, 
        {
            "location": "/os-clusters-access/index.html#cluster-summary", 
            "text": "The summary bar includes the following information about your cluster:     Item  Description      Cluster Name  The name that you selected for your cluster is displayed at the top of the page. Below it is the name of the cluster blueprint.    Time Remaining  If you enabled lifetime management for your cluster, the clock next to the cluster name indicates the amount of time that your cluster will run before it gets terminated. Note that the time remaining counter does not stop when you stop the cluster.    Cloud Provider  The logo of the cloud provider on which the cluster is running.    Credential  The name of the credential used to create the cluster.    Status  Current status. When a cluster is healthy, the status is  Running .    Nodes  The current number of cluster nodes, including the master node.    Uptime  The amount of time (HH:MM) that the cluster has been in the running state since it was started. Each time you stop and restart the cluster, the running time is reset to 0.    Created  The date when the cluster was created. The date format is Mon DD, YYYY. For example: Oct 27, 2017.", 
            "title": "Cluster summary"
        }, 
        {
            "location": "/os-clusters-access/index.html#cluster-information", 
            "text": "The following information is available on the cluster details page:      Item  Description      Cluster User  The name of the cluster user that you created when creating the cluster.    SSH Username  The SSH user which you must use when accessing cluster VMs via SSH. The SSH user is always \"cloudbreak\".    Ambari URL  Link to the Ambari web UI.    Region  The region in which the cluster is running in the cloud provider infrastructure.    Availability Zone  The availability zone within the region in which the cluster is running.    Blueprint  The name of the blueprint selected under \"Cluster Type\" to create this cluster.    Created With  The version of Cloudbreak used to create this cluster.    Ambari Version  The Ambari version which this cluster is currently running.    HDP/HDF Version  The HDP or HDF version which this cluster is currently running.    Authentication Source  If you are using an external authentication source (LDAP/AD) for your cluster, you can see it here. Refer to  Using an external authentication source .     Below this, you will see additional tabs that you can click on in order to see their content:     Item  Description      Hardware  This section includes information about your cluster instances: instance names, instance IDs, instance types, their status, fully qualified domain names (FQDNs), and private and public IPs.    Tags  This section lists keys and values of the user-defined tags, in the same order as you added them.    Recipes  This section includes recipe-related information. For each recipe, you can see the host group on which a recipe was executed, recipe name, and recipe type. Refer to  Using custom scripts (recipes) .    External Databases  If you are using an external database for your cluster, you can see it here. Refer to  using an external database .    Repository Details  This section includes Ambari and HDP/HDF repository information, as you provided it in the \"Base Images\" section when creating a cluster.    Image Details  This section includes information about the base image that was used for the Cloudbreak instance.    Network  This section includes information about the names of the network and subnet in which the cluster is running and the links to related cloud provider console.    Security  This section is only available if you have enabled Kerberos security. It provides you with the details of your Kerberos configuration.    Autoscaling  This section includes configuration options related to autoscaling. Refer to  Configuring autoscaling .", 
            "title": "Cluster information"
        }, 
        {
            "location": "/os-clusters-access/index.html#event-history", 
            "text": "The Event History section shows you events logged for the cluster, with the most recent event at the top. For example, after your cluster has been created, the following messages will be written to the log:  \nAmbari cluster built; Ambari ip:34.215.103.66\n10/26/2017, 9:41:58 AM\nBuilding Ambari cluster; Ambari ip:34.215.103.66\n10/26/2017, 9:30:20 AM\nStarting Ambari cluster services\n10/26/2017, 9:27:12 AM\nSetting up infrastructure metadata\n10/26/2017, 9:27:11 AM\nBootstrapping infrastructure cluster\n10/26/2017, 9:26:38 AM\nInfrastructure successfully provisioned\n10/26/2017, 9:26:37 AM\nBilling started, Infrastructure successfully provisioned\n10/26/2017, 9:26:37 AM\nInfrastructure metadata collection finished\n10/26/2017, 9:25:39 AM\nInfrastructure creation took 194 seconds\n10/26/2017, 9:25:37 AM\nCreating infrastructure\n10/26/2017, 9:22:22 AM\nSetting up HDP image\n10/26/2017, 9:22:21 AM", 
            "title": "Event history"
        }, 
        {
            "location": "/os-clusters-access/index.html#accessing-cluster-via-ssh", 
            "text": "If you plan to access the cluster via the command line clients, SSH into the master node instance in the cluster.    In order to use SSH, you must generate an SSH key pair or use an existing SSH key pair.    You can find the cluster instance public IP addresses on the cluster details page.    When accessing instances via SSH use the  cloudbreak  user.    On Mac OS, you can use the following syntax to SSH to the VM: ssh -i \"privatekey.pem\" cloudbreak@publicIP \nFor example: ssh -i \"dominika-kp.pem\" cloudbreak@p52.25.169.132  On Windows, you can SSH using an SSH client such as PuTTY.", 
            "title": "Accessing cluster via SSH"
        }, 
        {
            "location": "/os-clusters-access/index.html#access-ambari", 
            "text": "You can access Ambari web UI by clicking on the links provided in the  Cluster Information     Ambari URL .  Steps    From the cluster dashboard, click on the tile representing your cluster to navigate to cluster details.    Find the Ambari URL in the  Cluster Information  section. This URL is available once the Ambari cluster creation process has completed.      Click on the  Ambari URL  link.    The first time you access the server, your browser will attempt to confirm that the SSL Certificate is valid. Since Cloudbreak automatically generates a self-signed certificate, your browser will warn you about an Untrusted Connection and ask you to confirm a Security Exception. Depending on your browser, perform the steps below to proceed.     Browser  Steps      Firefox  Click  Advanced    Click  Add Exception...    Click  Confirm Security Exception    Safari  Click  Continue    Chrome  Click  Advanced    Click  Proceed...        Next: Manage and Monitor Clusters", 
            "title": "Access Ambari"
        }, 
        {
            "location": "/os-clusters-manage/index.html", 
            "text": "Managing and monitoring clusters\n\n\nYou can manage monitor your clusters from the Cloudbreak UI. To do that, click on the tile representing the cluster that you want to access. The actions available for your cluster are listed in the top right corner: \n\n\n \n\n\n\n  \nTips\n\n  \n\n  \nTo add or remove nodes from your cluster click \nACTIONS>Resize\n.\n\n  \nTo synchronize your cluster with the cloud provider account click \nACTIONS>Sync\n.\n\n  \nTo temporarily stop your cluster click \nSTOP\n.\n\n  \nTo terminate your cluster click \nTERMINATE\n.\n\n\n\n\n\n\n\n\n\nRetry a cluster\n\n\nWhen a stack provisioning or cluster creation failure occurs, the \"retry\" option allows you to resume the process from the last failed step. \n\n\nIn some cases the cause of a failed stack provisioning or cluster creation can be eliminated by simply retrying the process. For example, in case of a temporary network outage, a retry should be successful. In other cases, a manual modification is required before a retry can succeed. For example, if you are using a custom image but some configuration is missing, causing the process to fail, you must log in to the machine and fix the issue; Only after that you can retry the the process.\n\n\nOnly failed stack or cluster creations can be retried. A retry can be initiated any number of times on a failed creation process. \n\n\nTo retry provisioning a failed stack or cluster, follow these steps.  \n\n\nSteps\n\n\n\n\n\n\nBrowse to the cluster details.\n\n\n\n\n\n\nClick \nActions\n and select \nRetry\n. \n\n\nOnly failed stack or cluster creations can be retried, so the option is only available in these two cases.  \n\n\n\n\n\n\nClick \nYes\n to confirm. \n\n\nThe operation continues from the last failed step. \n\n\n\n\n\n\nResize a cluster\n\n\nTo resize a cluster, follow these steps.\n\n\n\n\nCluster resizing is not supported for HDF clusters.\n\nTo configure automated cluster scaling, refer to \nConfigure autoscaling\n.   \n\n\n\n\nSteps\n\n\n\n\n\n\nBrowse to the cluster details.\n\n\n\n\n\n\nClick \nActions\n and select \nResize\n. The cluster resize dialog is displayed.\n\n\n\n\n\n\nUsing the +/- controls, adjust the number of nodes for a chosen host group. \n\n\n\n\nYou can only modify one host group at a time. \n\nIt is not possible to resize the Ambari server host group.     \n\n\n\n\n\n\n\n\nClick \nYes\n to confirm the scale-up/scale-down.\n\n\nWhile nodes are being added or removed, cluster status changes to \"Update In Progress\". Once the operation has completed, cluster status changes back to \"Running\". \n\n\n\n\n\n\nSynchronize a cluster\n\n\nUse the \nsync\n option if you:  \n\n\n\n\nMade changes on your cloud provider side (for example, deleted cluster VMs) and you would like to synchronize Cloudbreak with the cloud provider.  \n\n\nManually changed service status in Ambari (for example, restarted services).   \n\n\n\n\nTo synchronize your cluster with the cloud provider, follow these steps. \n\n\nSteps\n\n\n\n\n\n\nBrowse to the cluster details.\n\n\n\n\n\n\nClick \nActions\n and select \nSync\n.\n\n\n\n\n\n\nClick \nYes\n to confirm.\n\n\nYour cluster infrastructure is synchronized based on changes on the cloud provider. The updates are written to the \"Event History\". \n\n\n\n\n\n\nStop a cluster\n\n\nCloudbreak supports stopping and restarting clusters. To stop and restart a cluster managed by Cloudbreak, use the options available from the Cloudbreak UI. \n\n\nSteps\n\n\n\n\n\n\nBrowse to the cluster details.\n\n\n\n\n\n\nClick \nStop\n to stop a currently running cluster.  \n\n\n\n\n\n\nClick \nYes\n to confirm. \n\n\n\n\n\n\nYour cluster status changes to \"Stopping in progress\" and then to \"Stopped\". Once stopping the infrastructure has completed, you will see a \nStart\n option to restart your cluster. \n\n\n\n\n\n\nWhen a cluster is in the \"stopped\" state, you are not charged for the VMs, but you are charged for external storage.  \n\n\nRestart a cluster\n\n\nIf your cluster is in the \"Stopped\" state, you can restart the cluster by follow these steps.\n\n\nSteps\n\n\n\n\n\n\nclick \nStart\n. This option is only available when the cluster has been stopped. \n\n\n\n\n\n\nClick \nYes\n to confirm.\n\n\nYour cluster status changes to \"Start in progress\" and then to \"Running\". \n\n\n\n\n\n\nTerminate a cluster\n\n\nTo terminate a cluster managed by Cloudbreak, use the option available from the Cloudbreak UI. \n\n\nSteps\n\n\n\n\n\n\nBrowse to the cluster details.\n\n\n\n\n\n\nClick \nTerminate\n. \n\n\n\n\n\n\nClick \nYes\n to confirm.\n\n\nAll cluster-related resources will be deleted, unless the resources (such as networks and subnets) existed prior to cluster creation or are used by other VMs in which case they will be preserved. \n\n\n\n\n\n\nForce terminate a cluster\n\n\nCluster deletion may fail if Cloudbreak is unable to delete one or more of the cloud resources that were part of your cluster infrastructure. In such as case, you can use the \nTerminate\n \n \nForce terminate\n option to remove the cluster entry from the Cloudbreak web UI, but you must also check your cloud provider account to see if there are any resources that must be deleted manually.\n\n\nSteps\n\n\n\n\n\n\nBrowse to the cluster details.\n\n\n\n\n\n\nClick \nTerminate\n. \n\n\n\n\n\n\nCheck  \nForce terminate\n.\n\n\n\n\n\n\nClick \nYes\n to confirm. \n\n\nWhen terminating a cluster with Kerberos enabled, you have an option to disable Kerberos prior to cluster termination. This option removes any cluster-related principals from the KDC.\n\n\n\n\n\n\nThis deletes the cluster tile from the UI.  \n\n\n\n\n\n\nLog in to your cloud provider account and \nmanually delete\n any resources that failed to be deleted.\n\n\n\n\n\n\nView cluster history\n\n\nFrom the navigation menu in the Cloudbreak UI, you can access the History page that allows you to generate a report showing basic information related to the clusters that were running within the specified range of dates.\n\n\nTo generate a report, follow these steps.\n\n\nSteps\n\n\n\n\n\n\nFrom the Cloudbreak UI navigation menu, select \nHistory\n.\n\n\n\n\n\n\nOn the History page, select the range of dates and click \nShow History\n to generate a tabular report for the selected period.\n\n\n\n\n\n\nHistory report content\n\n\nEach entry in the report represents one cluster instance group. For each entry, the report includes the following information:\n\n\n\n\nCreated\n - The date when your cluster was created (YYYY-MM-DD).\n\n\nProvider\n - The name of the cloud provider (AWS, Azure, Google, or OpenStack) on which the cluster instances are/were running.\n\n\nCluster Name\n - The name that you selected for the cluster.  \n\n\nInstance Group\n - The name of the host group.   \n\n\nInstance Count\n - The number of nodes in the host group. This number may be a decimal if a cluster has been resized.\n\n\nInstance Type\n - Provider-specific VM type of the cluster instances. \n\n\nRegion\n - The AWS region in which your cluster is/was running.\n\n\nAvailability Zone\n - The availability zone in which your cluster is/was running.      \n\n\nRunning Time (hours)\n - The sum of the running times for all the nodes in the instance group.\n\n\n\n\nThe \nAGGREGATE RUNNING TIME\n is the sum of the Running Times, adjusted for the selected time range.\n\n\nTo learn about how your cloud provider bills you for the VMs, refer to their documentation:\n\n\n\n\nAWS\n      \n\n\nAzure\n     \n\n\nGCP", 
            "title": "Manage and monitor clusters"
        }, 
        {
            "location": "/os-clusters-manage/index.html#managing-and-monitoring-clusters", 
            "text": "You can manage monitor your clusters from the Cloudbreak UI. To do that, click on the tile representing the cluster that you want to access. The actions available for your cluster are listed in the top right corner:      \n   Tips \n   \n   To add or remove nodes from your cluster click  ACTIONS>Resize . \n   To synchronize your cluster with the cloud provider account click  ACTIONS>Sync . \n   To temporarily stop your cluster click  STOP . \n   To terminate your cluster click  TERMINATE .", 
            "title": "Managing and monitoring clusters"
        }, 
        {
            "location": "/os-clusters-manage/index.html#retry-a-cluster", 
            "text": "When a stack provisioning or cluster creation failure occurs, the \"retry\" option allows you to resume the process from the last failed step.   In some cases the cause of a failed stack provisioning or cluster creation can be eliminated by simply retrying the process. For example, in case of a temporary network outage, a retry should be successful. In other cases, a manual modification is required before a retry can succeed. For example, if you are using a custom image but some configuration is missing, causing the process to fail, you must log in to the machine and fix the issue; Only after that you can retry the the process.  Only failed stack or cluster creations can be retried. A retry can be initiated any number of times on a failed creation process.   To retry provisioning a failed stack or cluster, follow these steps.    Steps    Browse to the cluster details.    Click  Actions  and select  Retry .   Only failed stack or cluster creations can be retried, so the option is only available in these two cases.      Click  Yes  to confirm.   The operation continues from the last failed step.", 
            "title": "Retry a cluster"
        }, 
        {
            "location": "/os-clusters-manage/index.html#resize-a-cluster", 
            "text": "To resize a cluster, follow these steps.   Cluster resizing is not supported for HDF clusters. \nTo configure automated cluster scaling, refer to  Configure autoscaling .      Steps    Browse to the cluster details.    Click  Actions  and select  Resize . The cluster resize dialog is displayed.    Using the +/- controls, adjust the number of nodes for a chosen host group.    You can only modify one host group at a time.  \nIt is not possible to resize the Ambari server host group.          Click  Yes  to confirm the scale-up/scale-down.  While nodes are being added or removed, cluster status changes to \"Update In Progress\". Once the operation has completed, cluster status changes back to \"Running\".", 
            "title": "Resize a cluster"
        }, 
        {
            "location": "/os-clusters-manage/index.html#synchronize-a-cluster", 
            "text": "Use the  sync  option if you:     Made changes on your cloud provider side (for example, deleted cluster VMs) and you would like to synchronize Cloudbreak with the cloud provider.    Manually changed service status in Ambari (for example, restarted services).      To synchronize your cluster with the cloud provider, follow these steps.   Steps    Browse to the cluster details.    Click  Actions  and select  Sync .    Click  Yes  to confirm.  Your cluster infrastructure is synchronized based on changes on the cloud provider. The updates are written to the \"Event History\".", 
            "title": "Synchronize a cluster"
        }, 
        {
            "location": "/os-clusters-manage/index.html#stop-a-cluster", 
            "text": "Cloudbreak supports stopping and restarting clusters. To stop and restart a cluster managed by Cloudbreak, use the options available from the Cloudbreak UI.   Steps    Browse to the cluster details.    Click  Stop  to stop a currently running cluster.      Click  Yes  to confirm.     Your cluster status changes to \"Stopping in progress\" and then to \"Stopped\". Once stopping the infrastructure has completed, you will see a  Start  option to restart your cluster.     When a cluster is in the \"stopped\" state, you are not charged for the VMs, but you are charged for external storage.", 
            "title": "Stop a cluster"
        }, 
        {
            "location": "/os-clusters-manage/index.html#restart-a-cluster", 
            "text": "If your cluster is in the \"Stopped\" state, you can restart the cluster by follow these steps.  Steps    click  Start . This option is only available when the cluster has been stopped.     Click  Yes  to confirm.  Your cluster status changes to \"Start in progress\" and then to \"Running\".", 
            "title": "Restart a cluster"
        }, 
        {
            "location": "/os-clusters-manage/index.html#terminate-a-cluster", 
            "text": "To terminate a cluster managed by Cloudbreak, use the option available from the Cloudbreak UI.   Steps    Browse to the cluster details.    Click  Terminate .     Click  Yes  to confirm.  All cluster-related resources will be deleted, unless the resources (such as networks and subnets) existed prior to cluster creation or are used by other VMs in which case they will be preserved.", 
            "title": "Terminate a cluster"
        }, 
        {
            "location": "/os-clusters-manage/index.html#force-terminate-a-cluster", 
            "text": "Cluster deletion may fail if Cloudbreak is unable to delete one or more of the cloud resources that were part of your cluster infrastructure. In such as case, you can use the  Terminate     Force terminate  option to remove the cluster entry from the Cloudbreak web UI, but you must also check your cloud provider account to see if there are any resources that must be deleted manually.  Steps    Browse to the cluster details.    Click  Terminate .     Check   Force terminate .    Click  Yes  to confirm.   When terminating a cluster with Kerberos enabled, you have an option to disable Kerberos prior to cluster termination. This option removes any cluster-related principals from the KDC.    This deletes the cluster tile from the UI.      Log in to your cloud provider account and  manually delete  any resources that failed to be deleted.", 
            "title": "Force terminate a cluster"
        }, 
        {
            "location": "/os-clusters-manage/index.html#view-cluster-history", 
            "text": "From the navigation menu in the Cloudbreak UI, you can access the History page that allows you to generate a report showing basic information related to the clusters that were running within the specified range of dates.  To generate a report, follow these steps.  Steps    From the Cloudbreak UI navigation menu, select  History .    On the History page, select the range of dates and click  Show History  to generate a tabular report for the selected period.", 
            "title": "View cluster history"
        }, 
        {
            "location": "/os-clusters-manage/index.html#history-report-content", 
            "text": "Each entry in the report represents one cluster instance group. For each entry, the report includes the following information:   Created  - The date when your cluster was created (YYYY-MM-DD).  Provider  - The name of the cloud provider (AWS, Azure, Google, or OpenStack) on which the cluster instances are/were running.  Cluster Name  - The name that you selected for the cluster.    Instance Group  - The name of the host group.     Instance Count  - The number of nodes in the host group. This number may be a decimal if a cluster has been resized.  Instance Type  - Provider-specific VM type of the cluster instances.   Region  - The AWS region in which your cluster is/was running.  Availability Zone  - The availability zone in which your cluster is/was running.        Running Time (hours)  - The sum of the running times for all the nodes in the instance group.   The  AGGREGATE RUNNING TIME  is the sum of the Running Times, adjusted for the selected time range.  To learn about how your cloud provider bills you for the VMs, refer to their documentation:   AWS         Azure        GCP", 
            "title": "History report content"
        }, 
        {
            "location": "/vm-launch/index.html", 
            "text": "Installing Cloudbreak on your own VM\n\n\nThis is an advanced deployment option. Select this option if you have custom VM requirements. Otherwise, you should use one of the pre-built images and follow these instructions:\n\n\n\n\nLaunch on AWS\n  \n\n\nLaunch on Azure\n  \n\n\nLaunch on GCP\n  \n\n\nLaunch on OpenStack\n   \n\n\n\n\nSystem requirements\n\n\nTo launch the Cloudbreak deployer and install the Cloudbreak application, your system must meet the following requirements:\n\n\n\n\nMinimum VM requirements: 16GB RAM, 40GB disk, 4 cores\n\n\nSupported operating systems: RHEL, CentOS, and Oracle Linux 7 (64-bit)\n\n\nDocker 1.9.1 must be installed\n\n\n\n\n\n\nYou can install Cloudbreak on Mac OS X for evaluation purposes only. Mac OS X is not supported for a production deployment of Cloudbreak.\n\n\n\n\nPrerequisites\n\n\nTo launch the Cloudbreak deployer and install the Cloudbreak application, you must first meet the following prerequisites:\n\n\nPorts\n\n\nPorts 22 (SSH), 80 (HTTPS), and 443 (HTTPS) must be open.\n\n\nRoot access\n\n\nEvery command must be executed as root. In order to get root privileges execute:\n\n\nsudo -i\n\n\n\nSystem updates\n\n\nEnsure that your system is up-to-date by executing:\n\n\nyum -y update\n\n\n\nReboot it if necessary.\n\n\nIptables\n\n\n\n\n\n\nInstall iptables-services:\n\n\nyum -y install net-tools ntp wget lsof unzip tar iptables-services\nsystemctl enable ntpd \n systemctl start ntpd\nsystemctl disable firewalld \n systemctl stop firewalld\n\n\n\n\nWithout iptables-services installed the \niptables save\n command will not be available.\n\n\n\n\n\n\n\n\nConfigure permissive iptables on your machine:\n\n\niptables --flush INPUT \n \\\niptables --flush FORWARD \n \\\nservice iptables save\n\n\n\n\n\n\nDisable SELINUX:\n\n\nsed -i --follow-symlinks 's/^SELINUX=.*/SELINUX=disabled/g' /etc/sysconfig/selinux\n\n\n\n\n\n\nCreate Docker repo:\n\n\ncat \n /etc/yum.repos.d/docker.repo \n\"EOF\"\n[dockerrepo]\nname=Docker Repository\nbaseurl=https://yum.dockerproject.org/repo/main/centos/7\nenabled=1\ngpgcheck=1\ngpgkey=https://yum.dockerproject.org/gpg\nEOF\n\n\n\n\n\n\nInstall Docker service:\n\n\nyum install -y docker-engine-1.9.1 docker-engine-selinux-1.9.1\nsystemctl start docker\nsystemctl enable docker\n\n\n\n\n\n\nMore\n\n\nAdditionally, review the following prerequisites:\n\n\n\n\nPrerequisites on AWS\n\n\nPrerequisites on Azure\n\n\nPrerequisites on GCP\n\n\nPrerequisites on OpenStack\n\n\n\n\nInstall Cloudbreak on your own VM\n\n\nInstall Cloudbreak using the following steps.\n\n\nSteps\n\n\n\n\n\n\nInstall the Cloudbreak deployer and unzip the platform-specific single binary to your PATH. For example:\n\n\nyum -y install unzip tar\ncurl -Ls public-repo-1.hortonworks.com/HDP/cloudbreak/cloudbreak-deployer_2.7.0_$(uname)_x86_64.tgz | sudo tar -xz -C /bin cbd\ncbd --version\n\n\nOnce the Cloudbreak deployer is installed, you can set up the Cloudbreak application.\n\n\n\n\n\n\nCreate a Cloudbreak deployment directory and navigate to it:\n\n\nmkdir cloudbreak-deployment\ncd cloudbreak-deployment\n\n\n\n\n\n\nIn the directory, create a file called \nProfile\n with the following content:\n\n\nexport UAA_DEFAULT_SECRET=MY-SECRET\nexport UAA_DEFAULT_USER_PW=MY-PASSWORD\nexport UAA_DEFAULT_USER_EMAIL=MY-EMAIL\nexport PUBLIC_IP=MY_VM_IP\n\n\n\n\nFor example:\n\n\nexport UAA_DEFAULT_SECRET=MySecret123\nexport UAA_DEFAULT_USER_PW=MySecurePassword123\nexport UAA_DEFAULT_USER_EMAIL=dbialek@hortonworks.com\nexport PUBLIC_IP=172.26.231.100\n\n\n\nYou will need to provide the email and password when logging in to the Cloudbreak web UI and when using the Cloudbreak CLI. The secret will be used by Cloudbreak for authentication.\n\n\nYou should set the CLOUDBREAK_SMTP_SENDER_USERNAME variable to the username you use to authenticate to your SMTP server. You should set the CLOUDBREAK_SMTP_SENDER_PASSWORD variable to the password you use to authenticate to your SMTP server.\n\n\n\n\n\n\nGenerate configurations by executing:\n\n\nrm *.yml\ncbd generate\n   \n\n\nThe cbd start command includes the cbd generate command which applies the following steps:\n\n\n\n\nCreates the \ndocker-compose.yml\n file, which describes the configuration of all the Docker containers required for the Cloudbreak deployment.  \n\n\nCreates the \nuaa.yml\n file, which holds the configuration of the identity server used to authenticate users with Cloudbreak.   \n\n\n\n\n\n\n\n\nStart the Cloudbreak application by using the following commands:\n\n\ncbd pull\ncbd start\n\n\nThis will start the Docker containers and initialize the application. The first time you start the Cloudbreak app, the process will take longer than usual due to the download of all the necessary docker images.\n\n\n\n\n\n\nNext, check Cloudbreak application logs:\n\n\ncbd logs cloudbreak\n\n\nYou should see a message like this in the log: \nStarted CloudbreakApplication in 36.823 seconds.\n Cloudbreak normally takes less than a minute to start.\n\n\n\n\n\n\nNext steps after installing on your own VM\n\n\nLog in to the Cloudbreak web UI and create a credential for Cloudbreak using the following platform-specific instructions:\n\n\n\n\nAccess Cloudbreak web UI on AWS\n  \n\n\nAccess Cloudbreak web UI on Azure\n  \n\n\nAccess Cloudbreak web UI on GCP\n  \n\n\nAccess Cloudbreak web UI on OpenStack", 
            "title": "Install Cloudbreak on your own VM"
        }, 
        {
            "location": "/vm-launch/index.html#installing-cloudbreak-on-your-own-vm", 
            "text": "This is an advanced deployment option. Select this option if you have custom VM requirements. Otherwise, you should use one of the pre-built images and follow these instructions:   Launch on AWS     Launch on Azure     Launch on GCP     Launch on OpenStack", 
            "title": "Installing Cloudbreak on your own VM"
        }, 
        {
            "location": "/vm-launch/index.html#system-requirements", 
            "text": "To launch the Cloudbreak deployer and install the Cloudbreak application, your system must meet the following requirements:   Minimum VM requirements: 16GB RAM, 40GB disk, 4 cores  Supported operating systems: RHEL, CentOS, and Oracle Linux 7 (64-bit)  Docker 1.9.1 must be installed    You can install Cloudbreak on Mac OS X for evaluation purposes only. Mac OS X is not supported for a production deployment of Cloudbreak.", 
            "title": "System requirements"
        }, 
        {
            "location": "/vm-launch/index.html#prerequisites", 
            "text": "To launch the Cloudbreak deployer and install the Cloudbreak application, you must first meet the following prerequisites:", 
            "title": "Prerequisites"
        }, 
        {
            "location": "/vm-launch/index.html#ports", 
            "text": "Ports 22 (SSH), 80 (HTTPS), and 443 (HTTPS) must be open.", 
            "title": "Ports"
        }, 
        {
            "location": "/vm-launch/index.html#root-access", 
            "text": "Every command must be executed as root. In order to get root privileges execute:  sudo -i", 
            "title": "Root access"
        }, 
        {
            "location": "/vm-launch/index.html#system-updates", 
            "text": "Ensure that your system is up-to-date by executing:  yum -y update  Reboot it if necessary.", 
            "title": "System updates"
        }, 
        {
            "location": "/vm-launch/index.html#iptables", 
            "text": "Install iptables-services:  yum -y install net-tools ntp wget lsof unzip tar iptables-services\nsystemctl enable ntpd   systemctl start ntpd\nsystemctl disable firewalld   systemctl stop firewalld   Without iptables-services installed the  iptables save  command will not be available.     Configure permissive iptables on your machine:  iptables --flush INPUT   \\\niptables --flush FORWARD   \\\nservice iptables save    Disable SELINUX:  sed -i --follow-symlinks 's/^SELINUX=.*/SELINUX=disabled/g' /etc/sysconfig/selinux    Create Docker repo:  cat   /etc/yum.repos.d/docker.repo  \"EOF\"\n[dockerrepo]\nname=Docker Repository\nbaseurl=https://yum.dockerproject.org/repo/main/centos/7\nenabled=1\ngpgcheck=1\ngpgkey=https://yum.dockerproject.org/gpg\nEOF    Install Docker service:  yum install -y docker-engine-1.9.1 docker-engine-selinux-1.9.1\nsystemctl start docker\nsystemctl enable docker", 
            "title": "Iptables"
        }, 
        {
            "location": "/vm-launch/index.html#more", 
            "text": "Additionally, review the following prerequisites:   Prerequisites on AWS  Prerequisites on Azure  Prerequisites on GCP  Prerequisites on OpenStack", 
            "title": "More"
        }, 
        {
            "location": "/vm-launch/index.html#install-cloudbreak-on-your-own-vm", 
            "text": "Install Cloudbreak using the following steps.  Steps    Install the Cloudbreak deployer and unzip the platform-specific single binary to your PATH. For example:  yum -y install unzip tar\ncurl -Ls public-repo-1.hortonworks.com/HDP/cloudbreak/cloudbreak-deployer_2.7.0_$(uname)_x86_64.tgz | sudo tar -xz -C /bin cbd\ncbd --version  Once the Cloudbreak deployer is installed, you can set up the Cloudbreak application.    Create a Cloudbreak deployment directory and navigate to it:  mkdir cloudbreak-deployment\ncd cloudbreak-deployment    In the directory, create a file called  Profile  with the following content:  export UAA_DEFAULT_SECRET=MY-SECRET\nexport UAA_DEFAULT_USER_PW=MY-PASSWORD\nexport UAA_DEFAULT_USER_EMAIL=MY-EMAIL\nexport PUBLIC_IP=MY_VM_IP   For example:  export UAA_DEFAULT_SECRET=MySecret123\nexport UAA_DEFAULT_USER_PW=MySecurePassword123\nexport UAA_DEFAULT_USER_EMAIL=dbialek@hortonworks.com\nexport PUBLIC_IP=172.26.231.100  You will need to provide the email and password when logging in to the Cloudbreak web UI and when using the Cloudbreak CLI. The secret will be used by Cloudbreak for authentication.  You should set the CLOUDBREAK_SMTP_SENDER_USERNAME variable to the username you use to authenticate to your SMTP server. You should set the CLOUDBREAK_SMTP_SENDER_PASSWORD variable to the password you use to authenticate to your SMTP server.    Generate configurations by executing:  rm *.yml\ncbd generate      The cbd start command includes the cbd generate command which applies the following steps:   Creates the  docker-compose.yml  file, which describes the configuration of all the Docker containers required for the Cloudbreak deployment.    Creates the  uaa.yml  file, which holds the configuration of the identity server used to authenticate users with Cloudbreak.        Start the Cloudbreak application by using the following commands:  cbd pull\ncbd start  This will start the Docker containers and initialize the application. The first time you start the Cloudbreak app, the process will take longer than usual due to the download of all the necessary docker images.    Next, check Cloudbreak application logs:  cbd logs cloudbreak  You should see a message like this in the log:  Started CloudbreakApplication in 36.823 seconds.  Cloudbreak normally takes less than a minute to start.", 
            "title": "Install Cloudbreak on your own VM"
        }, 
        {
            "location": "/vm-launch/index.html#next-steps-after-installing-on-your-own-vm", 
            "text": "Log in to the Cloudbreak web UI and create a credential for Cloudbreak using the following platform-specific instructions:   Access Cloudbreak web UI on AWS     Access Cloudbreak web UI on Azure     Access Cloudbreak web UI on GCP     Access Cloudbreak web UI on OpenStack", 
            "title": "Next steps after installing on your own VM"
        }, 
        {
            "location": "/blueprints/index.html", 
            "text": "Using custom blueprints\n\n\nThis option allows you to create and save your custom blueprints. \n\n\nAmbari blueprints\n are your declarative definition of your HDP or HDF cluster, defining the host groups and which components to install on which host group. Ambari uses them as a base for your clusters. \n\n\nYou have two options concerning using blueprints with Cloudbreak:\n\n\n\n\nUse one of the pre-defined blueprints\n: To use one of the default blueprints, simply select them when creating a cluster. The option is available on the \nGeneral Configuration\n page. First select the \nStack Version\n and then select your chosen blueprint under \nCluster Type\n. For the list of default blueprints, refer to \nDefault cluster configurations\n.       \n\n\nAdd your custom blueprint\n by uploading a JSON file or pasting the JSON text. \n\n\n\n\nWe recommend that you review the default blueprints to check if they meet your requirements. You can do this by selecting \nBlueprints\n from the navigation pane in the Cloudbreak web UI or by reading the documentation below.\n\n\nCreating a blueprint\n\n\nAmbari blueprints are specified in the JSON format. A blueprint can be exported from a running Ambari cluster and can be reused in Cloudbreak after slight modifications. When a blueprint is exported, it includes  some hardcoded configurations such as domain names, memory configurations, and so on, that are not applicable to the Cloudbreak cluster. There is no automatic way to modify an exported blueprint and make it instantly usable in Cloudbreak, the modifications have to be done manually. \n\n\nIn general, the blueprint should include the following elements:\n\n\n\"Blueprints\": {\n\u2002\u2002\u2002\u2002\"blueprint_name\": \"hdp-small-default\",\n\u2002\u2002\u2002\u2002\"stack_name\": \"HDP\",\n\u2002\u2002\u2002\u2002\"stack_version\": \"2.6\"\n\u2002\u2002},\n\u2002\u2002\"settings\": [],\n\u2002\u2002\"configurations\": [],\n\u2002\u2002\"host_groups\": [\n\u2002\u2002{\n      \"name\": \"master\",\n      \"configurations\": [],\n      \"components\": []\n    },\n    {\n      \"name\": \"worker\",\n      \"configurations\": [],\n      \"components\": [ ]\n    },\n    {\n      \"name\": \"compute\",\n      \"configurations\": [],\n      \"components\": []\n    }\n\u2002\u2002 ]\n\u2002\u2002}\n\n\n\nFor correct blueprint layout and other information about Ambari blueprints, refer to the \nAmbari cwiki\n page. \n\n\n\n    \nCreating Blueprints for Ambari 2.6.1+\n\n    \n\nAmbari 2.6.1 or newer does not install the mysqlconnector; therefore, when creating a blueprint for Ambari 2.6.1 or newer \nyou should not include the MYSQL_SERVER component\n for Hive Metastore in your blueprint. Instead, you have two options:\n\n\n\nConfigure an external RDBMS instance for Hive Metastore and include the JDBC connection information in your blueprint. If you choose to use an external database that is not PostgreSQL (such as Oracle, mysql) you must also set up Ambari with the appropriate connector; to do this, create a pre-ambari-start recipe and pass it when creating a cluster.\n\n\nIf a remote Hive RDBMS is not provided, Cloudbreak installs a Postgres instance and configures it for Hive Metastore during the cluster launch.\n\n\n\nFor information on how to configure an external database and pass your external database connection parameters, refer to \nAmbari blueprint\n documentation.\n\n\n\n\n\n\nCloudbreak requires you to define an additional element in the blueprint called \"blueprint_name\".\u2002This should be a unique name within Cloudbreak list of blueprints. For example:\n\n\n\"Blueprints\": {\n\u2002\u2002\u2002\u2002\"blueprint_name\": \"hdp-small-default\",\n\u2002\u2002\u2002\u2002\"stack_name\": \"HDP\",\n\u2002\u2002\u2002\u2002\"stack_version\": \"2.6\"\n\u2002\u2002},\n\u2002\u2002\"settings\": [],\n\u2002\u2002\"configurations\": [],\n\u2002\u2002\"host_groups\": [\n\u2002\u2002...\n\n\n\n\nThe \"blueprint_name\" is not included in the Ambari export. \n\n\nAfter you provide the blueprint to Cloudbreak, the host groups in the JSON will be mapped to a set of instances when starting the cluster, and the specified services and components will be installed on the corresponding nodes. It is not necessary to define a complete configuration in the blueprint. If a configuration is missing, Ambari will use a default value. \n\n\nHere are a few \nblueprint examples\n. You can also refer to the default blueprints provided in the Cloudbreak UI.\n\n\nRelated links\n\n\nBlueprint examples\n (Hortonworks)   \n\n\nAmbari cwiki\n (External)  \n\n\nCreating a template blueprint\n\n\nCloudbreak allows you to create special blueprints which include templating: the values of the variables specified in the blueprint will be dynamically generated \n replaced in the cluster creation phase, picking up the parameter values that you provided in the Cloudbreak UI or CLI.\nCloudbreak supports \nmustache\n kind of templating with {{{variable}}} syntax. You cannot use functions in the blueprint file; only variable injection is supported.\n\n\nExternal authentication source (LDAP/AD)\n\n\nWhen using \nexternal authentication sources\n, the following variables can be specified in your Blueprint for replacement:\n\n\n\n\n\n\n\n\nVariable\n\n\nDescription\n\n\nExample\n\n\n\n\n\n\n\n\n\n\nldap.connectionURL\n\n\nthe URL of the LDAP (host:port)\n\n\nldap://10.1.1.1:389\n\n\n\n\n\n\nldap.bindDn\n\n\nThe root Distinguished Name to search in the directory for users\n\n\nCN=Administrator,CN=Users,DC=ad,DC=hdc,DC=com\n\n\n\n\n\n\nldap.bindPassword\n\n\nThe root Distinguished Name password\n\n\nPassword1234!\n\n\n\n\n\n\nldap.directoryType\n\n\nThe directory of type\n\n\nLDAP or ACTIVE_DIRECTORY\n\n\n\n\n\n\nldap.userSearchBase\n\n\nUser search base\n\n\nCN=Users,DC=ad,DC=hdc,DC=com\n\n\n\n\n\n\nldap.userNameAttribute\n\n\nUsername attribute\n\n\ncn\n\n\n\n\n\n\nldap.userObjectClass\n\n\nObject class for users\n\n\nperson\n\n\n\n\n\n\nldap.groupSearchBase\n\n\nGroup search base\n\n\nOU=Groups,DC=ad,DC=hdc,DC=com\n\n\n\n\n\n\nldap.groupNameAttribute\n\n\nGroup attribute\n\n\ncb\n\n\n\n\n\n\nldap.groupObjectClass\n\n\nGroup object class\n\n\ngroup\n\n\n\n\n\n\nldap.groupMemberAttribute\n\n\nAttribute for membershio\n\n\nmember\n\n\n\n\n\n\nldap.domain\n\n\nYour domain\n\n\nexample.com\n\n\n\n\n\n\n\n\nExternal database (RDBMS)\n\n\nWhen using \nexternal databases\n, the following variables can be specified in your Blueprint for replacement:\n\n\n\n\n\n\n\n\nVariable\n\n\nDescription\n\n\nExample\n\n\n\n\n\n\n\n\n\n\nrds.[type].connectionURL\n\n\nThe jdbc url to the RDBMS\n\n\njdbc:postgresql://db.test:5432/test\n\n\n\n\n\n\nrds.[type].connectionDriver\n\n\nThe connection driver\n\n\norg.postgresql.Driver\n\n\n\n\n\n\nrds.[type].connectionUserName\n\n\nThe user name to the database\n\n\nadmin\n\n\n\n\n\n\nrds.[type].connectionPassword\n\n\nThe password for the connection\n\n\nPassword1234!\n\n\n\n\n\n\nrds.[type].databaseName\n\n\nThe name of the database\n\n\nhive\n\n\n\n\n\n\nrds.[type].host\n\n\nThe host of the database\n\n\n10.1.1.1\n\n\n\n\n\n\nrds.[type].hostWithPort\n\n\nDatabase host + port\n\n\n10.1.1.1:1234\n\n\n\n\n\n\nrds.[type].subprotocol\n\n\nParsed from jdbc url\n\n\npostgres\n\n\n\n\n\n\nrds.[type].databaseEngine\n\n\nCapital database name\n\n\nPOSTGRES\n\n\n\n\n\n\n\n\nUpload a blueprint\n\n\nOnce you have your blueprint ready, perform these steps.\n\n\nSteps\n\n\n\n\nIn the Cloudbreak UI, select \nBlueprints\n from the navigation pane. \n\n\n\n\nTo add your own blueprint, click \nCreate Blueprint\n and enter the following parameters:\n\n\n\n\n\n\n\n\nParameter\n\n\nValue\n\n\n\n\n\n\n\n\n\n\nName\n\n\nEnter a name for your blueprint.\n\n\n\n\n\n\nDescription\n\n\n(Optional) Enter a description for your blueprint.\n\n\n\n\n\n\nBlueprint Source\n\n\nSelect one of: \nText\n: Paste blueprint in JSON format.\n \nFile\n: Upload a file that contains the blueprint.\n \nURL\n: Specify the URL for your blueprint.\n\n\n\n\n\n\n\n\n \n\n\n\n\n\n\nTo use the uploaded blueprints, select it when creating a cluster. The option is available on the \nGeneral Configuration\n page. First select the \nPlatform Version\n and then select your chosen blueprint under \nCluster Type\n.", 
            "title": "Use custom blueprints"
        }, 
        {
            "location": "/blueprints/index.html#using-custom-blueprints", 
            "text": "This option allows you to create and save your custom blueprints.   Ambari blueprints  are your declarative definition of your HDP or HDF cluster, defining the host groups and which components to install on which host group. Ambari uses them as a base for your clusters.   You have two options concerning using blueprints with Cloudbreak:   Use one of the pre-defined blueprints : To use one of the default blueprints, simply select them when creating a cluster. The option is available on the  General Configuration  page. First select the  Stack Version  and then select your chosen blueprint under  Cluster Type . For the list of default blueprints, refer to  Default cluster configurations .         Add your custom blueprint  by uploading a JSON file or pasting the JSON text.    We recommend that you review the default blueprints to check if they meet your requirements. You can do this by selecting  Blueprints  from the navigation pane in the Cloudbreak web UI or by reading the documentation below.", 
            "title": "Using custom blueprints"
        }, 
        {
            "location": "/blueprints/index.html#creating-a-blueprint", 
            "text": "Ambari blueprints are specified in the JSON format. A blueprint can be exported from a running Ambari cluster and can be reused in Cloudbreak after slight modifications. When a blueprint is exported, it includes  some hardcoded configurations such as domain names, memory configurations, and so on, that are not applicable to the Cloudbreak cluster. There is no automatic way to modify an exported blueprint and make it instantly usable in Cloudbreak, the modifications have to be done manually.   In general, the blueprint should include the following elements:  \"Blueprints\": {\n\u2002\u2002\u2002\u2002\"blueprint_name\": \"hdp-small-default\",\n\u2002\u2002\u2002\u2002\"stack_name\": \"HDP\",\n\u2002\u2002\u2002\u2002\"stack_version\": \"2.6\"\n\u2002\u2002},\n\u2002\u2002\"settings\": [],\n\u2002\u2002\"configurations\": [],\n\u2002\u2002\"host_groups\": [\n\u2002\u2002{\n      \"name\": \"master\",\n      \"configurations\": [],\n      \"components\": []\n    },\n    {\n      \"name\": \"worker\",\n      \"configurations\": [],\n      \"components\": [ ]\n    },\n    {\n      \"name\": \"compute\",\n      \"configurations\": [],\n      \"components\": []\n    }\n\u2002\u2002 ]\n\u2002\u2002}  For correct blueprint layout and other information about Ambari blueprints, refer to the  Ambari cwiki  page.   \n     Creating Blueprints for Ambari 2.6.1+ \n     \nAmbari 2.6.1 or newer does not install the mysqlconnector; therefore, when creating a blueprint for Ambari 2.6.1 or newer  you should not include the MYSQL_SERVER component  for Hive Metastore in your blueprint. Instead, you have two options:  Configure an external RDBMS instance for Hive Metastore and include the JDBC connection information in your blueprint. If you choose to use an external database that is not PostgreSQL (such as Oracle, mysql) you must also set up Ambari with the appropriate connector; to do this, create a pre-ambari-start recipe and pass it when creating a cluster.  If a remote Hive RDBMS is not provided, Cloudbreak installs a Postgres instance and configures it for Hive Metastore during the cluster launch.  \nFor information on how to configure an external database and pass your external database connection parameters, refer to  Ambari blueprint  documentation.   Cloudbreak requires you to define an additional element in the blueprint called \"blueprint_name\".\u2002This should be a unique name within Cloudbreak list of blueprints. For example:  \"Blueprints\": {\n\u2002\u2002\u2002\u2002\"blueprint_name\": \"hdp-small-default\",\n\u2002\u2002\u2002\u2002\"stack_name\": \"HDP\",\n\u2002\u2002\u2002\u2002\"stack_version\": \"2.6\"\n\u2002\u2002},\n\u2002\u2002\"settings\": [],\n\u2002\u2002\"configurations\": [],\n\u2002\u2002\"host_groups\": [\n\u2002\u2002...  The \"blueprint_name\" is not included in the Ambari export.   After you provide the blueprint to Cloudbreak, the host groups in the JSON will be mapped to a set of instances when starting the cluster, and the specified services and components will be installed on the corresponding nodes. It is not necessary to define a complete configuration in the blueprint. If a configuration is missing, Ambari will use a default value.   Here are a few  blueprint examples . You can also refer to the default blueprints provided in the Cloudbreak UI.  Related links  Blueprint examples  (Hortonworks)     Ambari cwiki  (External)", 
            "title": "Creating a blueprint"
        }, 
        {
            "location": "/blueprints/index.html#creating-a-template-blueprint", 
            "text": "Cloudbreak allows you to create special blueprints which include templating: the values of the variables specified in the blueprint will be dynamically generated   replaced in the cluster creation phase, picking up the parameter values that you provided in the Cloudbreak UI or CLI.\nCloudbreak supports  mustache  kind of templating with {{{variable}}} syntax. You cannot use functions in the blueprint file; only variable injection is supported.  External authentication source (LDAP/AD)  When using  external authentication sources , the following variables can be specified in your Blueprint for replacement:     Variable  Description  Example      ldap.connectionURL  the URL of the LDAP (host:port)  ldap://10.1.1.1:389    ldap.bindDn  The root Distinguished Name to search in the directory for users  CN=Administrator,CN=Users,DC=ad,DC=hdc,DC=com    ldap.bindPassword  The root Distinguished Name password  Password1234!    ldap.directoryType  The directory of type  LDAP or ACTIVE_DIRECTORY    ldap.userSearchBase  User search base  CN=Users,DC=ad,DC=hdc,DC=com    ldap.userNameAttribute  Username attribute  cn    ldap.userObjectClass  Object class for users  person    ldap.groupSearchBase  Group search base  OU=Groups,DC=ad,DC=hdc,DC=com    ldap.groupNameAttribute  Group attribute  cb    ldap.groupObjectClass  Group object class  group    ldap.groupMemberAttribute  Attribute for membershio  member    ldap.domain  Your domain  example.com     External database (RDBMS)  When using  external databases , the following variables can be specified in your Blueprint for replacement:     Variable  Description  Example      rds.[type].connectionURL  The jdbc url to the RDBMS  jdbc:postgresql://db.test:5432/test    rds.[type].connectionDriver  The connection driver  org.postgresql.Driver    rds.[type].connectionUserName  The user name to the database  admin    rds.[type].connectionPassword  The password for the connection  Password1234!    rds.[type].databaseName  The name of the database  hive    rds.[type].host  The host of the database  10.1.1.1    rds.[type].hostWithPort  Database host + port  10.1.1.1:1234    rds.[type].subprotocol  Parsed from jdbc url  postgres    rds.[type].databaseEngine  Capital database name  POSTGRES", 
            "title": "Creating a template blueprint"
        }, 
        {
            "location": "/blueprints/index.html#upload-a-blueprint", 
            "text": "Once you have your blueprint ready, perform these steps.  Steps   In the Cloudbreak UI, select  Blueprints  from the navigation pane.    To add your own blueprint, click  Create Blueprint  and enter the following parameters:     Parameter  Value      Name  Enter a name for your blueprint.    Description  (Optional) Enter a description for your blueprint.    Blueprint Source  Select one of:  Text : Paste blueprint in JSON format.   File : Upload a file that contains the blueprint.   URL : Specify the URL for your blueprint.          To use the uploaded blueprints, select it when creating a cluster. The option is available on the  General Configuration  page. First select the  Platform Version  and then select your chosen blueprint under  Cluster Type .", 
            "title": "Upload a blueprint"
        }, 
        {
            "location": "/recipes/index.html", 
            "text": "Creating custom scripts (recipes)\n\n\nAlthough Cloudbreak lets you provision clusters in the cloud based on custom Ambari blueprints, Cloudbreak provisioning options don't consider all possible use cases. For that reason, we introduced recipes. \n\n\nA recipe is a script that runs on all nodes of a selected node group at a specific time. You can use recipes for tasks such as installing additional software or performing advanced cluster configuration. For example, you can use a recipe to put a JAR file on the Hadoop classpath.\n\n\nAvailable recipe execution times are:  \n\n\n\n\nBefore Ambari server start    \n\n\nAfter Ambari server start    \n\n\nAfter cluster installation    \n\n\nBefore cluster termination   \n\n\n\n\nYou can upload your recipes to Cloudbreak via the UI or CLI. Then, when creating a cluster, you can optionally attach one or more \"recipes\" and they will be executed on a specific host group at a specified time. \n\n\nWriting recipes\n\n\nWhen using recipes, consider the following:\n\n\n\n\nThe scripts will be executed on the node types you specify (such as \"master\", \"worker\", \"compute\"). If you want to run a a script on all nodes, define the recipe one per node type.  \n\n\nThe script will execute on all of the nodes of that type as root.  \n\n\nIn order to be executed, your script must be in a network location which is accessible from the cloud controller and cluster instances VPC.  \n\n\nMake sure to follow Linux best practices when creating your scripts. For example, don't forget to script \"Yes\" auto-answers where needed.  \n\n\nDo not execute yum update \u2013y since it may update other components on the instances (such as salt), which can create unintended or unstable behavior.   \n\n\nThe scripts will be executed as root. The recipe output is written to \n/var/log/recipes\n on each node on which it was executed.\n\n\n\n\nSample recipe for yum proxy setting\n\n\n#!/bin/bash\ncat \n /etc/yum.conf \nENDOF\nproxy=http://10.0.0.133:3128\nENDOF\n\n\n\n\nAdd recipes\n\n\nIn order to use your recipe for clusters, you must register it first by using the steps below.\n\n\nSteps\n\n\n\n\n\n\nPlace your script in a network location accessible from Cloudbreak and cluster instances virtual network. \n\n\n\n\n\n\nSelect \nExternal Sources \n Recipes\n from the navigation menu. \n\n\n\n\n\n\nClick on \nCreate Recipe\n. \n\n\n\n\n\n\nProvide the following:\n\n\n\n\n\n\n\n\nParameter\n\n\nValue\n\n\n\n\n\n\n\n\n\n\nName\n\n\nEnter a name for your recipe.\n\n\n\n\n\n\nDescription\n\n\n(Optional) Enter a description for your recipe.\n\n\n\n\n\n\nExecution Type\n\n\nSelect one of the following options: \npre-ambari-start\n: The script will be executed prior to Ambari server start.\npost-ambari-start\n: The script will be executed after Ambari server start but prior to cluster installation.\npost-cluster-install\n: The script will be executed after cluster deployment.\npre-termination\n: The script will be executed before cluster termination.\n\n\n\n\n\n\nScript\n\n\nSelect one of: \nScript\n: Paste the script.\n \nFile\n: Point to a file on your machine that contains the recipe.\n \nURL\n: Specify the URL for your recipe.\n\n\n\n\n\n\n\n\n\n\n\n\nWhen creating a cluster, you can select previously added recipes on the advanced \nCluster Extensions\n page of the create cluster wizard.", 
            "title": "Use custom scripts (recipes)"
        }, 
        {
            "location": "/recipes/index.html#creating-custom-scripts-recipes", 
            "text": "Although Cloudbreak lets you provision clusters in the cloud based on custom Ambari blueprints, Cloudbreak provisioning options don't consider all possible use cases. For that reason, we introduced recipes.   A recipe is a script that runs on all nodes of a selected node group at a specific time. You can use recipes for tasks such as installing additional software or performing advanced cluster configuration. For example, you can use a recipe to put a JAR file on the Hadoop classpath.  Available recipe execution times are:     Before Ambari server start      After Ambari server start      After cluster installation      Before cluster termination      You can upload your recipes to Cloudbreak via the UI or CLI. Then, when creating a cluster, you can optionally attach one or more \"recipes\" and they will be executed on a specific host group at a specified time.", 
            "title": "Creating custom scripts (recipes)"
        }, 
        {
            "location": "/recipes/index.html#writing-recipes", 
            "text": "When using recipes, consider the following:   The scripts will be executed on the node types you specify (such as \"master\", \"worker\", \"compute\"). If you want to run a a script on all nodes, define the recipe one per node type.    The script will execute on all of the nodes of that type as root.    In order to be executed, your script must be in a network location which is accessible from the cloud controller and cluster instances VPC.    Make sure to follow Linux best practices when creating your scripts. For example, don't forget to script \"Yes\" auto-answers where needed.    Do not execute yum update \u2013y since it may update other components on the instances (such as salt), which can create unintended or unstable behavior.     The scripts will be executed as root. The recipe output is written to  /var/log/recipes  on each node on which it was executed.", 
            "title": "Writing recipes"
        }, 
        {
            "location": "/recipes/index.html#sample-recipe-for-yum-proxy-setting", 
            "text": "#!/bin/bash\ncat   /etc/yum.conf  ENDOF\nproxy=http://10.0.0.133:3128\nENDOF", 
            "title": "Sample recipe for yum proxy setting"
        }, 
        {
            "location": "/recipes/index.html#add-recipes", 
            "text": "In order to use your recipe for clusters, you must register it first by using the steps below.  Steps    Place your script in a network location accessible from Cloudbreak and cluster instances virtual network.     Select  External Sources   Recipes  from the navigation menu.     Click on  Create Recipe .     Provide the following:     Parameter  Value      Name  Enter a name for your recipe.    Description  (Optional) Enter a description for your recipe.    Execution Type  Select one of the following options:  pre-ambari-start : The script will be executed prior to Ambari server start. post-ambari-start : The script will be executed after Ambari server start but prior to cluster installation. post-cluster-install : The script will be executed after cluster deployment. pre-termination : The script will be executed before cluster termination.    Script  Select one of:  Script : Paste the script.   File : Point to a file on your machine that contains the recipe.   URL : Specify the URL for your recipe.       When creating a cluster, you can select previously added recipes on the advanced  Cluster Extensions  page of the create cluster wizard.", 
            "title": "Add recipes"
        }, 
        {
            "location": "/mpacks/index.html", 
            "text": "Using management packs\n\n\nManagement packs allow you to deploy a range of services to your Ambari-managed cluster. You can use a management pack to deploy a specific component or service, like HDP Search, or to deploy an entire platform, like HDF.\n\n\nCloudbreak supports using management packs, allowing you to register them in Cloudbreak web UI and CLI and then select to install them as part of cluster creation. \n\n\nFor general information on management packs, refer to \nAmbari cwiki: Management+Packs\n.  \n\n\nRelated Links\n\n\nAmbari cwiki: Management+Packs\n  \n\n\nAdd management pack\n\n\nIn order to have a management stack installed for a specific cluster, you must register it with Cloudbreak by using the steps below.\n\n\nSteps\n\n\n\n\n\n\nObtain the URL for the management pack tarball file that you want to register in Cloudbreak. The tarball  must be available in a location accessible to clusters created by Cloudbreak.  \n\n\n\n\n\n\nIn Cloudbreak web UI, select \nExternal Sources \n Management Packs\n from the navigation menu. \n\n\n\n\n\n\nClick on \nRegister Management Pack\n. \n\n\n\n\n\n\nProvide the following:\n\n\n\n\n\n\n\n\nParameter\n\n\nValue\n\n\n\n\n\n\n\n\n\n\nName\n\n\nEnter a name for your managament pack.\n\n\n\n\n\n\nDescription\n\n\n(Optional) Enter a description.\n\n\n\n\n\n\nManagement pack URL\n\n\nProvide the URL to the location where the management pack tarball file is available.\n\n\n\n\n\n\nRemove all existing Ambari stack definitions prior to installing this Management Pack (\u201cmpack --purge\u201d).\n\n\n\n\n\n\n\n\nChecking this option allows you to purge any existing stack definition and should be included only when installing a stack management pack. Do not select this when installing an add-on service management pack.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWhen creating a cluster, on the advanced \nCluster Extensions\n page of the create cluster wizard, you can select one or more previously registered management packs. After selecting, click \nInstall\n to use the management pack for the cluster.", 
            "title": "Use management packs"
        }, 
        {
            "location": "/mpacks/index.html#using-management-packs", 
            "text": "Management packs allow you to deploy a range of services to your Ambari-managed cluster. You can use a management pack to deploy a specific component or service, like HDP Search, or to deploy an entire platform, like HDF.  Cloudbreak supports using management packs, allowing you to register them in Cloudbreak web UI and CLI and then select to install them as part of cluster creation.   For general information on management packs, refer to  Ambari cwiki: Management+Packs .    Related Links  Ambari cwiki: Management+Packs", 
            "title": "Using management packs"
        }, 
        {
            "location": "/mpacks/index.html#add-management-pack", 
            "text": "In order to have a management stack installed for a specific cluster, you must register it with Cloudbreak by using the steps below.  Steps    Obtain the URL for the management pack tarball file that you want to register in Cloudbreak. The tarball  must be available in a location accessible to clusters created by Cloudbreak.      In Cloudbreak web UI, select  External Sources   Management Packs  from the navigation menu.     Click on  Register Management Pack .     Provide the following:     Parameter  Value      Name  Enter a name for your managament pack.    Description  (Optional) Enter a description.    Management pack URL  Provide the URL to the location where the management pack tarball file is available.    Remove all existing Ambari stack definitions prior to installing this Management Pack (\u201cmpack --purge\u201d).     Checking this option allows you to purge any existing stack definition and should be included only when installing a stack management pack. Do not select this when installing an add-on service management pack.        When creating a cluster, on the advanced  Cluster Extensions  page of the create cluster wizard, you can select one or more previously registered management packs. After selecting, click  Install  to use the management pack for the cluster.", 
            "title": "Add management pack"
        }, 
        {
            "location": "/images/index.html", 
            "text": "Using custom images\n\n\nDefault images are available for each supported cloud provider and region. The following table lists the default base images available:\n\n\n\n\n\n\n\n\nCloud provider\n\n\nDefault image\n\n\n\n\n\n\n\n\n\n\nAWS\n\n\nAmazon Linux 2017\n\n\n\n\n\n\nAzure\n\n\nCentOS 7\n\n\n\n\n\n\nGCP\n\n\nCentOS 7\n\n\n\n\n\n\nOpenStack\n\n\nCentOS 7\n\n\n\n\n\n\n\n\nSince these default images may not fit the requirements of some users (for example when user requirements include custom OS hardening, custom libraries, custom tooling, and so on) Cloudbreak allows you to use your own \ncustom base images\n.\n\n\nIn order to use your own custom base images you must:\n\n\n\n\nBuild your custom images  \n\n\nPrepare the custom image catalog JSON file and save it in a location accessible to the Cloudbreak VM  \n\n\nRegister your custom image catalog with Cloudbreak    \n\n\nSelect a custom image when creating a cluster  \n\n\n\n\n\n    \nImportant\n\n    \n\nOnly \nbase images\n can be created and registered as custom images. Do not create or register \nprewarmed images\n as custom images.\n\n\n\n\n\n\nBuild custom images\n\n\nRefer to \nCustom images for Cloudbreak\n for information on how to build custom images.\n\n\nThis repository includes instructions and scripts to help you build custom images. Once you have the images, refer to the documentation below for information on how to create an image catalog and register it with Cloudbreak.\n\n\nPrepare the image catalog\n\n\nOnce you've built the custom images, prepare your custom image catalog JSON file. Once your image catalog JSON file is ready, save it in a location accessible via HTTP/HTTPS.\n\n\nStructure of the image catalog JSON file\n\n\nThe image catalog JSON file includes the following two high-level sections:\n\n\n\n\nimages\n: Contains information about the created images. The burned images are stored in the \nbase-images\n section.  \n\n\nversions\n: Contains the \ncloudbreak\n entry, which includes mapping between Cloudbreak versions and the image identifiers of burned images available for these Cloudbreak versions.\n\n\n\n\n\n\nAfter adding your image(s) to the \nimages\n section, make sure to also update the \nversions\n section. \n\n\n\n\nImages section\n  \n\n\nThe burned images are stored in the \nbase-images\n sub-section of \nimages\n. The \nbase-images\n section stores one or more image \"records\". Every image \"record\" must contain the date, description, images, os, os_type, and uuid fields.\n\n\n\n\n\n\n\n\nParameter\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\ndate\n\n\nDate for your image catalog entry.\n\n\n\n\n\n\ndescription\n\n\nDescription for your image catalog entry.\n\n\n\n\n\n\nimages\n\n\nThe image sets by cloud provider. An image set must store the virtual machine image IDs by the related region of the provider (AWS, Azure) or contain one default image for all regions (GCP, OpenStack). The virtual machine image IDs come from the result of the image burning process and must be an existing identifier of a virtual machine image on the related provider side. For the providers which use global rather than per-region images, the region should be replaced with \ndefault\n.\n\n\n\n\n\n\nos\n\n\nThe operating system used in the image.\n\n\n\n\n\n\nos_type\n\n\nThe type of operating system which will be used to determine the default Ambari and HDP/HDF repositories to use. Set \nos_type\n to \"redhat6\" for amazonlinux or centos6 images. Set \nos_type\n to \"redhat7\" for centos7 or rhel7 images.\n\n\n\n\n\n\nuuid\n\n\nThe \nuuid\n field must be a unique identifier within the file. You can generate it or select it manually. The utility \nuuidgen\n available from your command line is a convenient way to generate a unique ID.\n\n\n\n\n\n\n\n\nVersions section\n  \n\n\nThe \nversions\n section includes a single \"cloudbreak\" entry, which maps the uuids to a specific Cloudbreak version:\n\n\n\n\n\n\n\n\nParameter\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nimages\n\n\nImage \nuuid\n, same as the one that you specified in the \nbase-images\n section.\n\n\n\n\n\n\nversions\n\n\nThe Cloudbreak version(s) for which you would like to use the images.\n\n\n\n\n\n\n\n\nExample image catalog JSON file\n\n\nHere is an example image catalog JSON file that includes two sets of custom base images:\n\n\n\n\nA custom base image for AWS:\n\n\nThat is using Amazon Linux operating system\n\n\nThat will use the Redhat 6 repos as default Ambari and HDP repositories during cluster create     \n\n\nHas a unique ID of \"44b140a4-bd0b-457d-b174-e988bee3ca47\"\n\n\nIs available for Cloudbreak 2.4.0    \n\n\n\n\n\n\nA custom base image for Azure, Google, and OpenStack:\n\n\nThat is using CentOS 7 operating system\n\n\nThat will use the Redhat 7 repos as default Ambari and HDP repositories during cluster create   \n\n\nHas a unique ID of \"f6e778fc-7f17-4535-9021-515351df3692\"\n\n\nIs available to Cloudbreak 2.4.0      \n\n\n\n\n\n\n\n\nYou can also download it from \nhere\n.\n\n\n\n{\n  \"images\": {\n    \"base-images\": [\n      {\n        \"date\": \"2017-10-13\",\n        \"description\": \"Cloudbreak official base image\",\n        \"images\": {\n          \"aws\": {\n            \"ap-northeast-1\": \"ami-78e9311e\",\n            \"ap-northeast-2\": \"ami-84b613ea\",\n            \"ap-southeast-1\": \"ami-75226716\",\n            \"ap-southeast-2\": \"ami-92ce23f0\",\n            \"eu-central-1\": \"ami-d95be5b6\",\n            \"eu-west-1\": \"ami-46429e3f\",\n            \"sa-east-1\": \"ami-86d5abea\",\n            \"us-east-1\": \"ami-51a2742b\",\n            \"us-west-1\": \"ami-21ccfe41\",\n            \"us-west-2\": \"ami-2a1cdc52\"\n          }\n        },\n        \"os\": \"amazonlinux\",\n        \"os_type\": \"redhat6\",\n        \"uuid\": \"44b140a4-bd0b-457d-b174-e988bee3ca47\"\n      },\n      {\n        \"date\": \"2017-10-13\",\n        \"description\": \"Cloudbreak official base image\",\n        \"images\": {\n          \"azure\": {\n            \"Australia East\": \"https://hwxaustraliaeast.blob.core.windows.net/images/hdc-hdp--1710161226.vhd\",\n            \"Australia South East\": \"https://hwxaustralisoutheast.blob.core.windows.net/images/hdc-hdp--1710161226.vhd\",\n            \"Brazil South\": \"https://sequenceiqbrazilsouth2.blob.core.windows.net/images/hdc-hdp--1710161226.vhd\",\n            \"Canada Central\": \"https://sequenceiqcanadacentral.blob.core.windows.net/images/hdc-hdp--1710161226.vhd\",\n            \"Canada East\": \"https://sequenceiqcanadaeast.blob.core.windows.net/images/hdc-hdp--1710161226.vhd\",\n            \"Central India\": \"https://hwxcentralindia.blob.core.windows.net/images/hdc-hdp--1710161226.vhd\",\n            \"Central US\": \"https://sequenceiqcentralus2.blob.core.windows.net/images/hdc-hdp--1710161226.vhd\",\n            \"East Asia\": \"https://sequenceiqeastasia2.blob.core.windows.net/images/hdc-hdp--1710161226.vhd\",\n            \"East US\": \"https://sequenceiqeastus12.blob.core.windows.net/images/hdc-hdp--1710161226.vhd\",\n            \"East US 2\": \"https://sequenceiqeastus22.blob.core.windows.net/images/hdc-hdp--1710161226.vhd\",\n            \"Japan East\": \"https://sequenceiqjapaneast2.blob.core.windows.net/images/hdc-hdp--1710161226.vhd\",\n            \"Japan West\": \"https://sequenceiqjapanwest2.blob.core.windows.net/images/hdc-hdp--1710161226.vhd\",\n            \"Korea Central\": \"https://hwxkoreacentral.blob.core.windows.net/images/hdc-hdp--1710161226.vhd\",\n            \"Korea South\": \"https://hwxkoreasouth.blob.core.windows.net/images/hdc-hdp--1710161226.vhd\",\n            \"North Central US\": \"https://sequenceiqorthcentralus2.blob.core.windows.net/images/hdc-hdp--1710161226.vhd\",\n            \"North Europe\": \"https://sequenceiqnortheurope2.blob.core.windows.net/images/hdc-hdp--1710161226.vhd\",\n            \"South Central US\": \"https://sequenceiqouthcentralus2.blob.core.windows.net/images/hdc-hdp--1710161226.vhd\",\n            \"South India\": \"https://hwxsouthindia.blob.core.windows.net/images/hdc-hdp--1710161226.vhd\",\n            \"Southeast Asia\": \"https://sequenceiqsoutheastasia2.blob.core.windows.net/images/hdc-hdp--1710161226.vhd\",\n            \"UK South\": \"https://hwxsouthuk.blob.core.windows.net/images/hdc-hdp--1710161226.vhd\",\n            \"UK West\": \"https://hwxwestuk.blob.core.windows.net/images/hdc-hdp--1710161226.vhd\",\n            \"West Central US\": \"https://hwxwestcentralus.blob.core.windows.net/images/hdc-hdp--1710161226.vhd\",\n            \"West Europe\": \"https://sequenceiqwesteurope2.blob.core.windows.net/images/hdc-hdp--1710161226.vhd\",\n            \"West India\": \"https://hwxwestindia.blob.core.windows.net/images/hdc-hdp--1710161226.vhd\",\n            \"West US\": \"https://sequenceiqwestus2.blob.core.windows.net/images/hdc-hdp--1710161226.vhd\",\n            \"West US 2\": \"https://hwxwestus2.blob.core.windows.net/images/hdc-hdp--1710161226.vhd\"\n          },\n          \"gcp\": {\n            \"default\": \"sequenceiqimage/hdc-hdp--1710161226.tar.gz\"\n          },\n          \"openstack\": {\n            \"default\": \"hdc-hdp--1710161226\"\n          }\n        },\n        \"os\": \"centos7\",\n        \"os_type\": \"redhat7\",\n        \"uuid\": \"f6e778fc-7f17-4535-9021-515351df3691\"\n      }\n    ]\n},\n  \"versions\": {\n    \"cloudbreak\": [\n      {\n        \"images\": [\n          \"44b140a4-bd0b-457d-b174-e988bee3ca47\",\n          \"f6e778fc-7f17-4535-9021-515351df3692\"\n        ],\n        \"versions\": [\n          \"2.4.0\"\n        ]\n      }\n    ]\n  }\n}\n\n\n\n\nRegister image catalog\n\n\nNow that you have created your image catalog JSON file, register it with your Cloudbreak instance. You can do this via Cloudbreak UI, CLI, or be editing the Profile file. \n\n\n\n  \nImportant\n\n  \n\n  The content type of your image catalog file should be \n\"application/json\"\n for Cloudbreak to be able to process it.\n\n\n\n\n\n\nRegister image catalog in the UI\n\n\nUse these steps to register your custom image catalog in the Cloudbreak UI.\n\n\nSteps\n\n\n\n\nIn the Cloudbreak UI, select \nExternal Sources\n \n \nImage Catalogs\n from the navigation menu.  \n\n\nClick \nCreate Image Catalog\n.  \n\n\nEnter name for your image catalog and the URL to the location where it is stored.  \n\n\nClick \nCreate\n.\n\n\n\n\nAfter performing these steps, the image catalog will be available and automatically selected as the default entry in the image catalog drop-down list in the create cluster wizard.\n\n\nRegister image catalog in the CLI\n\n\nTo register your custom image catalog using the CLI, use the \ncb imagecatalog create\n command. Refer to \nCLI documentation\n.\n\n\nRegister image catalog in the Profile\n\n\nAs an alternative to using the UI or CLI, it is possible to place the catalog file to the Cloudbreak deployer`s etc directory and then set CB_IMAGE_CATALOG_URL variable in your Profile to IMAGE_CATALOG_FILE_NAME.JSON. \n\n\nSteps\n\n\n\n\nOn the Cloudbreak machine, switch to the root user by using \nsudo su\n  \n\n\nSave the image catalog file on your Cloudbreak machine in the /var/lib/cloudbreak-deployment/etc directory.  \n\n\nEdit the Profile file located in /var/lib/cloudbreak-deployment by adding export CB_IMAGE_CATALOG_URL to the file and set it to the name of your JSON file which declares your custom images. For example: \nexport CB_IMAGE_CATALOG_URL=custom-image-catalog.json\n    \n\n\nSave the Profile file.  \n\n\nRestart Cloudbreak by using \ncbd restart\n.  \n\n\n\n\nSelect a custom image when creating a cluster\n\n\nOnce you have registered your image catalog, you can use your custom image(s) when creating a cluster.\n\n\nSelect a custom image in Cloudbreak web UI\n\n\nPerform these steps in the advanced \nGeneral Configuration\n section of the create wizard wizard.\n\n\nSteps\n  \n\n\n\n\nIn the create cluster wizard, make sure that you are using the advanced wizard version. You need to perform the steps in the \nGeneral Configuration\n section.  \n\n\nUnder \nChoose Image Catalog\n, select your custom image catalog.  \n\n\nUnder \nBase Images\n \n \nChoose Image\n, select the provider-specific image that you would like to use. \n\n    The \"os\" that you specified in the image catalog will be displayed in the selection and the content of the \"description\" will be displayed in green.    \n\n\n\n\nYou can leave the default entries for the Ambari and HDP repositories, or you can customize to point to specific versions of Ambari and HDP/HDF that you want to use for the cluster.  \n\n\n\n\n\n\n\n\nSelect a custom image in the CLI\n\n\nTo use the custom image when creating a cluster via CLI, perform these steps.  \n\n\nSteps\n  \n\n\n\n\n\n\nObtain the image ID. For example:\n\n\ncb imagecatalog images aws --imagecatalog custom-catalog\n[\n  {\n\"Date\": \"2017-10-13\",\n\"Description\": \"Cloudbreak official base image\",\n\"Version\": \"2.5.1.0\",\n\"ImageID\": \"44b140a4-bd0b-457d-b174-e988bee3ca47\"\n  },\n  {\n\"Date\": \"2017-11-16\",\n\"Description\": \"Official Cloudbreak image\",\n\"Version\": \"2.5.1.0\",\n\"ImageID\": \"3c7598a4-ebd6-4a02-5638-882f5c7f7add\"\n  }\n]\n\n\n\n\n\n\nWhen preparing a CLI JSON template for your cluster, set the \"ImageCatalog\" parameter to the image catalog that you would like to use, and set the \"ImageId\" parameter to the uuid of the image from that catalog that you would like to use. For example:\n\n\n...\n  \"name\": \"aszegedi-cli-ci\",\n  \"network\": {\n\"subnetCIDR\": \"10.0.0.0/16\"\n  },\n  \"orchestrator\": {\n\"type\": \"SALT\"\n  },\n  \"parameters\": {\n\"instanceProfileStrategy\": \"CREATE\"\n  },\n  \"region\": \"eu-west-1\",\n  \"stackAuthentication\": {\n\"publicKeyId\": \"seq-master\"\n  },\n  \"userDefinedTags\": {\n\"owner\": \"aszegedi\"\n  },\n  \"imageCatalog\": \"custom-catalog\",\n  \"imageId\": \"3c7598a4-ebd6-4a02-5638-882f5c7f7add\"\n}\n\n\n\n\n\n\nRelated links\n\n\nCLI reference", 
            "title": "Use custom images"
        }, 
        {
            "location": "/images/index.html#using-custom-images", 
            "text": "Default images are available for each supported cloud provider and region. The following table lists the default base images available:     Cloud provider  Default image      AWS  Amazon Linux 2017    Azure  CentOS 7    GCP  CentOS 7    OpenStack  CentOS 7     Since these default images may not fit the requirements of some users (for example when user requirements include custom OS hardening, custom libraries, custom tooling, and so on) Cloudbreak allows you to use your own  custom base images .  In order to use your own custom base images you must:   Build your custom images    Prepare the custom image catalog JSON file and save it in a location accessible to the Cloudbreak VM    Register your custom image catalog with Cloudbreak      Select a custom image when creating a cluster     \n     Important \n     \nOnly  base images  can be created and registered as custom images. Do not create or register  prewarmed images  as custom images.", 
            "title": "Using custom images"
        }, 
        {
            "location": "/images/index.html#build-custom-images", 
            "text": "Refer to  Custom images for Cloudbreak  for information on how to build custom images.  This repository includes instructions and scripts to help you build custom images. Once you have the images, refer to the documentation below for information on how to create an image catalog and register it with Cloudbreak.", 
            "title": "Build custom images"
        }, 
        {
            "location": "/images/index.html#prepare-the-image-catalog", 
            "text": "Once you've built the custom images, prepare your custom image catalog JSON file. Once your image catalog JSON file is ready, save it in a location accessible via HTTP/HTTPS.", 
            "title": "Prepare the image catalog"
        }, 
        {
            "location": "/images/index.html#structure-of-the-image-catalog-json-file", 
            "text": "The image catalog JSON file includes the following two high-level sections:   images : Contains information about the created images. The burned images are stored in the  base-images  section.    versions : Contains the  cloudbreak  entry, which includes mapping between Cloudbreak versions and the image identifiers of burned images available for these Cloudbreak versions.    After adding your image(s) to the  images  section, make sure to also update the  versions  section.    Images section     The burned images are stored in the  base-images  sub-section of  images . The  base-images  section stores one or more image \"records\". Every image \"record\" must contain the date, description, images, os, os_type, and uuid fields.     Parameter  Description      date  Date for your image catalog entry.    description  Description for your image catalog entry.    images  The image sets by cloud provider. An image set must store the virtual machine image IDs by the related region of the provider (AWS, Azure) or contain one default image for all regions (GCP, OpenStack). The virtual machine image IDs come from the result of the image burning process and must be an existing identifier of a virtual machine image on the related provider side. For the providers which use global rather than per-region images, the region should be replaced with  default .    os  The operating system used in the image.    os_type  The type of operating system which will be used to determine the default Ambari and HDP/HDF repositories to use. Set  os_type  to \"redhat6\" for amazonlinux or centos6 images. Set  os_type  to \"redhat7\" for centos7 or rhel7 images.    uuid  The  uuid  field must be a unique identifier within the file. You can generate it or select it manually. The utility  uuidgen  available from your command line is a convenient way to generate a unique ID.     Versions section     The  versions  section includes a single \"cloudbreak\" entry, which maps the uuids to a specific Cloudbreak version:     Parameter  Description      images  Image  uuid , same as the one that you specified in the  base-images  section.    versions  The Cloudbreak version(s) for which you would like to use the images.", 
            "title": "Structure of the image catalog JSON file"
        }, 
        {
            "location": "/images/index.html#example-image-catalog-json-file", 
            "text": "Here is an example image catalog JSON file that includes two sets of custom base images:   A custom base image for AWS:  That is using Amazon Linux operating system  That will use the Redhat 6 repos as default Ambari and HDP repositories during cluster create       Has a unique ID of \"44b140a4-bd0b-457d-b174-e988bee3ca47\"  Is available for Cloudbreak 2.4.0        A custom base image for Azure, Google, and OpenStack:  That is using CentOS 7 operating system  That will use the Redhat 7 repos as default Ambari and HDP repositories during cluster create     Has a unique ID of \"f6e778fc-7f17-4535-9021-515351df3692\"  Is available to Cloudbreak 2.4.0           You can also download it from  here .  \n{\n  \"images\": {\n    \"base-images\": [\n      {\n        \"date\": \"2017-10-13\",\n        \"description\": \"Cloudbreak official base image\",\n        \"images\": {\n          \"aws\": {\n            \"ap-northeast-1\": \"ami-78e9311e\",\n            \"ap-northeast-2\": \"ami-84b613ea\",\n            \"ap-southeast-1\": \"ami-75226716\",\n            \"ap-southeast-2\": \"ami-92ce23f0\",\n            \"eu-central-1\": \"ami-d95be5b6\",\n            \"eu-west-1\": \"ami-46429e3f\",\n            \"sa-east-1\": \"ami-86d5abea\",\n            \"us-east-1\": \"ami-51a2742b\",\n            \"us-west-1\": \"ami-21ccfe41\",\n            \"us-west-2\": \"ami-2a1cdc52\"\n          }\n        },\n        \"os\": \"amazonlinux\",\n        \"os_type\": \"redhat6\",\n        \"uuid\": \"44b140a4-bd0b-457d-b174-e988bee3ca47\"\n      },\n      {\n        \"date\": \"2017-10-13\",\n        \"description\": \"Cloudbreak official base image\",\n        \"images\": {\n          \"azure\": {\n            \"Australia East\": \"https://hwxaustraliaeast.blob.core.windows.net/images/hdc-hdp--1710161226.vhd\",\n            \"Australia South East\": \"https://hwxaustralisoutheast.blob.core.windows.net/images/hdc-hdp--1710161226.vhd\",\n            \"Brazil South\": \"https://sequenceiqbrazilsouth2.blob.core.windows.net/images/hdc-hdp--1710161226.vhd\",\n            \"Canada Central\": \"https://sequenceiqcanadacentral.blob.core.windows.net/images/hdc-hdp--1710161226.vhd\",\n            \"Canada East\": \"https://sequenceiqcanadaeast.blob.core.windows.net/images/hdc-hdp--1710161226.vhd\",\n            \"Central India\": \"https://hwxcentralindia.blob.core.windows.net/images/hdc-hdp--1710161226.vhd\",\n            \"Central US\": \"https://sequenceiqcentralus2.blob.core.windows.net/images/hdc-hdp--1710161226.vhd\",\n            \"East Asia\": \"https://sequenceiqeastasia2.blob.core.windows.net/images/hdc-hdp--1710161226.vhd\",\n            \"East US\": \"https://sequenceiqeastus12.blob.core.windows.net/images/hdc-hdp--1710161226.vhd\",\n            \"East US 2\": \"https://sequenceiqeastus22.blob.core.windows.net/images/hdc-hdp--1710161226.vhd\",\n            \"Japan East\": \"https://sequenceiqjapaneast2.blob.core.windows.net/images/hdc-hdp--1710161226.vhd\",\n            \"Japan West\": \"https://sequenceiqjapanwest2.blob.core.windows.net/images/hdc-hdp--1710161226.vhd\",\n            \"Korea Central\": \"https://hwxkoreacentral.blob.core.windows.net/images/hdc-hdp--1710161226.vhd\",\n            \"Korea South\": \"https://hwxkoreasouth.blob.core.windows.net/images/hdc-hdp--1710161226.vhd\",\n            \"North Central US\": \"https://sequenceiqorthcentralus2.blob.core.windows.net/images/hdc-hdp--1710161226.vhd\",\n            \"North Europe\": \"https://sequenceiqnortheurope2.blob.core.windows.net/images/hdc-hdp--1710161226.vhd\",\n            \"South Central US\": \"https://sequenceiqouthcentralus2.blob.core.windows.net/images/hdc-hdp--1710161226.vhd\",\n            \"South India\": \"https://hwxsouthindia.blob.core.windows.net/images/hdc-hdp--1710161226.vhd\",\n            \"Southeast Asia\": \"https://sequenceiqsoutheastasia2.blob.core.windows.net/images/hdc-hdp--1710161226.vhd\",\n            \"UK South\": \"https://hwxsouthuk.blob.core.windows.net/images/hdc-hdp--1710161226.vhd\",\n            \"UK West\": \"https://hwxwestuk.blob.core.windows.net/images/hdc-hdp--1710161226.vhd\",\n            \"West Central US\": \"https://hwxwestcentralus.blob.core.windows.net/images/hdc-hdp--1710161226.vhd\",\n            \"West Europe\": \"https://sequenceiqwesteurope2.blob.core.windows.net/images/hdc-hdp--1710161226.vhd\",\n            \"West India\": \"https://hwxwestindia.blob.core.windows.net/images/hdc-hdp--1710161226.vhd\",\n            \"West US\": \"https://sequenceiqwestus2.blob.core.windows.net/images/hdc-hdp--1710161226.vhd\",\n            \"West US 2\": \"https://hwxwestus2.blob.core.windows.net/images/hdc-hdp--1710161226.vhd\"\n          },\n          \"gcp\": {\n            \"default\": \"sequenceiqimage/hdc-hdp--1710161226.tar.gz\"\n          },\n          \"openstack\": {\n            \"default\": \"hdc-hdp--1710161226\"\n          }\n        },\n        \"os\": \"centos7\",\n        \"os_type\": \"redhat7\",\n        \"uuid\": \"f6e778fc-7f17-4535-9021-515351df3691\"\n      }\n    ]\n},\n  \"versions\": {\n    \"cloudbreak\": [\n      {\n        \"images\": [\n          \"44b140a4-bd0b-457d-b174-e988bee3ca47\",\n          \"f6e778fc-7f17-4535-9021-515351df3692\"\n        ],\n        \"versions\": [\n          \"2.4.0\"\n        ]\n      }\n    ]\n  }\n}", 
            "title": "Example image catalog JSON file"
        }, 
        {
            "location": "/images/index.html#register-image-catalog", 
            "text": "Now that you have created your image catalog JSON file, register it with your Cloudbreak instance. You can do this via Cloudbreak UI, CLI, or be editing the Profile file.   \n   Important \n   \n  The content type of your image catalog file should be  \"application/json\"  for Cloudbreak to be able to process it.", 
            "title": "Register image catalog"
        }, 
        {
            "location": "/images/index.html#register-image-catalog-in-the-ui", 
            "text": "Use these steps to register your custom image catalog in the Cloudbreak UI.  Steps   In the Cloudbreak UI, select  External Sources     Image Catalogs  from the navigation menu.    Click  Create Image Catalog .    Enter name for your image catalog and the URL to the location where it is stored.    Click  Create .   After performing these steps, the image catalog will be available and automatically selected as the default entry in the image catalog drop-down list in the create cluster wizard.", 
            "title": "Register image catalog in the UI"
        }, 
        {
            "location": "/images/index.html#register-image-catalog-in-the-cli", 
            "text": "To register your custom image catalog using the CLI, use the  cb imagecatalog create  command. Refer to  CLI documentation .", 
            "title": "Register image catalog in the CLI"
        }, 
        {
            "location": "/images/index.html#register-image-catalog-in-the-profile", 
            "text": "As an alternative to using the UI or CLI, it is possible to place the catalog file to the Cloudbreak deployer`s etc directory and then set CB_IMAGE_CATALOG_URL variable in your Profile to IMAGE_CATALOG_FILE_NAME.JSON.   Steps   On the Cloudbreak machine, switch to the root user by using  sudo su     Save the image catalog file on your Cloudbreak machine in the /var/lib/cloudbreak-deployment/etc directory.    Edit the Profile file located in /var/lib/cloudbreak-deployment by adding export CB_IMAGE_CATALOG_URL to the file and set it to the name of your JSON file which declares your custom images. For example:  export CB_IMAGE_CATALOG_URL=custom-image-catalog.json       Save the Profile file.    Restart Cloudbreak by using  cbd restart .", 
            "title": "Register image catalog in the Profile"
        }, 
        {
            "location": "/images/index.html#select-a-custom-image-when-creating-a-cluster", 
            "text": "Once you have registered your image catalog, you can use your custom image(s) when creating a cluster.", 
            "title": "Select a custom image when creating a cluster"
        }, 
        {
            "location": "/images/index.html#select-a-custom-image-in-cloudbreak-web-ui", 
            "text": "Perform these steps in the advanced  General Configuration  section of the create wizard wizard.  Steps      In the create cluster wizard, make sure that you are using the advanced wizard version. You need to perform the steps in the  General Configuration  section.    Under  Choose Image Catalog , select your custom image catalog.    Under  Base Images     Choose Image , select the provider-specific image that you would like to use.  \n    The \"os\" that you specified in the image catalog will be displayed in the selection and the content of the \"description\" will be displayed in green.       You can leave the default entries for the Ambari and HDP repositories, or you can customize to point to specific versions of Ambari and HDP/HDF that you want to use for the cluster.", 
            "title": "Select a custom image in Cloudbreak web UI"
        }, 
        {
            "location": "/images/index.html#select-a-custom-image-in-the-cli", 
            "text": "To use the custom image when creating a cluster via CLI, perform these steps.    Steps       Obtain the image ID. For example:  cb imagecatalog images aws --imagecatalog custom-catalog\n[\n  {\n\"Date\": \"2017-10-13\",\n\"Description\": \"Cloudbreak official base image\",\n\"Version\": \"2.5.1.0\",\n\"ImageID\": \"44b140a4-bd0b-457d-b174-e988bee3ca47\"\n  },\n  {\n\"Date\": \"2017-11-16\",\n\"Description\": \"Official Cloudbreak image\",\n\"Version\": \"2.5.1.0\",\n\"ImageID\": \"3c7598a4-ebd6-4a02-5638-882f5c7f7add\"\n  }\n]    When preparing a CLI JSON template for your cluster, set the \"ImageCatalog\" parameter to the image catalog that you would like to use, and set the \"ImageId\" parameter to the uuid of the image from that catalog that you would like to use. For example:  ...\n  \"name\": \"aszegedi-cli-ci\",\n  \"network\": {\n\"subnetCIDR\": \"10.0.0.0/16\"\n  },\n  \"orchestrator\": {\n\"type\": \"SALT\"\n  },\n  \"parameters\": {\n\"instanceProfileStrategy\": \"CREATE\"\n  },\n  \"region\": \"eu-west-1\",\n  \"stackAuthentication\": {\n\"publicKeyId\": \"seq-master\"\n  },\n  \"userDefinedTags\": {\n\"owner\": \"aszegedi\"\n  },\n  \"imageCatalog\": \"custom-catalog\",\n  \"imageId\": \"3c7598a4-ebd6-4a02-5638-882f5c7f7add\"\n}    Related links  CLI reference", 
            "title": "Select a custom image in the CLI"
        }, 
        {
            "location": "/tags/index.html", 
            "text": "Tagging resources\n\n\nWhen you manually create resources in the cloud, you have an option to add custom tags that help you track these resources. Likewise, when creating clusters, you can instruct Cloudbreak to tag the cloud resources that it creates on your behalf. The tags added during cluster creation are displayed in your cloud account on the resources that Cloudbreak provisioned for your clusters. \n\n\nYou can use tags to categorize your cloud resources by purpose, owner, and so on. Tags come in especially handy when you are using a corporate AWS account and you want to quickly identify which resources belong to your cluster(s). In fact, your corporate cloud account admin may require you to tag all the resources that you create, in particular resources, such as VMs, which incur charges.\n\n\nAdd tags when creating a cluster\n\n\nYou can tag the cloud resources used for a cluster by providing custom tag names and values when creating a cluster via UI or CLI. In the Cloudbreak UI, this option is available in the create cluster wizard, in the advanced \nGeneral Configuration\n page \n \nTags\n section. \n\n\nIt is not possible to add tags via Cloudbreak after your cluster has been created.  \n\n\nTo learn more about tags and their restrictions, refer to the cloud provider documentation. \n\n\nRelated links\n\n\nTags on AWS\n  \n\n\nTags on Azure\n\n\nLabels on GCP\n\n\nTags on OpenStack", 
            "title": "Tag resources"
        }, 
        {
            "location": "/tags/index.html#tagging-resources", 
            "text": "When you manually create resources in the cloud, you have an option to add custom tags that help you track these resources. Likewise, when creating clusters, you can instruct Cloudbreak to tag the cloud resources that it creates on your behalf. The tags added during cluster creation are displayed in your cloud account on the resources that Cloudbreak provisioned for your clusters.   You can use tags to categorize your cloud resources by purpose, owner, and so on. Tags come in especially handy when you are using a corporate AWS account and you want to quickly identify which resources belong to your cluster(s). In fact, your corporate cloud account admin may require you to tag all the resources that you create, in particular resources, such as VMs, which incur charges.", 
            "title": "Tagging resources"
        }, 
        {
            "location": "/tags/index.html#add-tags-when-creating-a-cluster", 
            "text": "You can tag the cloud resources used for a cluster by providing custom tag names and values when creating a cluster via UI or CLI. In the Cloudbreak UI, this option is available in the create cluster wizard, in the advanced  General Configuration  page    Tags  section.   It is not possible to add tags via Cloudbreak after your cluster has been created.    To learn more about tags and their restrictions, refer to the cloud provider documentation.   Related links  Tags on AWS     Tags on Azure  Labels on GCP  Tags on OpenStack", 
            "title": "Add tags when creating a cluster"
        }, 
        {
            "location": "/security-kerberos/index.html", 
            "text": "Enabling Kerberos security\n\n\nWhen creating a cluster, you can optionally enable Kerberos security in that cluster and provide your Kerberos configuration details. Cloudbreak will automatically extend your blueprint configuration with the defined properties.\n\n\nKerberos overview\n\n\nKerberos is a third party authentication mechanism, in which users and services that users wish to access Hadoop rely on a third party - the Kerberos server - to authenticate each to the other. The Kerberos server itself is known as the \nKey Distribution Center\n, or \nKDC\n. At a high level, the KDC has three parts:\n\n\n\n\nA database of the users and services (known as \nprincipals\n) and their respective Kerberos passwords  \n\n\nAn \nAuthentication Server (AS)\n which performs the initial authentication and issues a Ticket Granting Ticket (TGT)  \n\n\nA \nTicket Granting Server (TGS)\n that issues subsequent service tickets based on the initial TGT  \n\n\n\n\nA user principal requests authentication from the AS. The AS returns a TGT that is encrypted using the user principal's Kerberos password, which is known only to the user principal and the AS. The user principal decrypts the TGT locally using its Kerberos password, and from that point forward, until the ticket expires, the user principal can use the TGT to get service tickets from the TGS. Service tickets are what allow the principal to access various services. \n\n\nSince cluster resources (hosts or services) cannot provide a password each time to decrypt the TGT, they use a special file, called a \nkeytab\n, which contains the resource principal authentication credentials. The set of hosts, users, and services over which the Kerberos server has control is called a \nrealm\n.\n\n\nThe following table explains the Kerberos related terminology:\n\n\n\n\n\n\n\n\nTerm\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nKey Distribution Center, or KDC\n\n\nThe trusted source for authentication in a Kerberos-enabled environment.\n\n\n\n\n\n\nKerberos KDC Server\n\n\nThe machine, or server, that serves as the Key Distribution Center (KDC).\n\n\n\n\n\n\nKerberos Client\n\n\nAny machine in the cluster that authenticates against the KDC.\n\n\n\n\n\n\nPrincipal\n\n\nThe unique name of a user or service that authenticates against the KDC.\n\n\n\n\n\n\nKeytab\n\n\nA file that includes one or more principals and their keys.\n\n\n\n\n\n\nRealm\n\n\nThe Kerberos network that includes a KDC and a number of clients.\n\n\n\n\n\n\n\n\nEnabling Kerberos\n\n\nThe option to enable Kerberos is available in the advanced \nSecurity\n section of the create cluster wizard.  \n\n\nYou have the following options for enabling Kerberos in a Cloudbreak  managed cluster:\n\n\n\n\n\n\n\n\nOption\n\n\nDescription\n\n\nEnvironment\n\n\n\n\n\n\n\n\n\n\nUse existing KDC\n\n\nAllows you to leverage an existing MIT KDC or Active Directory for enabling Kerberos with the cluster.\nYou can either provide the required parameters and Cloudbreak will generate the descriptors on your behalf, or provide the exact Ambari Kerberos descriptors to be injected into your blueprint in JSON format.\n\n\nSuitable for production\n\n\n\n\n\n\nUse test KDC\n\n\nInstalls a new MIT KDC on the master node and configures the cluster to leverage that KDC.\n\n\nSuitable for evaluation and testing only, not suitable for production\n\n\n\n\n\n\n\n\nUsing existing KDC\n\n\nTo use an existing KDC, in the advanced \nSecurity\n section of the create cluster wizard select \nEnable Kerberos Security\n. By default, \nUse Existing KDC\n option is selected.  \n\n\nYou must provide the following information about your MIT KDC or Active Directory. Based on these parameters, kerberos-env and krb5-conf JSON descriptors for Ambari are generated and injected into your Blueprint:\n\n\n\n\nBefore proceeding with the configuration, you must confirm that you met the requirements by checking the boxes next to all requirements listed. The configuration options are displayed only after you have confirmed all the requirements by checking every box.    \n\n\n\n\n\n\n\n\n\n\nParameter\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nKerberos Admin Principal\n\n\nThe admin principal in your existing MIT KDC or AD.\n\n\n\n\n\n\nKerberos Admin Password\n\n\nThe admin principal password in your existing MIT KDC or AD.\n\n\n\n\n\n\nMIT KDC or Active Directory\n\n\nSelect MIT KDC or Active Directory.\n\n\n\n\n\n\n\n\nUse basic configuration\n\n\n\n\n\n\n\n\nParameter\n\n\nRequired if using...\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nKerberos Url\n\n\nMIT, AD\n\n\nIP address or FQDN for the KDC host. Optionally a port number may be included. Example: \"kdc.example1.com:88\" or \"kdc.example1.com\"\n\n\n\n\n\n\nKerberos Admin URL\n\n\nMIT, AD\n\n\n(Optional) IP address or FQDN for the KDC admin host. Optionally a port number may be included. Example: \"kdc.example2.com:88\" or \"kdc.example2.com\"\n\n\n\n\n\n\nKerberos Realm\n\n\nMIT, AD\n\n\nThe default realm to use when creating service principals. Example: \"EXAMPLE.COM\"\n\n\n\n\n\n\nKerberos AD Ldap Url\n\n\nAD\n\n\nThe URL to the Active Directory LDAP Interface. This value must indicate a secure channel using LDAPS since it is required for creating and updating passwords for Active Directory accounts. Example: \"ldaps://ad.example.com:636\"\n\n\n\n\n\n\nKerberos AD Container DN\n\n\nAD\n\n\nThe distinguished name (DN) of the container used store service principals. Example:  \"OU=hadoop,DC=example,DC=com\"\n\n\n\n\n\n\nUse TCP Connection\n\n\nOptional\n\n\nBy default, Kerberos uses UDP. Checkmark this box to use TCP instead.\n\n\n\n\n\n\n\n\nUse advanced configuration\n \n\n\nChecking the \nUse Custom Configuration\n option allows you to provide the actual Ambari Kerberos descriptors to be injected into your blueprint (instead of Cloudbreak generating the descriptors on your behalf). This is the most powerful option which gives you full control of the Ambari Kerberos options that are available. You must provide: \n\n\n\n\nKerberos-env JSON Descriptor (required)\n\n\nkrb5-conf JSON Descriptor (optional)\n\n\n\n\nTo learn more about the Ambari Kerberos JSON descriptors, refer to \nApache cwiki\n.  \n\n\nUsing test KDC\n\n\nTo use a test KDC, in the advanced \nSecurity\n section of the create cluster wizard select \nEnable Kerberos Security\n and then select \nUse Test KDC\n.\n\n\n\n\nImportant\n\n\n\nUsing the Test KDC is for evaluation and testing purposes only, and cannot be used for production clusters. To enable Kerberos for production use, you must use the \nUse Existing KDC\n option. \n\n\n\n\n\nYou must provide the following parameters for your new test KDC:  \n\n\n\n\n\n\n\n\nParameter\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nKerberos Master Key\n\n\nThe master key for the KDC database.\n\n\n\n\n\n\nKerberos Admin Username\n\n\nThe admin principal to create that can administer the KDC.\n\n\n\n\n\n\nKerberos Admin Password\n\n\nThe admin principal password.\n\n\n\n\n\n\nConfirm Kerberos Admin Password\n\n\nThe admin principal password.\n\n\n\n\n\n\n\n\nWhen using the test KDC option:\n\n\n\n\nCloudbreak installs an MIT KDC instance on the Ambari server node.  \n\n\nKerberos clients are installed on all cluster nodes, and the krb5.conf is configured to use the MIT KDC.  \n\n\nThe cluster is configured for Kerberos to use the MIT KDC. Very basic Ambari KSON Kerberos descriptors are generated and used accordingly.\n\n\n\n\nExample kerberos-env JSON descriptor file:\n\n\n{\n      \"kerberos-env\" : {\n        \"properties\" : {\n          \"kdc_type\" : \"mit-kdc\",\n          \"kdc_hosts\" : \"ip-10-0-121-81.ec2.internal\",\n          \"realm\" : \"EC2.INTERNAL\",\n          \"encryption_types\" : \"aes des3-cbc-sha1 rc4 des-cbc-md5\",\n          \"ldap_url\" : \"\",\n          \"admin_server_host\" : \"ip-10-0-121-81.ec2.internal\",\n          \"container_dn\" : \"\"\n        }\n      }\n    }\n\n\n\nExample krb5-conf JSON  descriptor file: \n\n\n {\n      \"krb5-conf\" : {\n        \"properties\" : {\n          \"domains\" : \".ec2.internal\",\n          \"manage_krb5_conf\" : \"true\"\n        }\n      }\n    }\n\n\n\nRelated links\n \n\n\nApache cwiki", 
            "title": "Enable Kerberos security"
        }, 
        {
            "location": "/security-kerberos/index.html#enabling-kerberos-security", 
            "text": "When creating a cluster, you can optionally enable Kerberos security in that cluster and provide your Kerberos configuration details. Cloudbreak will automatically extend your blueprint configuration with the defined properties.", 
            "title": "Enabling Kerberos security"
        }, 
        {
            "location": "/security-kerberos/index.html#kerberos-overview", 
            "text": "Kerberos is a third party authentication mechanism, in which users and services that users wish to access Hadoop rely on a third party - the Kerberos server - to authenticate each to the other. The Kerberos server itself is known as the  Key Distribution Center , or  KDC . At a high level, the KDC has three parts:   A database of the users and services (known as  principals ) and their respective Kerberos passwords    An  Authentication Server (AS)  which performs the initial authentication and issues a Ticket Granting Ticket (TGT)    A  Ticket Granting Server (TGS)  that issues subsequent service tickets based on the initial TGT     A user principal requests authentication from the AS. The AS returns a TGT that is encrypted using the user principal's Kerberos password, which is known only to the user principal and the AS. The user principal decrypts the TGT locally using its Kerberos password, and from that point forward, until the ticket expires, the user principal can use the TGT to get service tickets from the TGS. Service tickets are what allow the principal to access various services.   Since cluster resources (hosts or services) cannot provide a password each time to decrypt the TGT, they use a special file, called a  keytab , which contains the resource principal authentication credentials. The set of hosts, users, and services over which the Kerberos server has control is called a  realm .  The following table explains the Kerberos related terminology:     Term  Description      Key Distribution Center, or KDC  The trusted source for authentication in a Kerberos-enabled environment.    Kerberos KDC Server  The machine, or server, that serves as the Key Distribution Center (KDC).    Kerberos Client  Any machine in the cluster that authenticates against the KDC.    Principal  The unique name of a user or service that authenticates against the KDC.    Keytab  A file that includes one or more principals and their keys.    Realm  The Kerberos network that includes a KDC and a number of clients.", 
            "title": "Kerberos overview"
        }, 
        {
            "location": "/security-kerberos/index.html#enabling-kerberos", 
            "text": "The option to enable Kerberos is available in the advanced  Security  section of the create cluster wizard.    You have the following options for enabling Kerberos in a Cloudbreak  managed cluster:     Option  Description  Environment      Use existing KDC  Allows you to leverage an existing MIT KDC or Active Directory for enabling Kerberos with the cluster. You can either provide the required parameters and Cloudbreak will generate the descriptors on your behalf, or provide the exact Ambari Kerberos descriptors to be injected into your blueprint in JSON format.  Suitable for production    Use test KDC  Installs a new MIT KDC on the master node and configures the cluster to leverage that KDC.  Suitable for evaluation and testing only, not suitable for production", 
            "title": "Enabling Kerberos"
        }, 
        {
            "location": "/security-kerberos/index.html#using-existing-kdc", 
            "text": "To use an existing KDC, in the advanced  Security  section of the create cluster wizard select  Enable Kerberos Security . By default,  Use Existing KDC  option is selected.    You must provide the following information about your MIT KDC or Active Directory. Based on these parameters, kerberos-env and krb5-conf JSON descriptors for Ambari are generated and injected into your Blueprint:   Before proceeding with the configuration, you must confirm that you met the requirements by checking the boxes next to all requirements listed. The configuration options are displayed only after you have confirmed all the requirements by checking every box.          Parameter  Description      Kerberos Admin Principal  The admin principal in your existing MIT KDC or AD.    Kerberos Admin Password  The admin principal password in your existing MIT KDC or AD.    MIT KDC or Active Directory  Select MIT KDC or Active Directory.     Use basic configuration     Parameter  Required if using...  Description      Kerberos Url  MIT, AD  IP address or FQDN for the KDC host. Optionally a port number may be included. Example: \"kdc.example1.com:88\" or \"kdc.example1.com\"    Kerberos Admin URL  MIT, AD  (Optional) IP address or FQDN for the KDC admin host. Optionally a port number may be included. Example: \"kdc.example2.com:88\" or \"kdc.example2.com\"    Kerberos Realm  MIT, AD  The default realm to use when creating service principals. Example: \"EXAMPLE.COM\"    Kerberos AD Ldap Url  AD  The URL to the Active Directory LDAP Interface. This value must indicate a secure channel using LDAPS since it is required for creating and updating passwords for Active Directory accounts. Example: \"ldaps://ad.example.com:636\"    Kerberos AD Container DN  AD  The distinguished name (DN) of the container used store service principals. Example:  \"OU=hadoop,DC=example,DC=com\"    Use TCP Connection  Optional  By default, Kerberos uses UDP. Checkmark this box to use TCP instead.     Use advanced configuration    Checking the  Use Custom Configuration  option allows you to provide the actual Ambari Kerberos descriptors to be injected into your blueprint (instead of Cloudbreak generating the descriptors on your behalf). This is the most powerful option which gives you full control of the Ambari Kerberos options that are available. You must provide:    Kerberos-env JSON Descriptor (required)  krb5-conf JSON Descriptor (optional)   To learn more about the Ambari Kerberos JSON descriptors, refer to  Apache cwiki .", 
            "title": "Using existing KDC"
        }, 
        {
            "location": "/security-kerberos/index.html#using-test-kdc", 
            "text": "To use a test KDC, in the advanced  Security  section of the create cluster wizard select  Enable Kerberos Security  and then select  Use Test KDC .   Important  \nUsing the Test KDC is for evaluation and testing purposes only, and cannot be used for production clusters. To enable Kerberos for production use, you must use the  Use Existing KDC  option.    You must provide the following parameters for your new test KDC:       Parameter  Description      Kerberos Master Key  The master key for the KDC database.    Kerberos Admin Username  The admin principal to create that can administer the KDC.    Kerberos Admin Password  The admin principal password.    Confirm Kerberos Admin Password  The admin principal password.     When using the test KDC option:   Cloudbreak installs an MIT KDC instance on the Ambari server node.    Kerberos clients are installed on all cluster nodes, and the krb5.conf is configured to use the MIT KDC.    The cluster is configured for Kerberos to use the MIT KDC. Very basic Ambari KSON Kerberos descriptors are generated and used accordingly.   Example kerberos-env JSON descriptor file:  {\n      \"kerberos-env\" : {\n        \"properties\" : {\n          \"kdc_type\" : \"mit-kdc\",\n          \"kdc_hosts\" : \"ip-10-0-121-81.ec2.internal\",\n          \"realm\" : \"EC2.INTERNAL\",\n          \"encryption_types\" : \"aes des3-cbc-sha1 rc4 des-cbc-md5\",\n          \"ldap_url\" : \"\",\n          \"admin_server_host\" : \"ip-10-0-121-81.ec2.internal\",\n          \"container_dn\" : \"\"\n        }\n      }\n    }  Example krb5-conf JSON  descriptor file:    {\n      \"krb5-conf\" : {\n        \"properties\" : {\n          \"domains\" : \".ec2.internal\",\n          \"manage_krb5_conf\" : \"true\"\n        }\n      }\n    }  Related links    Apache cwiki", 
            "title": "Using test KDC"
        }, 
        {
            "location": "/autoscaling/index.html", 
            "text": "Configuring autoscaling\n\n\nAutoscaling allows you to adjust cluster capacity based on Ambari metrics and alerts, as well as schedule time-based capacity adjustment. When creating an autoscaling policy, you define:\n\n\n\n\nAn \nalert\n that triggers a scaling policy. An alert can be based on an Ambari metric or can be time-based.     \n\n\nA \nscaling policy\n that adds or removes a set number of nodes to a selected host group when the conditions defined in the attached alert are met.    \n\n\n\n\nMetric-based autoscaling\n\n\nCloudbreak accesses all available Ambari metrics and allows you to define alerts based on these metrics. For example:\n\n\n\n\n\n\n\n\nAlert Definition\n\n\nPolicy Definition\n\n\n\n\n\n\n\n\n\n\nResourceManager CPU\n alert with \nCRITICAL\n status for 5 minutes\n\n\nAdd 10 worker nodes\n\n\n\n\n\n\nHDFS Capacity Utilization\n alert with \nWARN\n status for 20 minutes\n\n\nSet the number of worker nodes to 50\n\n\n\n\n\n\nAmbari Server Alerts\n alert with \nCRITICAL\n status for 15 minutes\n\n\nDecrease the number of worker nodes by 80%\n\n\n\n\n\n\n\n\nTime-based autoscaling\n\n\nTime-based alerts can be defined by providing a cron expression. For example: \n\n\n\n\n\n\n\n\nAlert Definition\n\n\nPolicy Definition\n\n\n\n\n\n\n\n\n\n\nEvery day at 07:00 AM (GMT-8)\n\n\nAdd 90 worker nodes\n\n\n\n\n\n\nEvery day at 08:00 PM (GMT-8)\n\n\nRemove 90 worker nodes\n\n\n\n\n\n\n\n\n\n\nCluster resizing is not supported for HDF clusters. \n\n\n\n\nEnable autoscaling\n\n\nFor each newly created cluster, autoscaling is disabled by default but it can be enabled once the cluster is in a running state. \n\n\n\n\nAutoscaling configuration is only available in the UI. It is currently not available in the CLI. \n\n\n\n\nSteps\n\n\n\n\nOn the cluster details page, navigate to the \nAutoscaling\n tab.   \n\n\n\n\nClick the toggle button to enable autoscaling:\n\n\n  \n\n\n\n\n\n\nThe toggle button turns green and you can see that \"Autoscaling is enabled\".   \n\n\n\n\nDefine alerts\n and then \ndefine scaling policies\n. You can also \nadjust the autoscaling settings\n. \n\n\n\n\nIf you decide to disable autoscaling, your previously defined alerts and policies will be preserved. \n\n\nDefining an alert\n\n\nAfter you have enabled autoscaling, define a metric-based or time-based alert.  \n\n\nDefine a metric-based alert\n\n\nAfter \nenabling autoscaling\n, perform the following steps to create a metric-based alert.  \n\n\n\n\nIf you would like to change default thresholds for an Ambari metric, refer to \nModifying alerts\n in Ambari documentation. \n\n\nIf you would like to create a custom Ambari alert, refer to \nHow to create a custom Ambari alert and use it for Cloudbreak autoscaling policies\n.\n\n\n\n\nSteps\n\n\n\n\nIn the \nAlert Configuration\n section, select \nMetric Based\n alert type.      \n\n\n\n\nProvide the following information:\n\n\n\n\n\n\n\n\nParameter\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nEnter alert name\n\n\nEnter a unique name for the alert.\n\n\n\n\n\n\nChoose metric type\n\n\nSelect the Ambari metric that should trigger the alert.\n\n\n\n\n\n\nAlert status\n\n\nSelect the alert status that should trigger an alert for the selected metric. One of: OK, CRITICAL, WARNING.\n\n\n\n\n\n\nAlert duration\n\n\nSelect the alert duration that should trigger an alert.\n\n\n\n\n\n\n\n\n\n\n\n\nClick \n+\n to save the alert.  \n\n\n\n\n\n\nOnce you have defined an alert, \ncreate a scaling policy\n that this metric should trigger.\n\n\nRelated links:\n\n\nHow to create a custom Ambari alert and use it for Cloudbreak autoscaling policies\n (HCC) \n\n\nModifying alerts\n (Hortonworks)   \n\n\nDefine a time-based alert\n\n\nAfter \nenabling autoscaling\n, perform the following steps to create a time-based alert.\n\n\nSteps\n\n\n\n\nIn the \nAlert Configuration\n section, select the \nTime Based\n alert type. \n\n\n\n\nProvide the following information: \n\n\n\n\n\n\n\n\nParameter\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nEnter alert name.\n\n\nEnter a unique name for the alert.\n\n\n\n\n\n\nSelect timezone.\n\n\nSelect your timezone.\n\n\n\n\n\n\nEnter cron expression\n\n\nEnter a cron expression that defines the frequency of the alert. Refer to \nCron expression generator\n.\n\n\n\n\n\n\n\n\n\n\n\n\nClick \n+\n to save the alert.   \n\n\n\n\n\n\nOnce you have defined an alert, \ncreate a scaling policy\n that this metric should trigger.\n\n\nCreate a scaling policy\n\n\nAfter \nenabling autoscaling\n and \ncreating at least one alert\n, perform the following steps to create a scaling policy.\n\n\nSteps\n\n\n\n\n\n\nIn the \nPolicy Configuration\n section, provide the following information:\n\n\n\n\n\n\n\n\nParameter\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nEnter policy name\n\n\nEnter a unique name for the policy.\n\n\n\n\n\n\nSelect action\n\n\nSelect one of the following actions: Add (to add nodes to a host group) Remove (to delete nodes from a host group), or Set (to set the number of nodes in a host group to the chosen number).\n\n\n\n\n\n\nEnter number or percentage\n\n\nEnter a number defining how many or what percentage of nodes to add or remove. If the action selected is \"set\", this defines the number of nodes that a host group will be set to after scaling.\n\n\n\n\n\n\nSelect nodes of percent\n\n\nSelect \"nodes\" or \"percent\", depending on whether you want to scale to a specific number, or percent of current number of nodes.\n\n\n\n\n\n\nSelect host group\n\n\nSelect the host group to which to apply the scaling.\n\n\n\n\n\n\nChoose an alert\n\n\nSelect the alert based on which the scaling should be applied.\n\n\n\n\n\n\n\n\n\n\n\n\nClick \n+\n to save the alert.   \n\n\n\n\n\n\nConfigure autoscaling settings\n\n\nAfter \nenabling autoscaling\n, perform these steps to configure the auto scaling settings for your cluster.   \n\n\nSteps\n\n\n\n\n\n\nIn the \nCluster Scaling Configuration\n, provide the following information: \n\n\n\n\n\n\n\n\nSetting\n\n\nDescription\n\n\nDefault Value\n\n\n\n\n\n\n\n\n\n\nCooldown time\n\n\nAfter an auto scaling event occurs, the amount of time to wait before enforcing another scaling policy. This means that the scaling events scheduled during cooldown time are dropped.\n\n\n30 minutes\n\n\n\n\n\n\nMinimum Cluster Size\n\n\nThe minimum size allowed for the cluster. Auto scaling policies cannot scale the cluster below or above this size.\n\n\n2 nodes\n\n\n\n\n\n\nMaximum Cluster Size\n\n\nThe maximum size allowed for the cluster. Auto scaling policies cannot scale the cluster below or above this size.\n\n\n100 nodes\n\n\n\n\n\n\n\n\n\n\n\n\nClick \nSave\n to save the changes.", 
            "title": "Configure autoscaling"
        }, 
        {
            "location": "/autoscaling/index.html#configuring-autoscaling", 
            "text": "Autoscaling allows you to adjust cluster capacity based on Ambari metrics and alerts, as well as schedule time-based capacity adjustment. When creating an autoscaling policy, you define:   An  alert  that triggers a scaling policy. An alert can be based on an Ambari metric or can be time-based.       A  scaling policy  that adds or removes a set number of nodes to a selected host group when the conditions defined in the attached alert are met.       Metric-based autoscaling  Cloudbreak accesses all available Ambari metrics and allows you to define alerts based on these metrics. For example:     Alert Definition  Policy Definition      ResourceManager CPU  alert with  CRITICAL  status for 5 minutes  Add 10 worker nodes    HDFS Capacity Utilization  alert with  WARN  status for 20 minutes  Set the number of worker nodes to 50    Ambari Server Alerts  alert with  CRITICAL  status for 15 minutes  Decrease the number of worker nodes by 80%     Time-based autoscaling  Time-based alerts can be defined by providing a cron expression. For example:      Alert Definition  Policy Definition      Every day at 07:00 AM (GMT-8)  Add 90 worker nodes    Every day at 08:00 PM (GMT-8)  Remove 90 worker nodes      Cluster resizing is not supported for HDF clusters.", 
            "title": "Configuring autoscaling"
        }, 
        {
            "location": "/autoscaling/index.html#enable-autoscaling", 
            "text": "For each newly created cluster, autoscaling is disabled by default but it can be enabled once the cluster is in a running state.    Autoscaling configuration is only available in the UI. It is currently not available in the CLI.    Steps   On the cluster details page, navigate to the  Autoscaling  tab.      Click the toggle button to enable autoscaling:        The toggle button turns green and you can see that \"Autoscaling is enabled\".      Define alerts  and then  define scaling policies . You can also  adjust the autoscaling settings .    If you decide to disable autoscaling, your previously defined alerts and policies will be preserved.", 
            "title": "Enable autoscaling"
        }, 
        {
            "location": "/autoscaling/index.html#defining-an-alert", 
            "text": "After you have enabled autoscaling, define a metric-based or time-based alert.", 
            "title": "Defining an alert"
        }, 
        {
            "location": "/autoscaling/index.html#define-a-metric-based-alert", 
            "text": "After  enabling autoscaling , perform the following steps to create a metric-based alert.     If you would like to change default thresholds for an Ambari metric, refer to  Modifying alerts  in Ambari documentation.   If you would like to create a custom Ambari alert, refer to  How to create a custom Ambari alert and use it for Cloudbreak autoscaling policies .   Steps   In the  Alert Configuration  section, select  Metric Based  alert type.         Provide the following information:     Parameter  Description      Enter alert name  Enter a unique name for the alert.    Choose metric type  Select the Ambari metric that should trigger the alert.    Alert status  Select the alert status that should trigger an alert for the selected metric. One of: OK, CRITICAL, WARNING.    Alert duration  Select the alert duration that should trigger an alert.       Click  +  to save the alert.      Once you have defined an alert,  create a scaling policy  that this metric should trigger.  Related links:  How to create a custom Ambari alert and use it for Cloudbreak autoscaling policies  (HCC)   Modifying alerts  (Hortonworks)", 
            "title": "Define a metric-based alert"
        }, 
        {
            "location": "/autoscaling/index.html#define-a-time-based-alert", 
            "text": "After  enabling autoscaling , perform the following steps to create a time-based alert.  Steps   In the  Alert Configuration  section, select the  Time Based  alert type.    Provide the following information:      Parameter  Description      Enter alert name.  Enter a unique name for the alert.    Select timezone.  Select your timezone.    Enter cron expression  Enter a cron expression that defines the frequency of the alert. Refer to  Cron expression generator .       Click  +  to save the alert.       Once you have defined an alert,  create a scaling policy  that this metric should trigger.", 
            "title": "Define a time-based alert"
        }, 
        {
            "location": "/autoscaling/index.html#create-a-scaling-policy", 
            "text": "After  enabling autoscaling  and  creating at least one alert , perform the following steps to create a scaling policy.  Steps    In the  Policy Configuration  section, provide the following information:     Parameter  Description      Enter policy name  Enter a unique name for the policy.    Select action  Select one of the following actions: Add (to add nodes to a host group) Remove (to delete nodes from a host group), or Set (to set the number of nodes in a host group to the chosen number).    Enter number or percentage  Enter a number defining how many or what percentage of nodes to add or remove. If the action selected is \"set\", this defines the number of nodes that a host group will be set to after scaling.    Select nodes of percent  Select \"nodes\" or \"percent\", depending on whether you want to scale to a specific number, or percent of current number of nodes.    Select host group  Select the host group to which to apply the scaling.    Choose an alert  Select the alert based on which the scaling should be applied.       Click  +  to save the alert.", 
            "title": "Create a scaling policy"
        }, 
        {
            "location": "/autoscaling/index.html#configure-autoscaling-settings", 
            "text": "After  enabling autoscaling , perform these steps to configure the auto scaling settings for your cluster.     Steps    In the  Cluster Scaling Configuration , provide the following information:      Setting  Description  Default Value      Cooldown time  After an auto scaling event occurs, the amount of time to wait before enforcing another scaling policy. This means that the scaling events scheduled during cooldown time are dropped.  30 minutes    Minimum Cluster Size  The minimum size allowed for the cluster. Auto scaling policies cannot scale the cluster below or above this size.  2 nodes    Maximum Cluster Size  The maximum size allowed for the cluster. Auto scaling policies cannot scale the cluster below or above this size.  100 nodes       Click  Save  to save the changes.", 
            "title": "Configure autoscaling settings"
        }, 
        {
            "location": "/external-db/index.html", 
            "text": "Using an external database for cluster services\n\n\nCloudbreak allows you to register an existing RDBMS instance to be used for a database for certain services. After you register the RDBMS with Cloudbreak, you can use it for multiple clusters. \n\n\nSupported databases\n\n\nIf you would like to use an external database for one of the components that support it, you may use the following database types and versions: \n\n\n\n\n\n\n\n\nComponent\n\n\nSupported databases\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAmbari\n\n\nPostgreSQL, MySQL\n\n\nBy default, Ambari uses an embedded PostgreSQL instance.\n\n\n\n\n\n\n\n\n\n\nDruid\n\n\nPostgreSQL, MySQL\n\n\nYou must provide an external database.\n\n\n\n\n\n\n\n\n\n\nHive\n\n\nPostgreSQL, MySQL, Oracle 11, Oracle 12\n\n\nBy default, Cloudbreak installs a PostgreSQL instance on the Hive Metastore host.\n\n\n\n\n\n\n\n\n\n\nOozie\n\n\nPostgreSQL, MySQL, Oracle 11, Oracle 12\n\n\nYou must provide an external database.\n\n\n\n\n\n\n\n\n\n\nRanger\n\n\nPostgreSQL, MySQL, Oracle 11, Oracle 12\n\n\nYou must provide an external database.\n\n\n\n\n\n\n\n\n\n\nSuperset\n\n\nPostgreSQL, MySQL\n\n\nYou must provide an external database.\n\n\n\n\n\n\n\n\n\n\nOther\n\n\nPostgreSQL, MySQL, Oracle 11, Oracle 12\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExternal database options\n\n\nCloudbreak includes the following external database options:\n\n\n\n\n\n\n\n\nOption\n\n\nDescription\n\n\nBlueprint requirements\n\n\nSteps\n\n\nExample\n\n\n\n\n\n\n\n\n\n\nBuilt-in types\n\n\nCloudbreak includes a few built-in types: Hive, Druid, Ranger, Superset, and Oozie.\n\n\nUse a standard blueprint which does not include any JDBC  parameters. Cloudbreak automatically injects the JDBC property variables into the blueprint.\n\n\nSimply \nregister the database in the UI\n. After that, you can attach the database config to your clusters.\n\n\nRefer to \nExample 1\n\n\n\n\n\n\nOther types\n\n\nIn addition to the built-in types, Cloudbreak allows you to specify custom types. In the UI, this corresponds to the UI option is called \"Other\" \n \"Enter the type\".\n\n\nYou must provide a custom dynamic blueprint which includes RDBMS-specific variables. Refer to \nCreating a template blueprint\n.\n\n\nPrepare your custom blueprint first. Next, \nregister the database in the UI\n. After that, you can attach the database config to your clusters.\n\n\nRefer to \nExample 2\n\n\n\n\n\n\n\n\nDuring cluster create, Cloudbreak checks whether the JDBC properties are present in the blueprint:\n\n\n\n\nExample 1: Built-in type Hive\n\n\nIn this scenario, you start up with a standard blueprint, and Cloudbreak injects the JDBC properties into the blueprint.\n\n\n\n\n\n\nRegister an existing external database of \"Hive\" type (built-in type):\n\n\n\n\n\n\n\n\n\n\nProperty variable\n\n\nExample value\n\n\n\n\n\n\n\n\n\n\nrds.hive.connectionURL\n\n\njdbc:postgresql://hive.test.eu-west-1:5432/hive\n\n\n\n\n\n\nrds.hive.connectionDriver\n\n\norg.postgresql.Driver\n\n\n\n\n\n\nrds.hive.connectionUserName\n\n\nmydatabaseuser\n\n\n\n\n\n\nrds.hive.connectionPassword\n\n\nHadoop123!\n\n\n\n\n\n\nrds.hive.subprotocol\n\n\npostgres\n\n\n\n\n\n\nrds.hive.databaseEngine\n\n\nPOSTGRES\n\n\n\n\n\n\n\n\n\n\n\n\nCreate a cluster by using a standard blueprint (i.e. one without JDBC related variables) and by attaching the external Hive database configuration.  \n\n\n\n\n\n\nUpon cluster create, Hive JDBC properties will be injected into the blueprint according to the following template:\n\n\n...\n\"hive-site\": {\n \"properties\": {\n   \"javax.jdo.option.ConnectionURL\": \"{{{ rds.hive.connectionURL }}}\",\n   \"javax.jdo.option.ConnectionDriverName\": \"{{{ rds.hive.connectionDriver }}}\",\n   \"javax.jdo.option.ConnectionUserName\": \"{{{ rds.hive.connectionUserName }}}\",\n   \"javax.jdo.option.ConnectionPassword\": \"{{{ rds.hive.connectionPassword }}}\"\n  }\n},\n\"hive-env\" : {\n \"properties\" : {\n   \"hive_database\" : \"Existing {{{ rds.hive.subprotocol }}} Database\",\n   \"hive_database_type\" : \"{{{ rds.hive.databaseEngine }}}\"\n  }\n}\n...\n \n\n\n\n\n\n\nExample 2: Other type\n\n\nIn this scenario, you start up with a special blueprint including JDBC property variables, and Cloudbreak replaces JDBC-related property variables in the blueprint. \n\n\n\n\n\n\nPrepare a blueprint blueprint that includes property variables. Use \nmustache template\n syntax. For example:\n\n\n...\n\"test-site\": {\n\"properties\": {\n   \"javax.jdo.option.ConnectionURL\":\"{{{rds.test.connectionURL}}}\"\n  }\n...\n\n\n\n\n\n\nRegister an existing external database of some \"Other\" type. For example:\n\n\n\n\n\n\n\n\n\n\nProperty variable\n\n\nExample value\n\n\n\n\n\n\n\n\n\n\nrds.hive.connectionURL\n\n\ndb.test.eu-west-1:5432/sometest\n\n\n\n\n\n\nrds.hive.connectionDriver\n\n\norg.postgresql.Driver\n\n\n\n\n\n\nrds.hive.connectionUserName\n\n\nmydatabaseuser\n\n\n\n\n\n\nrds.hive.connectionPassword\n\n\nHadoop123!\n\n\n\n\n\n\nrds.hive.subprotocol\n\n\npostgres\n\n\n\n\n\n\nrds.hive.databaseEngine\n\n\nPOSTGRES\n\n\n\n\n\n\n\n\n\n\n\n\nCreate a cluster by using your custom blueprint and by attaching the external database configuration.  \n\n\n\n\n\n\nUpon cluster create, Cloudbreak replaces JDBC-related property variables in the blueprint. \n\n\n\n\n\n\nRelated links\n\n\nMustache template syntax\n  \n\n\nCreating a template blueprint for RDMBS\n\n\nIn order to use an external RDBMS for some component other than the built-in components, you must include JDBC property variables in your blueprint. You must use \nmustache template\n syntax. See \nExample 2: Other type\n for an example configuration. \n\n\nRelated links\n\n\nCreating a template blueprint\n\n\nMustache template syntax\n (External)  \n\n\nRegister an external database\n\n\nYou must create the external RDBMS instance and database prior to registering it with Cloudbreak. Once you have it ready, you can:\n\n\n\n\nRegister it in Cloudbreak web UI or CLI.  \n\n\nUse it with one or more clusters. Once registered, the database will now show up in the list of available databases when creating a cluster under advanced \nExternal Sources\n \n \nConfigure Databases\n.  \n\n\n\n\nPrerequisites\n  \n\n\nIf you are planning to use an external MySQL or Oracle database, you must download the JDBC connector's JAR file and place it in a location available to the cluster host on which Ambari is installed. The steps below require that you provide the URL to the JDBC connector's JAR file. \n\n\n\n\nIf you are using your own \ncustom image\n, you may place the JDBC connector's JAR file directly on the machine as part of the image burning process. \n\n\n\n\nSteps\n \n\n\n\n\nFrom the navigation pane, select \nExternal Sources\n \n \nDatabase Configurations\n.  \n\n\nSelect \nRegister Database Configuration\n.    \n\n\n\n\nProvide the following information:\n\n\n\n\n\n\n\n\nParameter\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nName\n\n\nEnter the name to use when registering this database to Cloudbreak. This is \nnot\n the database name.\n\n\n\n\n\n\nType\n\n\nSelect the service for which you would like to use the external database. If you selected \"Other\", you must provide a special blueprint.\n\n\n\n\n\n\nJDBC Connection\n\n\nSelect the database \ntype\n and enter the \nJDBC connection\n string (HOST:PORT/DB_NAME).\n\n\n\n\n\n\nConnector's JAR URL\n\n\n(MySQL and Oracle only) Provide a URL to the JDBC connector's JAR file. The JAR file must be hosted in a location accessible to the cluster host on which Ambari is installed. At cluster creation time, Cloudbreak places the JAR file in the /opts/jdbc-drivers directory. You do not need to provide the \"Connector's JAR URL if you are using a custom image and the JAR file was either manually placed on the VM as part of custom image burning or it was placed there by using a recipe.\n\n\n\n\n\n\nUsername\n\n\nEnter the JDBC connection username.\n\n\n\n\n\n\nPassword\n\n\nEnter the JDBC connection password.\n\n\n\n\n\n\n\n\n\n\n\n\nClick \nTest Connection\n to validate and test the RDS connection information.  \n\n\n\n\nOnce your settings are validated and working, click \nREGISTER\n to save the configuration.  \n\n\nThe database will now show up on the list of available databases when creating a cluster under advanced \nExternal Sources\n \n \nConfigure Databases\n. You can select it and click \nAttach\n each time you would like to use it for a cluster.", 
            "title": "Register external database"
        }, 
        {
            "location": "/external-db/index.html#using-an-external-database-for-cluster-services", 
            "text": "Cloudbreak allows you to register an existing RDBMS instance to be used for a database for certain services. After you register the RDBMS with Cloudbreak, you can use it for multiple clusters.", 
            "title": "Using an external database for cluster services"
        }, 
        {
            "location": "/external-db/index.html#supported-databases", 
            "text": "If you would like to use an external database for one of the components that support it, you may use the following database types and versions:      Component  Supported databases  Description        Ambari  PostgreSQL, MySQL  By default, Ambari uses an embedded PostgreSQL instance.      Druid  PostgreSQL, MySQL  You must provide an external database.      Hive  PostgreSQL, MySQL, Oracle 11, Oracle 12  By default, Cloudbreak installs a PostgreSQL instance on the Hive Metastore host.      Oozie  PostgreSQL, MySQL, Oracle 11, Oracle 12  You must provide an external database.      Ranger  PostgreSQL, MySQL, Oracle 11, Oracle 12  You must provide an external database.      Superset  PostgreSQL, MySQL  You must provide an external database.      Other  PostgreSQL, MySQL, Oracle 11, Oracle 12", 
            "title": "Supported databases"
        }, 
        {
            "location": "/external-db/index.html#external-database-options", 
            "text": "Cloudbreak includes the following external database options:     Option  Description  Blueprint requirements  Steps  Example      Built-in types  Cloudbreak includes a few built-in types: Hive, Druid, Ranger, Superset, and Oozie.  Use a standard blueprint which does not include any JDBC  parameters. Cloudbreak automatically injects the JDBC property variables into the blueprint.  Simply  register the database in the UI . After that, you can attach the database config to your clusters.  Refer to  Example 1    Other types  In addition to the built-in types, Cloudbreak allows you to specify custom types. In the UI, this corresponds to the UI option is called \"Other\"   \"Enter the type\".  You must provide a custom dynamic blueprint which includes RDBMS-specific variables. Refer to  Creating a template blueprint .  Prepare your custom blueprint first. Next,  register the database in the UI . After that, you can attach the database config to your clusters.  Refer to  Example 2     During cluster create, Cloudbreak checks whether the JDBC properties are present in the blueprint:", 
            "title": "External database options"
        }, 
        {
            "location": "/external-db/index.html#example-1-built-in-type-hive", 
            "text": "In this scenario, you start up with a standard blueprint, and Cloudbreak injects the JDBC properties into the blueprint.    Register an existing external database of \"Hive\" type (built-in type):      Property variable  Example value      rds.hive.connectionURL  jdbc:postgresql://hive.test.eu-west-1:5432/hive    rds.hive.connectionDriver  org.postgresql.Driver    rds.hive.connectionUserName  mydatabaseuser    rds.hive.connectionPassword  Hadoop123!    rds.hive.subprotocol  postgres    rds.hive.databaseEngine  POSTGRES       Create a cluster by using a standard blueprint (i.e. one without JDBC related variables) and by attaching the external Hive database configuration.      Upon cluster create, Hive JDBC properties will be injected into the blueprint according to the following template:  ...\n\"hive-site\": {\n \"properties\": {\n   \"javax.jdo.option.ConnectionURL\": \"{{{ rds.hive.connectionURL }}}\",\n   \"javax.jdo.option.ConnectionDriverName\": \"{{{ rds.hive.connectionDriver }}}\",\n   \"javax.jdo.option.ConnectionUserName\": \"{{{ rds.hive.connectionUserName }}}\",\n   \"javax.jdo.option.ConnectionPassword\": \"{{{ rds.hive.connectionPassword }}}\"\n  }\n},\n\"hive-env\" : {\n \"properties\" : {\n   \"hive_database\" : \"Existing {{{ rds.hive.subprotocol }}} Database\",\n   \"hive_database_type\" : \"{{{ rds.hive.databaseEngine }}}\"\n  }\n}\n...", 
            "title": "Example 1: Built-in type Hive"
        }, 
        {
            "location": "/external-db/index.html#example-2-other-type", 
            "text": "In this scenario, you start up with a special blueprint including JDBC property variables, and Cloudbreak replaces JDBC-related property variables in the blueprint.     Prepare a blueprint blueprint that includes property variables. Use  mustache template  syntax. For example:  ...\n\"test-site\": {\n\"properties\": {\n   \"javax.jdo.option.ConnectionURL\":\"{{{rds.test.connectionURL}}}\"\n  }\n...    Register an existing external database of some \"Other\" type. For example:      Property variable  Example value      rds.hive.connectionURL  db.test.eu-west-1:5432/sometest    rds.hive.connectionDriver  org.postgresql.Driver    rds.hive.connectionUserName  mydatabaseuser    rds.hive.connectionPassword  Hadoop123!    rds.hive.subprotocol  postgres    rds.hive.databaseEngine  POSTGRES       Create a cluster by using your custom blueprint and by attaching the external database configuration.      Upon cluster create, Cloudbreak replaces JDBC-related property variables in the blueprint.     Related links  Mustache template syntax", 
            "title": "Example 2: Other type"
        }, 
        {
            "location": "/external-db/index.html#creating-a-template-blueprint-for-rdmbs", 
            "text": "In order to use an external RDBMS for some component other than the built-in components, you must include JDBC property variables in your blueprint. You must use  mustache template  syntax. See  Example 2: Other type  for an example configuration.   Related links  Creating a template blueprint  Mustache template syntax  (External)", 
            "title": "Creating a template blueprint for RDMBS"
        }, 
        {
            "location": "/external-db/index.html#register-an-external-database", 
            "text": "You must create the external RDBMS instance and database prior to registering it with Cloudbreak. Once you have it ready, you can:   Register it in Cloudbreak web UI or CLI.    Use it with one or more clusters. Once registered, the database will now show up in the list of available databases when creating a cluster under advanced  External Sources     Configure Databases .     Prerequisites     If you are planning to use an external MySQL or Oracle database, you must download the JDBC connector's JAR file and place it in a location available to the cluster host on which Ambari is installed. The steps below require that you provide the URL to the JDBC connector's JAR file.    If you are using your own  custom image , you may place the JDBC connector's JAR file directly on the machine as part of the image burning process.    Steps     From the navigation pane, select  External Sources     Database Configurations .    Select  Register Database Configuration .       Provide the following information:     Parameter  Description      Name  Enter the name to use when registering this database to Cloudbreak. This is  not  the database name.    Type  Select the service for which you would like to use the external database. If you selected \"Other\", you must provide a special blueprint.    JDBC Connection  Select the database  type  and enter the  JDBC connection  string (HOST:PORT/DB_NAME).    Connector's JAR URL  (MySQL and Oracle only) Provide a URL to the JDBC connector's JAR file. The JAR file must be hosted in a location accessible to the cluster host on which Ambari is installed. At cluster creation time, Cloudbreak places the JAR file in the /opts/jdbc-drivers directory. You do not need to provide the \"Connector's JAR URL if you are using a custom image and the JAR file was either manually placed on the VM as part of custom image burning or it was placed there by using a recipe.    Username  Enter the JDBC connection username.    Password  Enter the JDBC connection password.       Click  Test Connection  to validate and test the RDS connection information.     Once your settings are validated and working, click  REGISTER  to save the configuration.    The database will now show up on the list of available databases when creating a cluster under advanced  External Sources     Configure Databases . You can select it and click  Attach  each time you would like to use it for a cluster.", 
            "title": "Register an external database"
        }, 
        {
            "location": "/external-ldap/index.html", 
            "text": "Using an external authentication source for clusters\n\n\nCloudbreak allows you to register an existing LDAP/AD instance and use it for multiple clusters. You must create the LDAP/AD prior to registering it with Cloudbreak. Once you have it ready, the overall steps are:\n\n\n\n\nRegister an existing LDAP in Cloudbreak web UI or CLI.  \n\n\nOnce registered, the LDAP will now show up in the list of available authentication sources when creating a cluster under advanced \nExternal Sources\n \n \nConfigure Authentication\n.  \n\n\nPrepare a blueprint as described in \nPreparing a blueprint for LDAP/AD \n.  \n\n\nCreate a cluster by using the blueprint and by attaching the authentication source. Cloudbreak automatically injects the LDAP property variables into the blueprint. \n\n\n\n\nPreparing a blueprint for LDAP/AD\n\n\nIn order to use LDAP/AD for your cluster, you must provide a suitable cluster blueprint:\n\n\n\n\nThe blueprint must include one or more of the following supported components: Atlas, Hadoop, Hive LLAP, Ranger Admin, Ranger UserSync.  \n\n\nThe blueprint should not include any LDAP properties. Before injecting the properties, Cloudbreak checks if LDAP related properties already exist in the blueprint. If they exist, they are not injected.  \n\n\n\n\nDuring cluster creation the following properties will be injected in the blueprint:\n\n\n\n\nldap.connectionURL  \n\n\nldap.domain  \n\n\nldap.bindDn  \n\n\nldap.bindPassword  \n\n\nldap.userSearchBase  \n\n\nldap.userObjectClass  \n\n\nldap.userNameAttribute  \n\n\nldap.groupSearchBase  \n\n\nldap.groupObjectClass  \n\n\nldap.groupNameAttribute  \n\n\nldap.groupMemberAttribute  \n\n\nldap.directoryType  \n\n\nldap.directoryTypeShort  \n\n\n\n\nTheir values will be the values that you provided to Cloudbreak: \n\n\n\n\nRegister an authentication source\n\n\nCloudbreak allows you to register an existing LDAP/AD instance and use it for multiple clusters. You must create the LDAP/AD prior to registering it with Cloudbreak. Once you have it ready, you can:\n\n\n\n\nRegister an existing LDAP in Cloudbreak web UI or CLI.  \n\n\nUse it as an authentication source for your clusters. Once registered, the LDAP will now show up in the list of available authentication sources when creating a cluster under advanced \nExternal Sources\n \n \nConfigure Authentication\n.   \n\n\n\n\nSteps\n\n\n\n\nFrom the navigation pane, select \nExternal Sources\n \n \nAuthentication Configurations\n.  \n\n\nSelect \nRegister Authentication Source\n.     \n\n\n\n\nProvide the following parameters related to your existing LDAP/AD: \n\n\nGENERAL CONFIGURATION\n\n\n\n\n\n\n\n\nParameter\n\n\nDescription\n\n\nExample\n\n\n\n\n\n\n\n\n\n\nName\n\n\nEnter a name for your LDAP.\n\n\ncb-ldap\n\n\n\n\n\n\nDirectory Type\n\n\nChoose whether your directory is \nLDAP\n or \nActive Directory\n.\n\n\nLDAP\n\n\n\n\n\n\nLDAP Server Connection\n\n\nSelect \nLDAP\n or \nLDAPS\n.\n\n\nLDAP\n\n\n\n\n\n\nServer Host\n\n\nEnter the hostname for the LDAP or AD server .\n\n\n10.0.3.128\n\n\n\n\n\n\nServer Port\n\n\nEnter the port.\n\n\n389\n\n\n\n\n\n\nLDAP Bind DN\n\n\nEnter the root Distinguished Name to search in the directory for users.\n\n\nCN=Administrator,CN=Users,DC=ad,DC=hdc,DC=com\n\n\n\n\n\n\nLDAP Bind Password\n\n\nEnter your root Distinguished Name password.\n\n\nMyPassword1234!\n\n\n\n\n\n\n\n\nUSER CONFIGURATION\n\n\n\n\n\n\n\n\nParameter\n\n\nDescription\n\n\nExample\n\n\n\n\n\n\n\n\n\n\nLDAP User Search Base\n\n\nEnter your LDAP user search base. This defines the location in the directory from which the LDAP search begins.\n\n\nCN=Users,DC=ad,DC=hdc,DC=com\n\n\n\n\n\n\nLDAP User Name Attribute\n\n\nEnter the attribute for which to conduct a search on the user base.\n\n\nHDCaccountName\n\n\n\n\n\n\nLDAP User Object Class\n\n\nEnter the directory object class for users.\n\n\nperson\n\n\n\n\n\n\n\n\nGROUP CONFIGURATION\n\n\n\n\n\n\n\n\nParameter\n\n\nDescription\n\n\nExample\n\n\n\n\n\n\n\n\n\n\nLDAP Group Search Base\n\n\nEnter your LDAP group search base. This defines the location in the directory from which the LDAP search begins.\n\n\nOU-scoDC=ad,DC=hdc,DC=com\n\n\n\n\n\n\nLDAP Admin Group\n\n\n(Optional) Enter your LDAP admin group, if needed.\n\n\n\n\n\n\n\n\nLDAP Group Name Attribute\n\n\nEnter the attribute for which to conduct a search on groups.\n\n\ncn\n\n\n\n\n\n\nLDAP Group Object Class\n\n\nEnter the directory object class for groups.\n\n\ngroup\n\n\n\n\n\n\nLDAP Group Member Attribute\n\n\nEnter the attribute on the group object class that represents members.\n\n\nmember\n\n\n\n\n\n\n\n\n\n\n\n\nClick \nTest Connection\n to verify that the connection information that you entered is correct.\n\n\n\n\n\n\nClick \nREGISTER\n. \n\n\n\n\n\n\nThe LDAP will now show up on the list of available authentication sources when creating a cluster under advanced \nExternal Sources\n \n \nConfigure Authentication\n. It can be reused with multiple clusters. Just select it if you would like to use it for a given cluster.", 
            "title": "Register external authentication source"
        }, 
        {
            "location": "/external-ldap/index.html#using-an-external-authentication-source-for-clusters", 
            "text": "Cloudbreak allows you to register an existing LDAP/AD instance and use it for multiple clusters. You must create the LDAP/AD prior to registering it with Cloudbreak. Once you have it ready, the overall steps are:   Register an existing LDAP in Cloudbreak web UI or CLI.    Once registered, the LDAP will now show up in the list of available authentication sources when creating a cluster under advanced  External Sources     Configure Authentication .    Prepare a blueprint as described in  Preparing a blueprint for LDAP/AD  .    Create a cluster by using the blueprint and by attaching the authentication source. Cloudbreak automatically injects the LDAP property variables into the blueprint.", 
            "title": "Using an external authentication source for clusters"
        }, 
        {
            "location": "/external-ldap/index.html#preparing-a-blueprint-for-ldapad", 
            "text": "In order to use LDAP/AD for your cluster, you must provide a suitable cluster blueprint:   The blueprint must include one or more of the following supported components: Atlas, Hadoop, Hive LLAP, Ranger Admin, Ranger UserSync.    The blueprint should not include any LDAP properties. Before injecting the properties, Cloudbreak checks if LDAP related properties already exist in the blueprint. If they exist, they are not injected.     During cluster creation the following properties will be injected in the blueprint:   ldap.connectionURL    ldap.domain    ldap.bindDn    ldap.bindPassword    ldap.userSearchBase    ldap.userObjectClass    ldap.userNameAttribute    ldap.groupSearchBase    ldap.groupObjectClass    ldap.groupNameAttribute    ldap.groupMemberAttribute    ldap.directoryType    ldap.directoryTypeShort     Their values will be the values that you provided to Cloudbreak:", 
            "title": "Preparing a blueprint for LDAP/AD"
        }, 
        {
            "location": "/external-ldap/index.html#register-an-authentication-source", 
            "text": "Cloudbreak allows you to register an existing LDAP/AD instance and use it for multiple clusters. You must create the LDAP/AD prior to registering it with Cloudbreak. Once you have it ready, you can:   Register an existing LDAP in Cloudbreak web UI or CLI.    Use it as an authentication source for your clusters. Once registered, the LDAP will now show up in the list of available authentication sources when creating a cluster under advanced  External Sources     Configure Authentication .      Steps   From the navigation pane, select  External Sources     Authentication Configurations .    Select  Register Authentication Source .        Provide the following parameters related to your existing LDAP/AD:   GENERAL CONFIGURATION     Parameter  Description  Example      Name  Enter a name for your LDAP.  cb-ldap    Directory Type  Choose whether your directory is  LDAP  or  Active Directory .  LDAP    LDAP Server Connection  Select  LDAP  or  LDAPS .  LDAP    Server Host  Enter the hostname for the LDAP or AD server .  10.0.3.128    Server Port  Enter the port.  389    LDAP Bind DN  Enter the root Distinguished Name to search in the directory for users.  CN=Administrator,CN=Users,DC=ad,DC=hdc,DC=com    LDAP Bind Password  Enter your root Distinguished Name password.  MyPassword1234!     USER CONFIGURATION     Parameter  Description  Example      LDAP User Search Base  Enter your LDAP user search base. This defines the location in the directory from which the LDAP search begins.  CN=Users,DC=ad,DC=hdc,DC=com    LDAP User Name Attribute  Enter the attribute for which to conduct a search on the user base.  HDCaccountName    LDAP User Object Class  Enter the directory object class for users.  person     GROUP CONFIGURATION     Parameter  Description  Example      LDAP Group Search Base  Enter your LDAP group search base. This defines the location in the directory from which the LDAP search begins.  OU-scoDC=ad,DC=hdc,DC=com    LDAP Admin Group  (Optional) Enter your LDAP admin group, if needed.     LDAP Group Name Attribute  Enter the attribute for which to conduct a search on groups.  cn    LDAP Group Object Class  Enter the directory object class for groups.  group    LDAP Group Member Attribute  Enter the attribute on the group object class that represents members.  member       Click  Test Connection  to verify that the connection information that you entered is correct.    Click  REGISTER .     The LDAP will now show up on the list of available authentication sources when creating a cluster under advanced  External Sources     Configure Authentication . It can be reused with multiple clusters. Just select it if you would like to use it for a given cluster.", 
            "title": "Register an authentication source"
        }, 
        {
            "location": "/external-proxy/index.html", 
            "text": "Register a proxy\n\n\nCloudbreak allows you to save your existing proxy configuration information so that you can provide the proxy information to the clusters that you create with Cloudbreak. The steps are:       \n\n\n\n\nRegister your proxy in Cloudbreak web UI or CLI.   \n\n\nOnce the proxy has been registered with Cloudbreak, it will show up in the list of available proxies when creating a cluster under advanced \nExternal Sources\n \n \nConfigure Proxy\n.  \n\n\n\n\nSteps\n \n\n\n\n\nFrom the navigation pane, select \nExternal Sources\n \n \nProxy Configurations\n.  \n\n\nSelect \nRegister Proxy Configuration\n.    \n\n\n\n\nProvide the following information:\n\n\n\n\n\n\n\n\nParameter\n\n\nDescription\n\n\nExample\n\n\n\n\n\n\n\n\n\n\nName\n\n\nEnter the name to use when registering this database to Cloudbreak. This is \nnot\n the database name.\n\n\nmy-proxy\n\n\n\n\n\n\nDescription\n\n\nProvide description.\n\n\n\n\n\n\n\n\nProtocol\n\n\nSelect HTTP or HTTPS.\n\n\nHTTPS\n\n\n\n\n\n\nServer Host\n\n\nEnter the URL of your proxy server host.\n\n\n10.0.2.237\n\n\n\n\n\n\nServer Port\n\n\nEnter proxy server port.\n\n\n3128\n\n\n\n\n\n\nUsername\n\n\nEnter the username for the proxy.\n\n\ntestuser\n\n\n\n\n\n\nPassword\n\n\nEnter the password for the proxy.\n\n\nMyPassword123\n\n\n\n\n\n\n\n\n\n\n\n\nClick \nREGISTER\n to save the configuration. \n\n\n\n\n\n\nThe proxy will now show up when creating a cluster under advanced \nExternal Sources\n \n \nConfigure Proxy\n. You can select it each time you would like to use it for a cluster.", 
            "title": "Register proxy configuration"
        }, 
        {
            "location": "/external-proxy/index.html#register-a-proxy", 
            "text": "Cloudbreak allows you to save your existing proxy configuration information so that you can provide the proxy information to the clusters that you create with Cloudbreak. The steps are:          Register your proxy in Cloudbreak web UI or CLI.     Once the proxy has been registered with Cloudbreak, it will show up in the list of available proxies when creating a cluster under advanced  External Sources     Configure Proxy .     Steps     From the navigation pane, select  External Sources     Proxy Configurations .    Select  Register Proxy Configuration .       Provide the following information:     Parameter  Description  Example      Name  Enter the name to use when registering this database to Cloudbreak. This is  not  the database name.  my-proxy    Description  Provide description.     Protocol  Select HTTP or HTTPS.  HTTPS    Server Host  Enter the URL of your proxy server host.  10.0.2.237    Server Port  Enter proxy server port.  3128    Username  Enter the username for the proxy.  testuser    Password  Enter the password for the proxy.  MyPassword123       Click  REGISTER  to save the configuration.     The proxy will now show up when creating a cluster under advanced  External Sources     Configure Proxy . You can select it each time you would like to use it for a cluster.", 
            "title": "Register a proxy"
        }, 
        {
            "location": "/cb-credentials/index.html", 
            "text": "Managing Cloudbreak credentials\n\n\nYou can view and manage Cloudbreak credentials in the \nCredentials\n tab by clicking \nCreate credential\n and providing required parameters. You must create at least one credential in order to be able to create a cluster. \n\n\nCreate Cloudbreak credential\n\n\nFor steps, refer to:\n\n\n\n\nCreate Cloudbreak credential on AWS\n  \n\n\nCreate Cloudbreak credential on Azure\n  \n\n\nCreate Cloudbreak credential on GCP\n \n\n\nCreate Cloudbreak credential on OpenStack\n\n\n\n\nModify an existing credential\n\n\nYou can modify an existing Cloudbreak credential by following these steps.\n\n\n\n\nThe value of the \"Name\" parameter cannot be changed.  \n\nThe values of sensitive parameters will not be displayed and you will have to reenter them.    \n\n\n\n\nSteps\n\n\n\n\nIn the Cloudbreak UI, select \nCredentials\n from the navigation pane.\n\n2.Click on \n  next to the credential that you want to edit.  \n\n\nWhen done making changes, click \nSave\n to save your changes.  \n\n\n\n\nSet a default credential\n\n\nIf using multiple Cloudbreak credentials, you can select one credential and use it as default for creating clusters. This default credential will be pre-selected in the create cluster wizard.\n\n\nSteps\n\n\n\n\nIn the Cloudbreak UI, select \nCredentials\n from the navigation pane.  \n\n\nClick \nSet as default\n next to the credential that you would like to set as default.  \n\n\nClick \nYes\n to confirm.", 
            "title": "Manage Cloudbreak credentials"
        }, 
        {
            "location": "/cb-credentials/index.html#managing-cloudbreak-credentials", 
            "text": "You can view and manage Cloudbreak credentials in the  Credentials  tab by clicking  Create credential  and providing required parameters. You must create at least one credential in order to be able to create a cluster.", 
            "title": "Managing Cloudbreak credentials"
        }, 
        {
            "location": "/cb-credentials/index.html#create-cloudbreak-credential", 
            "text": "For steps, refer to:   Create Cloudbreak credential on AWS     Create Cloudbreak credential on Azure     Create Cloudbreak credential on GCP    Create Cloudbreak credential on OpenStack", 
            "title": "Create Cloudbreak credential"
        }, 
        {
            "location": "/cb-credentials/index.html#modify-an-existing-credential", 
            "text": "You can modify an existing Cloudbreak credential by following these steps.   The value of the \"Name\" parameter cannot be changed.   \nThe values of sensitive parameters will not be displayed and you will have to reenter them.       Steps   In the Cloudbreak UI, select  Credentials  from the navigation pane. \n2.Click on    next to the credential that you want to edit.    When done making changes, click  Save  to save your changes.", 
            "title": "Modify an existing credential"
        }, 
        {
            "location": "/cb-credentials/index.html#set-a-default-credential", 
            "text": "If using multiple Cloudbreak credentials, you can select one credential and use it as default for creating clusters. This default credential will be pre-selected in the create cluster wizard.  Steps   In the Cloudbreak UI, select  Credentials  from the navigation pane.    Click  Set as default  next to the credential that you would like to set as default.    Click  Yes  to confirm.", 
            "title": "Set a default credential"
        }, 
        {
            "location": "/cb-migrate/index.html", 
            "text": "Moving a Cloudbreak instance\n\n\nTo transfer a Cloudbreak instance from one host to another, perform these tasks:\n\n\n\n\nIf you are using the embedded PostgreSQL database, \nback up current Cloudbreak database\n data  \n\n\nLaunch a new Cloudbreak instance and start Cloudbreak. Refer to \nLaunch Cloudbreak\n   \n\n\nIf you are using the embedded PostgreSQL database, \npopulate the new Cloudbreak instance database with the dump from the original Cloudbreak instance\n on the new host.  \n\n\nModify Cloudbreak Profile\n  \n\n\n\n\nBack up Cloudbreak database\n\n\nTo create a backup of the embedded PostgreSQL database, perform these steps.\n\n\nSteps\n \n\n\n\n\n\n\nOn your Cloudbreak host machine, execute the following  command to enter the container of the database:\n\n\ndocker exec -it cbreak_commondb_1 bash\n \nIf it is not running, start the database container by using the \ndocker start cbreak_commondb_1\n command.\n\n\n\n\n\n\nCreate three database dumps (cbdb, uaadb, periscopedb):  \n\n\npg_dump -Fc -U postgres cbdb \n cbdb.dump\npg_dump -Fc -U postgres uaadb \n uaadb.dump\npg_dump -Fc -U postgres periscopedb \n periscopedb.dump\n\n\n\n\n\n\nQuit from the container with shortcut \nCTRL+d\n.\n\n\n\n\n\n\nSave the previously created dumps to the host instance:               \n\n\ndocker cp cbreak_commondb_1:/cbdb.dump ./cbdb.dump\ndocker cp cbreak_commondb_1:/uaadb.dump ./uaadb.dump\ndocker cp cbreak_commondb_1:/periscopedb.dump ./periscopedb.dump\n\n\n\n\n\n\nPopulate database with dump from original Cloudbreak instance\n\n\nPerform these steps to populate databases with information from the Cloudbreak server.\n\n\nSteps\n \n\n\n\n\n\n\nCopy the saved database files from \nBack up Cloudbreak database\n to the new Cloudbreak server host.\n\n\n\n\n\n\nCopy the dump files into the database container with the following commands. Modify the location as necessary (The example below assumes that the files are in \n/tmp\n):\n\n\ndocker cp /tmp/cbdb.dump cbreak_commondb_1:/cbdb.dump\ndocker cp /tmp/uaadb.dump cbreak_commondb_1:/uaadb.dump\ndocker cp /tmp/periscopedb.dump cbreak_commondb_1:/periscopedb.dump\n\n\n\n\n\n\nExecute the following command to stop the container:\n\n\ndocker stop cbreak_identity_1\n\n\n\n\n\n\nExecute the following command to enter the container of the database:\n\n\ndocker exec -it cbreak_commondb_1 bash\n\n\n\n\n\n\nExecute the following commands:\n\n\npsql -U postgres\ndrop database uaadb;\ndrop database cbdb;\ndrop database periscopedb;\ncreate database uaadb;\ncreate database cbdb;\ncreate database periscopedb;\n\n\n\n\nIf you get \nERROR:  database \"uaadb\" is being accessed by other users\n error, ensure that    cbreak_identity_1 container is not running and then retry dropping uaadb.  \n\n\n\n\n\n\n\n\nExit the PostgreSQL interactive terminal.\n    \n\\q\n \n\n\n\n\n\n\nRestore the databases from the original backups:\n\n\npg_restore -U postgres -d periscopedb periscopedb.dump\npg_restore -U postgres -d cbdb cbdb.dump\npg_restore -U postgres -d uaadb uaadb.dump\n\n\n\n\n\n\nQuit from the container with the shortcut \nCTRL+d\n.     \n\n\n\n\n\n\nModify Cloudbreak Profile\n\n\nPerform these steps to ensure that your new Profile file is correctly set up. \n\n\nSteps\n \n\n\n\n\n\n\nEnsure that the following parameter values match in the origin and target Profile files and modify Profile file of the target environment if necessary:\n\n\nexport UAA_DEFAULT_USER_EMAIL=admin@example.com\nexport UAA_DEFAULT_SECRET=cbsecret\nexport UAA_DEFAULT_USER_PW=cbuser\n\n\n\n\n\n\nRestart Cloudbreak application by using the \ncbd restart\n command.  \n\n\nAfter performing these steps the migration is complete. To verify, log in to the UI of your new Cloudbreak instance and make sure that it contains the information from your old instance.", 
            "title": "Move Cloudbreak instance"
        }, 
        {
            "location": "/cb-migrate/index.html#moving-a-cloudbreak-instance", 
            "text": "To transfer a Cloudbreak instance from one host to another, perform these tasks:   If you are using the embedded PostgreSQL database,  back up current Cloudbreak database  data    Launch a new Cloudbreak instance and start Cloudbreak. Refer to  Launch Cloudbreak      If you are using the embedded PostgreSQL database,  populate the new Cloudbreak instance database with the dump from the original Cloudbreak instance  on the new host.    Modify Cloudbreak Profile", 
            "title": "Moving a Cloudbreak instance"
        }, 
        {
            "location": "/cb-migrate/index.html#back-up-cloudbreak-database", 
            "text": "To create a backup of the embedded PostgreSQL database, perform these steps.  Steps      On your Cloudbreak host machine, execute the following  command to enter the container of the database:  docker exec -it cbreak_commondb_1 bash  \nIf it is not running, start the database container by using the  docker start cbreak_commondb_1  command.    Create three database dumps (cbdb, uaadb, periscopedb):    pg_dump -Fc -U postgres cbdb   cbdb.dump\npg_dump -Fc -U postgres uaadb   uaadb.dump\npg_dump -Fc -U postgres periscopedb   periscopedb.dump    Quit from the container with shortcut  CTRL+d .    Save the previously created dumps to the host instance:                 docker cp cbreak_commondb_1:/cbdb.dump ./cbdb.dump\ndocker cp cbreak_commondb_1:/uaadb.dump ./uaadb.dump\ndocker cp cbreak_commondb_1:/periscopedb.dump ./periscopedb.dump", 
            "title": "Back up Cloudbreak database"
        }, 
        {
            "location": "/cb-migrate/index.html#populate-database-with-dump-from-original-cloudbreak-instance", 
            "text": "Perform these steps to populate databases with information from the Cloudbreak server.  Steps      Copy the saved database files from  Back up Cloudbreak database  to the new Cloudbreak server host.    Copy the dump files into the database container with the following commands. Modify the location as necessary (The example below assumes that the files are in  /tmp ):  docker cp /tmp/cbdb.dump cbreak_commondb_1:/cbdb.dump\ndocker cp /tmp/uaadb.dump cbreak_commondb_1:/uaadb.dump\ndocker cp /tmp/periscopedb.dump cbreak_commondb_1:/periscopedb.dump    Execute the following command to stop the container:  docker stop cbreak_identity_1    Execute the following command to enter the container of the database:  docker exec -it cbreak_commondb_1 bash    Execute the following commands:  psql -U postgres\ndrop database uaadb;\ndrop database cbdb;\ndrop database periscopedb;\ncreate database uaadb;\ncreate database cbdb;\ncreate database periscopedb;   If you get  ERROR:  database \"uaadb\" is being accessed by other users  error, ensure that    cbreak_identity_1 container is not running and then retry dropping uaadb.       Exit the PostgreSQL interactive terminal.\n     \\q      Restore the databases from the original backups:  pg_restore -U postgres -d periscopedb periscopedb.dump\npg_restore -U postgres -d cbdb cbdb.dump\npg_restore -U postgres -d uaadb uaadb.dump    Quit from the container with the shortcut  CTRL+d .", 
            "title": "Populate database with dump from original Cloudbreak instance"
        }, 
        {
            "location": "/cb-migrate/index.html#modify-cloudbreak-profile", 
            "text": "Perform these steps to ensure that your new Profile file is correctly set up.   Steps      Ensure that the following parameter values match in the origin and target Profile files and modify Profile file of the target environment if necessary:  export UAA_DEFAULT_USER_EMAIL=admin@example.com\nexport UAA_DEFAULT_SECRET=cbsecret\nexport UAA_DEFAULT_USER_PW=cbuser    Restart Cloudbreak application by using the  cbd restart  command.    After performing these steps the migration is complete. To verify, log in to the UI of your new Cloudbreak instance and make sure that it contains the information from your old instance.", 
            "title": "Modify Cloudbreak Profile"
        }, 
        {
            "location": "/cb-ldap/index.html", 
            "text": "Configuring Cloudbreak for LDAP/AD authentication\n\n\nBy default Cloudbreak uses an internal system as the user store for authentication (enabled by using \nCloudFoundry UAA\n). If you would like to configure LDAP or Active Directory (AD) external authentication, you need to:  \n\n\n\n\nCollect the \nfollowing information\n about your LDAP/AD setup    \n\n\nConfigure Cloudbreak\n to work with that LDAP/AD setup\n\n\n\n\nLDAP/AD information\n\n\nThe following table details the properties and values that you need to know about your LDAP/AD environment on order to use the LDAP/AD with Cloudbreak: \n\n\n\n\n\n\n\n\nParameter\n\n\nDescription\n\n\nExample\n\n\n\n\n\n\n\n\n\n\nbase\n\n\n\n\n\n\n\n\n\n\nurl\n\n\nThe LDAP url with port\n\n\nldap://10.0.3.128:389/\n\n\n\n\n\n\nuserDn\n\n\nEnter the root Distinguished Name to search in the directory for users.\n\n\ncn=Administrator,ou=srv,dc=hortonworks,dc=local\n\n\n\n\n\n\npassword\n\n\nEnter your root Distinguished Name password.\n\n\nMyPassword1234!\n\n\n\n\n\n\nsearchBase\n\n\nEnter your LDAP user search base. This defines the location in the directory from which the LDAP search begins.\n\n\nou=Users,dc=hortonworks,dc=local\n\n\n\n\n\n\nsearchFilter\n\n\nEnter the attribute for which to conduct a search on the user base.\n\n\nmail={0}\n\n\n\n\n\n\ngroups\n\n\n\n\n\n\n\n\n\n\nsearchBase\n\n\nEnter your LDAP group search base. This defines the location in the directory from which the LDAP search begins.\n\n\nou=Groups,dc=hortonworks,dc=local\n\n\n\n\n\n\ngroupSearchFilter\n\n\nEnter the attribute for which to conduct a search on the group base.\n\n\nmember={0}\n\n\n\n\n\n\n\n\nConfiguring Cloudbreak for LDAP/AD\n\n\nThere are two parts to configuring Cloudbreak for LDAP/AD:\n\n\n\n\nConfiguring LDAP/AD user authentication for Cloudbreak  \n\n\nConfiguring LDAP/AD group authorization for Cloudbreak  \n\n\n\n\nConfigure user authentication\n\n\nConfigure LDAP/AD user authentication for Cloudbreak by using these steps. \n\n\nSteps\n \n\n\n\n\nOn the Cloudbreak host, browse to \n/var/lib/cloudbreak-deployment\n.  \n\n\n\n\nCreate a new file called \nuaa-changes.yml\n. \n\n\n\n\nThe name of this file can be customized by setting the following in Profile: \nexport UAA_SETTINGS_FILE=myldap.yml\n\n\n\n\n\n\n\n\nIn the yml file enter the following using your \nLDAP/AD information\n. Next, save the file and restart Cloudbreak.  \n\n\n\n\n\n\nspring_profiles: postgresql,ldap\n\nldap:\n  profile:\n    file: ldap/ldap-search-and-bind.xml\n  base:\n    url: ldap://10.0.3.138:389\n    userDn: cn=Administrator,ou=srv,dc=hortonworks,dc=local\n    password: \u2019mypassword\u2019\n    searchBase: ou=Users,dc=hortonworks,dc=local\n    searchFilter: mail={0}\n  groups:\n    file: ldap/ldap-groups-map-to-scopes.xml\n    searchBase: ou=Groups,dc=hortonworks,dc=local\n    searchSubtree: false\n    maxSearchDepth: 1\n    groupSearchFilter: member={0}\n    autoAdd: true\n\n\n\n\nConfigure group authorization\n\n\nOnce user authentication is configured, you need to configure which group(s) can access Cloudbreak. Users (once authenticated) will be granted permission to access Cloudbreak and use the capabilities of Cloudbreak based on their group member. The following describes how to create (i.e. execute-and-map) a group authorization and how to remove (i.e. delete-mapping) an authorization. \n\n\nTo create a group authorization, execute the following (for example: to add \u201cAnalysts\u201d group):\n\n\ncbd util execute-ldap-mapping cn=Analysts,ou=Groups,dc=hortonworks,dc=local\n\n\n\nTo remove a group authorization, execute the following (for example: to remove \u201cAnalysts\u201d group):\n\n\ncbd util delete-ldap-mapping cn=Analysts,ou=Groups,dc=hortonworks,dc=local", 
            "title": "Configure LDAP/AD authentication for Cloudbreak"
        }, 
        {
            "location": "/cb-ldap/index.html#configuring-cloudbreak-for-ldapad-authentication", 
            "text": "By default Cloudbreak uses an internal system as the user store for authentication (enabled by using  CloudFoundry UAA ). If you would like to configure LDAP or Active Directory (AD) external authentication, you need to:     Collect the  following information  about your LDAP/AD setup      Configure Cloudbreak  to work with that LDAP/AD setup", 
            "title": "Configuring Cloudbreak for LDAP/AD authentication"
        }, 
        {
            "location": "/cb-ldap/index.html#ldapad-information", 
            "text": "The following table details the properties and values that you need to know about your LDAP/AD environment on order to use the LDAP/AD with Cloudbreak:      Parameter  Description  Example      base      url  The LDAP url with port  ldap://10.0.3.128:389/    userDn  Enter the root Distinguished Name to search in the directory for users.  cn=Administrator,ou=srv,dc=hortonworks,dc=local    password  Enter your root Distinguished Name password.  MyPassword1234!    searchBase  Enter your LDAP user search base. This defines the location in the directory from which the LDAP search begins.  ou=Users,dc=hortonworks,dc=local    searchFilter  Enter the attribute for which to conduct a search on the user base.  mail={0}    groups      searchBase  Enter your LDAP group search base. This defines the location in the directory from which the LDAP search begins.  ou=Groups,dc=hortonworks,dc=local    groupSearchFilter  Enter the attribute for which to conduct a search on the group base.  member={0}", 
            "title": "LDAP/AD information"
        }, 
        {
            "location": "/cb-ldap/index.html#configuring-cloudbreak-for-ldapad", 
            "text": "There are two parts to configuring Cloudbreak for LDAP/AD:   Configuring LDAP/AD user authentication for Cloudbreak    Configuring LDAP/AD group authorization for Cloudbreak", 
            "title": "Configuring Cloudbreak for LDAP/AD"
        }, 
        {
            "location": "/cb-ldap/index.html#configure-user-authentication", 
            "text": "Configure LDAP/AD user authentication for Cloudbreak by using these steps.   Steps     On the Cloudbreak host, browse to  /var/lib/cloudbreak-deployment .     Create a new file called  uaa-changes.yml .    The name of this file can be customized by setting the following in Profile:  export UAA_SETTINGS_FILE=myldap.yml     In the yml file enter the following using your  LDAP/AD information . Next, save the file and restart Cloudbreak.      spring_profiles: postgresql,ldap\n\nldap:\n  profile:\n    file: ldap/ldap-search-and-bind.xml\n  base:\n    url: ldap://10.0.3.138:389\n    userDn: cn=Administrator,ou=srv,dc=hortonworks,dc=local\n    password: \u2019mypassword\u2019\n    searchBase: ou=Users,dc=hortonworks,dc=local\n    searchFilter: mail={0}\n  groups:\n    file: ldap/ldap-groups-map-to-scopes.xml\n    searchBase: ou=Groups,dc=hortonworks,dc=local\n    searchSubtree: false\n    maxSearchDepth: 1\n    groupSearchFilter: member={0}\n    autoAdd: true", 
            "title": "Configure user authentication"
        }, 
        {
            "location": "/cb-ldap/index.html#configure-group-authorization", 
            "text": "Once user authentication is configured, you need to configure which group(s) can access Cloudbreak. Users (once authenticated) will be granted permission to access Cloudbreak and use the capabilities of Cloudbreak based on their group member. The following describes how to create (i.e. execute-and-map) a group authorization and how to remove (i.e. delete-mapping) an authorization.   To create a group authorization, execute the following (for example: to add \u201cAnalysts\u201d group):  cbd util execute-ldap-mapping cn=Analysts,ou=Groups,dc=hortonworks,dc=local  To remove a group authorization, execute the following (for example: to remove \u201cAnalysts\u201d group):  cbd util delete-ldap-mapping cn=Analysts,ou=Groups,dc=hortonworks,dc=local", 
            "title": "Configure group authorization"
        }, 
        {
            "location": "/security-cb-ssl/index.html", 
            "text": "Add SSL certificate for Cloudbreak web UI\n\n\nBy default Cloudbreak has been configured with a self-signed certificate for access via HTTPS. This is sufficient for many deployments such as trials, development, testing, or staging. However, for production deployments, a trusted certificate is preferred and can be configured in the controller. Follow these steps to configure the cloud controller to use your own trusted certificate. \n\n\nPrerequisites\n\n\nTo use your own certificate, you must have:\n\n\n\n\nA resolvable fully qualified domain name (FQDN) for the controller host IP address. For example, this can be set up in \nAmazon Route 53\n.  \n\n\nA valid SSL certificate for this fully qualified domain name. The certificate can be obtained from a number of certificate providers.  \n\n\n\n\nSteps\n\n\n\n\n\n\nSSH to the Cloudbreak host instance:\n\n\nssh -i mykeypair.pem cloudbreak@[CONTROLLER-IP-ADDRESS]\n\n\n\n\n\n\nMake sure that the target fully qualified domain name (FQDN) which you plan to use for Cloudbreak is resolvable:\n\n\nnslookup [TARGET-CONTROLLER-FQDN]\n\n\nFor example:\n\n\nnslookup hdcloud.example.com\n\n\n\n\n\n\nBrowse to the Cloudbreak deployment directory and edit the \nProfile\n file:\n\n\nvi /var/lib/cloudbreak-deployment/Profile\n\n\n\n\n\n\nReplace the value of the \nPUBLIC_IP\n variable with the \nTARGET-CONTROLLER-FQDN\n value:\n\n\nPUBLIC_IP=[TARGET-CONTROLLER-FQDN]\n\n\n\n\n\n\nCopy your private key and certificate files for the FQDN onto the Cloudbreak host. These files must be placed under \n/var/lib/cloudbreak-deployment/certs/traefik/\n directory.\n\n\n\n\nFile permissions for the private key and certificate files can be set to 600.\n\n\n\n\n\n\n\n\n\n\nFile\n\n\nExample\n\n\n\n\n\n\n\n\n\n\nPRIV-KEY-LOCATION\n\n\n/var/lib/cloudbreak-deployment/certs/traefik/hdcloud.example.com.key\n\n\n\n\n\n\nCERT-LOCATION\n\n\n/var/lib/cloudbreak-deployment/certs/traefik/hdcloud.example.com.crt\n\n\n\n\n\n\n\n\n\n\n\n\nConfigure TLS details in your \nProfile\n by adding the following line at the end of the file.\n\n\n\n\nNotice that \nCERT-LOCATION\n and \nPRIV-KEY-LOCATION\n are file locations from Step 5, starting at the \n/certs/...\n path.\n\n\n\n\nexport CBD_TRAEFIK_TLS=\u201d[CERT-LOCATION],[PRIV-KEY-LOCATION]\u201d\n\n\nFor example:\n\n\nexport CBD_TRAEFIK_TLS=\"/certs/traefik/hdcloud.example.com.crt,/certs/traefik/hdcloud.example.com.key\"\n\n\n\n\n\n\nRestart Cloudbreak deployer:\n\n\ncbd restart\n\n\n\n\n\n\nUsing your web browser, access the Cloudbreak UI using the new resolvable fully qualified domain name.\n\n\n\n\n\n\nConfirm that the connection is SSL-protected and that the certificate used is the certificate that you provided to Cloudbreak.", 
            "title": "Add SSL certificate for Cloudbreak UI"
        }, 
        {
            "location": "/security-cb-ssl/index.html#add-ssl-certificate-for-cloudbreak-web-ui", 
            "text": "By default Cloudbreak has been configured with a self-signed certificate for access via HTTPS. This is sufficient for many deployments such as trials, development, testing, or staging. However, for production deployments, a trusted certificate is preferred and can be configured in the controller. Follow these steps to configure the cloud controller to use your own trusted certificate.   Prerequisites  To use your own certificate, you must have:   A resolvable fully qualified domain name (FQDN) for the controller host IP address. For example, this can be set up in  Amazon Route 53 .    A valid SSL certificate for this fully qualified domain name. The certificate can be obtained from a number of certificate providers.     Steps    SSH to the Cloudbreak host instance:  ssh -i mykeypair.pem cloudbreak@[CONTROLLER-IP-ADDRESS]    Make sure that the target fully qualified domain name (FQDN) which you plan to use for Cloudbreak is resolvable:  nslookup [TARGET-CONTROLLER-FQDN]  For example:  nslookup hdcloud.example.com    Browse to the Cloudbreak deployment directory and edit the  Profile  file:  vi /var/lib/cloudbreak-deployment/Profile    Replace the value of the  PUBLIC_IP  variable with the  TARGET-CONTROLLER-FQDN  value:  PUBLIC_IP=[TARGET-CONTROLLER-FQDN]    Copy your private key and certificate files for the FQDN onto the Cloudbreak host. These files must be placed under  /var/lib/cloudbreak-deployment/certs/traefik/  directory.   File permissions for the private key and certificate files can be set to 600.      File  Example      PRIV-KEY-LOCATION  /var/lib/cloudbreak-deployment/certs/traefik/hdcloud.example.com.key    CERT-LOCATION  /var/lib/cloudbreak-deployment/certs/traefik/hdcloud.example.com.crt       Configure TLS details in your  Profile  by adding the following line at the end of the file.   Notice that  CERT-LOCATION  and  PRIV-KEY-LOCATION  are file locations from Step 5, starting at the  /certs/...  path.   export CBD_TRAEFIK_TLS=\u201d[CERT-LOCATION],[PRIV-KEY-LOCATION]\u201d  For example:  export CBD_TRAEFIK_TLS=\"/certs/traefik/hdcloud.example.com.crt,/certs/traefik/hdcloud.example.com.key\"    Restart Cloudbreak deployer:  cbd restart    Using your web browser, access the Cloudbreak UI using the new resolvable fully qualified domain name.    Confirm that the connection is SSL-protected and that the certificate used is the certificate that you provided to Cloudbreak.", 
            "title": "Add SSL certificate for Cloudbreak web UI"
        }, 
        {
            "location": "/cb-proxy/index.html", 
            "text": "Configuring outbound internet access and proxy\n\n\nDepending on your enterprise requirements, you may have limited or restricted outbound network access and/or require the use of an internet proxy. Installing and configuring Cloudbreak, as well as creating cloud resources and clusters on those resources requires outbound network access to certain destinations, and in some cases must go through a proxy.\n\n\nThis section provides information on the outbound network destinations for Cloudbreak, and instructions on how to configure Cloudbreak to use a proxy for outbound access (if required).\n\n\n\n\n\n\n\n\nScenario\n\n\nDocumentation\n\n\n\n\n\n\n\n\n\n\nMy environment has limited outbound internet access\n\n\nRefer to \nOutbound network access destinations\n for information on network rules.\n\n\n\n\n\n\nMy environment requires use of a proxy for outbound internet access\n\n\nRefer to \nUsing a proxy\n for information on using a proxy with Cloudbreak.\n\n\n\n\n\n\n\n\nOutbound network access destinations\n\n\nTo install and configure Cloudbreak, you will need the following outbound destinations available:\n\n\n\n\n\n    \nDestination\n\n    \nDescription\n \n  \n\n  \n\n    \n*.docker.io\n\n    \nObtain the Docker images for Cloudbreak.\n \n  \n\n  \n\n    \nraw.githubusercontent.com\ngithub.com\ns3.amazonaws.com\n*.cloudfront.net\n\n    \nObtain Cloudbreak dependencies.\n \n  \n\n  \n\n  \ncloudbreak-imagecatalog.s3.amazonaws.com \n The default Cloudbreak image catalog used for VMs. Refer to \nCustom images\n for more information on image catalogs. \n\n  \n\n\n\n\n\nOnce Cloudbreak is installed and configured, you will need the following outbound destinations available in order to communicate with the cloud provider APIs to obtain cloud resources for clusters.\n\n\n\n\n\n    \nCloud provider\n\n    \nCloud provider API destinations\n \n  \n\n  \n\n  \nAmazon Web Services\n\n  \n*.amazonaws.com\n\n \n\n  \n Microsoft Azure \n\n  \n \n*.microsoftonline.com\n*.windows.net\n*.azure.com\n\n   \n\n   \n\n  \n Google Cloud Platform  \n  \n  \n \naccounts.google.com\n*.googleapis.com\n\n  \n\n\n\n\n\nTo install the cluster software, you can: \n\n\na) use the public hosted repositories provided by Hortonworks, or\n\nb) specify your own local hosted repositories when you create a cluster. \n\n\nIf you choose to (a) use the public hosted repositories, be sure to allow outbound access to the following destinations:\n\n\n\n\nprivate-repo-1.hortonworks.com  \n\n\npublic-repo-1.hortonworks.com  \n\n\n\n\nUsing a proxy\n\n\nIn some cases, your environment requires all internet traffic to go through an internet proxy. This section describes the following:\n\n\n\n\nHow to \nset up Cloudbreak to use a proxy\n  \n\n\nHow to \nconfigure your cluster hosts to use a proxy\n  \n\n\n\n\nSet up Cloudbreak to use a proxy\n\n\nUse these steps if you would like to set up Cloudbreak to use your proxy. \n\n\nSteps\n\n\n\n\n\n\nAfter downloading and installing Cloudbreak, configure the Docker daemon to use proxy by adding the following to the Docker service file:\n\n\nEnvironment=\"HTTP_PROXY=http://my-proxy-host:my-proxy-port\" \"NO_PROXY=localhost,127.0.0.1\"\n   \n\n\nFor example:\n\n\nvi /etc/systemd/system/docker.service -\n Environment=\"HTTP_PROXY=http://10.0.2.237:3128\" \"NO_PROXY=localhost,127.0.0.1\"\n\n\nFor more information refer to\n\nDocker docs\n.     \n\n\n\n\n\n\nEnsure that ports 9443 and 8443 are handled as SSL connections in the proxy config.   \n\n\n\n\n\n\nConfigure proxy settings in the Profile file by setting the following variables:  \n\n\n\n\n\n\nHTTP_PROXY_HOST=your-proxy-host\nHTTPS_PROXY_HOST=your-proxy-host\nPROXY_PORT=your-proxy-port\nPROXY_USER=your-proxy-user\nPROXY_PASSWORD=your-proxy-password\n#NON_PROXY_HOSTS\n#HTTPS_PROXYFORCLUSTERCONNECTION=false\n\n\n\nFor example:\n\n\nHTTP_PROXY_HOST=10.0.2.237\nHTTPS_PROXY_HOST=10.0.2.237\nPROXY_PORT=3128\nPROXY_USER=squid\nPROXY_PASSWORD=squid\n#NON_PROXY_HOSTS\n#HTTPS_PROXYFORCLUSTERCONNECTION=false\n\n\n\nSet up clusters to use a proxy\n\n\nUse the following guidelines to find out what steps to perform in order to set up your clusters to use a proxy:  \n\n\n\n\n\n\n\n\nWhat base image are you using?\n\n\nWhere are the platform repositories?\n\n\nWhat to do\n\n\n\n\n\n\n\n\n\n\nDefault\n\n\nPublic\n\n\nUse \nRegister a Proxy\n\n\n\n\n\n\nDefault\n\n\nLocal\n\n\nUse \nRegister a Proxy\n\n\n\n\n\n\nCustom\n\n\nPublic\n\n\nSet up the proxy on your custom image OR use \nRegister a Proxy\n.\n\n\n\n\n\n\nCustom\n\n\nLocal\n\n\nNot required. Skip this section.\n\n\n\n\n\n\n\n\nYou can define a proxy configuration as an external source in Cloudbreak web UI or CLI, and then (optionally) specify to configure that proxy configuration on the hosts that are part of the cluster during cluster create. Refer to \nRegister a Proxy\n for more information.  \n\n\nAdvanced proxy setup scenarios\n\n\nIn some cases, Cloudbreak using the proxy might vary depending on your Cloudbreak -\n cluster deployment. This section describes two scenarios:\n\n\n\n\nScenario 1\n: Cloudbreak needs to go through a proxy to access the Cloud provider APIs (and other public internet resources) but can talk to the cluster hosts directly.  \n\n\nScenario 2\n: Cloudbreak needs to go through a proxy to access the Cloud provider APIs (and other public internet resources) and the cluster hosts.  \n\n\n\n\nScenario 1\n\n\nIn this scenario, Cloudbreak can resolve and communicate with the Ambari Server in the cluster hosts directly. For example, this can be a scenario where Cloudbreak is deployed in the same VPC/VNet as the clusters and will not go through the proxy. However, Cloudbreak will communicate to the public Cloud Provider APIs via the proxy.\n\n\nTo configure this scenario, set this setting in your Profile file:\n\n\nHTTPS_PROXYFORCLUSTERCONNECTION = false\n\n\n\n \n\n\nScenario 2\n\n\nIn this scenario, Cloudbreak will connect to the Ambari Server through the configured proxy. For example, this can be a scenario where Cloudbreak is deployed to a different VPC/VNet than the cluster and must go through a proxy. Communication to the public cloud provider APIs also is via the proxy.\n\n\nTo configure this scenario, set this setting in your Profile file:\n\n\nHTTPS_PROXYFORCLUSTERCONNECTION = true", 
            "title": "Configure outbound internet access and proxy"
        }, 
        {
            "location": "/cb-proxy/index.html#configuring-outbound-internet-access-and-proxy", 
            "text": "Depending on your enterprise requirements, you may have limited or restricted outbound network access and/or require the use of an internet proxy. Installing and configuring Cloudbreak, as well as creating cloud resources and clusters on those resources requires outbound network access to certain destinations, and in some cases must go through a proxy.  This section provides information on the outbound network destinations for Cloudbreak, and instructions on how to configure Cloudbreak to use a proxy for outbound access (if required).     Scenario  Documentation      My environment has limited outbound internet access  Refer to  Outbound network access destinations  for information on network rules.    My environment requires use of a proxy for outbound internet access  Refer to  Using a proxy  for information on using a proxy with Cloudbreak.", 
            "title": "Configuring outbound internet access and proxy"
        }, 
        {
            "location": "/cb-proxy/index.html#outbound-network-access-destinations", 
            "text": "To install and configure Cloudbreak, you will need the following outbound destinations available:   \n     Destination \n     Description  \n   \n   \n     *.docker.io \n     Obtain the Docker images for Cloudbreak.  \n   \n   \n     raw.githubusercontent.com github.com s3.amazonaws.com *.cloudfront.net \n     Obtain Cloudbreak dependencies.  \n   \n   \n   cloudbreak-imagecatalog.s3.amazonaws.com   The default Cloudbreak image catalog used for VMs. Refer to  Custom images  for more information on image catalogs.  \n     Once Cloudbreak is installed and configured, you will need the following outbound destinations available in order to communicate with the cloud provider APIs to obtain cloud resources for clusters.   \n     Cloud provider \n     Cloud provider API destinations  \n   \n   \n   Amazon Web Services \n   *.amazonaws.com \n  \n    Microsoft Azure  \n     *.microsoftonline.com *.windows.net *.azure.com \n    \n    \n    Google Cloud Platform     \n     accounts.google.com *.googleapis.com \n     To install the cluster software, you can:   a) use the public hosted repositories provided by Hortonworks, or \nb) specify your own local hosted repositories when you create a cluster.   If you choose to (a) use the public hosted repositories, be sure to allow outbound access to the following destinations:   private-repo-1.hortonworks.com    public-repo-1.hortonworks.com", 
            "title": "Outbound network access destinations"
        }, 
        {
            "location": "/cb-proxy/index.html#using-a-proxy", 
            "text": "In some cases, your environment requires all internet traffic to go through an internet proxy. This section describes the following:   How to  set up Cloudbreak to use a proxy     How to  configure your cluster hosts to use a proxy", 
            "title": "Using a proxy"
        }, 
        {
            "location": "/cb-proxy/index.html#set-up-cloudbreak-to-use-a-proxy", 
            "text": "Use these steps if you would like to set up Cloudbreak to use your proxy.   Steps    After downloading and installing Cloudbreak, configure the Docker daemon to use proxy by adding the following to the Docker service file:  Environment=\"HTTP_PROXY=http://my-proxy-host:my-proxy-port\" \"NO_PROXY=localhost,127.0.0.1\"      For example:  vi /etc/systemd/system/docker.service -  Environment=\"HTTP_PROXY=http://10.0.2.237:3128\" \"NO_PROXY=localhost,127.0.0.1\"  For more information refer to Docker docs .         Ensure that ports 9443 and 8443 are handled as SSL connections in the proxy config.       Configure proxy settings in the Profile file by setting the following variables:      HTTP_PROXY_HOST=your-proxy-host\nHTTPS_PROXY_HOST=your-proxy-host\nPROXY_PORT=your-proxy-port\nPROXY_USER=your-proxy-user\nPROXY_PASSWORD=your-proxy-password\n#NON_PROXY_HOSTS\n#HTTPS_PROXYFORCLUSTERCONNECTION=false  For example:  HTTP_PROXY_HOST=10.0.2.237\nHTTPS_PROXY_HOST=10.0.2.237\nPROXY_PORT=3128\nPROXY_USER=squid\nPROXY_PASSWORD=squid\n#NON_PROXY_HOSTS\n#HTTPS_PROXYFORCLUSTERCONNECTION=false", 
            "title": "Set up Cloudbreak to use a proxy"
        }, 
        {
            "location": "/cb-proxy/index.html#set-up-clusters-to-use-a-proxy", 
            "text": "Use the following guidelines to find out what steps to perform in order to set up your clusters to use a proxy:       What base image are you using?  Where are the platform repositories?  What to do      Default  Public  Use  Register a Proxy    Default  Local  Use  Register a Proxy    Custom  Public  Set up the proxy on your custom image OR use  Register a Proxy .    Custom  Local  Not required. Skip this section.     You can define a proxy configuration as an external source in Cloudbreak web UI or CLI, and then (optionally) specify to configure that proxy configuration on the hosts that are part of the cluster during cluster create. Refer to  Register a Proxy  for more information.", 
            "title": "Set up clusters to use a proxy"
        }, 
        {
            "location": "/cb-proxy/index.html#advanced-proxy-setup-scenarios", 
            "text": "In some cases, Cloudbreak using the proxy might vary depending on your Cloudbreak -  cluster deployment. This section describes two scenarios:   Scenario 1 : Cloudbreak needs to go through a proxy to access the Cloud provider APIs (and other public internet resources) but can talk to the cluster hosts directly.    Scenario 2 : Cloudbreak needs to go through a proxy to access the Cloud provider APIs (and other public internet resources) and the cluster hosts.     Scenario 1  In this scenario, Cloudbreak can resolve and communicate with the Ambari Server in the cluster hosts directly. For example, this can be a scenario where Cloudbreak is deployed in the same VPC/VNet as the clusters and will not go through the proxy. However, Cloudbreak will communicate to the public Cloud Provider APIs via the proxy.  To configure this scenario, set this setting in your Profile file:  HTTPS_PROXYFORCLUSTERCONNECTION = false     Scenario 2  In this scenario, Cloudbreak will connect to the Ambari Server through the configured proxy. For example, this can be a scenario where Cloudbreak is deployed to a different VPC/VNet than the cluster and must go through a proxy. Communication to the public cloud provider APIs also is via the proxy.  To configure this scenario, set this setting in your Profile file:  HTTPS_PROXYFORCLUSTERCONNECTION = true", 
            "title": "Advanced proxy setup scenarios"
        }, 
        {
            "location": "/security-cb-inbound/index.html", 
            "text": "Restrict inbound access to clusters\n\n\nWe recommend that after launching Cloudbreak you set CB_DEFAULT_GATEWAY_CIDR in your Cloudbreak's Profile file.\nWhen you launch a cluster, and Cloudbreak proposes security groups, this CIDR will be used\nfor the Cloudbreak to the Cluster master node (i.e. the host with Ambari Server) with this IP. This limits access\nfrom Cloudbreak to this cluster name for ports 9443 and 22 for Cloudbreak communication and management of the cluster.\n\n\nSteps\n \n\n\n\n\n\n\nSet CB_DEFAULT_GATEWAY_CIDR to the CIDR address range which is used by Cloudbreak to communicate with the cluster:\n\n\nexport CB_DEFAULT_GATEWAY_CIDR=14.15.16.17/32\n\n\nOr, if your Cloudbreak communicates with the cluster through multiple addresses, set multiple addresses separated with a comma:\n\n\nexport CB_DEFAULT_GATEWAY_CIDR=14.15.16.17/32,18.17.16.15/32\n\n\n\n\n\n\nIf Cloudbreak has already been started, restart it using \ncbd restart\n.     \n\n\n\n\n\n\nWhen CB_DEFAULT_GATEWAY_CIDR is set, two additional rules are added to your Ambari node security group: (1) port 9443 open to your Cloudbreak IP, and (2) port 22 open to your Cloudbreak IP. You can view and edit these default rules in the create cluster wizard.", 
            "title": "Restrict inbound access to clusters"
        }, 
        {
            "location": "/security-cb-inbound/index.html#restrict-inbound-access-to-clusters", 
            "text": "We recommend that after launching Cloudbreak you set CB_DEFAULT_GATEWAY_CIDR in your Cloudbreak's Profile file.\nWhen you launch a cluster, and Cloudbreak proposes security groups, this CIDR will be used\nfor the Cloudbreak to the Cluster master node (i.e. the host with Ambari Server) with this IP. This limits access\nfrom Cloudbreak to this cluster name for ports 9443 and 22 for Cloudbreak communication and management of the cluster.  Steps      Set CB_DEFAULT_GATEWAY_CIDR to the CIDR address range which is used by Cloudbreak to communicate with the cluster:  export CB_DEFAULT_GATEWAY_CIDR=14.15.16.17/32  Or, if your Cloudbreak communicates with the cluster through multiple addresses, set multiple addresses separated with a comma:  export CB_DEFAULT_GATEWAY_CIDR=14.15.16.17/32,18.17.16.15/32    If Cloudbreak has already been started, restart it using  cbd restart .         When CB_DEFAULT_GATEWAY_CIDR is set, two additional rules are added to your Ambari node security group: (1) port 9443 open to your Cloudbreak IP, and (2) port 22 open to your Cloudbreak IP. You can view and edit these default rules in the create cluster wizard.", 
            "title": "Restrict inbound access to clusters"
        }, 
        {
            "location": "/security-cb-customdom/index.html", 
            "text": "Configure sccess from custom domains\n\n\nCloudbreak deployer, which uses UAA as an identity provider, supports multitenancy. In UAA, multitenancy is managed through identity zones. An identity zone is accessed through a unique subdomain. For example, if the standard UAA responds to \nhttps://uaa.10.244.0.34.xip.io\n, a zone on this UAA can be accessed through a unique subdomain \nhttps://testzone1.uaa.10.244.0.34.xip.io\n.\n\n\nIf you want to use a custom domain for your identity or deployment, add the \nUAA_ZONE_DOMAIN\n line to your \nProfile\n:\n\n\nexport UAA_ZONE_DOMAIN=my-subdomain.example.com\n\n\n\nThis variable is necessary for UAA to identify which zone provider should handle the requests that arrive to that domain.", 
            "title": "Configure access from custom domains"
        }, 
        {
            "location": "/security-cb-customdom/index.html#configure-sccess-from-custom-domains", 
            "text": "Cloudbreak deployer, which uses UAA as an identity provider, supports multitenancy. In UAA, multitenancy is managed through identity zones. An identity zone is accessed through a unique subdomain. For example, if the standard UAA responds to  https://uaa.10.244.0.34.xip.io , a zone on this UAA can be accessed through a unique subdomain  https://testzone1.uaa.10.244.0.34.xip.io .  If you want to use a custom domain for your identity or deployment, add the  UAA_ZONE_DOMAIN  line to your  Profile :  export UAA_ZONE_DOMAIN=my-subdomain.example.com  This variable is necessary for UAA to identify which zone provider should handle the requests that arrive to that domain.", 
            "title": "Configure sccess from custom domains"
        }, 
        {
            "location": "/cb-db/index.html", 
            "text": "Configuring external Cloudbreak database\n\n\nBy default, Cloudbreak uses an embedded PostgreSQL database to persist data related to Cloudbreak\nconfiguration, setup and so on. For a production Cloudbreak deployment, we suggest that you configure an external database. \n\n\nSupported databases\n\n\nAn embedded PostgreSQL 9.6.1 database is used by Cloudbreak by default. If you would like to\nuse an external database for Cloudbreak, you may use the following supported database types and versions: \n\n\n\n\n\n\n\n\nDatabase type\n\n\nSupported version\n\n\n\n\n\n\n\n\n\n\nExternal PostgreSQL\n\n\n9.6.1 or above\n\n\n\n\n\n\nExternal MySQL\n\n\nNot supported\n\n\n\n\n\n\nExternal MariaDB\n\n\nNot supported\n\n\n\n\n\n\nExternal Oracle\n\n\nNot supported\n\n\n\n\n\n\nExternal SQL Server\n\n\nNot supported\n\n\n\n\n\n\n\n\nConfigure external Cloudbreak database\n\n\nThe following section describes how to use Cloudbreak with an existing external database, other than\nthe embedded PostgreSQL database instance that Cloudbreak uses by default. To configure an external PostgreSQL database for Cloudbreak, perform these steps. \n\n\nSteps\n\n\n\n\n\n\nOn your Cloudbreak host machine, set the following environment variables according to the settings of your external database: \n\n\nexport DATABASE_HOST=my.database.host\nexport DATABASE_PORT=5432\nexport DATABASE_USERNAME=admin\nexport DATABASE_PASSWORD=Admin123!\n\n\n\n\n\n\n\nOn your external database, create three databases: \ncbdb, uaadb, periscopedb\n. You can create these databases using the \ncreatedb\n utility with the following commands:\n\n\ncreatedb -h $DATABASE_HOST -p $DATABASE_PORT -U $DATABASE_USERNAME cbdb\ncreatedb -h $DATABASE_HOST -p $DATABASE_PORT -U $DATABASE_USERNAME uaadb\ncreatedb -h $DATABASE_HOST -p $DATABASE_PORT -U $DATABASE_USERNAME periscopedb\n\n\nFor more information refer to the \nPostgreSQL documentation\n. \n\nAlternatively, you can log in to the management interface of your external database and execute \ncreate database\n commands directly. \n\n\n\n\n\n\nSet the following variables in your Cloudbreak Profile file. Modify the database parameters according to your external database.\n\n\nexport DATABASE_HOST=my.database.host\nexport DATABASE_PORT=5432\nexport DATABASE_USERNAME=admin\nexport DATABASE_PASSWORD=Admin123!\n\n\nexport CB_DB_PORT_5432_TCP_ADDR=$DATABASE_HOST\nexport CB_DB_PORT_5432_TCP_PORT=$DATABASE_PORT\nexport CB_DB_ENV_USER=$DATABASE_USERNAME\nexport CB_DB_ENV_PASS=$DATABASE_PASSWORD\nexport CB_DB_ENV_DB=cbdb\n\n\nexport PERISCOPE_DB_TCP_ADDR=$DATABASE_HOST\nexport PERISCOPE_DB_TCP_PORT=$DATABASE_PORT\nexport PERISCOPE_DB_USER=$DATABASE_USERNAME\nexport PERISCOPE_DB_PASS=$DATABASE_PASSWORD\nexport PERISCOPE_DB_NAME=periscopedb\nexport PERISCOPE_DB_SCHEMA_NAME=public\n\n\nexport IDENTITY_DB_URL=$DATABASE_HOST:$DATABASE_PORT\nexport IDENTITY_DB_USER=$DATABASE_USERNAME\nexport IDENTITY_DB_PASS=$DATABASE_PASSWORD\nexport IDENTITY_DB_NAME=uaadb\n\n\n\n\n\n\nRestart Cloudbreak application by using the \ncbd restart\n command. \n\n\n\n\n\n\nAfter performing these steps, your external database will be used for Cloudbreak instead of the built-in database. \n\n\n      \n\nData Migration\n  \n\n If you want to migrate your existing data (such as  blueprints, recipes, and so on) from the embedded database to the external one, then after completing these steps, you should also  \ncreate a backup\n of your original database and then \nrestore\n it in the external database.", 
            "title": "Configure external Cloudbreak database"
        }, 
        {
            "location": "/cb-db/index.html#configuring-external-cloudbreak-database", 
            "text": "By default, Cloudbreak uses an embedded PostgreSQL database to persist data related to Cloudbreak\nconfiguration, setup and so on. For a production Cloudbreak deployment, we suggest that you configure an external database.", 
            "title": "Configuring external Cloudbreak database"
        }, 
        {
            "location": "/cb-db/index.html#supported-databases", 
            "text": "An embedded PostgreSQL 9.6.1 database is used by Cloudbreak by default. If you would like to\nuse an external database for Cloudbreak, you may use the following supported database types and versions:      Database type  Supported version      External PostgreSQL  9.6.1 or above    External MySQL  Not supported    External MariaDB  Not supported    External Oracle  Not supported    External SQL Server  Not supported", 
            "title": "Supported databases"
        }, 
        {
            "location": "/cb-db/index.html#configure-external-cloudbreak-database", 
            "text": "The following section describes how to use Cloudbreak with an existing external database, other than\nthe embedded PostgreSQL database instance that Cloudbreak uses by default. To configure an external PostgreSQL database for Cloudbreak, perform these steps.   Steps    On your Cloudbreak host machine, set the following environment variables according to the settings of your external database:   export DATABASE_HOST=my.database.host\nexport DATABASE_PORT=5432\nexport DATABASE_USERNAME=admin\nexport DATABASE_PASSWORD=Admin123!    On your external database, create three databases:  cbdb, uaadb, periscopedb . You can create these databases using the  createdb  utility with the following commands:  createdb -h $DATABASE_HOST -p $DATABASE_PORT -U $DATABASE_USERNAME cbdb\ncreatedb -h $DATABASE_HOST -p $DATABASE_PORT -U $DATABASE_USERNAME uaadb\ncreatedb -h $DATABASE_HOST -p $DATABASE_PORT -U $DATABASE_USERNAME periscopedb  For more information refer to the  PostgreSQL documentation .  \nAlternatively, you can log in to the management interface of your external database and execute  create database  commands directly.     Set the following variables in your Cloudbreak Profile file. Modify the database parameters according to your external database.  export DATABASE_HOST=my.database.host\nexport DATABASE_PORT=5432\nexport DATABASE_USERNAME=admin\nexport DATABASE_PASSWORD=Admin123!  export CB_DB_PORT_5432_TCP_ADDR=$DATABASE_HOST\nexport CB_DB_PORT_5432_TCP_PORT=$DATABASE_PORT\nexport CB_DB_ENV_USER=$DATABASE_USERNAME\nexport CB_DB_ENV_PASS=$DATABASE_PASSWORD\nexport CB_DB_ENV_DB=cbdb  export PERISCOPE_DB_TCP_ADDR=$DATABASE_HOST\nexport PERISCOPE_DB_TCP_PORT=$DATABASE_PORT\nexport PERISCOPE_DB_USER=$DATABASE_USERNAME\nexport PERISCOPE_DB_PASS=$DATABASE_PASSWORD\nexport PERISCOPE_DB_NAME=periscopedb\nexport PERISCOPE_DB_SCHEMA_NAME=public  export IDENTITY_DB_URL=$DATABASE_HOST:$DATABASE_PORT\nexport IDENTITY_DB_USER=$DATABASE_USERNAME\nexport IDENTITY_DB_PASS=$DATABASE_PASSWORD\nexport IDENTITY_DB_NAME=uaadb    Restart Cloudbreak application by using the  cbd restart  command.     After performing these steps, your external database will be used for Cloudbreak instead of the built-in database.          Data Migration     If you want to migrate your existing data (such as  blueprints, recipes, and so on) from the embedded database to the external one, then after completing these steps, you should also   create a backup  of your original database and then  restore  it in the external database.", 
            "title": "Configure external Cloudbreak database"
        }, 
        {
            "location": "/cb-disable-provider/index.html", 
            "text": "Disable providers\n\n\nIf you are planning to use Cloudbreak with a specific cloud provider or a specific set of cloud providers, you may want to disable the remaining providers. For example, if you are planning to use Cloudbreak with Azure only, you may want to disable AWS, Google Cloud, and OpenStack. \n\n\nSteps\n\n\n\n\n\n\nNavigate to the Cloudbreak deployment directory and edit Profile. For example:\n\n\ncd /var/lib/cloudbreak-deployment/\nvi Profile\n\n\n\n\n\n\nAdd the following entry, setting it to the provider that you  would like to see. For example, if you would like to see Azure only, set this to \"AZURE\":\n\n\nexport CB_ENABLEDPLATFORMS=AZURE\n\n\nAccepted values are:\n\n\n\n\nAZURE\n\n\nAWS\n\n\nGCP\n\n\nOPENSTACK\n\n\n\n\nAny combination of platforms can be used; for example if you would like to see AWS and OpenStack, then use:\n\n\nexport CB_ENABLEDPLATFORMS=AWS,OPENSTACK\n\n\nIf you want to reverse the change and see all providers, then either delete CB_ENABLEDPLATFORMS from the Profile or add the following: \n\n\nexport CB_ENABLEDPLATFORMS=AZURE,AWS,GCP,OPENSTACK\n\n\n\n\n\n\nRestart Cloudbreak by using \ncbd restart\n.", 
            "title": "Disable providers"
        }, 
        {
            "location": "/cb-disable-provider/index.html#disable-providers", 
            "text": "If you are planning to use Cloudbreak with a specific cloud provider or a specific set of cloud providers, you may want to disable the remaining providers. For example, if you are planning to use Cloudbreak with Azure only, you may want to disable AWS, Google Cloud, and OpenStack.   Steps    Navigate to the Cloudbreak deployment directory and edit Profile. For example:  cd /var/lib/cloudbreak-deployment/\nvi Profile    Add the following entry, setting it to the provider that you  would like to see. For example, if you would like to see Azure only, set this to \"AZURE\":  export CB_ENABLEDPLATFORMS=AZURE  Accepted values are:   AZURE  AWS  GCP  OPENSTACK   Any combination of platforms can be used; for example if you would like to see AWS and OpenStack, then use:  export CB_ENABLEDPLATFORMS=AWS,OPENSTACK  If you want to reverse the change and see all providers, then either delete CB_ENABLEDPLATFORMS from the Profile or add the following:   export CB_ENABLEDPLATFORMS=AZURE,AWS,GCP,OPENSTACK    Restart Cloudbreak by using  cbd restart .", 
            "title": "Disable providers"
        }, 
        {
            "location": "/cb-ports/index.html", 
            "text": "Change default Cloudbreak ports\n\n\nBy default, Cloudbreak uses ports 80 (HTTP) and 443 (HTTPS) to access the Cloudbreak server (for the web UI and for the CLI). To change these port numbers, you must edit the Profile file on your Cloudbreak host. \n\n\nCloudbreak should not be running when you change the port numbers. Edit Profile either before you start Cloudbreak the first time or stop Cloudbreak before editing the file.\n\n\nSteps\n  \n\n\n\n\n\n\nNavigate to the Cloudbreak deployment directory (typically \n/var/lib/cloudbreak-deployment\n) and open the Profile file with a text editor. \n\n\n\n\n\n\nAdd one or both of the following parameters, setting them to the port numbers that you want to use:\n\n\nexport PUBLIC_HTTP_PORT=111\nexport PUBLIC_HTTPS_PORT=222\n\n\n\n\n\n\nStart or restart Cloudbreak by using \ncbd start\n or \ncbd restart\n. \n\n\n\n\n\n\nThis change affects Cloudbreak CLI configuration. When \nconfiguring the CLI\n, you must provide these ports as part of the server URL. For example: \n\n\ncb configure --server http://cb.server.address:111 --username  test@hortonworks.com\ncb configure --server https://cb.server.address:222 --username  test@hortonworks.com", 
            "title": "Change default Cloudbreak ports"
        }, 
        {
            "location": "/cb-ports/index.html#change-default-cloudbreak-ports", 
            "text": "By default, Cloudbreak uses ports 80 (HTTP) and 443 (HTTPS) to access the Cloudbreak server (for the web UI and for the CLI). To change these port numbers, you must edit the Profile file on your Cloudbreak host.   Cloudbreak should not be running when you change the port numbers. Edit Profile either before you start Cloudbreak the first time or stop Cloudbreak before editing the file.  Steps       Navigate to the Cloudbreak deployment directory (typically  /var/lib/cloudbreak-deployment ) and open the Profile file with a text editor.     Add one or both of the following parameters, setting them to the port numbers that you want to use:  export PUBLIC_HTTP_PORT=111\nexport PUBLIC_HTTPS_PORT=222    Start or restart Cloudbreak by using  cbd start  or  cbd restart .     This change affects Cloudbreak CLI configuration. When  configuring the CLI , you must provide these ports as part of the server URL. For example:   cb configure --server http://cb.server.address:111 --username  test@hortonworks.com\ncb configure --server https://cb.server.address:222 --username  test@hortonworks.com", 
            "title": "Change default Cloudbreak ports"
        }, 
        {
            "location": "/cb-profile/index.html", 
            "text": "Customizing Cloudbreak Profile file\n\n\nCloudbreak deployer configuration is based on environment variables.  \n\n\nDuring startup, Cloudbreak deployer tries to determine the underlying infrastructure and then sets required environment variables with appropriate default values. If these environment variables are not sufficient for your use case, you can set additional environment variables in your \nProfile\n file. \n\n\nSet Profile variables\n\n\nTo set environment variables relevant for Cloudbreak Deployer, add them to a file called \nProfile\n located in the Cloudbreak deployment directory (typically \n/var/lib/cloudbreak-deployment\n).\n\n\nThe \nProfile\n file is sourced, so you can use the usual syntax to set configuration values:\n\n\nexport MY_VAR=some_value\nexport MY_OTHER_VAR=another_value \n\n\n\n\nAfter changing a property, you must regenerate the config file and restart the application by using \ncbd restart\n.\n\n\nCheck available Profile variables\n\n\nTo see all available environment variables with their default values, use:\n\n\ncbd env show\n\n\n\n\nCreate environment-specific profiles\n\n\nIf you would like to use a different versions of Cloudbreak for prod and qa profile, you must create two environment specific configurations that can be sourced. For example:\n\n\n\n\nProfile.prod  \n\n\nProfile.qa   \n\n\n\n\nFor example, to create and use a prod profile, you need to:\n\n\n\n\nCreate a file called \nProfile.prod\n  \n\n\nWrite the environment-specific \nexport DOCKER_TAG_CLOUDBREAK=0.3.99\n into \nProfile.prod\n to specify Docker image.  \n\n\nSet the environment variable: \nCBD_DEFAULT_PROFILE=prod\n  \n\n\n\n\nTo use the prod specific profile once, set:  \n\n\nCBD_DEFAULT_PROFILE=prod cbd some_commands\n\n\n\nTo permanently use the prod profile, set \nexport CBD_DEFAULT_PROFILE=prod\n in your \n.bash_profile\n.\n\n\nSecure the Profile file\n\n\nBefore starting Cloudbreak for the first time, configure the Profile file as directed below. Changes are applied during startup so a restart (\ncbd restart\n) is required after each change.\n\n\n\n\n\n\nExecute the following command in the directory where you want to store Cloudbreak-related files:\n\n\n\necho export PUBLIC_IP=[the ip or hostname to bind] \n Profile\n\n\n\n\n\n\n\nAfter you have a base Profile file, add the following custom properties to it:\n\n\n\nexport UAA_DEFAULT_SECRET='[custom secret]'\nexport UAA_DEFAULT_USER_EMAIL='[default admin email address]'\nexport UAA_DEFAULT_USER_PW='[default admin password]'\nexport UAA_DEFAULT_USER_FIRSTNAME='[default admin first name]'\nexport UAA_DEFAULT_USER_LASTNAME='[default admin last name]'\n\n\n\nCloudbreak has additional secrets which by default inherit their values from \nUAA_DEFAULT_SECRET\n. Instead of using the default, you can define different values in the Profile for each of these service clients:\n\n\n\nexport UAA_CLOUDBREAK_SECRET='[cloudbreak secret]'\nexport UAA_PERISCOPE_SECRET='[auto scaling secret]'\nexport UAA_ULUWATU_SECRET='[web ui secret]'\nexport UAA_SULTANS_SECRET='[authenticator secret]'\n\n\n\nYou can change these secrets at any time, except \nUAA_CLOUDBREAK_SECRET\n which is used to encrypt sensitive information at database level. \n\n\nUAA_DEFAULT_USER_PW\n is stored in plain text format, but if \nUAA_DEFAULT_USER_PW\n is missing from the Profile, it gets a default value. Because default password is not an option, if you set an empty password explicitly in the Profile Cloudbreak deployer will ask for password all the time when it is needed for the operation.\n\n\n\nexport UAA_DEFAULT_USER_PW=''\n\n\n\nIn this case, Cloudbreak deployer wouldn't be able to add the default user, so you have to do it manually by executing the following command:\n\n\n\ncbd util add-default-user\n\n\n\n\n\n\n\nFor more information about setting environment variables in Profile, refer to \nSet Profile variables\n.\n\n\nChange SMTP parameters\n\n\nIf you want to change SMTP parameters, add them your \nProfile\n.\n\n\nThe default values of the SMTP parameters are:\n\n\nexport CLOUDBREAK_SMTP_SENDER_USERNAME=\nexport CLOUDBREAK_SMTP_SENDER_PASSWORD=\nexport CLOUDBREAK_SMTP_SENDER_HOST=\nexport CLOUDBREAK_SMTP_SENDER_PORT=25\nexport CLOUDBREAK_SMTP_SENDER_FROM=\nexport CLOUDBREAK_SMTP_AUTH=true\nexport CLOUDBREAK_SMTP_STARTTLS_ENABLE=true\nexport CLOUDBREAK_SMTP_TYPE=smtp\n\n\n\n\nFor example: \n\n\nexport CLOUDBREAK_SMTP_SENDER_USERNAME='myemail@gmail.com'  \nexport CLOUDBREAK_SMTP_SENDER_PASSWORD='Mypassword123'  \nexport CLOUDBREAK_SMTP_SENDER_HOST='smtp.gmail.com'  \nexport CLOUDBREAK_SMTP_SENDER_PORT=25  \nexport CLOUDBREAK_SMTP_SENDER_FROM='myemail@gmail.com'  \nexport CLOUDBREAK_SMTP_AUTH=true  \nexport CLOUDBREAK_SMTP_STARTTLS_ENABLE=true  \nexport CLOUDBREAK_SMTP_TYPE=smtp  \n\n\n\n\n\n\nThe example assumes you are using gmail. You should use the settings appropriate for your SMTP server.\n\n\n\n\nIf your SMTP server uses SMTPS, you must set the protocol in your \nProfile\n to smtps:\n\n\nexport CLOUDBREAK_SMTP_TYPE=smtps\n\n\n\n\nConfigure Consul\n\n\nCloudbreak uses \nConsul\n for DNS resolution. All Cloudbreak related services are registered as someservice.service.consul.\n\n\nConsul\u2019s built-in DNS server is able to fallback on another DNS server. This option is called \n-recursor\n. Cloudbreak Deployer first tries to discover the DNS settings of the host by looking for nameserver entry in the \n/etc/resolv.conf\n file. If it finds one, consul will use it as a recursor. Otherwise, it will use \n8.8.8.8\n.\n\n\nFor a full list of available consul config options, refer to \nConsul documentation\n.\n\n\nTo pass any additional Consul configuration, define the \nDOCKER_CONSUL_OPTIONS\n variable in the Profile file.\n\n\nAdd tags in Profile (AWS)\n\n\nIn order to differentiate launched instances, you can optionally define custom tags for your AWS resources deployed by Cloudbreak. \n\n\n\n\n\n\nIf you want just one custom tag for your CloudFormation resources, set this variable in the \nProfile\n:\n\n\nexport CB_AWS_DEFAULT_CF_TAG=mytagcontent\n\n\nIn this example, the name of the tag will be \nCloudbreakId\n and the value will be \nmytagcontent\n.\n\n\n\n\n\n\nIf you prefer to customize the tag name, set this variable:\n\n\nexport CB_AWS_CUSTOM_CF_TAGS=mytagname:mytagvalue\n\n\nIn this example the name of the tag will be \nmytagname\n and the value will be \nmytagvalue\n. \n\n\n\n\n\n\nYou can specify a list of tags with a comma separated list: \n\n\nexport CB_AWS_CUSTOM_CF_TAGS=tag1:value1,tag2:value2,tag3:value3", 
            "title": "Modify settings in Cloudbreak Profile"
        }, 
        {
            "location": "/cb-profile/index.html#customizing-cloudbreak-profile-file", 
            "text": "Cloudbreak deployer configuration is based on environment variables.    During startup, Cloudbreak deployer tries to determine the underlying infrastructure and then sets required environment variables with appropriate default values. If these environment variables are not sufficient for your use case, you can set additional environment variables in your  Profile  file.", 
            "title": "Customizing Cloudbreak Profile file"
        }, 
        {
            "location": "/cb-profile/index.html#set-profile-variables", 
            "text": "To set environment variables relevant for Cloudbreak Deployer, add them to a file called  Profile  located in the Cloudbreak deployment directory (typically  /var/lib/cloudbreak-deployment ).  The  Profile  file is sourced, so you can use the usual syntax to set configuration values:  export MY_VAR=some_value\nexport MY_OTHER_VAR=another_value   After changing a property, you must regenerate the config file and restart the application by using  cbd restart .", 
            "title": "Set Profile variables"
        }, 
        {
            "location": "/cb-profile/index.html#check-available-profile-variables", 
            "text": "To see all available environment variables with their default values, use:  cbd env show", 
            "title": "Check available Profile variables"
        }, 
        {
            "location": "/cb-profile/index.html#create-environment-specific-profiles", 
            "text": "If you would like to use a different versions of Cloudbreak for prod and qa profile, you must create two environment specific configurations that can be sourced. For example:   Profile.prod    Profile.qa      For example, to create and use a prod profile, you need to:   Create a file called  Profile.prod     Write the environment-specific  export DOCKER_TAG_CLOUDBREAK=0.3.99  into  Profile.prod  to specify Docker image.    Set the environment variable:  CBD_DEFAULT_PROFILE=prod      To use the prod specific profile once, set:    CBD_DEFAULT_PROFILE=prod cbd some_commands  To permanently use the prod profile, set  export CBD_DEFAULT_PROFILE=prod  in your  .bash_profile .", 
            "title": "Create environment-specific profiles"
        }, 
        {
            "location": "/cb-profile/index.html#secure-the-profile-file", 
            "text": "Before starting Cloudbreak for the first time, configure the Profile file as directed below. Changes are applied during startup so a restart ( cbd restart ) is required after each change.    Execute the following command in the directory where you want to store Cloudbreak-related files:  \necho export PUBLIC_IP=[the ip or hostname to bind]   Profile    After you have a base Profile file, add the following custom properties to it:  \nexport UAA_DEFAULT_SECRET='[custom secret]'\nexport UAA_DEFAULT_USER_EMAIL='[default admin email address]'\nexport UAA_DEFAULT_USER_PW='[default admin password]'\nexport UAA_DEFAULT_USER_FIRSTNAME='[default admin first name]'\nexport UAA_DEFAULT_USER_LASTNAME='[default admin last name]'  Cloudbreak has additional secrets which by default inherit their values from  UAA_DEFAULT_SECRET . Instead of using the default, you can define different values in the Profile for each of these service clients:  \nexport UAA_CLOUDBREAK_SECRET='[cloudbreak secret]'\nexport UAA_PERISCOPE_SECRET='[auto scaling secret]'\nexport UAA_ULUWATU_SECRET='[web ui secret]'\nexport UAA_SULTANS_SECRET='[authenticator secret]'  You can change these secrets at any time, except  UAA_CLOUDBREAK_SECRET  which is used to encrypt sensitive information at database level.   UAA_DEFAULT_USER_PW  is stored in plain text format, but if  UAA_DEFAULT_USER_PW  is missing from the Profile, it gets a default value. Because default password is not an option, if you set an empty password explicitly in the Profile Cloudbreak deployer will ask for password all the time when it is needed for the operation.  \nexport UAA_DEFAULT_USER_PW=''  In this case, Cloudbreak deployer wouldn't be able to add the default user, so you have to do it manually by executing the following command:  \ncbd util add-default-user    For more information about setting environment variables in Profile, refer to  Set Profile variables .", 
            "title": "Secure the Profile file"
        }, 
        {
            "location": "/cb-profile/index.html#change-smtp-parameters", 
            "text": "If you want to change SMTP parameters, add them your  Profile .  The default values of the SMTP parameters are:  export CLOUDBREAK_SMTP_SENDER_USERNAME=\nexport CLOUDBREAK_SMTP_SENDER_PASSWORD=\nexport CLOUDBREAK_SMTP_SENDER_HOST=\nexport CLOUDBREAK_SMTP_SENDER_PORT=25\nexport CLOUDBREAK_SMTP_SENDER_FROM=\nexport CLOUDBREAK_SMTP_AUTH=true\nexport CLOUDBREAK_SMTP_STARTTLS_ENABLE=true\nexport CLOUDBREAK_SMTP_TYPE=smtp  For example:   export CLOUDBREAK_SMTP_SENDER_USERNAME='myemail@gmail.com'  \nexport CLOUDBREAK_SMTP_SENDER_PASSWORD='Mypassword123'  \nexport CLOUDBREAK_SMTP_SENDER_HOST='smtp.gmail.com'  \nexport CLOUDBREAK_SMTP_SENDER_PORT=25  \nexport CLOUDBREAK_SMTP_SENDER_FROM='myemail@gmail.com'  \nexport CLOUDBREAK_SMTP_AUTH=true  \nexport CLOUDBREAK_SMTP_STARTTLS_ENABLE=true  \nexport CLOUDBREAK_SMTP_TYPE=smtp     The example assumes you are using gmail. You should use the settings appropriate for your SMTP server.   If your SMTP server uses SMTPS, you must set the protocol in your  Profile  to smtps:  export CLOUDBREAK_SMTP_TYPE=smtps", 
            "title": "Change SMTP parameters"
        }, 
        {
            "location": "/cb-profile/index.html#configure-consul", 
            "text": "Cloudbreak uses  Consul  for DNS resolution. All Cloudbreak related services are registered as someservice.service.consul.  Consul\u2019s built-in DNS server is able to fallback on another DNS server. This option is called  -recursor . Cloudbreak Deployer first tries to discover the DNS settings of the host by looking for nameserver entry in the  /etc/resolv.conf  file. If it finds one, consul will use it as a recursor. Otherwise, it will use  8.8.8.8 .  For a full list of available consul config options, refer to  Consul documentation .  To pass any additional Consul configuration, define the  DOCKER_CONSUL_OPTIONS  variable in the Profile file.", 
            "title": "Configure Consul"
        }, 
        {
            "location": "/cb-profile/index.html#add-tags-in-profile-aws", 
            "text": "In order to differentiate launched instances, you can optionally define custom tags for your AWS resources deployed by Cloudbreak.     If you want just one custom tag for your CloudFormation resources, set this variable in the  Profile :  export CB_AWS_DEFAULT_CF_TAG=mytagcontent  In this example, the name of the tag will be  CloudbreakId  and the value will be  mytagcontent .    If you prefer to customize the tag name, set this variable:  export CB_AWS_CUSTOM_CF_TAGS=mytagname:mytagvalue  In this example the name of the tag will be  mytagname  and the value will be  mytagvalue .     You can specify a list of tags with a comma separated list:   export CB_AWS_CUSTOM_CF_TAGS=tag1:value1,tag2:value2,tag3:value3", 
            "title": "Add tags in Profile (AWS)"
        }, 
        {
            "location": "/cli-install/index.html", 
            "text": "Installing Cloudbreak CLI\n\n\nThe Cloudbreak Command Line Interface (CLI) is a tool to help you manage your Cloudbreak cluster instances. This tool can be used to interact with Cloudbreak for automating cluster creation, management, monitoring, and termination. \n\n\nThe CLI is available for Linux, Mac OS X, and Windows. \n\n\nInstall the CLI\n\n\nAfter you have launched Cloudbreak, the CLI is available for download from that Cloudbreak instance.\n\n\nSteps\n\n\n\n\nBrowse to your Cloudbreak instance and log in to the Cloudbreak web UI.  \n\n\nSelect \nDownload CLI\n from the navigation pane. \n\n\nSelect your operating system. The CLI is available for Linux, Mac OS X, and Windows.   \n\n\nDownload the selected bundle to your local machine.  \n\n\nExtract the bundle.  \n\n\nYou can optionally add \ncb\n to your system path.\n\n\n\n\nRun the executable to verify the CLI: \n\n\ncb --version\n\n\n\n\n\n\nConfigure the CLI\n\n\nOnce you have installed the CLI, you need to configure the CLI to work with Cloudbreak.\n\n\nSteps\n\n\n\n\n\n\nUse the \ncb configure\n command to set up the CLI configuration file. The configuration options are:  \n\n\n\n\n--server\n server address [$CB_SERVER_ADDRESS]  \n\n\n--username\n user name (e-mail address) [$CB_USER_NAME]  \n\n\n--password\n password [$CB_PASSWORD]  \n\n\n\n\nThe password configuration is optional. If you do not provide the password, no password is stored in the CLI configuration file. Therefore, you will need to provide the password with each command you execute or via an environment variable.\n\n\nFor example:\n\n\ncb configure --server https://ec2-11-111-111-11.compute-1.amazonaws.com --username admin@hortonworks.com\n\n\n\n\n\n\nThe CLI configuration file will be saved at \n~/.cb/config\n. The content will look similar to the following:\n\n\ndefault:\n  username: admin@hortonworks.com\n  server: https://ec2-11-111-111-11.compute-1.amazonaws.com\n\n\n\n\n\n\nRun any command to verify that you can connect to the Cloudbreak instance via CLI. For example:\n\n\ncb cluster list\n  \n\n\n\n\n\n\n\n    \nConfiguration precedence\n\n    \n\n    The CLI can look for configuration options from different locations. You can optionally\n    pass the configuration options on each command or from environment variables. The following\n    order is used for the CLI to look for configuration options: \nCommand line\n, \nEnvironment variables\n\n    and the \nConfiguration file\n.\n    \n\n\n\n\n\nAdd multiple configurations\n\n\nIf you are using multiple profiles for multiple environments, you can configure them using the \ncb configure\n command and passing the name of your environment-specific profile file using the \n--profile\n parameter. After running the command, the configuration will be added as a new entry to the \nconfig\n file. For example, running the following command \ncb configure --server https://192.167.65.4 --username test@hortonworks.com --profile staging\n will add the \"staging\" entry:\n\n\ndefault:\n  username: admin@hortonworks.com\n  server: https://192.167.65.4\nstaging:\n  username: test@hortonworks.com\n  server: https://192.167.65.4  \n\n\n\n\nFor example:\n\n\n#cb configure --server https://192.167.65.4 --username test@hortonworks.com --profile staging\nINFO:  [writeConfigToFile] dir already exists: /Users/rkovacs/.cb\nINFO:  [writeConfigToFile] writing credentials to file: /Users/rkovacs/.cb/config\n# cat /Users/rkovacs/.cb/config\ndefault:\n  username: admin@example.com\n  server: https://192.167.65.4\n  output: table\nstaging:\n  username: test@hortonworks.com\n  server: https://192.167.65.4\n\n\n\nConfigure default output\n\n\nBy default, JSON format is used in command output. For example, if you run \ncb list-clusters\n without specifying output type, the output will be JSON. If you would like to change default output, add it to the config file. For example:\n\n\ndefault:\n  username: admin@hortonworks.com\n  server: https://192.167.65.4\n  output: table\n\n\n\n\n\nNext: Get Started with CLI", 
            "title": "Install the CLI"
        }, 
        {
            "location": "/cli-install/index.html#installing-cloudbreak-cli", 
            "text": "The Cloudbreak Command Line Interface (CLI) is a tool to help you manage your Cloudbreak cluster instances. This tool can be used to interact with Cloudbreak for automating cluster creation, management, monitoring, and termination.   The CLI is available for Linux, Mac OS X, and Windows.", 
            "title": "Installing Cloudbreak CLI"
        }, 
        {
            "location": "/cli-install/index.html#install-the-cli", 
            "text": "After you have launched Cloudbreak, the CLI is available for download from that Cloudbreak instance.  Steps   Browse to your Cloudbreak instance and log in to the Cloudbreak web UI.    Select  Download CLI  from the navigation pane.   Select your operating system. The CLI is available for Linux, Mac OS X, and Windows.     Download the selected bundle to your local machine.    Extract the bundle.    You can optionally add  cb  to your system path.   Run the executable to verify the CLI:   cb --version", 
            "title": "Install the CLI"
        }, 
        {
            "location": "/cli-install/index.html#configure-the-cli", 
            "text": "Once you have installed the CLI, you need to configure the CLI to work with Cloudbreak.  Steps    Use the  cb configure  command to set up the CLI configuration file. The configuration options are:     --server  server address [$CB_SERVER_ADDRESS]    --username  user name (e-mail address) [$CB_USER_NAME]    --password  password [$CB_PASSWORD]     The password configuration is optional. If you do not provide the password, no password is stored in the CLI configuration file. Therefore, you will need to provide the password with each command you execute or via an environment variable.  For example:  cb configure --server https://ec2-11-111-111-11.compute-1.amazonaws.com --username admin@hortonworks.com    The CLI configuration file will be saved at  ~/.cb/config . The content will look similar to the following:  default:\n  username: admin@hortonworks.com\n  server: https://ec2-11-111-111-11.compute-1.amazonaws.com    Run any command to verify that you can connect to the Cloudbreak instance via CLI. For example:  cb cluster list       \n     Configuration precedence \n     \n    The CLI can look for configuration options from different locations. You can optionally\n    pass the configuration options on each command or from environment variables. The following\n    order is used for the CLI to look for configuration options:  Command line ,  Environment variables \n    and the  Configuration file .", 
            "title": "Configure the CLI"
        }, 
        {
            "location": "/cli-install/index.html#add-multiple-configurations", 
            "text": "If you are using multiple profiles for multiple environments, you can configure them using the  cb configure  command and passing the name of your environment-specific profile file using the  --profile  parameter. After running the command, the configuration will be added as a new entry to the  config  file. For example, running the following command  cb configure --server https://192.167.65.4 --username test@hortonworks.com --profile staging  will add the \"staging\" entry:  default:\n  username: admin@hortonworks.com\n  server: https://192.167.65.4\nstaging:\n  username: test@hortonworks.com\n  server: https://192.167.65.4    For example:  #cb configure --server https://192.167.65.4 --username test@hortonworks.com --profile staging\nINFO:  [writeConfigToFile] dir already exists: /Users/rkovacs/.cb\nINFO:  [writeConfigToFile] writing credentials to file: /Users/rkovacs/.cb/config\n# cat /Users/rkovacs/.cb/config\ndefault:\n  username: admin@example.com\n  server: https://192.167.65.4\n  output: table\nstaging:\n  username: test@hortonworks.com\n  server: https://192.167.65.4", 
            "title": "Add multiple configurations"
        }, 
        {
            "location": "/cli-install/index.html#configure-default-output", 
            "text": "By default, JSON format is used in command output. For example, if you run  cb list-clusters  without specifying output type, the output will be JSON. If you would like to change default output, add it to the config file. For example:  default:\n  username: admin@hortonworks.com\n  server: https://192.167.65.4\n  output: table   Next: Get Started with CLI", 
            "title": "Configure default output"
        }, 
        {
            "location": "/cli-get-started/index.html", 
            "text": "Getting started with the CLI\n\n\nGet started with the CLI\n\n\nAfter \ninstalling\n and \nconfiguring\n the CLI, you can use it to perform the same tasks as are available in the Cloudbreak UI: create and manage clusters, credentials, blueprints, and recipes.\n\n\nSteps\n\n\n\n\n\n\nBefore you start using the CLI, familiarize yourself with Cloudbreak concepts and the Cloudbreak web UI. \n\n\n\n\n\n\nIf you haven't already, create at least one Cloudbreak credential by using Cloudbreak UI or the \ncredential create\n command. \n\n\n\n\n\n\nTo create a cluster, you must first generate a JSON skeleton. Although it is possible to generate it by using the \ncluster generate-template\n command, it is easiest to obtain it from the Cloudbreak UI, as described in \nObtain cluster JSON template from the UI\n.\n\n\n\n\n\n\nSave the template in the JSON format and edit it if needed.\n\n\n\n\n\n\nOnce your JSON file is ready, you can use it to create a cluster via the \ncluster create\n command.\n\n\n\n\n\n\nOnce your cluster is running, use can use the CLI to manage and monitor your cluster. For a full list of commands, refer to \nCLI reference\n.    \n\n\n\n\n\n\nObtain cluster JSON template from the UI\n\n\nThe simplest way to obtain a valid JSON template for your cluster is to get it from the Cloudbreak UI. You can do this in two ways:\n\n\nFrom create cluster\n\n\nOnce you've provided all the cluster parameters, on the last page of the create cluster wizard, click \nShow CLI Command\n to obtain the JSON template:\n\n\n    \n\n\nClick \nCopy the JSON\n to copy the content and then use a text editor to edit and save it. \n\n\nFrom cluster details\n\n\nYou can obtain the JSON template for a cluster from the cluster details page by selecting \nActions\n \n \nShow CLI Command\n. This option is available for all clusters that have been initiated, so the cluster does not need to be in the running state to obtain this information. In fact, this option is useful when troubleshooting cluster failures.  \n\n\n   \n\n\nClick \nCopy the JSON\n to copy the content and then use a text editor to edit and save it. \n\n\n \n\n\nObtain CLI command from the UI\n\n\nCloudbreak web UI includes an option in the UI which allows you to generate the  \ncreate\n command for resources such as credentials, blueprints, clusters, and recipes. This option is available when creating a resource and for existing resources, from the resource details page.   \n\n\nFrom Create Resource\n\n\nWhen creating a resource (credential, blueprint, cluster, or recipe), provide all information and then click \nShow CLI Command\n. The UI will display the \ncreate\n CLI command for the resource.\n\n\nFrom Resource Details\n\n\nNavigate to credential, blueprint, cluster, or recipe details and  click \nShow CLI Command\n. The UI will display the \ncreate\n CLI command for the resource.\n\n\nGet help\n\n\nTo get CLI help, you can add help to the end of a command. The following will list help for the CLI at the top-level:\n\n\ncb --help\n\n\n\nor \n\n\ncb --h\n\n\n\nThe following will list help for the create-cluster command, including its command options and global options:\n\n\ncb cluster --help\n\n\n\nor\n\n\ncb cluster --h\n\n\n\n\n\nNext: CLI Reference", 
            "title": "Get started with the CLI"
        }, 
        {
            "location": "/cli-get-started/index.html#getting-started-with-the-cli", 
            "text": "", 
            "title": "Getting started with the CLI"
        }, 
        {
            "location": "/cli-get-started/index.html#get-started-with-the-cli", 
            "text": "After  installing  and  configuring  the CLI, you can use it to perform the same tasks as are available in the Cloudbreak UI: create and manage clusters, credentials, blueprints, and recipes.  Steps    Before you start using the CLI, familiarize yourself with Cloudbreak concepts and the Cloudbreak web UI.     If you haven't already, create at least one Cloudbreak credential by using Cloudbreak UI or the  credential create  command.     To create a cluster, you must first generate a JSON skeleton. Although it is possible to generate it by using the  cluster generate-template  command, it is easiest to obtain it from the Cloudbreak UI, as described in  Obtain cluster JSON template from the UI .    Save the template in the JSON format and edit it if needed.    Once your JSON file is ready, you can use it to create a cluster via the  cluster create  command.    Once your cluster is running, use can use the CLI to manage and monitor your cluster. For a full list of commands, refer to  CLI reference .", 
            "title": "Get started with the CLI"
        }, 
        {
            "location": "/cli-get-started/index.html#obtain-cluster-json-template-from-the-ui", 
            "text": "The simplest way to obtain a valid JSON template for your cluster is to get it from the Cloudbreak UI. You can do this in two ways:  From create cluster  Once you've provided all the cluster parameters, on the last page of the create cluster wizard, click  Show CLI Command  to obtain the JSON template:        Click  Copy the JSON  to copy the content and then use a text editor to edit and save it.   From cluster details  You can obtain the JSON template for a cluster from the cluster details page by selecting  Actions     Show CLI Command . This option is available for all clusters that have been initiated, so the cluster does not need to be in the running state to obtain this information. In fact, this option is useful when troubleshooting cluster failures.         Click  Copy the JSON  to copy the content and then use a text editor to edit and save it.", 
            "title": "Obtain cluster JSON template from the UI"
        }, 
        {
            "location": "/cli-get-started/index.html#obtain-cli-command-from-the-ui", 
            "text": "Cloudbreak web UI includes an option in the UI which allows you to generate the   create  command for resources such as credentials, blueprints, clusters, and recipes. This option is available when creating a resource and for existing resources, from the resource details page.     From Create Resource  When creating a resource (credential, blueprint, cluster, or recipe), provide all information and then click  Show CLI Command . The UI will display the  create  CLI command for the resource.  From Resource Details  Navigate to credential, blueprint, cluster, or recipe details and  click  Show CLI Command . The UI will display the  create  CLI command for the resource.", 
            "title": "Obtain CLI command from the UI"
        }, 
        {
            "location": "/cli-get-started/index.html#get-help", 
            "text": "To get CLI help, you can add help to the end of a command. The following will list help for the CLI at the top-level:  cb --help  or   cb --h  The following will list help for the create-cluster command, including its command options and global options:  cb cluster --help  or  cb cluster --h   Next: CLI Reference", 
            "title": "Get help"
        }, 
        {
            "location": "/cli-reference/index.html", 
            "text": "Cloudbreak CLI reference\n\n\nThis section will help you get started with the Cloudbreak CLI after you have \ninstalled and configured it\n.\n\n\nCommand structure\n\n\nThe CLI command can contain multiple parts. The first part is a set of global options. The next part is the command. The next part is a set of command options and arguments which could include sub-commands.\n\n\ncb [global options] command [command options] [arguments...]\n\n\n\nCommand output\n\n\nYou can control the output from the CLI using the --output argument. The possible output formats include:\n\n\n\n\nJSON (\njson\n)\n\n\nYAML (\nyaml\n)\n\n\nFormatted table (\ntable\n)\n\n\n\n\nFor example:\n\n\ncb cluster list --output json\n\n\n\ncb clusters list --output yaml\n\n\n\ncb cluster list --output table\n\n\n\nCommands\n\n\nConfigure CLI:  \n\n\n\n\nconfigure\n  \n\n\n\n\nCloud provider:\n\n\n\n\ncloud availability-zones\n  \n\n\ncloud regions\n       \n\n\ncloud volumes\n   \n\n\ncloud instances\n   \n\n\n\n\nCredential:  \n\n\n\n\ncredential create\n   \n\n\ncredential delete\n        \n\n\ncredential describe\n        \n\n\ncredential list\n         \n\n\ncredential modify\n   \n\n\n\n\nBlueprint:   \n\n\n\n\nblueprint create\n  \n\n\nblueprint delete\n           \n\n\nblueprint describe\n         \n\n\nblueprint list\n           \n\n\n\n\nCluster: \n\n\n\n\ncluster change-ambari-password\n   \n\n\ncluster create\n \n\n\ncluster delete\n              \n\n\ncluster describe\n    \n\n\ncluster generate-template\n   \n\n\ncluster generate-reinstall-template\n  \n\n\ncluster list\n    \n\n\ncluster repair\n    \n\n\ncluster retry\n         \n\n\ncluster scale\n      \n\n\ncluster start\n        \n\n\ncluster stop\n              \n\n\ncluster sync\n \n\n\n\n\nDatabase:            \n\n\n\n\ndatabase create\n  \n\n\ndatabase delete\n  \n\n\ndatabase list\n  \n\n\ndatabase test\n  \n\n\n\n\nImage catalog\n\n\n\n\nimagecatalog create\n     \n\n\nimagecatalog delete\n \n\n\nimagecatalog images\n                      \n\n\nimagecatalog list\n  \n\n\nimagecatalog set-default\n            \n\n\n\n\nLDAP:\n\n\n\n\nldap create\n  \n\n\nldap delete\n  \n\n\nldap list\n\n\n\n\nMpack\n\n\n\n\nmpack create\n  \n\n\nmpack delete\n  \n\n\nmpack list\n\n\n\n\nProxy:\n\n\n\n\nproxy create\n  \n\n\nproxy delete\n  \n\n\nproxy list\n\n\n\n\nRecipe:  \n\n\n\n\nrecipe create\n     \n\n\nrecipe delete\n            \n\n\nrecipe describe\n            \n\n\nrecipe list\n              \n\n\n\n\n\n\nblueprint create\n\n\nAdds a new blueprint from a file or from a URL.\n\n\nSub-commands\n\n\nfrom-url\n Creates a blueprint by downloading it from a URL location\n\n\nfrom-file\n Creates a blueprint by reading it from a local file\n\n\nRequired options\n\n\nfrom-url\n \n\n\n--name \nvalue\n Name for the blueprint\n\n\n--url \nvalue\n URL location of the Ambari blueprint JSON file\n\n\nfrom-file\n \n\n\n--name \nvalue\n Name for the blueprint\n\n\n--file \nvalue\n Location of the Ambari blueprint JSON file on the local machine\n\n\nOptions\n\n\n--description \nvalue\n  Description of the resource\n\n\n--server \nvalue\n  Cloudbreak server address [$CB_SERVER_ADDRESS]\n\n\n--username \nvalue\n  Cloudbreak user name (e-mail address) [$CB_USER_NAME]\n\n\n--password \nvalue\n  Cloudbreak password [$CB_PASSWORD]\n\n\n--profile \nvalue\n Selects a config profile to use [$CB_PROFILE]\n\n\n--auth-type \nvalue\n  Authentication method to use. Values: oauth2, basic [$CB_AUTH_TYPE]\n\n\n--public\n   Public in account  \n\n\nExamples\n\n\nAdds a blueprint from a URL:\n\n\ncb blueprint create from-url --url https://someurl.com/test.bp --name test1\n\n\n\nAdds a blueprint from a local file:\n\n\ncb blueprint create from-file --file /Users/test/Documents/blueprints/test.bp --name test2\n\n\n\nRelated links\n\n\nUsing Custom Blueprints\n\n\n\n\nblueprint delete\n\n\nDeletes an existing blueprint.\n\n\nRequired options\n\n\n--name \nvalue\n Blueprint name \n\n\nOptions\n\n\n--server \nvalue\n  Cloudbreak server address [$CB_SERVER_ADDRESS]\n\n\n--username \nvalue\n  Cloudbreak user name (e-mail address) [$CB_USER_NAME]\n\n\n--password \nvalue\n  Cloudbreak password [$CB_PASSWORD]\n\n\n--profile \nvalue\n Selects a config profile to use [$CB_PROFILE] \n\n\n--auth-type \nvalue\n  Authentication method to use. Values: oauth2, basic [$CB_AUTH_TYPE]  \n\n\nExamples\n\n\ncb blueprint delete --name \"testbp\"\n\n\n\n\n\nblueprint describe\n\n\nDescribes an existing blueprint.\n\n\nRequired options\n\n\n--name \nvalue\n Blueprint name \n\n\nOptions\n\n\n--output \nvalue\n  Supported formats: json, yaml, table (default: \"json\") [$CB_OUT_FORMAT]\n\n\n--server \nvalue\n  Cloudbreak server address [$CB_SERVER_ADDRESS]\n\n\n--username \nvalue\n  Cloudbreak user name (e-mail address) [$CB_USER_NAME]\n\n\n--password \nvalue\n  Cloudbreak password [$CB_PASSWORD]\n\n\n--profile \nvalue\n Selects a config profile to use [$CB_PROFILE]\n\n\n--auth-type \nvalue\n  Authentication method to use. Values: oauth2, basic [$CB_AUTH_TYPE]  \n\n\nExamples\n\n\ncb blueprint describe --name \"Data Science: Apache Spark 2.1, Apache Zeppelin 0.7.0\"\n{\n  \"Name\": \"Data Science: Apache Spark 2.1, Apache Zeppelin 0.7.0\",\n  \"Description\": \"Data Science: Apache Spark 2.1, Apache Zeppelin 0.7.0\",\n  \"HDPVersion\": \"2.6\",\n  \"HostgroupCount\": \"3\",\n  \"Tags\": \"DEFAULT\"\n}\n\n\n\n\n\nblueprint list\n\n\nLists available blueprints.\n\n\nRequired options\n\n\nNome\n\n\nOptions\n\n\n--output \nvalue\n Supported formats: json, yaml, table (default: \"json\") [$CB_OUT_FORMAT] \n\n\n--server \nvalue\n  Cloudbreak server address [$CB_SERVER_ADDRESS]\n\n\n--username \nvalue\n  Cloudbreak user name (e-mail address) [$CB_USER_NAME]\n\n\n--password \nvalue\n  Cloudbreak password [$CB_PASSWORD]\n\n\n--profile \nvalue\n Selects a config profile to use [$CB_PROFILE]\n\n\n--auth-type \nvalue\n  Authentication method to use. Values: oauth2, basic [$CB_AUTH_TYPE]  \n\n\nExamples\n\n\ncb blueprint list\n[\n  {\n    \"Name\": \"EDW-Analytics: Apache Hive 2 LLAP, Apache Zeppelin\",\n    \"Description\": \"Useful for EDW analytics using Hive LLAP\",\n    \"HDPVersion\": \"2.6\",\n    \"HostgroupCount\": \"3\",\n    \"Tags\": \"DEFAULT\"\n  },\n  {\n    \"Name\": \"Data Science: Apache Spark 2, Apache Zeppelin\",\n    \"Description\": \"Useful for data science with Spark and Zeppelin\",\n    \"HDPVersion\": \"2.6\",\n    \"HostgroupCount\": \"3\",\n    \"Tags\": \"DEFAULT\"\n  },\n  {\n    \"Name\": \"EDW-ETL: Apache Hive, Apache Spark 2\",\n    \"Description\": \"Useful for ETL data processing with Hive and Spark\",\n    \"HDPVersion\": \"2.6\",\n    \"HostgroupCount\": \"3\",\n    \"Tags\": \"DEFAULT\"\n  },\n  {\n    \"Name\": \"Flow Management: Apache NiFi\",\n    \"Description\": \"Useful for data-flow management with Apache NiFi\",\n    \"HDPVersion\": \"3.1\",\n    \"HostgroupCount\": \"2\",\n    \"Tags\": \"DEFAULT\"\n  },\n  {\n    \"Name\": \"my-hdf-test\",\n    \"Description\": \"\",\n    \"HDPVersion\": \"3.1\",\n    \"HostgroupCount\": \"3\",\n    \"Tags\": \"USER_MANAGED\"\n }\n]\n\n\n\n\n\ncloud availability-zones\n\n\nLists all availability zones available in the specified cloud provider region. \n\n\nRequired options\n\n\n--credential \nvalue\n  Name of the credential\n\n\n--region \nvalue\n   Name of the region  \n\n\nOptions\n\n\n--output \nvalue\n  Supported formats: json, yaml, table (default: \"json\") [$CB_OUT_FORMAT]\n\n\n--server \nvalue\n  Cloudbreak server address [$CB_SERVER_ADDRESS]\n\n\n--username \nvalue\n  Cloudbreak user name (e-mail address) [$CB_USER_NAME]\n\n\n--password \nvalue\n  Cloudbreak password [$CB_PASSWORD] \n\n\n--profile \nvalue\n  Selects a config profile to use [$CB_PROFILE] \n\n\n--auth-type \nvalue\n  Authentication method to use. Values: oauth2, basic [$CB_AUTH_TYPE]   \n\n\nExamples\n\n\nLists availability zones in the us-west-2 (Oregon) region on the AWS account identified by the credential called \"aws-cred\":\n\n\ncb cloud availability-zones --credential aws-cred --region us-west-2\n[\n  {\n    \"Name\": \"us-west-2a\"\n  },\n  {\n    \"Name\": \"us-west-2b\"\n  },\n  {\n    \"Name\": \"us-west-2c\"\n  }\n]\n\n\n\n\n\ncloud regions\n\n\nLists the available cloud provider regions. \n\n\nRequired options\n\n\n--credential \nvalue\n  Name of the credential  \n\n\nOptions\n\n\n--output \nvalue\n  Supported formats: json, yaml, table (default: \"json\") [$CB_OUT_FORMAT] \n\n\n--server \nvalue\n  Cloudbreak server address [$CB_SERVER_ADDRESS]\n\n\n--username \nvalue\n  Cloudbreak user name (e-mail address) [$CB_USER_NAME]\n\n\n--password \nvalue\n  Cloudbreak password [$CB_PASSWORD]\n\n\n--profile \nvalue\n  Selects a config profile to use [$CB_PROFILE] \n\n\n--auth-type \nvalue\n  Authentication method to use. Values: oauth2, basic [$CB_AUTH_TYPE]   \n\n\nExamples\n\n\nLists regions available on the AWS account identified by the credential called \"aws-cred\":\n\n\ncb cloud regions --credential aws-cred\n[\n  {\n    \"Name\": \"ap-northeast-1\",\n    \"Description\": \"Asia Pacific (Tokyo)\"\n  },\n  {\n    \"Name\": \"ap-northeast-2\",\n    \"Description\": \"Asia Pacific (Seoul)\"\n  },\n  ...\n\n\n\n\n\ncloud volumes\n\n\nLists the available cloud provider volume types. \n\n\nSub-commands\n\n\naws\n     Lists the available aws volume types\n\n\nazure\n   Lists the available azure volume types\n\n\ngcp\n     Lists the available gcp volume types  \n\n\nRequired options\n\n\nNone\n\n\nOptions\n\n\n--output \nvalue\n  Supported formats: json, yaml, table (default: \"json\") [$CB_OUT_FORMAT]\n\n\n--server \nvalue\n  Cloudbreak server address [$CB_SERVER_ADDRESS]\n\n\n--username \nvalue\n  Cloudbreak user name (e-mail address) [$CB_USER_NAME]\n\n\n--password \nvalue\n  Cloudbreak password [$CB_PASSWORD]  \n\n\n--profile \nvalue\n  Selects a config profile to use [$CB_PROFILE] \n\n\n--auth-type \nvalue\n  Authentication method to use. Values: oauth2, basic [$CB_AUTH_TYPE]   \n\n\nExamples\n\n\nLists volumes available on AWS:\n\n\ncb cloud volumes aws\n[\n  {\n    \"Name\": \"ephemeral\",\n    \"Description\": \"Ephemeral\"\n  },\n  {\n    \"Name\": \"gp2\",\n    \"Description\": \"General Purpose (SSD)\"\n  },\n  {\n    \"Name\": \"st1\",\n    \"Description\": \"Throughput Optimized HDD\"\n  },\n  {\n    \"Name\": \"standard\",\n    \"Description\": \"Magnetic\"\n  }\n]\n\n\n\n\n\ncloud instances\n\n\nLists the available cloud provider instance types.\n\n\nRequired options\n\n\n--credential \nvalue\n  Name of the credential\n\n\n--region \nvalue\n   Name of the region  \n\n\nOptions\n\n\n--availability-zone \nvalue\n  Name of the availability zone \n\n\n--output \nvalue\n      Supported formats: json, yaml, table (default: \"json\") [$CB_OUT_FORMAT]\n\n\n--server \nvalue\n  Cloudbreak server address [$CB_SERVER_ADDRESS]\n\n\n--username \nvalue\n  Cloudbreak user name (e-mail address) [$CB_USER_NAME]\n\n\n--password \nvalue\n  Cloudbreak password [$CB_PASSWORD]\n\n\n--profile \nvalue\n     Selects a config profile to use [$CB_PROFILE] \n\n\n--auth-type \nvalue\n  Authentication method to use. Values: oauth2, basic [$CB_AUTH_TYPE]    \n\n\nExamples\n\n\nLists instance types available in the us-west-2 (Oregon) region on the AWS account identified by the credential called \"aws-cred\":\n\n\ncb cloud instances --credential aws-cred --region us-west-2\n\n  {\n    \"Name\": \"c3.2xlarge\",\n    \"Cpu\": \"8\",\n    \"Memory\": \"15.0\",\n    \"AvailabilityZone\": \"us-west-2b\"\n  },\n  {\n    \"Name\": \"c3.4xlarge\",\n    \"Cpu\": \"16\",\n    \"Memory\": \"30.0\",\n    \"AvailabilityZone\": \"us-west-2b\"\n  },\n  ...\n\n\n\n\n\ncluster change-ambari-password\n\n\nChanges Ambari password.\n\n\nRequired options\n\n\n--name \nvalue\n  Cluster name\n\n\n--old-password \nvalue\n Old Ambari password \n\n\n--new-password \nvalue\n  New Ambari password  \n\n\n--ambari-user \nvalue\n  Ambari user   \n\n\nOptions\n\n\n--output \nvalue\n  Supported formats: json, yaml, table (default: \"json\") [$CB_OUT_FORMAT]\n\n\n--server \nvalue\n  Cloudbreak server address [$CB_SERVER_ADDRESS]\n\n\n--username \nvalue\n  Cloudbreak user name (e-mail address) [$CB_USER_NAME]\n\n\n--password \nvalue\n  Cloudbreak password [$CB_PASSWORD]\n\n\n--profile \nvalue\n  Selects a config profile to use [$CB_PROFILE]\n\n\n--auth-type \nvalue\n  Authentication method to use. Values: oauth2, basic [$CB_AUTH_TYPE]    \n\n\nExamples\n\n\nChanges password for Ambari user called \"admin\" for a cluster called \"test1234\":\n\n\ncb cluster change-ambari-password --name test1234 --old-password 123456 --new-password Ambari123456 --ambari-user admin\n\n\n\n\n\ncluster create\n\n\nCreates a new cluster based on a JSON template. \n\n\nRequired options\n\n\n--cli-input-json \nvalue\n  User provided file in JSON format  \n\n\nOptions\n\n\n--name \nvalue\n  Name for the cluster\n\n\n--description \nvalue\n  Description of resource \n\n\n--input-json-param-password \nvalue\n  Password for the cluster and Ambari \n\n\n--wait\n  Wait for the operation to finish. No argument is required \n\n\n--server \nvalue\n  Cloudbreak server address [$CB_SERVER_ADDRESS]\n\n\n--username \nvalue\n  Cloudbreak user name (e-mail address) [$CB_USER_NAME]\n\n\n--password \nvalue\n  Cloudbreak password [$CB_PASSWORD] \n\n\n--profile \nvalue\n  Selects a config profile to use [$CB_PROFILE]   \n\n\n--auth-type \nvalue\n  Authentication method to use. Values: oauth2, basic [$CB_AUTH_TYPE]  \n\n\n--public\n   Public in account  \n\n\nExamples\n\n\nCreates a cluster called \"testcluster\" based on a local JSON file called \"mytemplate.json\" located in the /Users/test/Documents directory:   \n\n\ncb cluster create --name testcluster --cli-input-json /Users/test/Documents/mytemplate.json\n\n\n\nRelated links\n\n\nObtain Cluster JSON Template from the UI\n   \n\n\n\n\ncluster delete\n\n\nDeletes an existing cluster. \n\n\nRequired options\n\n\n--name \nvalue\n  Cluster name\n\n\nOptions\n\n\n--force\n  Force the operation\n\n\n--wait\n  Wait for the operation to finish. No argument is required\n\n\n--server \nvalue\n  Cloudbreak server address [$CB_SERVER_ADDRESS]\n\n\n--username \nvalue\n  Cloudbreak user name (e-mail address) [$CB_USER_NAME]\n\n\n--password \nvalue\n  Cloudbreak password [$CB_PASSWORD]\n\n\n--profile \nvalue\n  Selects a config profile to use [$CB_PROFILE] \n\n\n--auth-type \nvalue\n  Authentication method to use. Values: oauth2, basic [$CB_AUTH_TYPE]   \n\n\nExamples\n\n\ncb cluster delete --name test1234\n\n\n\n\n\ncluster describe\n\n\nDescribes an existing cluster.\n\n\nRequired options\n\n\n--name \nvalue\n  Cluster name\n\n\nOptions\n\n\n--output \nvalue\n  Supported formats: json, yaml, table (default: \"json\") [$CB_OUT_FORMAT] \n\n\n--server \nvalue\n  Cloudbreak server address [$CB_SERVER_ADDRESS]\n\n\n--username \nvalue\n  Cloudbreak user name (e-mail address) [$CB_USER_NAME]\n\n\n--password \nvalue\n  Cloudbreak password [$CB_PASSWORD]\n\n\n--profile \nvalue\n  Selects a config profile to use [$CB_PROFILE]\n\n\n--auth-type \nvalue\n  Authentication method to use. Values: oauth2, basic [$CB_AUTH_TYPE]   \n\n\nExamples\n\n\nReturns a JSON file describing an existing cluster called \"test1234\":\n\n\n./cb cluster describe --name test1234\n\n\n\nThe command returns JSON output which due to space limitation was not captured in the example.\n\n\n\n\ncluster generate-template\n\n\nGenerates a provider-specific cluster template in JSON format.\n\n\nSub-commands\n\n\naws new-network\n Generates an AWS cluster JSON template with new network\n\n\naws existing-network\n Generates an AWS cluster JSON template with existing network\n\n\naws existing-subnet\n Generates an AWS cluster JSON template with existing network and subnet  \n\n\nazure new-network\n Generates an Azure cluster JSON template with new network\n\n\nazure existing-subnet\n Generates an Azure cluster JSON template with existing network and subnet  \n\n\ngcp new-network\n Generates an GCP cluster JSON template with new network\n\n\ngcp existing-network\n Generates an GCP cluster JSON template with existing network \n\n\ngcp existing-subnet\n Generates an GCP cluster JSON template with existing network and subnet\n\n\ngcp legacy-network\n Generates an GCP cluster JSON template with legacy network without subnets    \n\n\nopenstack new-network\n Generates an OS cluster JSON template with new network \n\n\nopenstack existing-network\n Generates an OS cluster JSON template with existing network \n\n\nopenstack existing-subnet\n Generates an OS cluster JSON template with existing network and subnet   \n\n\nExamples\n\n\ncb cluster generate-template aws new-network\n\n\n\nRelated Commands\n\n\nRelated Links\n\n\nObtain Cluster JSON Template from the UI\n   \n\n\n\n\ncluster generate-reinstall-template\n\n\nGenerates a cluster template that you can use to reinstall the cluster if installation went fail. \n\n\nRequired options\n\n\n--blueprint-name \nvalue\n  Name of the blueprint \n\n\nOptions\n\n\n--output \nvalue\n  Supported formats: json, yaml, table (default: \"json\") [$CB_OUT_FORMAT]\n\n\n--server \nvalue\n  Cloudbreak server address [$CB_SERVER_ADDRESS]\n\n\n--username \nvalue\n  Cloudbreak user name (e-mail address) [$CB_USER_NAME]\n\n\n--password \nvalue\n  Cloudbreak password [$CB_PASSWORD]  \n\n\n--profile \nvalue\n  Selects a config profile to use [$CB_PROFILE]\n\n\n--auth-type \nvalue\n  Authentication method to use. Values: oauth2, basic [$CB_AUTH_TYPE]   \n\n\nExamples\n\n\ncb cluster generate-reinstall-template --blueprint-name \"EDW-ETL: Apache Hive 1.2.1, Apache Spark 2.1\"\n\n\n\n\n\ncluster list\n\n\nLists all clusters which are currently associated with the Cloudbreak instance.\n\n\nRequired options\n\n\nNone\n\n\nOptions\n\n\n--output \nvalue\n  Supported formats: json, yaml, table (default: \"json\") [$CB_OUT_FORMAT]\n\n\n--server \nvalue\n  Cloudbreak server address [$CB_SERVER_ADDRESS]\n\n\n--username \nvalue\n  Cloudbreak user name (e-mail address) [$CB_USER_NAME]\n\n\n--password \nvalue\n  Cloudbreak password [$CB_PASSWORD]  \n\n\n--profile \nvalue\n  Selects a config profile to use [$CB_PROFILE]\n\n\n--auth-type \nvalue\n  Authentication method to use. Values: oauth2, basic [$CB_AUTH_TYPE]  \n\n\nExamples\n\n\nLists available clusters: \n\n\ncb cluster list\n[\n  {\n    \"Name\": \"test1234\",\n    \"Description\": \"\",\n    \"CloudPlatform\": \"AZURE\",\n    \"StackStatus\": \"UPDATE_IN_PROGRESS\",\n    \"ClusterStatus\": \"REQUESTED\"\n  }\n]\n\n\n\nLists available clusters, with output in a table format:\n\n\ncb cluster list --output table\n+----------+-------------+---------------+--------------------+---------------+\n|   NAME   | DESCRIPTION | CLOUDPLATFORM |    STACKSTATUS     | CLUSTERSTATUS |\n+----------+-------------+---------------+--------------------+---------------+\n| test1234 |             | AZURE         | UPDATE_IN_PROGRESS | REQUESTED     |\n+----------+-------------+---------------+--------------------+---------------+\n\n\n\n\n\ncluster repair\n\n\nRepairs a cluster if cluster installation failed.\n\n\nRequired options\n\n\n--name \nvalue\n  Cluster name\n\n\nOptions\n\n\n--wait\n  Wait for the operation to finish. No argument is required\n\n\n--output \nvalue\n  Supported formats: json, yaml, table (default: \"json\") [$CB_OUT_FORMAT] \n\n--server \nvalue\n  Cloudbreak server address [$CB_SERVER_ADDRESS]\n\n\n--username \nvalue\n  Cloudbreak user name (e-mail address) [$CB_USER_NAME]\n\n\n--password \nvalue\n  Cloudbreak password [$CB_PASSWORD]  \n\n\n--profile \nvalue\n  Selects a config profile to use [$CB_PROFILE]\n\n\n--auth-type \nvalue\n  Authentication method to use. Values: oauth2, basic [$CB_AUTH_TYPE]   \n\n\nExamples\n\n\ncb cluster repair --name test1234\n\n\n\n\n\ncluster retry\n\n\nRetries the process if cluster or stack provisioning failed.\n\n\nRequired options\n\n\n--name \nvalue\n  Cluster name\n\n\nOptions\n\n\n--wait\n  Wait for the operation to finish. No argument is required\n\n\n--output \nvalue\n  Supported formats: json, yaml, table (default: \"json\") [$CB_OUT_FORMAT] \n\n--server \nvalue\n  Cloudbreak server address [$CB_SERVER_ADDRESS]\n\n\n--username \nvalue\n  Cloudbreak user name (e-mail address) [$CB_USER_NAME]\n\n\n--password \nvalue\n  Cloudbreak password [$CB_PASSWORD]  \n\n\n--profile \nvalue\n  Selects a config profile to use [$CB_PROFILE]\n\n\n--auth-type \nvalue\n  Authentication method to use. Values: oauth2, basic [$CB_AUTH_TYPE]   \n\n\nExamples\n\n\ncb cluster retry --name test1234\n\n\n\n\n\ncluster scale\n\n\nScales a cluster by adding or removing nodes.\n\n\nRequired options\n\n\n--name \nvalue\n  Cluster name\n\n\n--group-name \nvalue\n  Name of the group to scale\n\n\n--desired-node-count \nvalue\n  Desired number of nodes  \n\n\nOptions\n\n\n--wait\n  Wait for the operation to finish. No argument is required \n\n\n--output \nvalue\n  Supported formats: json, yaml, table (default: \"json\") [$CB_OUT_FORMAT]\n\n\n--server \nvalue\n  Cloudbreak server address [$CB_SERVER_ADDRESS]\n\n\n--username \nvalue\n  Cloudbreak user name (e-mail address) [$CB_USER_NAME]\n\n\n--password \nvalue\n  Cloudbreak password [$CB_PASSWORD]  \n\n\n--profile \nvalue\n  Selects a config profile to use [$CB_PROFILE]\n\n\n--auth-type \nvalue\n  Authentication method to use. Values: oauth2, basic [$CB_AUTH_TYPE]   \n\n\nExamples\n\n\ncb cluster scale --name test1234 --group-name worker --desired node-count 3\n\n\n\n\n\ncluster start\n\n\nStarts a cluster which has previously been stopped.\n\n\nRequired options\n\n\n--name \nvalue\n  Cluster name\n\n\nOptions\n\n\n--wait\n  Wait for the operation to finish. No argument is required\n\n\n--output \nvalue\n  Supported formats: json, yaml, table (default: \"json\") [$CB_OUT_FORMAT]\n\n\n--server \nvalue\n  Cloudbreak server address [$CB_SERVER_ADDRESS]\n\n\n--username \nvalue\n  Cloudbreak user name (e-mail address) [$CB_USER_NAME]\n\n\n--password \nvalue\n  Cloudbreak password [$CB_PASSWORD]  \n\n\n--profile \nvalue\n  Selects a config profile to use [$CB_PROFILE]\n\n\n--auth-type \nvalue\n  Authentication method to use. Values: oauth2, basic [$CB_AUTH_TYPE]   \n\n\nExamples\n\n\ncb cluster start --name test1234\n\n\n\n\n\ncluster stop\n\n\nStops a cluster.\n\n\nRequired options\n\n\n--name \nvalue\n  Cluster name\n\n\nOptions\n\n\n--wait\n  Wait for the operation to finish. No argument is required\n\n\n--output \nvalue\n  Supported formats: json, yaml, table (default: \"json\") [$CB_OUT_FORMAT]\n\n\n--server \nvalue\n  Cloudbreak server address [$CB_SERVER_ADDRESS]\n\n\n--username \nvalue\n  Cloudbreak user name (e-mail address) [$CB_USER_NAME]\n\n\n--password \nvalue\n  Cloudbreak password [$CB_PASSWORD]  \n\n\n--profile \nvalue\n  Selects a config profile to use [$CB_PROFILE]\n\n\n--auth-type \nvalue\n  Authentication method to use. Values: oauth2, basic [$CB_AUTH_TYPE]  \n\n\nExamples\n\n\ncb cluster stop --name test1234\n\n\n\n\n\ncluster sync\n\n\nSynchronizes a cluster with the cloud provider.\n\n\nRequired options\n\n\n--name \nvalue\n  Cluster name\n\n\nOptions\n\n\n--output \nvalue\n  Supported formats: json, yaml, table (default: \"json\") [$CB_OUT_FORMAT]\n\n\n--server \nvalue\n  Cloudbreak server address [$CB_SERVER_ADDRESS]\n\n\n--username \nvalue\n  Cloudbreak user name (e-mail address) [$CB_USER_NAME]\n\n\n--password \nvalue\n  Cloudbreak password [$CB_PASSWORD]  \n\n\n--profile \nvalue\n  Selects a config profile to use [$CB_PROFILE]\n\n\n--auth-type \nvalue\n  Authentication method to use. Values: oauth2, basic [$CB_AUTH_TYPE]  \n\n\nExamples\n\n\ncb cluster sync --name test1234\n\n\n\n\n\nconfigure\n\n\nConfigures the Cloudbreak server address and credentials used to communicate with this server.\n\n\nRequired options\n\n\n--server \nvalue\n  Server address [$CB_SERVER_ADDRESS]\n\n\n--username \nvalue\n     User name (e-mail address) [$CB_USER_NAME]  \n\n\nOptions\n\n\n--password \nvalue\n  Password [$CB_PASSWORD]\n\n\n--profile \nvalue\n  Select a config profile to use [$CB_PROFILE] \n\n\n--output \nvalue\n  Supported formats: json, yaml, table (default: \"json\") [$CB_OUT_FORMAT]\n\n\n--auth-type \nvalue\n  Authentication method to use. Values: oauth2, basic [$CB_AUTH_TYPE]  \n\n\nExamples\n\n\nThis example configures the server address with username and password:\n\n\ncb configure --server https://ec2-11-111-111-11.compute-1.amazonaws.com --username admin@hortonworks.com --password MySecurePassword123\n\n\n\nThis example configures the server address with username but without a password:\n\n\ncb configure --server https://ec2-11-111-111-11.compute-1.amazonaws.com --username admin@hortonworks.com\n\n\n\nRelated links\n\n\nConfigure CLI\n\n\n\n\ncredential create\n\n\nCreates a new Cloudbreak credential.\n\n\nSub-commands\n\n\naws role-based\n  Creates a new AWS credential\n\n\naws key-based\n  Creates a new AWS credential\n\n\nazure app-based\n Creates a new app-based Azure credential\n\n\ngcp\n Creates a new gcp credential\n\n\nopenstack keystone-v2\n Creates a new OpenStack credential\n\n\nopenstack keystone-v3\n Creates a new OpenStack credential \n\n\nRequired options\n\n\naws role-based\n \n\n\n--name \nvalue\n  Name for the credential \n\n\n--role-arn \nvalue\n IAM Role ARN of the role used for Cloudbreak credential  \n\n\naws key-based\n \n\n\n--name \nvalue\n  Name for the credential  \n\n\n--access-key \nvalue\n  AWS Access Key\n\n\n--secret-key \nvalue\n  AWS Secret Key  \n\n\nazure app-based\n \n\n\n--name \nvalue\n  Name for the credential  \n\n\n--subscription-id \nvalue\n  Subscription ID from your Azure Subscriptions\n\n\n--tenant-id \nvalue\n  Directory ID from your Azure Active Directory \n Properties    \n\n\n--app-id \nvalue\n  Application ID of your app from your Azure Active Directory \n App Registrations          \n\n\n--app-password \nvalue\n  Your application key from app registration's Settings \n Keys  \n\n\ngcp\n \n\n\n--name \nvalue\n  Name for the credential  \n\n\n--project-id \nvalue\n  Project ID from your GCP account                      \n\n\n--service-account-id \nvalue\n  Your GCP Service account ID from IAM \n Admin \n Service accounts               \n\n\n--service-account-private-key-file \nvalue\n  P12 key from your GCP service account  \n\n\nopenstack keystone-v2\n  \n\n\n--name \nvalue\n  Name for the credential  \n\n\n--tenant-user \nvalue\n  OpenStack user name   \n\n\n--tenant-password \nvalue\n  OpenStack password\n\n\n--tenant-name \nvalue\n  OpenStack tenant name    \n\n\n--endpoint \nvalue\n   OpenStack endpoint   \n\n\nopenstack keystone-v3\n \n\n\n--name \nvalue\n  Name for the credential \n\n\n--tenant-user \nvalue\n  OpenStack user name   \n\n\n--tenant-password \nvalue\n  OpenStack password\n\n\n--user-domain \nvalue\n  OpenStack user domain    \n\n\n--endpoint \nvalue\n   OpenStack endpoint  \n\n\nOptions\n\n\n--description \nvalue\n  Description of the resource\n\n\n--server \nvalue\n  Cloudbreak server address [$CB_SERVER_ADDRESS]\n\n\n--username \nvalue\n  Cloudbreak user name (e-mail address) [$CB_USER_NAME]\n\n\n--password \nvalue\n  Cloudbreak password [$CB_PASSWORD]  \n\n\n--profile \nvalue\n Selects a config profile to use [$CB_PROFILE]\n\n\n--auth-type \nvalue\n  Authentication method to use. Values: oauth2, basic [$CB_AUTH_TYPE]\n\n\n--public\n   Public in account  \n\n\nAdditionally, the following option is available for OpenStack Keystone2 and Keystone3:\n\n\n--facing \nvalue\n API facing. One of: public, admin, internal\n\n\nAdditionally, the following options are available for OpenStack Keystone3:\n\n\n--project-domain-name \nvalue\n  OpenStack project domain name  \n\n\n--project-name \nvalue\n  OpenStack project name         \n\n\n--domain-name \nvalue\n  OpenStack domain name  \n\n\n--keystone-scope \nvalue\n OpenStack keystone scope. One of: default, domain, project   \n\n\nExamples\n\n\nCreates a role-based credential on AWS:\n\n\ncb credential create aws role-based --name my-credential1 --role-arn arn:aws:iam::517127065441:role/CredentialRole\n\n\n\nCreates a key-based credential on AWS:\n\n\ncb credential create aws key-based --name my-credential2 --access-key ABDVIRDFV3K4HLJ45SKA --secret-key D89L5pOPM+426Rtj3curKzJEJL3lYoNcP8GvguBV\n\n\n\nCreates an app-based credential on Azure:\n\n\ncb credential create azure app-based --name my-credential3 --subscription-id b8e7379e-568g-55d3-na82-45b8d421e998 --tenant-id  c79n5399-3231-65ba-8dgg-2g4e2a40085e --app-id 6d147d89-48d2-5de2-eef8-b89775bbfcg1 --app-password 4a8hBgfI52s/C8R5Sea2YHGnBFrD3fRONfdG8w7F2Ua=\n\n\n\nCreates a credential on Google Cloud:\n\n\ncb credential create gcp --name my-credential4 --project-id test-proj --service-account-id test@test-proj.iam.gserviceaccount.com --service-account-private-key-file /Users/test/3fff57a6f68e.p12\n\n\n\nCreates a role-based credential on OpenStack with Keystone-v2:\n\n\ncb credential create openstack keystone-v2 --name my-credential5 --tenant-user test --tenant-password MySecurePass123 --tenant-name test --endpoint http://openstack.test.organization.com:5000/v2.0\n\n\n\nRelated links\n  \n\n\nCreate Credential on AWS\n\n\nCreate Credential on Azure\n\n\nCreate Credential on GCP\n\n\nCreate Credential on OpenStack\n  \n\n\n\n\ncredential delete\n\n\nDeletes an existing Cloudbreak credential.\n\n\nRequired options\n\n\n--name \nvalue\n  Credential name \n\n\nOptions\n\n\n--server \nvalue\n  Cloudbreak server address [$CB_SERVER_ADDRESS]\n\n\n--username \nvalue\n  Cloudbreak user name (e-mail address) [$CB_USER_NAME]\n\n\n--password \nvalue\n  Cloudbreak password [$CB_PASSWORD]  \n\n\n--profile \nvalue\n Selects a config profile to use [$CB_PROFILE]\n\n\n--auth-type \nvalue\n  Authentication method to use. Values: oauth2, basic [$CB_AUTH_TYPE]  \n\n\nExamples\n\n\ncb credential delete --name test-cred\n\n\n\n\n\ncredential describe\n\n\nDescribes an existing credential.\n\n\nRequired options\n\n\n--name \nvalue\n  Credential name \n\n\nOptions\n\n\n--output \nvalue\n  Supported formats: json, yaml, table (default: \"json\") [$CB_OUT_FORMAT]  \n\n\n--server \nvalue\n  Cloudbreak server address [$CB_SERVER_ADDRESS]\n\n\n--username \nvalue\n  Cloudbreak user name (e-mail address) [$CB_USER_NAME]\n\n\n--password \nvalue\n  Cloudbreak password [$CB_PASSWORD]  \n\n\n--profile \nvalue\n Selects a config profile to use [$CB_PROFILE]\n\n\n--auth-type \nvalue\n  Authentication method to use. Values: oauth2, basic [$CB_AUTH_TYPE]  \n\n\nExamples\n\n\ncb credential describe --name testcred\n{\n  \"Name\": \"testcred\",\n  \"Description\": \"\",\n  \"CloudPlatform\": \"AZURE\"\n}\n\n\n\n\n\ncredential list\n\n\nLists existing Cloudbreak credentials.\n\n\nRequired options\n\n\nNone\n\n\nOptions\n\n\n--output \nvalue\n  Supported formats: json, yaml, table (default: \"json\") [$CB_OUT_FORMAT]\n\n\n--server \nvalue\n  Cloudbreak server address [$CB_SERVER_ADDRESS]\n\n\n--username \nvalue\n  Cloudbreak user name (e-mail address) [$CB_USER_NAME]\n\n\n--password \nvalue\n  Cloudbreak password [$CB_PASSWORD]  \n\n\n--profile \nvalue\n Selects a config profile to use [$CB_PROFILE]\n\n\n--auth-type \nvalue\n  Authentication method to use. Values: oauth2, basic [$CB_AUTH_TYPE]  \n\n\nExamples\n\n\nLists credentials:\n\n\ncb credential list \n[\n  {\n    \"Name\": \"testcred\",\n    \"Description\": \"\",\n    \"CloudPlatform\": \"AZURE\"\n  }\n]\n\n\n\n\nLists credentials, with output formatted in a table format:\n\n\ncb credential list --output table\n+---------+-------------+---------------+\n|  NAME   | DESCRIPTION | CLOUDPLATFORM |\n+---------+-------------+---------------+\n| armcred |             | AZURE         |\n+---------+-------------+---------------+\n\n\n\n\n\ncredential modify\n\n\nModifies an existing Cloudbreak credential. \n\n\n\n\nThe \n--name\n parameter is used to identify the credential that is being modified, and therefore its value cannot be modified.    \n\n\n\n\nSub-commands\n\n\naws role-based\n  Modifies an AWS role-based credential\n\n\naws key-based\n  Modifies an AWS key-based credential\n\n\nazure app-based\n Modifies an app-based Azure credential\n\n\ngcp\n Modifies a Google Cloud credential\n\n\nopenstack keystone-v2\n Modifies an OpenStack v2 credential\n\n\nopenstack keystone-v3\n Modifies an  OpenStack v3 credential \n\n\nRequired options\n\n\naws role-based\n \n\n\n--name \nvalue\n  Credential name\n\n\n--role-arn \nvalue\n IAM Role ARN of the role used for Cloudbreak credential  \n\n\naws key-based\n \n\n\n--name \nvalue\n  Credential name\n\n\n--access-key \nvalue\n  AWS Access Key\n\n\n--secret-key \nvalue\n  AWS Secret Key  \n\n\nazure app-based\n \n\n\n--name \nvalue\n  Credential name\n\n\n--subscription-id \nvalue\n  Subscription ID from your Azure Subscriptions\n\n\n--tenant-id \nvalue\n  Directory ID from your Azure Active Directory \n Properties    \n\n\n--app-id \nvalue\n  Application ID of your app from your Azure Active Directory \n App Registrations          \n\n\n--app-password \nvalue\n  Your application key from app registration's Settings \n Keys  \n\n\ngcp\n \n\n\n--name \nvalue\n  Credential name \n\n\n--project-id \nvalue\n  Project ID from your GCP account                      \n\n\n--service-account-id \nvalue\n  Your GCP Service account ID from IAM \n Admin \n Service accounts               \n\n\n--service-account-private-key-file \nvalue\n  P12 key from your GCP service account  \n\n\nopenstack keystone-v2\n  \n\n\n--name \nvalue\n  Credential name\n\n\n--tenant-user \nvalue\n  OpenStack user name   \n\n\n--tenant-password \nvalue\n  OpenStack password\n\n\n--tenant-name \nvalue\n  OpenStack tenant name    \n\n\n--endpoint \nvalue\n   OpenStack endpoint   \n\n\nopenstack keystone-v3\n \n\n\n--name \nvalue\n  Credential name\n\n\n--tenant-user \nvalue\n  OpenStack user name   \n\n\n--tenant-password \nvalue\n  OpenStack password\n\n\n--user-domain \nvalue\n  OpenStack user domain    \n\n\n--endpoint \nvalue\n   OpenStack endpoint  \n\n\nOptions\n\n\n--description \nvalue\n  Description of the resource\n\n\n--server \nvalue\n  Cloudbreak server address [$CB_SERVER_ADDRESS]\n\n\n--username \nvalue\n  Cloudbreak user name (e-mail address) [$CB_USER_NAME]\n\n\n--password \nvalue\n  Cloudbreak password [$CB_PASSWORD]  \n\n\n--profile \nvalue\n Selects a config profile to use [$CB_PROFILE]\n\n\n--auth-type \nvalue\n  Authentication method to use. Values: oauth2, basic [$CB_AUTH_TYPE]    \n\n\nAdditionally, the following option is available for OpenStack Keystone2 and Keystone3:\n\n\n--facing \nvalue\n API facing. One of: public, admin, internal\n\n\nAdditionally, the following options are available for OpenStack Keystone3:\n\n\n--project-domain-name \nvalue\n  OpenStack project domain name  \n\n\n--project-name \nvalue\n  OpenStack project name         \n\n\n--domain-name \nvalue\n  OpenStack domain name  \n\n\n--keystone-scope \nvalue\n OpenStack keystone scope. One of: default, domain, project  \n\n\nExamples\n\n\nModifies a role-based AWS credential:\n\n\ncb credential modify aws role-based --name my-credential1 --role-arn arn:aws:iam::517127065441:role/CredentialRole\n\n\n\nModifies a key-based AWS credential:\n\n\ncb credential modify aws key-based --name my-credential2 --access-key ABDVIRDFV3K4HLJ45SKA --secret-key D89L5pOPM+426Rtj3curKzJEJL3lYoNcP8GvguBV\n\n\n\nModifies an app-based Azure credential:\n\n\ncb credential modify azure app-based --name my-credential3 --subscription-id b8e7379e-568g-55d3-na82-45b8d421e998 --tenant-id  c79n5399-3231-65ba-8dgg-2g4e2a40085e --app-id 6d147d89-48d2-5de2-eef8-b89775bbfcg1 --app-password 4a8hBgfI52s/C8R5Sea2YHGnBFrD3fRONfdG8w7F2Ua=\n\n\n\nModifies a Google Cloud credential:\n\n\ncb credential modify gcp --name my-credential4 --project-id test-proj --service-account-id test@test-proj.iam.gserviceaccount.com --service-account-private-key-file /Users/test/3fff57a6f68e.p12\n\n\n\nModifies a role-based OpenStack credential which uses Keystone-v2:\n\n\ncb credential modify openstack keystone-v2 --name my-credential5 --tenant-user test --tenant-password MySecurePass123 --tenant-name test --endpoint http://openstack.test.organization.com:5000/v2.0\n\n\n\n\n\ndatabase create\n\n\nRegisters an existing external database with Cloudbreak.\n\n\nSub-commands\n  \n\n\nmysql\n  Registers a MySQL database configuration\n\n\noracle11\n  Registers an Oracle 11 database configuration\n\n\noracle12\n  Registers an Oracle 12 database configuration\n\n\npostgres\n  Registers a Postgres database configuration  \n\n\nRequired options\n\n\n--name \nvalue\n   Name for the database   \n\n\n--db-username \nvalue\n  Username for the JDBC connection\n\n\n--db-password \nvalue\n  Password for the JDBC connection\n\n\n--url \nvalue\n  JDBC connection URL in the form of jdbc:db-type://address:port/db\n\n\n--type \nvalue\n   Name if the service that will use the database (AMBARI, DRUID, HIVE, OOZIE, RANGER, SUPERSET, or other custom type)    \n\n\nIf using MySQL and Oracle, the \n--connector-jar-url value \nvalue\n parameter is required in all cases except the following: If you are using a custom image and you already placed the JAR file on the machine, then this parameter is not required.\n\n\nOptions\n\n\n--description \nvalue\n  Description for the database     \n\n\n--server \nvalue\n  Cloudbreak server address [$CB_SERVER_ADDRESS]\n\n\n--username \nvalue\n  Cloudbreak user name (e-mail address) [$CB_USER_NAME]\n\n\n--password \nvalue\n  Cloudbreak password [$CB_PASSWORD] \n\n\n--profile \nvalue\n  Selects a config profile to use [$CB_PROFILE]\n\n\n--auth-type \nvalue\n  Authentication method to use. Values: oauth2, basic [$CB_AUTH_TYPE] \n\n\n--public\n   Public in account  \n\n\nExamples\n\n\nRegisters an existing Postgres database called \"test-postgres\" with Cloudbreak:\n\n\ncb database create postgres --name testpostgres  --type HIVE --url jdbc:postgresql://test-db.cic6nusrpqec.us-west-2.rds.amazonaws.com:5432/testdb --db-username testuser --db-password MySecurePassword123\n\n\n\nThe connection URL includes three components db-type://address:port/db: \n\n\n\n\nDatabase type \"jdbc:postgresql\"  \n\n\nEndpoint \"test-db.cic6nusrpqec.us-west-2.rds.amazonaws.com:5432\"  \n\n\nPort \"5432\"  \n\n\nDatabase name \"testdb\"  \n\n\n\n\nRegisters an existing MySQL database called \"testmysql\" with Cloudbreak:  \n\n\ncb database create mysql --name testmysql --type OOZIE --url jdbc:mysql://test-db.cic6nusrpqec.us-west-2.rds.amazonaws.com:5432/testdb  --db-username test --db-password test --connector-jar-url http://example-page/driver-file.JAR\n\n\n\n\n\ndatabase delete\n\n\nUnregisters a previously registered database with Cloudbreak. It does not delete the database instance. \n\n\nRequired options\n\n\n--name \nvalue\n  Database registration name     \n\n\nOptions\n\n\n--server \nvalue\n  Cloudbreak server address [$CB_SERVER_ADDRESS]\n\n\n--username \nvalue\n  Cloudbreak user name (e-mail address) [$CB_USER_NAME]\n\n\n--password \nvalue\n  Cloudbreak password [$CB_PASSWORD]\n\n\n--profile \nvalue\n  Selects a config profile to use [$CB_PROFILE]\n\n\n--auth-type \nvalue\n  Authentication method to use. Values: oauth2, basic [$CB_AUTH_TYPE]\n\n\nExamples\n\n\ncb database delete --name testdatabase\n\n\n\n\n\ndatabase list\n\n\nLists all available database registrations.\n\n\nRequired options\n\n\nNone\n\n\nOptions\n\n\n--output \nvalue\n  Supported formats: json, yaml, table (default: \"json\") [$CB_OUT_FORMAT]\n\n\n--server \nvalue\n  Cloudbreak server address [$CB_SERVER_ADDRESS]\n\n\n--username \nvalue\n  Cloudbreak user name (e-mail address) [$CB_USER_NAME]\n\n\n--password \nvalue\n  Cloudbreak password [$CB_PASSWORD]\n\n\n--profile \nvalue\n  Selects a config profile to use [$CB_PROFILE]\n\n\n--auth-type \nvalue\n  Authentication method to use. Values: oauth2, basic [$CB_AUTH_TYPE]\n\n\nExamples\n\n\nLists existing database registrations:\n\n\ncb database list\n\n\n\nLists existing database registrations, with output presented in a table format:\n\n\ncb database list --output table\n\n\n\n\n\ndatabase test\n\n\nTest database connection.\n\n\nSub-commands\n  \n\n\nby-name\n    Tests a stored database configuration identified by its name\n\n\nby-params\n  Tests database connection parameters   \n\n\nRequired options\n  \n\n\nby-name\n  \n\n\n--name \nvalue\n  Database registration name    \n\n\nby-params\n  \n\n\n--db-username \nvalue\n  Username to use for the JDBC connection\n\n\n--db-password \nvalue\n  Password to use for the JDBC connection\n\n\n--url \nvalue\n  JDBC connection URL in the form of jdbc:db-type://address:port/db\n\n\n--type \nvalue\n  Type of database (the service name that will use the database  \n\n\nOptions\n\n\n--server \nvalue\n  Cloudbreak server address [$CB_SERVER_ADDRESS]\n\n\n--username \nvalue\n  Cloudbreak user name (e-mail address) [$CB_USER_NAME]\n\n\n--password \nvalue\n  Cloudbreak password [$CB_PASSWORD]  \n\n\n--profile \nvalue\n  Selects a config profile to use [$CB_PROFILE]\n\n\n--auth-type \nvalue\n  Authentication method to use. Values: oauth2, basic [$CB_AUTH_TYPE]  \n\n\nExamples\n\n\nTests connection to a previously registered database called \"testpostgres\":\n\n\ndatabase test --name testpostgres\n\n\n\nTests connection to a database based on connection parameters provided: \n\n\ncb database test by-params --type HIVE --url jdbc:postgresql://test-db.cic6nusrpqec.us-west-2.rds.amazonaws.com:5432/testdb --db-username testuser --db-password MySecurePassword123\n\n\n\n\n\nimagecatalog create\n\n\nRegisters a new custom image catalog based on the URL provided.  \n\n\nRequired options\n  \n\n\n--name \nvalue\n  Name for the image catalog\n\n\n--url \nvalue\n   URL location of the image catalog JSON file  \n\n\nOptions\n\n\n--description \nvalue\n  Description for the recipe \n\n\n--server \nvalue\n  Cloudbreak server address [$CB_SERVER_ADDRESS]\n\n\n--username \nvalue\n  Cloudbreak user name (e-mail address) [$CB_USER_NAME]\n\n\n--password \nvalue\n  Cloudbreak password [$CB_PASSWORD]  \n\n\n--profile \nvalue\n  Selects a config profile to use [$CB_PROFILE]\n\n\n--auth-type \nvalue\n  Authentication method to use. Values: oauth2, basic [$CB_AUTH_TYPE]\n\n\n--public\n   Public in account  \n\n\nExamples\n\n\nRegisters an image catalog called \"mycustomcatalog\" which is available at https://example.com/myimagecatalog.json: \n\n\ncb imagecatalog create --name mycustomcatalog --url https://example.com/myimagecatalog.json\n\n\n\nRelated links\n  \n\n\nCustom Images\n   \n\n\n\n\nimagecatalog delete\n\n\nDeletes a previously registered custom image catalog. It does not delete any cloud provider resources that you created as a prerequisite for creating the Cloudbreak credential.    \n\n\nRequired options\n  \n\n\n--name \nvalue\n  Image catalog name    \n\n\nOptions\n\n\n--output \nvalue\n  Supported formats: json, yaml, table (default: \"json\") [$CB_OUT_FORMAT]\n\n\n--server \nvalue\n  Cloudbreak server address [$CB_SERVER_ADDRESS]\n\n\n--username \nvalue\n  Cloudbreak user name (e-mail address) [$CB_USER_NAME]\n\n\n--password \nvalue\n  Cloudbreak password [$CB_PASSWORD]  \n\n\n--profile \nvalue\n  Selects a config profile to use [$CB_PROFILE]\n\n\n--auth-type \nvalue\n  Authentication method to use. Values: oauth2, basic [$CB_AUTH_TYPE]  \n\n\nExamples\n\n\nDeletes an image catalog called \"mycustomcatalog\":\n\n\ncb imagecatalog delete --name mycustomcatalog\n\n\n\nRelated links\n  \n\n\nCustom Images\n  \n\n\n\n\nimagecatalog images\n\n\nLists images from the specified image catalog available for the specified cloud provider.   \n\n\nSub-commands\n  \n\n\naws\n         Lists available aws images   \n\n\nazure\n       Lists available azure images   \n\n\ngcp\n         Lists available gcp images  \n\n\nopenstack\n   Lists available openstack images    \n\n\nRequired options\n\n\n--imagecatalog \nvalue\n  Name of the imagecatalog     \n\n\nOptions\n\n\n--output \nvalue\n  Supported formats: json, yaml, table (default: \"json\") [$CB_OUT_FORMAT]  \n\n\n--server \nvalue\n  Cloudbreak server address [$CB_SERVER_ADDRESS]\n\n\n--username \nvalue\n  Cloudbreak user name (e-mail address) [$CB_USER_NAME]\n\n\n--password \nvalue\n  Cloudbreak password [$CB_PASSWORD]   \n\n\n--profile \nvalue\n  Selects a config profile to use [$CB_PROFILE] \n\n\n--auth-type \nvalue\n  Authentication method to use. Values: oauth2, basic [$CB_AUTH_TYPE]  \n\n\nExamples\n  \n\n\nReturns date, description, Ambari version, and image ID for all AWS images from an image catalog called \"myimagecatalog\":\n\n\n./cb imagecatalog images aws --imagecatalog cloudbreak-default\n[\n  {\n    \"Date\": \"2017-10-13\",\n    \"Description\": \"Cloudbreak official base image\",\n    \"Version\": \"2.6.0.0\",\n    \"ImageID\": \"44b140a4-bd0b-457d-b174-e988bee3ca47\"\n  },\n  {\n    \"Date\": \"2017-11-16\",\n    \"Description\": \"Official Cloudbreak image\",\n    \"Version\": \"2.6.0.0\",\n    \"ImageID\": \"3c7598a4-ebd6-4a02-5638-882f5c7f7add\"\n  }\n]\n\n\n\nRelated links\n  \n\n\nCustom Images\n  \n\n\n\n\nimagecatalog list\n\n\nLists default and custom image catalogs registered with Cloudbreak instance.   \n\n\nRequired options\n  \n\n\nNone  \n\n\nOptions\n  \n\n\n--output \nvalue\n  Supported formats: json, yaml, table (default: \"json\") [$CB_OUT_FORMAT] \n\n\n--server \nvalue\n  Cloudbreak server address [$CB_SERVER_ADDRESS]\n\n\n--username \nvalue\n  Cloudbreak user name (e-mail address) [$CB_USER_NAME]\n\n\n--password \nvalue\n  Cloudbreak password [$CB_PASSWORD]  \n\n\n--profile \nvalue\n  Selects a config profile to use [$CB_PROFILE]\n\n\n--auth-type \nvalue\n  Authentication method to use. Values: oauth2, basic [$CB_AUTH_TYPE]  \n\n\nExamples\n\n\nLists existing image catalogs:  \n\n\ncb  imagecatalog list \n[\n  {\n    \"Name\": \"mycustomcatalog\",\n    \"Default\": false,\n    \"URL\": \"https://example.com/imagecatalog.json\"\n  },\n  {\n    \"Name\": \"cloudbreak-default\",\n    \"Default\": true,\n    \"URL\": \"https://s3-eu-west-1.amazonaws.com/cloudbreak-info/v2-dev-cb-image-catalog.json\"\n  }\n]\n\n\n\nRelated links\n  \n\n\nCustom Images\n  \n\n\n\n\nimagecatalog set-default\n\n\nSets the specified image catalog as default.  \n\n\nRequired options\n   \n\n\n--name \nvalue\n  Image catalog name     \n\n\nOptions\n\n\n--output \nvalue\n  Supported formats: json, yaml, table (default: \"json\") [$CB_OUT_FORMAT]\n\n\n--server \nvalue\n  Cloudbreak server address [$CB_SERVER_ADDRESS]\n\n\n--username \nvalue\n  Cloudbreak user name (e-mail address) [$CB_USER_NAME]\n\n\n--password \nvalue\n  Cloudbreak password [$CB_PASSWORD]  \n\n\n--profile \nvalue\n  Selects a config profile to use [$CB_PROFILE]\n\n\n--auth-type \nvalue\n  Authentication method to use. Values: oauth2, basic [$CB_AUTH_TYPE]  \n\n\nExamples\n\n\nSets \"mycustomcatalog\" as default:  \n\n\nimagecatalog set-default --name mycustomcatalog\n\n\n\nRelated links\n  \n\n\nCustom Images\n  \n\n\n\n\nldap create\n\n\nRegisters an existing LDAP with Cloudbreak.\n\n\nRequired options\n\n\n--name \nvalue\n  Name for the LDAP     \n\n\n--ldap-server \nvalue\n  Address of the LDAP server (e.g. ldap://10.0.0.1:384)\n\n\n--ldap-domain \nvalue\n   LDAP domain (e.g. ad.cb.com)\n\n\n--ldap-bind-dn \nvalue\n  LDAP bind DN (e.g. CN=Administrator,CN=Users,DC=ad,DC=cb,DC=com)\n\n\n--ldap-bind-password \nvalue\n  LDAP bind password\n\n\n--ldap-directory-type \nvalue\n   LDAP directory type (LDAP or ACTIVE_DIRECTORY) \n\n\n--ldap-user-search-base \nvalue\n   LDAP user search base (e.g. CN=Users,DC=ad,DC=cb,DC=com)\n\n\n--ldap-user-name-attribute \nvalue\n   LDAP user name attribute\n\n\n--ldap-user-object-class \nvalue\n   LDAP user object class\n\n\n--ldap-group-member-attribute \nvalue\n LDAP group member attribute\n\n\n--ldap-group-name-attribute \nvalue\n  LDAP group name attribute\n\n\n--ldap-group-object-class \nvalue\n  LDAP group object class\n\n\n--ldap-group-search-base \nvalue\n   LDAP group search base (e.g. OU=scopes,DC=ad,DC=cb,DC=com)  \n\n\nOptions\n\n\n--ldap-admin-group \nvalue\n  LDAP group of administrators\n\n\n--description \nvalue\n  Description for the LDAP    \n\n\n--server \nvalue\n  Cloudbreak server address [$CB_SERVER_ADDRESS]\n\n\n--username \nvalue\n  Cloudbreak user name (e-mail address) [$CB_USER_NAME]\n\n\n--password \nvalue\n  Cloudbreak password [$CB_PASSWORD] \n\n\n--profile \nvalue\n  Selects a config profile to use [$CB_PROFILE]\n\n\n--auth-type \nvalue\n  Authentication method to use. Values: oauth2, basic [$CB_AUTH_TYPE]\n\n\n--public\n   Public in account  \n\n\n\n\nldap delete\n\n\nDeletes selected LDAP registration from Cloudbreak. It does not delete the LDAP. \n\n\nRequired options\n\n\n--name \nvalue\n  LDAP name      \n\n\nOptions\n\n\n--output \nvalue\n  Supported formats: json, yaml, table (default: \"json\") [$CB_OUT_FORMAT]\n\n\n--server \nvalue\n  Cloudbreak server address [$CB_SERVER_ADDRESS]\n\n\n--username \nvalue\n  Cloudbreak user name (e-mail address) [$CB_USER_NAME]\n\n\n--password \nvalue\n  Cloudbreak password [$CB_PASSWORD]\n\n\n--profile \nvalue\n  Selects a config profile to use [$CB_PROFILE]\n\n\n--auth-type \nvalue\n  Authentication method to use. Values: oauth2, basic [$CB_AUTH_TYPE]\n\n\nExamples\n\n\ncb ldap delete --name testldap\n\n\n\n\n\nldap list\n\n\nLists all available LDAPs.\n\n\nRequired options\n\n\nNone\n\n\nOptions\n\n\n--output \nvalue\n  Supported formats: json, yaml, table (default: \"json\") [$CB_OUT_FORMAT]\n\n\n--server \nvalue\n  Cloudbreak server address [$CB_SERVER_ADDRESS]\n\n\n--username \nvalue\n  Cloudbreak user name (e-mail address) [$CB_USER_NAME]\n\n\n--password \nvalue\n  Cloudbreak password [$CB_PASSWORD]\n\n\n--profile \nvalue\n  Selects a config profile to use [$CB_PROFILE]\n\n\n--auth-type \nvalue\n  Authentication method to use. Values: oauth2, basic [$CB_AUTH_TYPE]\n\n\nExamples\n\n\nLists existing LDAPs:\n\n\ncb ldap list\n\n\n\nLists existing LDAPs, with output presented in a table format:\n\n\ncb ldap list --output table\n\n\n\n\n\nmpack create\n\n\nRegisters an existing management pack with Cloudbreak.\n\n\nRequired options\n\n\n--name \nvalue\n  Name for the mpack      \n\n\n--url \nvalue\n  URL that points to the location of the mpack tarball   \n\n\nOptions\n\n\n--purge\n  Purge existing resources specified in purge-list \n\n\n--purge-list \nvalue\n  Comma-separated list of resources to purge (stack-definitions,service-definitions,mpacks). By default (stack-definitions,mpacks) will be purged\n\n\n--force\n  Force install management pack  \n\n\n--description \nvalue\n  Description for the LDAP    \n\n\n--server \nvalue\n  Cloudbreak server address [$CB_SERVER_ADDRESS]\n\n\n--username \nvalue\n  Cloudbreak user name (e-mail address) [$CB_USER_NAME]\n\n\n--password \nvalue\n  Cloudbreak password [$CB_PASSWORD] \n\n\n--profile \nvalue\n  Selects a config profile to use [$CB_PROFILE]\n\n\n--auth-type \nvalue\n  Authentication method to use. Values: oauth2, basic [$CB_AUTH_TYPE]\n\n\n--public\n   Public in account  \n\n\nExamples\n\n\nRegisters a new mpack without purging:\n\n\ncb mpack create --name test-hdp-search --url http://public-repo-1.hortonworks.com/HDP-SOLR/hdp-solr-ambari-mp/solr-service-mpack-3.0.0.tar.gz\n\n\n\n\n\nmpack delete\n\n\nDeletes selected management registration from Cloudbreak. It does not delete the management pack. \n\n\nRequired options\n\n\n--name \nvalue\n  Management pack name      \n\n\nOptions\n\n\n--server \nvalue\n  Cloudbreak server address [$CB_SERVER_ADDRESS]\n\n\n--username \nvalue\n  Cloudbreak user name (e-mail address) [$CB_USER_NAME]\n\n\n--password \nvalue\n  Cloudbreak password [$CB_PASSWORD]\n\n\n--profile \nvalue\n  Selects a config profile to use [$CB_PROFILE]\n\n\n--auth-type \nvalue\n  Authentication method to use. Values: oauth2, basic [$CB_AUTH_TYPE]\n\n\nExamples\n\n\ncb mpack delete --name testmpack\n\n\n\n\n\nmpack list\n\n\nLists all available management packs.\n\n\nRequired options\n\n\nNone\n\n\nOptions\n\n\n--output \nvalue\n  Supported formats: json, yaml, table (default: \"json\") [$CB_OUT_FORMAT]\n\n\n--server \nvalue\n  Cloudbreak server address [$CB_SERVER_ADDRESS]\n\n\n--username \nvalue\n  Cloudbreak user name (e-mail address) [$CB_USER_NAME]\n\n\n--password \nvalue\n  Cloudbreak password [$CB_PASSWORD]\n\n\n--profile \nvalue\n  Selects a config profile to use [$CB_PROFILE]\n\n\n--auth-type \nvalue\n  Authentication method to use. Values: oauth2, basic [$CB_AUTH_TYPE]\n\n\nExamples\n\n\nLists all currently registered management packs and provides information about each:\n\n\ncb mpack list\n[\n  {\n    \"Name\": \"hdp-search-3\",\n    \"Description\": \"\",\n    \"URL\": \"http://public-repo-1.hortonworks.com/HDP-SOLR/hdp-solr-ambari-mp/solr-service-mpack-3.0.0.tar.gz\",\n    \"Purge\": \"false\",\n    \"PurgeList\": \"\",\n    \"Force\": \"false\"\n  }\n]\n\n\n\n\n\nproxy create\n\n\nRegisters an existing proxy with Cloudbreak.\n\n\nRequired options\n\n\n--name \nvalue\n  Name for the proxy   \n\n\n--proxy-host \nvalue\n  Hostname or IP address of the proxy\n\n\n--proxy-port \nvalue\n  Port of the proxy  \n\n\nOptions\n\n\n--proxy-protocol \nvalue\n  Protocol for the proxy (http or https) (default: \"http\")\n\n\n--proxy-user \nvalue\n   User for the proxy if basic auth is required\n\n\n--proxy-password \nvalue\n  Password for the proxy if basic auth is required\n\n\n--description \nvalue\n  Description for the proxy    \n\n\n--server \nvalue\n  Cloudbreak server address [$CB_SERVER_ADDRESS]\n\n\n--username \nvalue\n  Cloudbreak user name (e-mail address) [$CB_USER_NAME]\n\n\n--password \nvalue\n  Cloudbreak password [$CB_PASSWORD] \n\n\n--profile \nvalue\n  Selects a config profile to use [$CB_PROFILE]\n\n\n--auth-type \nvalue\n  Authentication method to use. Values: oauth2, basic [$CB_AUTH_TYPE]\n\n\n--public\n   Public in account  \n\n\n\n\nproxy delete\n\n\nUnregisters a previously registered proxy with Cloudbreak. It does not delete the proxy. \n\n\nRequired options\n\n\n--name \nvalue\n  Proxy registration name     \n\n\nOptions\n\n\n--output \nvalue\n  Supported formats: json, yaml, table (default: \"json\") [$CB_OUT_FORMAT]\n\n\n--server \nvalue\n  Cloudbreak server address [$CB_SERVER_ADDRESS]\n\n\n--username \nvalue\n  Cloudbreak user name (e-mail address) [$CB_USER_NAME]\n\n\n--password \nvalue\n  Cloudbreak password [$CB_PASSWORD]\n\n\n--profile \nvalue\n  Selects a config profile to use [$CB_PROFILE]\n\n\n--auth-type \nvalue\n  Authentication method to use. Values: oauth2, basic [$CB_AUTH_TYPE]\n\n\nExamples\n\n\ncb proxy delete --name testproxy\n\n\n\n\n\nproxy list\n\n\nLists all proxies that were previously registered with Cloudbreak.\n\n\nRequired options\n\n\nNone\n\n\nOptions\n\n\n--output \nvalue\n  Supported formats: json, yaml, table (default: \"json\") [$CB_OUT_FORMAT]\n\n\n--server \nvalue\n  Cloudbreak server address [$CB_SERVER_ADDRESS]\n\n\n--username \nvalue\n  Cloudbreak user name (e-mail address) [$CB_USER_NAME]\n\n\n--password \nvalue\n  Cloudbreak password [$CB_PASSWORD]\n\n\n--profile \nvalue\n  Selects a config profile to use [$CB_PROFILE]\n\n\n--auth-type \nvalue\n  Authentication method to use. Values: oauth2, basic [$CB_AUTH_TYPE]\n\n\nExamples\n\n\nLists existing proxy registrations:\n\n\ncb proxy list\n\n\n\nLists existing proxy registrations, with output presented in a table format:\n\n\ncb proxy list --output table\n\n\n\n\n\nrecipe create\n\n\nAdds a new recipe from a file or from a URL.\n\n\nSub-commands\n\n\nfrom-url\n  Creates a recipe by downloading it from a URL location\n\n\nfrom-file\n  Creates a recipe by reading it from a local file  \n\n\nRequired options\n\n\nfrom-url\n  \n\n\n--name \nvalue\n  Name for the recipe \n\n\n--execution-type \nvalue\n  Type of execution [pre-ambari-start, pre-termination, post-ambari-start, post-cluster-install]  \n\n\n--url \nvalue\n  URL location of the Ambari blueprint JSON file  \n\n\nfrom-file\n \n\n\n--name \nvalue\n  Name for the recipe\n\n\n--execution-type \nvalue\n  Type of execution [pre-ambari-start, pre-termination, post-ambari-start, post-cluster-install] \n\n\n--file \nvalue\n  Location of the Ambari blueprint JSON file  \n\n\nOptions\n\n\n--description \nvalue\n  Description for the recipe \n\n\n--server \nvalue\n  Cloudbreak server address [$CB_SERVER_ADDRESS]\n\n\n--username \nvalue\n  Cloudbreak user name (e-mail address) [$CB_USER_NAME]\n\n\n--password \nvalue\n  Cloudbreak password [$CB_PASSWORD]\n\n\n--profile \nvalue\n  Selects a config profile to use [$CB_PROFILE]\n\n\n--auth-type \nvalue\n  Authentication method to use. Values: oauth2, basic [$CB_AUTH_TYPE]\n\n\n--public\n   Public in account  \n\n\nExamples\n\n\nAdds a new recipe called \"test1\" from a URL:\n\n\ncb recipe create from-url --name \"test1\" --execution-type post-ambari-start --url http://some-site.com/test.sh\n\n\n\nAdds a new recipe called \"test2\" from a file:\n\n\ncb recipe create from-url --name \"test2\" --execution-type post-ambari-start --file /Users/test/Documents/test.sh\n\n\n\nRelated links\n\n\nRecipes\n\n\n\n\nrecipe delete\n\n\nDeletes an existing recipe.\n\n\nRequired options\n\n\n--name \nvalue\n  Recipe name  \n\n\nOptions\n\n\n--output \nvalue\n  Supported formats: json, yaml, table (default: \"json\") [$CB_OUT_FORMAT]\n\n\n--server \nvalue\n  Cloudbreak server address [$CB_SERVER_ADDRESS]\n\n\n--username \nvalue\n  Cloudbreak user name (e-mail address) [$CB_USER_NAME]\n\n\n--password \nvalue\n  Cloudbreak password [$CB_PASSWORD]\n\n\n--profile \nvalue\n  Selects a config profile to use [$CB_PROFILE] \n\n\n--auth-type \nvalue\n  Authentication method to use. Values: oauth2, basic [$CB_AUTH_TYPE]  \n\n\nExamples\n\n\ncb recipe delete --name test\n\n\n\n\n\nrecipe describe\n\n\nDescribes an existing recipe.\n\n\nRequired options\n\n\n--name \nvalue\n  Recipe name     \n\n\nOptions\n\n\n--output \nvalue\n  Supported formats: json, yaml, table (default: \"json\") [$CB_OUT_FORMAT]\n\n\n--server \nvalue\n  Cloudbreak server address [$CB_SERVER_ADDRESS]\n\n\n--username \nvalue\n  Cloudbreak user name (e-mail address) [$CB_USER_NAME]\n\n\n--password \nvalue\n  Cloudbreak password [$CB_PASSWORD]\n\n\n--profile \nvalue\n  Selects a config profile to use [$CB_PROFILE]\n\n\n--auth-type \nvalue\n  Authentication method to use. Values: oauth2, basic [$CB_AUTH_TYPE]  \n\n\nExamples\n\n\nDescribes a recipe called \"test\":\n\n\ncb recipe describe --name test\n{\n  \"Name\": \"test\",\n  \"Description\": \"\",\n  \"ExecutionType\": \"POST\"\n}\n\n\n\nDescribes a recipe called \"test\", with output presented in a table format:\n\n\ncb describe-recipe --name test --output table\n+------+-------------+----------------+\n| NAME | DESCRIPTION | EXECUTION TYPE |\n+------+-------------+----------------+\n| test |             | POST           |\n+------+-------------+----------------+\n\n\n\n\n\nrecipe list\n\n\nLists all available recipes.\n\n\nRequired options\n\n\nNone\n\n\nOptions\n\n\n--output \nvalue\n  Supported formats: json, yaml, table (default: \"json\") [$CB_OUT_FORMAT]\n\n\n--server \nvalue\n  Cloudbreak server address [$CB_SERVER_ADDRESS]\n\n\n--username \nvalue\n  Cloudbreak user name (e-mail address) [$CB_USER_NAME]\n\n\n--password \nvalue\n  Cloudbreak password [$CB_PASSWORD]\n\n\n--profile \nvalue\n  Selects a config profile to use [$CB_PROFILE]\n\n\n--auth-type \nvalue\n  Authentication method to use. Values: oauth2, basic [$CB_AUTH_TYPE]  \n\n\nExamples\n\n\nLists existing recipes:\n\n\ncb recipe list\n[\n  {\n    \"Name\": \"test\",\n    \"Description\": \"\",\n    \"ExecutionType\": \"POST\"\n  }\n]\n\n\n\nLists existing recipes, with output presented in a table format:\n\n\ncb recipe list --output table\n+------+-------------+-------------------+\n| NAME | DESCRIPTION | EXECUTION TYPE    |\n+------+-------------+-------------------+\n| test |             | POST-AMBARI-START |\n+------+-------------+-------------------+\n\n\n\n\n\n\nDebugging\n\n\nTo use debugging mode, pass the \n--debug\n option. \n\n\nChecking CLI Version\n\n\nTo check CLI version, use \ncb --version\n.", 
            "title": "CLI Reference"
        }, 
        {
            "location": "/cli-reference/index.html#cloudbreak-cli-reference", 
            "text": "This section will help you get started with the Cloudbreak CLI after you have  installed and configured it .", 
            "title": "Cloudbreak CLI reference"
        }, 
        {
            "location": "/cli-reference/index.html#command-structure", 
            "text": "The CLI command can contain multiple parts. The first part is a set of global options. The next part is the command. The next part is a set of command options and arguments which could include sub-commands.  cb [global options] command [command options] [arguments...]", 
            "title": "Command structure"
        }, 
        {
            "location": "/cli-reference/index.html#command-output", 
            "text": "You can control the output from the CLI using the --output argument. The possible output formats include:   JSON ( json )  YAML ( yaml )  Formatted table ( table )   For example:  cb cluster list --output json  cb clusters list --output yaml  cb cluster list --output table", 
            "title": "Command output"
        }, 
        {
            "location": "/cli-reference/index.html#commands", 
            "text": "Configure CLI:     configure      Cloud provider:   cloud availability-zones     cloud regions          cloud volumes      cloud instances       Credential:     credential create      credential delete           credential describe           credential list            credential modify       Blueprint:      blueprint create     blueprint delete              blueprint describe            blueprint list               Cluster:    cluster change-ambari-password      cluster create    cluster delete                 cluster describe       cluster generate-template      cluster generate-reinstall-template     cluster list       cluster repair       cluster retry            cluster scale         cluster start           cluster stop                 cluster sync     Database:               database create     database delete     database list     database test      Image catalog   imagecatalog create        imagecatalog delete    imagecatalog images                         imagecatalog list     imagecatalog set-default                LDAP:   ldap create     ldap delete     ldap list   Mpack   mpack create     mpack delete     mpack list   Proxy:   proxy create     proxy delete     proxy list   Recipe:     recipe create        recipe delete               recipe describe               recipe list", 
            "title": "Commands"
        }, 
        {
            "location": "/cli-reference/index.html#blueprint-create", 
            "text": "Adds a new blueprint from a file or from a URL.  Sub-commands  from-url  Creates a blueprint by downloading it from a URL location  from-file  Creates a blueprint by reading it from a local file  Required options  from-url    --name  value  Name for the blueprint  --url  value  URL location of the Ambari blueprint JSON file  from-file    --name  value  Name for the blueprint  --file  value  Location of the Ambari blueprint JSON file on the local machine  Options  --description  value   Description of the resource  --server  value   Cloudbreak server address [$CB_SERVER_ADDRESS]  --username  value   Cloudbreak user name (e-mail address) [$CB_USER_NAME]  --password  value   Cloudbreak password [$CB_PASSWORD]  --profile  value  Selects a config profile to use [$CB_PROFILE]  --auth-type  value   Authentication method to use. Values: oauth2, basic [$CB_AUTH_TYPE]  --public    Public in account    Examples  Adds a blueprint from a URL:  cb blueprint create from-url --url https://someurl.com/test.bp --name test1  Adds a blueprint from a local file:  cb blueprint create from-file --file /Users/test/Documents/blueprints/test.bp --name test2  Related links  Using Custom Blueprints", 
            "title": "blueprint create"
        }, 
        {
            "location": "/cli-reference/index.html#blueprint-delete", 
            "text": "Deletes an existing blueprint.  Required options  --name  value  Blueprint name   Options  --server  value   Cloudbreak server address [$CB_SERVER_ADDRESS]  --username  value   Cloudbreak user name (e-mail address) [$CB_USER_NAME]  --password  value   Cloudbreak password [$CB_PASSWORD]  --profile  value  Selects a config profile to use [$CB_PROFILE]   --auth-type  value   Authentication method to use. Values: oauth2, basic [$CB_AUTH_TYPE]    Examples  cb blueprint delete --name \"testbp\"", 
            "title": "blueprint delete"
        }, 
        {
            "location": "/cli-reference/index.html#blueprint-describe", 
            "text": "Describes an existing blueprint.  Required options  --name  value  Blueprint name   Options  --output  value   Supported formats: json, yaml, table (default: \"json\") [$CB_OUT_FORMAT]  --server  value   Cloudbreak server address [$CB_SERVER_ADDRESS]  --username  value   Cloudbreak user name (e-mail address) [$CB_USER_NAME]  --password  value   Cloudbreak password [$CB_PASSWORD]  --profile  value  Selects a config profile to use [$CB_PROFILE]  --auth-type  value   Authentication method to use. Values: oauth2, basic [$CB_AUTH_TYPE]    Examples  cb blueprint describe --name \"Data Science: Apache Spark 2.1, Apache Zeppelin 0.7.0\"\n{\n  \"Name\": \"Data Science: Apache Spark 2.1, Apache Zeppelin 0.7.0\",\n  \"Description\": \"Data Science: Apache Spark 2.1, Apache Zeppelin 0.7.0\",\n  \"HDPVersion\": \"2.6\",\n  \"HostgroupCount\": \"3\",\n  \"Tags\": \"DEFAULT\"\n}", 
            "title": "blueprint describe"
        }, 
        {
            "location": "/cli-reference/index.html#blueprint-list", 
            "text": "Lists available blueprints.  Required options  Nome  Options  --output  value  Supported formats: json, yaml, table (default: \"json\") [$CB_OUT_FORMAT]   --server  value   Cloudbreak server address [$CB_SERVER_ADDRESS]  --username  value   Cloudbreak user name (e-mail address) [$CB_USER_NAME]  --password  value   Cloudbreak password [$CB_PASSWORD]  --profile  value  Selects a config profile to use [$CB_PROFILE]  --auth-type  value   Authentication method to use. Values: oauth2, basic [$CB_AUTH_TYPE]    Examples  cb blueprint list\n[\n  {\n    \"Name\": \"EDW-Analytics: Apache Hive 2 LLAP, Apache Zeppelin\",\n    \"Description\": \"Useful for EDW analytics using Hive LLAP\",\n    \"HDPVersion\": \"2.6\",\n    \"HostgroupCount\": \"3\",\n    \"Tags\": \"DEFAULT\"\n  },\n  {\n    \"Name\": \"Data Science: Apache Spark 2, Apache Zeppelin\",\n    \"Description\": \"Useful for data science with Spark and Zeppelin\",\n    \"HDPVersion\": \"2.6\",\n    \"HostgroupCount\": \"3\",\n    \"Tags\": \"DEFAULT\"\n  },\n  {\n    \"Name\": \"EDW-ETL: Apache Hive, Apache Spark 2\",\n    \"Description\": \"Useful for ETL data processing with Hive and Spark\",\n    \"HDPVersion\": \"2.6\",\n    \"HostgroupCount\": \"3\",\n    \"Tags\": \"DEFAULT\"\n  },\n  {\n    \"Name\": \"Flow Management: Apache NiFi\",\n    \"Description\": \"Useful for data-flow management with Apache NiFi\",\n    \"HDPVersion\": \"3.1\",\n    \"HostgroupCount\": \"2\",\n    \"Tags\": \"DEFAULT\"\n  },\n  {\n    \"Name\": \"my-hdf-test\",\n    \"Description\": \"\",\n    \"HDPVersion\": \"3.1\",\n    \"HostgroupCount\": \"3\",\n    \"Tags\": \"USER_MANAGED\"\n }\n]", 
            "title": "blueprint list"
        }, 
        {
            "location": "/cli-reference/index.html#cloud-availability-zones", 
            "text": "Lists all availability zones available in the specified cloud provider region.   Required options  --credential  value   Name of the credential  --region  value    Name of the region    Options  --output  value   Supported formats: json, yaml, table (default: \"json\") [$CB_OUT_FORMAT]  --server  value   Cloudbreak server address [$CB_SERVER_ADDRESS]  --username  value   Cloudbreak user name (e-mail address) [$CB_USER_NAME]  --password  value   Cloudbreak password [$CB_PASSWORD]   --profile  value   Selects a config profile to use [$CB_PROFILE]   --auth-type  value   Authentication method to use. Values: oauth2, basic [$CB_AUTH_TYPE]     Examples  Lists availability zones in the us-west-2 (Oregon) region on the AWS account identified by the credential called \"aws-cred\":  cb cloud availability-zones --credential aws-cred --region us-west-2\n[\n  {\n    \"Name\": \"us-west-2a\"\n  },\n  {\n    \"Name\": \"us-west-2b\"\n  },\n  {\n    \"Name\": \"us-west-2c\"\n  }\n]", 
            "title": "cloud availability-zones"
        }, 
        {
            "location": "/cli-reference/index.html#cloud-regions", 
            "text": "Lists the available cloud provider regions.   Required options  --credential  value   Name of the credential    Options  --output  value   Supported formats: json, yaml, table (default: \"json\") [$CB_OUT_FORMAT]   --server  value   Cloudbreak server address [$CB_SERVER_ADDRESS]  --username  value   Cloudbreak user name (e-mail address) [$CB_USER_NAME]  --password  value   Cloudbreak password [$CB_PASSWORD]  --profile  value   Selects a config profile to use [$CB_PROFILE]   --auth-type  value   Authentication method to use. Values: oauth2, basic [$CB_AUTH_TYPE]     Examples  Lists regions available on the AWS account identified by the credential called \"aws-cred\":  cb cloud regions --credential aws-cred\n[\n  {\n    \"Name\": \"ap-northeast-1\",\n    \"Description\": \"Asia Pacific (Tokyo)\"\n  },\n  {\n    \"Name\": \"ap-northeast-2\",\n    \"Description\": \"Asia Pacific (Seoul)\"\n  },\n  ...", 
            "title": "cloud regions"
        }, 
        {
            "location": "/cli-reference/index.html#cloud-volumes", 
            "text": "Lists the available cloud provider volume types.   Sub-commands  aws      Lists the available aws volume types  azure    Lists the available azure volume types  gcp      Lists the available gcp volume types    Required options  None  Options  --output  value   Supported formats: json, yaml, table (default: \"json\") [$CB_OUT_FORMAT]  --server  value   Cloudbreak server address [$CB_SERVER_ADDRESS]  --username  value   Cloudbreak user name (e-mail address) [$CB_USER_NAME]  --password  value   Cloudbreak password [$CB_PASSWORD]    --profile  value   Selects a config profile to use [$CB_PROFILE]   --auth-type  value   Authentication method to use. Values: oauth2, basic [$CB_AUTH_TYPE]     Examples  Lists volumes available on AWS:  cb cloud volumes aws\n[\n  {\n    \"Name\": \"ephemeral\",\n    \"Description\": \"Ephemeral\"\n  },\n  {\n    \"Name\": \"gp2\",\n    \"Description\": \"General Purpose (SSD)\"\n  },\n  {\n    \"Name\": \"st1\",\n    \"Description\": \"Throughput Optimized HDD\"\n  },\n  {\n    \"Name\": \"standard\",\n    \"Description\": \"Magnetic\"\n  }\n]", 
            "title": "cloud volumes"
        }, 
        {
            "location": "/cli-reference/index.html#cloud-instances", 
            "text": "Lists the available cloud provider instance types.  Required options  --credential  value   Name of the credential  --region  value    Name of the region    Options  --availability-zone  value   Name of the availability zone   --output  value       Supported formats: json, yaml, table (default: \"json\") [$CB_OUT_FORMAT]  --server  value   Cloudbreak server address [$CB_SERVER_ADDRESS]  --username  value   Cloudbreak user name (e-mail address) [$CB_USER_NAME]  --password  value   Cloudbreak password [$CB_PASSWORD]  --profile  value      Selects a config profile to use [$CB_PROFILE]   --auth-type  value   Authentication method to use. Values: oauth2, basic [$CB_AUTH_TYPE]      Examples  Lists instance types available in the us-west-2 (Oregon) region on the AWS account identified by the credential called \"aws-cred\":  cb cloud instances --credential aws-cred --region us-west-2\n\n  {\n    \"Name\": \"c3.2xlarge\",\n    \"Cpu\": \"8\",\n    \"Memory\": \"15.0\",\n    \"AvailabilityZone\": \"us-west-2b\"\n  },\n  {\n    \"Name\": \"c3.4xlarge\",\n    \"Cpu\": \"16\",\n    \"Memory\": \"30.0\",\n    \"AvailabilityZone\": \"us-west-2b\"\n  },\n  ...", 
            "title": "cloud instances"
        }, 
        {
            "location": "/cli-reference/index.html#cluster-change-ambari-password", 
            "text": "Changes Ambari password.  Required options  --name  value   Cluster name  --old-password  value  Old Ambari password   --new-password  value   New Ambari password    --ambari-user  value   Ambari user     Options  --output  value   Supported formats: json, yaml, table (default: \"json\") [$CB_OUT_FORMAT]  --server  value   Cloudbreak server address [$CB_SERVER_ADDRESS]  --username  value   Cloudbreak user name (e-mail address) [$CB_USER_NAME]  --password  value   Cloudbreak password [$CB_PASSWORD]  --profile  value   Selects a config profile to use [$CB_PROFILE]  --auth-type  value   Authentication method to use. Values: oauth2, basic [$CB_AUTH_TYPE]      Examples  Changes password for Ambari user called \"admin\" for a cluster called \"test1234\":  cb cluster change-ambari-password --name test1234 --old-password 123456 --new-password Ambari123456 --ambari-user admin", 
            "title": "cluster change-ambari-password"
        }, 
        {
            "location": "/cli-reference/index.html#cluster-create", 
            "text": "Creates a new cluster based on a JSON template.   Required options  --cli-input-json  value   User provided file in JSON format    Options  --name  value   Name for the cluster  --description  value   Description of resource   --input-json-param-password  value   Password for the cluster and Ambari   --wait   Wait for the operation to finish. No argument is required   --server  value   Cloudbreak server address [$CB_SERVER_ADDRESS]  --username  value   Cloudbreak user name (e-mail address) [$CB_USER_NAME]  --password  value   Cloudbreak password [$CB_PASSWORD]   --profile  value   Selects a config profile to use [$CB_PROFILE]     --auth-type  value   Authentication method to use. Values: oauth2, basic [$CB_AUTH_TYPE]    --public    Public in account    Examples  Creates a cluster called \"testcluster\" based on a local JSON file called \"mytemplate.json\" located in the /Users/test/Documents directory:     cb cluster create --name testcluster --cli-input-json /Users/test/Documents/mytemplate.json  Related links  Obtain Cluster JSON Template from the UI", 
            "title": "cluster create"
        }, 
        {
            "location": "/cli-reference/index.html#cluster-delete", 
            "text": "Deletes an existing cluster.   Required options  --name  value   Cluster name  Options  --force   Force the operation  --wait   Wait for the operation to finish. No argument is required  --server  value   Cloudbreak server address [$CB_SERVER_ADDRESS]  --username  value   Cloudbreak user name (e-mail address) [$CB_USER_NAME]  --password  value   Cloudbreak password [$CB_PASSWORD]  --profile  value   Selects a config profile to use [$CB_PROFILE]   --auth-type  value   Authentication method to use. Values: oauth2, basic [$CB_AUTH_TYPE]     Examples  cb cluster delete --name test1234", 
            "title": "cluster delete"
        }, 
        {
            "location": "/cli-reference/index.html#cluster-describe", 
            "text": "Describes an existing cluster.  Required options  --name  value   Cluster name  Options  --output  value   Supported formats: json, yaml, table (default: \"json\") [$CB_OUT_FORMAT]   --server  value   Cloudbreak server address [$CB_SERVER_ADDRESS]  --username  value   Cloudbreak user name (e-mail address) [$CB_USER_NAME]  --password  value   Cloudbreak password [$CB_PASSWORD]  --profile  value   Selects a config profile to use [$CB_PROFILE]  --auth-type  value   Authentication method to use. Values: oauth2, basic [$CB_AUTH_TYPE]     Examples  Returns a JSON file describing an existing cluster called \"test1234\":  ./cb cluster describe --name test1234  The command returns JSON output which due to space limitation was not captured in the example.", 
            "title": "cluster describe"
        }, 
        {
            "location": "/cli-reference/index.html#cluster-generate-template", 
            "text": "Generates a provider-specific cluster template in JSON format.  Sub-commands  aws new-network  Generates an AWS cluster JSON template with new network  aws existing-network  Generates an AWS cluster JSON template with existing network  aws existing-subnet  Generates an AWS cluster JSON template with existing network and subnet    azure new-network  Generates an Azure cluster JSON template with new network  azure existing-subnet  Generates an Azure cluster JSON template with existing network and subnet    gcp new-network  Generates an GCP cluster JSON template with new network  gcp existing-network  Generates an GCP cluster JSON template with existing network   gcp existing-subnet  Generates an GCP cluster JSON template with existing network and subnet  gcp legacy-network  Generates an GCP cluster JSON template with legacy network without subnets      openstack new-network  Generates an OS cluster JSON template with new network   openstack existing-network  Generates an OS cluster JSON template with existing network   openstack existing-subnet  Generates an OS cluster JSON template with existing network and subnet     Examples  cb cluster generate-template aws new-network  Related Commands  Related Links  Obtain Cluster JSON Template from the UI", 
            "title": "cluster generate-template"
        }, 
        {
            "location": "/cli-reference/index.html#cluster-generate-reinstall-template", 
            "text": "Generates a cluster template that you can use to reinstall the cluster if installation went fail.   Required options  --blueprint-name  value   Name of the blueprint   Options  --output  value   Supported formats: json, yaml, table (default: \"json\") [$CB_OUT_FORMAT]  --server  value   Cloudbreak server address [$CB_SERVER_ADDRESS]  --username  value   Cloudbreak user name (e-mail address) [$CB_USER_NAME]  --password  value   Cloudbreak password [$CB_PASSWORD]    --profile  value   Selects a config profile to use [$CB_PROFILE]  --auth-type  value   Authentication method to use. Values: oauth2, basic [$CB_AUTH_TYPE]     Examples  cb cluster generate-reinstall-template --blueprint-name \"EDW-ETL: Apache Hive 1.2.1, Apache Spark 2.1\"", 
            "title": "cluster generate-reinstall-template"
        }, 
        {
            "location": "/cli-reference/index.html#cluster-list", 
            "text": "Lists all clusters which are currently associated with the Cloudbreak instance.  Required options  None  Options  --output  value   Supported formats: json, yaml, table (default: \"json\") [$CB_OUT_FORMAT]  --server  value   Cloudbreak server address [$CB_SERVER_ADDRESS]  --username  value   Cloudbreak user name (e-mail address) [$CB_USER_NAME]  --password  value   Cloudbreak password [$CB_PASSWORD]    --profile  value   Selects a config profile to use [$CB_PROFILE]  --auth-type  value   Authentication method to use. Values: oauth2, basic [$CB_AUTH_TYPE]    Examples  Lists available clusters:   cb cluster list\n[\n  {\n    \"Name\": \"test1234\",\n    \"Description\": \"\",\n    \"CloudPlatform\": \"AZURE\",\n    \"StackStatus\": \"UPDATE_IN_PROGRESS\",\n    \"ClusterStatus\": \"REQUESTED\"\n  }\n]  Lists available clusters, with output in a table format:  cb cluster list --output table\n+----------+-------------+---------------+--------------------+---------------+\n|   NAME   | DESCRIPTION | CLOUDPLATFORM |    STACKSTATUS     | CLUSTERSTATUS |\n+----------+-------------+---------------+--------------------+---------------+\n| test1234 |             | AZURE         | UPDATE_IN_PROGRESS | REQUESTED     |\n+----------+-------------+---------------+--------------------+---------------+", 
            "title": "cluster list"
        }, 
        {
            "location": "/cli-reference/index.html#cluster-repair", 
            "text": "Repairs a cluster if cluster installation failed.  Required options  --name  value   Cluster name  Options  --wait   Wait for the operation to finish. No argument is required  --output  value   Supported formats: json, yaml, table (default: \"json\") [$CB_OUT_FORMAT]  --server  value   Cloudbreak server address [$CB_SERVER_ADDRESS]  --username  value   Cloudbreak user name (e-mail address) [$CB_USER_NAME]  --password  value   Cloudbreak password [$CB_PASSWORD]    --profile  value   Selects a config profile to use [$CB_PROFILE]  --auth-type  value   Authentication method to use. Values: oauth2, basic [$CB_AUTH_TYPE]     Examples  cb cluster repair --name test1234", 
            "title": "cluster repair"
        }, 
        {
            "location": "/cli-reference/index.html#cluster-retry", 
            "text": "Retries the process if cluster or stack provisioning failed.  Required options  --name  value   Cluster name  Options  --wait   Wait for the operation to finish. No argument is required  --output  value   Supported formats: json, yaml, table (default: \"json\") [$CB_OUT_FORMAT]  --server  value   Cloudbreak server address [$CB_SERVER_ADDRESS]  --username  value   Cloudbreak user name (e-mail address) [$CB_USER_NAME]  --password  value   Cloudbreak password [$CB_PASSWORD]    --profile  value   Selects a config profile to use [$CB_PROFILE]  --auth-type  value   Authentication method to use. Values: oauth2, basic [$CB_AUTH_TYPE]     Examples  cb cluster retry --name test1234", 
            "title": "cluster retry"
        }, 
        {
            "location": "/cli-reference/index.html#cluster-scale", 
            "text": "Scales a cluster by adding or removing nodes.  Required options  --name  value   Cluster name  --group-name  value   Name of the group to scale  --desired-node-count  value   Desired number of nodes    Options  --wait   Wait for the operation to finish. No argument is required   --output  value   Supported formats: json, yaml, table (default: \"json\") [$CB_OUT_FORMAT]  --server  value   Cloudbreak server address [$CB_SERVER_ADDRESS]  --username  value   Cloudbreak user name (e-mail address) [$CB_USER_NAME]  --password  value   Cloudbreak password [$CB_PASSWORD]    --profile  value   Selects a config profile to use [$CB_PROFILE]  --auth-type  value   Authentication method to use. Values: oauth2, basic [$CB_AUTH_TYPE]     Examples  cb cluster scale --name test1234 --group-name worker --desired node-count 3", 
            "title": "cluster scale"
        }, 
        {
            "location": "/cli-reference/index.html#cluster-start", 
            "text": "Starts a cluster which has previously been stopped.  Required options  --name  value   Cluster name  Options  --wait   Wait for the operation to finish. No argument is required  --output  value   Supported formats: json, yaml, table (default: \"json\") [$CB_OUT_FORMAT]  --server  value   Cloudbreak server address [$CB_SERVER_ADDRESS]  --username  value   Cloudbreak user name (e-mail address) [$CB_USER_NAME]  --password  value   Cloudbreak password [$CB_PASSWORD]    --profile  value   Selects a config profile to use [$CB_PROFILE]  --auth-type  value   Authentication method to use. Values: oauth2, basic [$CB_AUTH_TYPE]     Examples  cb cluster start --name test1234", 
            "title": "cluster start"
        }, 
        {
            "location": "/cli-reference/index.html#cluster-stop", 
            "text": "Stops a cluster.  Required options  --name  value   Cluster name  Options  --wait   Wait for the operation to finish. No argument is required  --output  value   Supported formats: json, yaml, table (default: \"json\") [$CB_OUT_FORMAT]  --server  value   Cloudbreak server address [$CB_SERVER_ADDRESS]  --username  value   Cloudbreak user name (e-mail address) [$CB_USER_NAME]  --password  value   Cloudbreak password [$CB_PASSWORD]    --profile  value   Selects a config profile to use [$CB_PROFILE]  --auth-type  value   Authentication method to use. Values: oauth2, basic [$CB_AUTH_TYPE]    Examples  cb cluster stop --name test1234", 
            "title": "cluster stop"
        }, 
        {
            "location": "/cli-reference/index.html#cluster-sync", 
            "text": "Synchronizes a cluster with the cloud provider.  Required options  --name  value   Cluster name  Options  --output  value   Supported formats: json, yaml, table (default: \"json\") [$CB_OUT_FORMAT]  --server  value   Cloudbreak server address [$CB_SERVER_ADDRESS]  --username  value   Cloudbreak user name (e-mail address) [$CB_USER_NAME]  --password  value   Cloudbreak password [$CB_PASSWORD]    --profile  value   Selects a config profile to use [$CB_PROFILE]  --auth-type  value   Authentication method to use. Values: oauth2, basic [$CB_AUTH_TYPE]    Examples  cb cluster sync --name test1234", 
            "title": "cluster sync"
        }, 
        {
            "location": "/cli-reference/index.html#configure", 
            "text": "Configures the Cloudbreak server address and credentials used to communicate with this server.  Required options  --server  value   Server address [$CB_SERVER_ADDRESS]  --username  value      User name (e-mail address) [$CB_USER_NAME]    Options  --password  value   Password [$CB_PASSWORD]  --profile  value   Select a config profile to use [$CB_PROFILE]   --output  value   Supported formats: json, yaml, table (default: \"json\") [$CB_OUT_FORMAT]  --auth-type  value   Authentication method to use. Values: oauth2, basic [$CB_AUTH_TYPE]    Examples  This example configures the server address with username and password:  cb configure --server https://ec2-11-111-111-11.compute-1.amazonaws.com --username admin@hortonworks.com --password MySecurePassword123  This example configures the server address with username but without a password:  cb configure --server https://ec2-11-111-111-11.compute-1.amazonaws.com --username admin@hortonworks.com  Related links  Configure CLI", 
            "title": "configure"
        }, 
        {
            "location": "/cli-reference/index.html#credential-create", 
            "text": "Creates a new Cloudbreak credential.  Sub-commands  aws role-based   Creates a new AWS credential  aws key-based   Creates a new AWS credential  azure app-based  Creates a new app-based Azure credential  gcp  Creates a new gcp credential  openstack keystone-v2  Creates a new OpenStack credential  openstack keystone-v3  Creates a new OpenStack credential   Required options  aws role-based    --name  value   Name for the credential   --role-arn  value  IAM Role ARN of the role used for Cloudbreak credential    aws key-based    --name  value   Name for the credential    --access-key  value   AWS Access Key  --secret-key  value   AWS Secret Key    azure app-based    --name  value   Name for the credential    --subscription-id  value   Subscription ID from your Azure Subscriptions  --tenant-id  value   Directory ID from your Azure Active Directory   Properties      --app-id  value   Application ID of your app from your Azure Active Directory   App Registrations            --app-password  value   Your application key from app registration's Settings   Keys    gcp    --name  value   Name for the credential    --project-id  value   Project ID from your GCP account                        --service-account-id  value   Your GCP Service account ID from IAM   Admin   Service accounts                 --service-account-private-key-file  value   P12 key from your GCP service account    openstack keystone-v2     --name  value   Name for the credential    --tenant-user  value   OpenStack user name     --tenant-password  value   OpenStack password  --tenant-name  value   OpenStack tenant name      --endpoint  value    OpenStack endpoint     openstack keystone-v3    --name  value   Name for the credential   --tenant-user  value   OpenStack user name     --tenant-password  value   OpenStack password  --user-domain  value   OpenStack user domain      --endpoint  value    OpenStack endpoint    Options  --description  value   Description of the resource  --server  value   Cloudbreak server address [$CB_SERVER_ADDRESS]  --username  value   Cloudbreak user name (e-mail address) [$CB_USER_NAME]  --password  value   Cloudbreak password [$CB_PASSWORD]    --profile  value  Selects a config profile to use [$CB_PROFILE]  --auth-type  value   Authentication method to use. Values: oauth2, basic [$CB_AUTH_TYPE]  --public    Public in account    Additionally, the following option is available for OpenStack Keystone2 and Keystone3:  --facing  value  API facing. One of: public, admin, internal  Additionally, the following options are available for OpenStack Keystone3:  --project-domain-name  value   OpenStack project domain name    --project-name  value   OpenStack project name           --domain-name  value   OpenStack domain name    --keystone-scope  value  OpenStack keystone scope. One of: default, domain, project     Examples  Creates a role-based credential on AWS:  cb credential create aws role-based --name my-credential1 --role-arn arn:aws:iam::517127065441:role/CredentialRole  Creates a key-based credential on AWS:  cb credential create aws key-based --name my-credential2 --access-key ABDVIRDFV3K4HLJ45SKA --secret-key D89L5pOPM+426Rtj3curKzJEJL3lYoNcP8GvguBV  Creates an app-based credential on Azure:  cb credential create azure app-based --name my-credential3 --subscription-id b8e7379e-568g-55d3-na82-45b8d421e998 --tenant-id  c79n5399-3231-65ba-8dgg-2g4e2a40085e --app-id 6d147d89-48d2-5de2-eef8-b89775bbfcg1 --app-password 4a8hBgfI52s/C8R5Sea2YHGnBFrD3fRONfdG8w7F2Ua=  Creates a credential on Google Cloud:  cb credential create gcp --name my-credential4 --project-id test-proj --service-account-id test@test-proj.iam.gserviceaccount.com --service-account-private-key-file /Users/test/3fff57a6f68e.p12  Creates a role-based credential on OpenStack with Keystone-v2:  cb credential create openstack keystone-v2 --name my-credential5 --tenant-user test --tenant-password MySecurePass123 --tenant-name test --endpoint http://openstack.test.organization.com:5000/v2.0  Related links     Create Credential on AWS  Create Credential on Azure  Create Credential on GCP  Create Credential on OpenStack", 
            "title": "credential create"
        }, 
        {
            "location": "/cli-reference/index.html#credential-delete", 
            "text": "Deletes an existing Cloudbreak credential.  Required options  --name  value   Credential name   Options  --server  value   Cloudbreak server address [$CB_SERVER_ADDRESS]  --username  value   Cloudbreak user name (e-mail address) [$CB_USER_NAME]  --password  value   Cloudbreak password [$CB_PASSWORD]    --profile  value  Selects a config profile to use [$CB_PROFILE]  --auth-type  value   Authentication method to use. Values: oauth2, basic [$CB_AUTH_TYPE]    Examples  cb credential delete --name test-cred", 
            "title": "credential delete"
        }, 
        {
            "location": "/cli-reference/index.html#credential-describe", 
            "text": "Describes an existing credential.  Required options  --name  value   Credential name   Options  --output  value   Supported formats: json, yaml, table (default: \"json\") [$CB_OUT_FORMAT]    --server  value   Cloudbreak server address [$CB_SERVER_ADDRESS]  --username  value   Cloudbreak user name (e-mail address) [$CB_USER_NAME]  --password  value   Cloudbreak password [$CB_PASSWORD]    --profile  value  Selects a config profile to use [$CB_PROFILE]  --auth-type  value   Authentication method to use. Values: oauth2, basic [$CB_AUTH_TYPE]    Examples  cb credential describe --name testcred\n{\n  \"Name\": \"testcred\",\n  \"Description\": \"\",\n  \"CloudPlatform\": \"AZURE\"\n}", 
            "title": "credential describe"
        }, 
        {
            "location": "/cli-reference/index.html#credential-list", 
            "text": "Lists existing Cloudbreak credentials.  Required options  None  Options  --output  value   Supported formats: json, yaml, table (default: \"json\") [$CB_OUT_FORMAT]  --server  value   Cloudbreak server address [$CB_SERVER_ADDRESS]  --username  value   Cloudbreak user name (e-mail address) [$CB_USER_NAME]  --password  value   Cloudbreak password [$CB_PASSWORD]    --profile  value  Selects a config profile to use [$CB_PROFILE]  --auth-type  value   Authentication method to use. Values: oauth2, basic [$CB_AUTH_TYPE]    Examples  Lists credentials:  cb credential list \n[\n  {\n    \"Name\": \"testcred\",\n    \"Description\": \"\",\n    \"CloudPlatform\": \"AZURE\"\n  }\n]  Lists credentials, with output formatted in a table format:  cb credential list --output table\n+---------+-------------+---------------+\n|  NAME   | DESCRIPTION | CLOUDPLATFORM |\n+---------+-------------+---------------+\n| armcred |             | AZURE         |\n+---------+-------------+---------------+", 
            "title": "credential list"
        }, 
        {
            "location": "/cli-reference/index.html#credential-modify", 
            "text": "Modifies an existing Cloudbreak credential.    The  --name  parameter is used to identify the credential that is being modified, and therefore its value cannot be modified.       Sub-commands  aws role-based   Modifies an AWS role-based credential  aws key-based   Modifies an AWS key-based credential  azure app-based  Modifies an app-based Azure credential  gcp  Modifies a Google Cloud credential  openstack keystone-v2  Modifies an OpenStack v2 credential  openstack keystone-v3  Modifies an  OpenStack v3 credential   Required options  aws role-based    --name  value   Credential name  --role-arn  value  IAM Role ARN of the role used for Cloudbreak credential    aws key-based    --name  value   Credential name  --access-key  value   AWS Access Key  --secret-key  value   AWS Secret Key    azure app-based    --name  value   Credential name  --subscription-id  value   Subscription ID from your Azure Subscriptions  --tenant-id  value   Directory ID from your Azure Active Directory   Properties      --app-id  value   Application ID of your app from your Azure Active Directory   App Registrations            --app-password  value   Your application key from app registration's Settings   Keys    gcp    --name  value   Credential name   --project-id  value   Project ID from your GCP account                        --service-account-id  value   Your GCP Service account ID from IAM   Admin   Service accounts                 --service-account-private-key-file  value   P12 key from your GCP service account    openstack keystone-v2     --name  value   Credential name  --tenant-user  value   OpenStack user name     --tenant-password  value   OpenStack password  --tenant-name  value   OpenStack tenant name      --endpoint  value    OpenStack endpoint     openstack keystone-v3    --name  value   Credential name  --tenant-user  value   OpenStack user name     --tenant-password  value   OpenStack password  --user-domain  value   OpenStack user domain      --endpoint  value    OpenStack endpoint    Options  --description  value   Description of the resource  --server  value   Cloudbreak server address [$CB_SERVER_ADDRESS]  --username  value   Cloudbreak user name (e-mail address) [$CB_USER_NAME]  --password  value   Cloudbreak password [$CB_PASSWORD]    --profile  value  Selects a config profile to use [$CB_PROFILE]  --auth-type  value   Authentication method to use. Values: oauth2, basic [$CB_AUTH_TYPE]      Additionally, the following option is available for OpenStack Keystone2 and Keystone3:  --facing  value  API facing. One of: public, admin, internal  Additionally, the following options are available for OpenStack Keystone3:  --project-domain-name  value   OpenStack project domain name    --project-name  value   OpenStack project name           --domain-name  value   OpenStack domain name    --keystone-scope  value  OpenStack keystone scope. One of: default, domain, project    Examples  Modifies a role-based AWS credential:  cb credential modify aws role-based --name my-credential1 --role-arn arn:aws:iam::517127065441:role/CredentialRole  Modifies a key-based AWS credential:  cb credential modify aws key-based --name my-credential2 --access-key ABDVIRDFV3K4HLJ45SKA --secret-key D89L5pOPM+426Rtj3curKzJEJL3lYoNcP8GvguBV  Modifies an app-based Azure credential:  cb credential modify azure app-based --name my-credential3 --subscription-id b8e7379e-568g-55d3-na82-45b8d421e998 --tenant-id  c79n5399-3231-65ba-8dgg-2g4e2a40085e --app-id 6d147d89-48d2-5de2-eef8-b89775bbfcg1 --app-password 4a8hBgfI52s/C8R5Sea2YHGnBFrD3fRONfdG8w7F2Ua=  Modifies a Google Cloud credential:  cb credential modify gcp --name my-credential4 --project-id test-proj --service-account-id test@test-proj.iam.gserviceaccount.com --service-account-private-key-file /Users/test/3fff57a6f68e.p12  Modifies a role-based OpenStack credential which uses Keystone-v2:  cb credential modify openstack keystone-v2 --name my-credential5 --tenant-user test --tenant-password MySecurePass123 --tenant-name test --endpoint http://openstack.test.organization.com:5000/v2.0", 
            "title": "credential modify"
        }, 
        {
            "location": "/cli-reference/index.html#database-create", 
            "text": "Registers an existing external database with Cloudbreak.  Sub-commands     mysql   Registers a MySQL database configuration  oracle11   Registers an Oracle 11 database configuration  oracle12   Registers an Oracle 12 database configuration  postgres   Registers a Postgres database configuration    Required options  --name  value    Name for the database     --db-username  value   Username for the JDBC connection  --db-password  value   Password for the JDBC connection  --url  value   JDBC connection URL in the form of jdbc:db-type://address:port/db  --type  value    Name if the service that will use the database (AMBARI, DRUID, HIVE, OOZIE, RANGER, SUPERSET, or other custom type)      If using MySQL and Oracle, the  --connector-jar-url value  value  parameter is required in all cases except the following: If you are using a custom image and you already placed the JAR file on the machine, then this parameter is not required.  Options  --description  value   Description for the database       --server  value   Cloudbreak server address [$CB_SERVER_ADDRESS]  --username  value   Cloudbreak user name (e-mail address) [$CB_USER_NAME]  --password  value   Cloudbreak password [$CB_PASSWORD]   --profile  value   Selects a config profile to use [$CB_PROFILE]  --auth-type  value   Authentication method to use. Values: oauth2, basic [$CB_AUTH_TYPE]   --public    Public in account    Examples  Registers an existing Postgres database called \"test-postgres\" with Cloudbreak:  cb database create postgres --name testpostgres  --type HIVE --url jdbc:postgresql://test-db.cic6nusrpqec.us-west-2.rds.amazonaws.com:5432/testdb --db-username testuser --db-password MySecurePassword123  The connection URL includes three components db-type://address:port/db:    Database type \"jdbc:postgresql\"    Endpoint \"test-db.cic6nusrpqec.us-west-2.rds.amazonaws.com:5432\"    Port \"5432\"    Database name \"testdb\"     Registers an existing MySQL database called \"testmysql\" with Cloudbreak:    cb database create mysql --name testmysql --type OOZIE --url jdbc:mysql://test-db.cic6nusrpqec.us-west-2.rds.amazonaws.com:5432/testdb  --db-username test --db-password test --connector-jar-url http://example-page/driver-file.JAR", 
            "title": "database create"
        }, 
        {
            "location": "/cli-reference/index.html#database-delete", 
            "text": "Unregisters a previously registered database with Cloudbreak. It does not delete the database instance.   Required options  --name  value   Database registration name       Options  --server  value   Cloudbreak server address [$CB_SERVER_ADDRESS]  --username  value   Cloudbreak user name (e-mail address) [$CB_USER_NAME]  --password  value   Cloudbreak password [$CB_PASSWORD]  --profile  value   Selects a config profile to use [$CB_PROFILE]  --auth-type  value   Authentication method to use. Values: oauth2, basic [$CB_AUTH_TYPE]  Examples  cb database delete --name testdatabase", 
            "title": "database delete"
        }, 
        {
            "location": "/cli-reference/index.html#database-list", 
            "text": "Lists all available database registrations.  Required options  None  Options  --output  value   Supported formats: json, yaml, table (default: \"json\") [$CB_OUT_FORMAT]  --server  value   Cloudbreak server address [$CB_SERVER_ADDRESS]  --username  value   Cloudbreak user name (e-mail address) [$CB_USER_NAME]  --password  value   Cloudbreak password [$CB_PASSWORD]  --profile  value   Selects a config profile to use [$CB_PROFILE]  --auth-type  value   Authentication method to use. Values: oauth2, basic [$CB_AUTH_TYPE]  Examples  Lists existing database registrations:  cb database list  Lists existing database registrations, with output presented in a table format:  cb database list --output table", 
            "title": "database list"
        }, 
        {
            "location": "/cli-reference/index.html#database-test", 
            "text": "Test database connection.  Sub-commands     by-name     Tests a stored database configuration identified by its name  by-params   Tests database connection parameters     Required options     by-name     --name  value   Database registration name      by-params     --db-username  value   Username to use for the JDBC connection  --db-password  value   Password to use for the JDBC connection  --url  value   JDBC connection URL in the form of jdbc:db-type://address:port/db  --type  value   Type of database (the service name that will use the database    Options  --server  value   Cloudbreak server address [$CB_SERVER_ADDRESS]  --username  value   Cloudbreak user name (e-mail address) [$CB_USER_NAME]  --password  value   Cloudbreak password [$CB_PASSWORD]    --profile  value   Selects a config profile to use [$CB_PROFILE]  --auth-type  value   Authentication method to use. Values: oauth2, basic [$CB_AUTH_TYPE]    Examples  Tests connection to a previously registered database called \"testpostgres\":  database test --name testpostgres  Tests connection to a database based on connection parameters provided:   cb database test by-params --type HIVE --url jdbc:postgresql://test-db.cic6nusrpqec.us-west-2.rds.amazonaws.com:5432/testdb --db-username testuser --db-password MySecurePassword123", 
            "title": "database test"
        }, 
        {
            "location": "/cli-reference/index.html#imagecatalog-create", 
            "text": "Registers a new custom image catalog based on the URL provided.    Required options     --name  value   Name for the image catalog  --url  value    URL location of the image catalog JSON file    Options  --description  value   Description for the recipe   --server  value   Cloudbreak server address [$CB_SERVER_ADDRESS]  --username  value   Cloudbreak user name (e-mail address) [$CB_USER_NAME]  --password  value   Cloudbreak password [$CB_PASSWORD]    --profile  value   Selects a config profile to use [$CB_PROFILE]  --auth-type  value   Authentication method to use. Values: oauth2, basic [$CB_AUTH_TYPE]  --public    Public in account    Examples  Registers an image catalog called \"mycustomcatalog\" which is available at https://example.com/myimagecatalog.json:   cb imagecatalog create --name mycustomcatalog --url https://example.com/myimagecatalog.json  Related links     Custom Images", 
            "title": "imagecatalog create"
        }, 
        {
            "location": "/cli-reference/index.html#imagecatalog-delete", 
            "text": "Deletes a previously registered custom image catalog. It does not delete any cloud provider resources that you created as a prerequisite for creating the Cloudbreak credential.      Required options     --name  value   Image catalog name      Options  --output  value   Supported formats: json, yaml, table (default: \"json\") [$CB_OUT_FORMAT]  --server  value   Cloudbreak server address [$CB_SERVER_ADDRESS]  --username  value   Cloudbreak user name (e-mail address) [$CB_USER_NAME]  --password  value   Cloudbreak password [$CB_PASSWORD]    --profile  value   Selects a config profile to use [$CB_PROFILE]  --auth-type  value   Authentication method to use. Values: oauth2, basic [$CB_AUTH_TYPE]    Examples  Deletes an image catalog called \"mycustomcatalog\":  cb imagecatalog delete --name mycustomcatalog  Related links     Custom Images", 
            "title": "imagecatalog delete"
        }, 
        {
            "location": "/cli-reference/index.html#imagecatalog-images", 
            "text": "Lists images from the specified image catalog available for the specified cloud provider.     Sub-commands     aws          Lists available aws images     azure        Lists available azure images     gcp          Lists available gcp images    openstack    Lists available openstack images      Required options  --imagecatalog  value   Name of the imagecatalog       Options  --output  value   Supported formats: json, yaml, table (default: \"json\") [$CB_OUT_FORMAT]    --server  value   Cloudbreak server address [$CB_SERVER_ADDRESS]  --username  value   Cloudbreak user name (e-mail address) [$CB_USER_NAME]  --password  value   Cloudbreak password [$CB_PASSWORD]     --profile  value   Selects a config profile to use [$CB_PROFILE]   --auth-type  value   Authentication method to use. Values: oauth2, basic [$CB_AUTH_TYPE]    Examples     Returns date, description, Ambari version, and image ID for all AWS images from an image catalog called \"myimagecatalog\":  ./cb imagecatalog images aws --imagecatalog cloudbreak-default\n[\n  {\n    \"Date\": \"2017-10-13\",\n    \"Description\": \"Cloudbreak official base image\",\n    \"Version\": \"2.6.0.0\",\n    \"ImageID\": \"44b140a4-bd0b-457d-b174-e988bee3ca47\"\n  },\n  {\n    \"Date\": \"2017-11-16\",\n    \"Description\": \"Official Cloudbreak image\",\n    \"Version\": \"2.6.0.0\",\n    \"ImageID\": \"3c7598a4-ebd6-4a02-5638-882f5c7f7add\"\n  }\n]  Related links     Custom Images", 
            "title": "imagecatalog images"
        }, 
        {
            "location": "/cli-reference/index.html#imagecatalog-list", 
            "text": "Lists default and custom image catalogs registered with Cloudbreak instance.     Required options     None    Options     --output  value   Supported formats: json, yaml, table (default: \"json\") [$CB_OUT_FORMAT]   --server  value   Cloudbreak server address [$CB_SERVER_ADDRESS]  --username  value   Cloudbreak user name (e-mail address) [$CB_USER_NAME]  --password  value   Cloudbreak password [$CB_PASSWORD]    --profile  value   Selects a config profile to use [$CB_PROFILE]  --auth-type  value   Authentication method to use. Values: oauth2, basic [$CB_AUTH_TYPE]    Examples  Lists existing image catalogs:    cb  imagecatalog list \n[\n  {\n    \"Name\": \"mycustomcatalog\",\n    \"Default\": false,\n    \"URL\": \"https://example.com/imagecatalog.json\"\n  },\n  {\n    \"Name\": \"cloudbreak-default\",\n    \"Default\": true,\n    \"URL\": \"https://s3-eu-west-1.amazonaws.com/cloudbreak-info/v2-dev-cb-image-catalog.json\"\n  }\n]  Related links     Custom Images", 
            "title": "imagecatalog list"
        }, 
        {
            "location": "/cli-reference/index.html#imagecatalog-set-default", 
            "text": "Sets the specified image catalog as default.    Required options      --name  value   Image catalog name       Options  --output  value   Supported formats: json, yaml, table (default: \"json\") [$CB_OUT_FORMAT]  --server  value   Cloudbreak server address [$CB_SERVER_ADDRESS]  --username  value   Cloudbreak user name (e-mail address) [$CB_USER_NAME]  --password  value   Cloudbreak password [$CB_PASSWORD]    --profile  value   Selects a config profile to use [$CB_PROFILE]  --auth-type  value   Authentication method to use. Values: oauth2, basic [$CB_AUTH_TYPE]    Examples  Sets \"mycustomcatalog\" as default:    imagecatalog set-default --name mycustomcatalog  Related links     Custom Images", 
            "title": "imagecatalog set-default"
        }, 
        {
            "location": "/cli-reference/index.html#ldap-create", 
            "text": "Registers an existing LDAP with Cloudbreak.  Required options  --name  value   Name for the LDAP       --ldap-server  value   Address of the LDAP server (e.g. ldap://10.0.0.1:384)  --ldap-domain  value    LDAP domain (e.g. ad.cb.com)  --ldap-bind-dn  value   LDAP bind DN (e.g. CN=Administrator,CN=Users,DC=ad,DC=cb,DC=com)  --ldap-bind-password  value   LDAP bind password  --ldap-directory-type  value    LDAP directory type (LDAP or ACTIVE_DIRECTORY)   --ldap-user-search-base  value    LDAP user search base (e.g. CN=Users,DC=ad,DC=cb,DC=com)  --ldap-user-name-attribute  value    LDAP user name attribute  --ldap-user-object-class  value    LDAP user object class  --ldap-group-member-attribute  value  LDAP group member attribute  --ldap-group-name-attribute  value   LDAP group name attribute  --ldap-group-object-class  value   LDAP group object class  --ldap-group-search-base  value    LDAP group search base (e.g. OU=scopes,DC=ad,DC=cb,DC=com)    Options  --ldap-admin-group  value   LDAP group of administrators  --description  value   Description for the LDAP      --server  value   Cloudbreak server address [$CB_SERVER_ADDRESS]  --username  value   Cloudbreak user name (e-mail address) [$CB_USER_NAME]  --password  value   Cloudbreak password [$CB_PASSWORD]   --profile  value   Selects a config profile to use [$CB_PROFILE]  --auth-type  value   Authentication method to use. Values: oauth2, basic [$CB_AUTH_TYPE]  --public    Public in account", 
            "title": "ldap create"
        }, 
        {
            "location": "/cli-reference/index.html#ldap-delete", 
            "text": "Deletes selected LDAP registration from Cloudbreak. It does not delete the LDAP.   Required options  --name  value   LDAP name        Options  --output  value   Supported formats: json, yaml, table (default: \"json\") [$CB_OUT_FORMAT]  --server  value   Cloudbreak server address [$CB_SERVER_ADDRESS]  --username  value   Cloudbreak user name (e-mail address) [$CB_USER_NAME]  --password  value   Cloudbreak password [$CB_PASSWORD]  --profile  value   Selects a config profile to use [$CB_PROFILE]  --auth-type  value   Authentication method to use. Values: oauth2, basic [$CB_AUTH_TYPE]  Examples  cb ldap delete --name testldap", 
            "title": "ldap delete"
        }, 
        {
            "location": "/cli-reference/index.html#ldap-list", 
            "text": "Lists all available LDAPs.  Required options  None  Options  --output  value   Supported formats: json, yaml, table (default: \"json\") [$CB_OUT_FORMAT]  --server  value   Cloudbreak server address [$CB_SERVER_ADDRESS]  --username  value   Cloudbreak user name (e-mail address) [$CB_USER_NAME]  --password  value   Cloudbreak password [$CB_PASSWORD]  --profile  value   Selects a config profile to use [$CB_PROFILE]  --auth-type  value   Authentication method to use. Values: oauth2, basic [$CB_AUTH_TYPE]  Examples  Lists existing LDAPs:  cb ldap list  Lists existing LDAPs, with output presented in a table format:  cb ldap list --output table", 
            "title": "ldap list"
        }, 
        {
            "location": "/cli-reference/index.html#mpack-create", 
            "text": "Registers an existing management pack with Cloudbreak.  Required options  --name  value   Name for the mpack        --url  value   URL that points to the location of the mpack tarball     Options  --purge   Purge existing resources specified in purge-list   --purge-list  value   Comma-separated list of resources to purge (stack-definitions,service-definitions,mpacks). By default (stack-definitions,mpacks) will be purged  --force   Force install management pack    --description  value   Description for the LDAP      --server  value   Cloudbreak server address [$CB_SERVER_ADDRESS]  --username  value   Cloudbreak user name (e-mail address) [$CB_USER_NAME]  --password  value   Cloudbreak password [$CB_PASSWORD]   --profile  value   Selects a config profile to use [$CB_PROFILE]  --auth-type  value   Authentication method to use. Values: oauth2, basic [$CB_AUTH_TYPE]  --public    Public in account    Examples  Registers a new mpack without purging:  cb mpack create --name test-hdp-search --url http://public-repo-1.hortonworks.com/HDP-SOLR/hdp-solr-ambari-mp/solr-service-mpack-3.0.0.tar.gz", 
            "title": "mpack create"
        }, 
        {
            "location": "/cli-reference/index.html#mpack-delete", 
            "text": "Deletes selected management registration from Cloudbreak. It does not delete the management pack.   Required options  --name  value   Management pack name        Options  --server  value   Cloudbreak server address [$CB_SERVER_ADDRESS]  --username  value   Cloudbreak user name (e-mail address) [$CB_USER_NAME]  --password  value   Cloudbreak password [$CB_PASSWORD]  --profile  value   Selects a config profile to use [$CB_PROFILE]  --auth-type  value   Authentication method to use. Values: oauth2, basic [$CB_AUTH_TYPE]  Examples  cb mpack delete --name testmpack", 
            "title": "mpack delete"
        }, 
        {
            "location": "/cli-reference/index.html#mpack-list", 
            "text": "Lists all available management packs.  Required options  None  Options  --output  value   Supported formats: json, yaml, table (default: \"json\") [$CB_OUT_FORMAT]  --server  value   Cloudbreak server address [$CB_SERVER_ADDRESS]  --username  value   Cloudbreak user name (e-mail address) [$CB_USER_NAME]  --password  value   Cloudbreak password [$CB_PASSWORD]  --profile  value   Selects a config profile to use [$CB_PROFILE]  --auth-type  value   Authentication method to use. Values: oauth2, basic [$CB_AUTH_TYPE]  Examples  Lists all currently registered management packs and provides information about each:  cb mpack list\n[\n  {\n    \"Name\": \"hdp-search-3\",\n    \"Description\": \"\",\n    \"URL\": \"http://public-repo-1.hortonworks.com/HDP-SOLR/hdp-solr-ambari-mp/solr-service-mpack-3.0.0.tar.gz\",\n    \"Purge\": \"false\",\n    \"PurgeList\": \"\",\n    \"Force\": \"false\"\n  }\n]", 
            "title": "mpack list"
        }, 
        {
            "location": "/cli-reference/index.html#proxy-create", 
            "text": "Registers an existing proxy with Cloudbreak.  Required options  --name  value   Name for the proxy     --proxy-host  value   Hostname or IP address of the proxy  --proxy-port  value   Port of the proxy    Options  --proxy-protocol  value   Protocol for the proxy (http or https) (default: \"http\")  --proxy-user  value    User for the proxy if basic auth is required  --proxy-password  value   Password for the proxy if basic auth is required  --description  value   Description for the proxy      --server  value   Cloudbreak server address [$CB_SERVER_ADDRESS]  --username  value   Cloudbreak user name (e-mail address) [$CB_USER_NAME]  --password  value   Cloudbreak password [$CB_PASSWORD]   --profile  value   Selects a config profile to use [$CB_PROFILE]  --auth-type  value   Authentication method to use. Values: oauth2, basic [$CB_AUTH_TYPE]  --public    Public in account", 
            "title": "proxy create"
        }, 
        {
            "location": "/cli-reference/index.html#proxy-delete", 
            "text": "Unregisters a previously registered proxy with Cloudbreak. It does not delete the proxy.   Required options  --name  value   Proxy registration name       Options  --output  value   Supported formats: json, yaml, table (default: \"json\") [$CB_OUT_FORMAT]  --server  value   Cloudbreak server address [$CB_SERVER_ADDRESS]  --username  value   Cloudbreak user name (e-mail address) [$CB_USER_NAME]  --password  value   Cloudbreak password [$CB_PASSWORD]  --profile  value   Selects a config profile to use [$CB_PROFILE]  --auth-type  value   Authentication method to use. Values: oauth2, basic [$CB_AUTH_TYPE]  Examples  cb proxy delete --name testproxy", 
            "title": "proxy delete"
        }, 
        {
            "location": "/cli-reference/index.html#proxy-list", 
            "text": "Lists all proxies that were previously registered with Cloudbreak.  Required options  None  Options  --output  value   Supported formats: json, yaml, table (default: \"json\") [$CB_OUT_FORMAT]  --server  value   Cloudbreak server address [$CB_SERVER_ADDRESS]  --username  value   Cloudbreak user name (e-mail address) [$CB_USER_NAME]  --password  value   Cloudbreak password [$CB_PASSWORD]  --profile  value   Selects a config profile to use [$CB_PROFILE]  --auth-type  value   Authentication method to use. Values: oauth2, basic [$CB_AUTH_TYPE]  Examples  Lists existing proxy registrations:  cb proxy list  Lists existing proxy registrations, with output presented in a table format:  cb proxy list --output table", 
            "title": "proxy list"
        }, 
        {
            "location": "/cli-reference/index.html#recipe-create", 
            "text": "Adds a new recipe from a file or from a URL.  Sub-commands  from-url   Creates a recipe by downloading it from a URL location  from-file   Creates a recipe by reading it from a local file    Required options  from-url     --name  value   Name for the recipe   --execution-type  value   Type of execution [pre-ambari-start, pre-termination, post-ambari-start, post-cluster-install]    --url  value   URL location of the Ambari blueprint JSON file    from-file    --name  value   Name for the recipe  --execution-type  value   Type of execution [pre-ambari-start, pre-termination, post-ambari-start, post-cluster-install]   --file  value   Location of the Ambari blueprint JSON file    Options  --description  value   Description for the recipe   --server  value   Cloudbreak server address [$CB_SERVER_ADDRESS]  --username  value   Cloudbreak user name (e-mail address) [$CB_USER_NAME]  --password  value   Cloudbreak password [$CB_PASSWORD]  --profile  value   Selects a config profile to use [$CB_PROFILE]  --auth-type  value   Authentication method to use. Values: oauth2, basic [$CB_AUTH_TYPE]  --public    Public in account    Examples  Adds a new recipe called \"test1\" from a URL:  cb recipe create from-url --name \"test1\" --execution-type post-ambari-start --url http://some-site.com/test.sh  Adds a new recipe called \"test2\" from a file:  cb recipe create from-url --name \"test2\" --execution-type post-ambari-start --file /Users/test/Documents/test.sh  Related links  Recipes", 
            "title": "recipe create"
        }, 
        {
            "location": "/cli-reference/index.html#recipe-delete", 
            "text": "Deletes an existing recipe.  Required options  --name  value   Recipe name    Options  --output  value   Supported formats: json, yaml, table (default: \"json\") [$CB_OUT_FORMAT]  --server  value   Cloudbreak server address [$CB_SERVER_ADDRESS]  --username  value   Cloudbreak user name (e-mail address) [$CB_USER_NAME]  --password  value   Cloudbreak password [$CB_PASSWORD]  --profile  value   Selects a config profile to use [$CB_PROFILE]   --auth-type  value   Authentication method to use. Values: oauth2, basic [$CB_AUTH_TYPE]    Examples  cb recipe delete --name test", 
            "title": "recipe delete"
        }, 
        {
            "location": "/cli-reference/index.html#recipe-describe", 
            "text": "Describes an existing recipe.  Required options  --name  value   Recipe name       Options  --output  value   Supported formats: json, yaml, table (default: \"json\") [$CB_OUT_FORMAT]  --server  value   Cloudbreak server address [$CB_SERVER_ADDRESS]  --username  value   Cloudbreak user name (e-mail address) [$CB_USER_NAME]  --password  value   Cloudbreak password [$CB_PASSWORD]  --profile  value   Selects a config profile to use [$CB_PROFILE]  --auth-type  value   Authentication method to use. Values: oauth2, basic [$CB_AUTH_TYPE]    Examples  Describes a recipe called \"test\":  cb recipe describe --name test\n{\n  \"Name\": \"test\",\n  \"Description\": \"\",\n  \"ExecutionType\": \"POST\"\n}  Describes a recipe called \"test\", with output presented in a table format:  cb describe-recipe --name test --output table\n+------+-------------+----------------+\n| NAME | DESCRIPTION | EXECUTION TYPE |\n+------+-------------+----------------+\n| test |             | POST           |\n+------+-------------+----------------+", 
            "title": "recipe describe"
        }, 
        {
            "location": "/cli-reference/index.html#recipe-list", 
            "text": "Lists all available recipes.  Required options  None  Options  --output  value   Supported formats: json, yaml, table (default: \"json\") [$CB_OUT_FORMAT]  --server  value   Cloudbreak server address [$CB_SERVER_ADDRESS]  --username  value   Cloudbreak user name (e-mail address) [$CB_USER_NAME]  --password  value   Cloudbreak password [$CB_PASSWORD]  --profile  value   Selects a config profile to use [$CB_PROFILE]  --auth-type  value   Authentication method to use. Values: oauth2, basic [$CB_AUTH_TYPE]    Examples  Lists existing recipes:  cb recipe list\n[\n  {\n    \"Name\": \"test\",\n    \"Description\": \"\",\n    \"ExecutionType\": \"POST\"\n  }\n]  Lists existing recipes, with output presented in a table format:  cb recipe list --output table\n+------+-------------+-------------------+\n| NAME | DESCRIPTION | EXECUTION TYPE    |\n+------+-------------+-------------------+\n| test |             | POST-AMBARI-START |\n+------+-------------+-------------------+", 
            "title": "recipe list"
        }, 
        {
            "location": "/cli-reference/index.html#debugging", 
            "text": "To use debugging mode, pass the  --debug  option.", 
            "title": "Debugging"
        }, 
        {
            "location": "/cli-reference/index.html#checking-cli-version", 
            "text": "To check CLI version, use  cb --version .", 
            "title": "Checking CLI Version"
        }, 
        {
            "location": "/trouble-cb-logs/index.html", 
            "text": "Checking Cloudbreak logs\n\n\nWhen troubleshooting, you can access the following Cloudbreak logs.\n\n\nCloudbreak logs\n\n\nWhen installing Cloudbreak using a pre-built cloud image, the  Cloudbreak deployer location and the cbd root folder is \n/var/lib/cloudbreak-deployment\n. You must execute all cbd actions from the cbd root folder as a cloudbreak user. \n\n\n\n\nYour cbd root directory may be different if you installed Cloudbreak on your own VM. \n\n\n\n\nAggregated logs\n\n\nCloudbreak consists of multiple microservices deployed into Docker containers. \n\n\nTo check aggregated service logs, use the following commands:\n\n\ncbd logs\n shows all service logs.\n\n\ncbd logs | tee cloudbreak.log\n allows you to redirect the input into a file for sharing these logs.\n\n\nIndividual service logs\n\n\nTo check individual service logs, use the following commands:\n\n\ncbd logs cloudbreak\n shows Cloudbreak logs. This service is the backend service that handles all deployments.\n\n\ncbd logs uluwatu\n shows Cloudbreak UI logs. Uluwatu is the UI component of Cloudbreak.\n\n\ncbd logs identity\n shows Identity logs. Identity is responsible for authentication and authorization.\n\n\ncbd logs periscope\n shows Periscope logs. Periscope is responsible for triggering autoscaling rules.\n\n\nDocker logs\n\n\nThe same logs can be accessed via Docker commands:\n\n\ndocker logs cbreak_cloudbreak_1\n shows the same logs as \ncbd logs cloudbreak\n.\n\n\nCloudbreak logs are rotated and can be accessed later from the Cloudbreak deployment folder. Each time you restart the application via cbd restart a new log file is created with a timestamp in the name (for example, cbreak-20170821-105900.log). \n\n\n\n\nThere is a symlink called \ncbreak.log\n which points to the latest log file. Sharing this symlink does not share the log itself.\n\n\n\n\nSaltstack logs\n\n\nCloudbreak uses Saltstack to install Ambari and the necessary packages for the HDP provisioning. Salt Master always runs alongside the Ambari Server node. Each instance in the cluster runs a Salt Minion, which connects to the Salt Master. There can be multiple Salt Masters if the cluster is configured to run in HA (High Availability) mode and in this case each Salt Minion connects to each Salt Master.\n\n\nCloudbreak also uses SaltStack to execute user-provided customization scripts called \"recipes\". \n\n\nSalt Master and Salt Minion logs can be found at the following location: \n/var/log/salt\n\n\nAmbari logs\n\n\nCloudbreak uses Ambari to orchestrate the installation of the different HDP components. Each instance in the cluster runs an Ambari agent which connects to the Ambari server. Ambari server is declared by the user during the cluster installation wizard. \n\n\nAmbari server logs\n\n\nAmbari server logs can be found on the nodes where Ambari server is installed in the following locations:\n\n\n/var/log/ambari-server/ambari-server.log\n\n\n/var/log/ambari-server/ambari-server.out\n\n\nBoth files contain important information about the root cause of a certain issue so it is advised to check both.\n\n\nAmbari agent logs\n\n\nAmbari agent logs can be found on the nodes where Ambari agent is installed in the following locations:\n\n\n/var/log/ambari-agent/ambari-agent.log\n\n\nRecipe logs\n\n\nCloudbreak supports \"recipes\" - user-provided customization scripts that can be run prior to or after cluster installation. It is the user\u2019s responsibility to provide an idempotent well tested script. If the execution fails, the recipe logs can be found at \n/var/log/recipes\n on the nodes on which the recipes were executed.\n\n\nIt is advised, but not required to have an advanced logging mechanism in the script, as Cloudbreak always logs every script that are run. Recipes are often the sources of installation failures as users might try to remove necessary packages or reconfigure services.", 
            "title": "Cloudbreak logs"
        }, 
        {
            "location": "/trouble-cb-logs/index.html#checking-cloudbreak-logs", 
            "text": "When troubleshooting, you can access the following Cloudbreak logs.", 
            "title": "Checking Cloudbreak logs"
        }, 
        {
            "location": "/trouble-cb-logs/index.html#cloudbreak-logs", 
            "text": "When installing Cloudbreak using a pre-built cloud image, the  Cloudbreak deployer location and the cbd root folder is  /var/lib/cloudbreak-deployment . You must execute all cbd actions from the cbd root folder as a cloudbreak user.    Your cbd root directory may be different if you installed Cloudbreak on your own VM.", 
            "title": "Cloudbreak logs"
        }, 
        {
            "location": "/trouble-cb-logs/index.html#aggregated-logs", 
            "text": "Cloudbreak consists of multiple microservices deployed into Docker containers.   To check aggregated service logs, use the following commands:  cbd logs  shows all service logs.  cbd logs | tee cloudbreak.log  allows you to redirect the input into a file for sharing these logs.", 
            "title": "Aggregated logs"
        }, 
        {
            "location": "/trouble-cb-logs/index.html#individual-service-logs", 
            "text": "To check individual service logs, use the following commands:  cbd logs cloudbreak  shows Cloudbreak logs. This service is the backend service that handles all deployments.  cbd logs uluwatu  shows Cloudbreak UI logs. Uluwatu is the UI component of Cloudbreak.  cbd logs identity  shows Identity logs. Identity is responsible for authentication and authorization.  cbd logs periscope  shows Periscope logs. Periscope is responsible for triggering autoscaling rules.", 
            "title": "Individual service logs"
        }, 
        {
            "location": "/trouble-cb-logs/index.html#docker-logs", 
            "text": "The same logs can be accessed via Docker commands:  docker logs cbreak_cloudbreak_1  shows the same logs as  cbd logs cloudbreak .  Cloudbreak logs are rotated and can be accessed later from the Cloudbreak deployment folder. Each time you restart the application via cbd restart a new log file is created with a timestamp in the name (for example, cbreak-20170821-105900.log).    There is a symlink called  cbreak.log  which points to the latest log file. Sharing this symlink does not share the log itself.", 
            "title": "Docker logs"
        }, 
        {
            "location": "/trouble-cb-logs/index.html#saltstack-logs", 
            "text": "Cloudbreak uses Saltstack to install Ambari and the necessary packages for the HDP provisioning. Salt Master always runs alongside the Ambari Server node. Each instance in the cluster runs a Salt Minion, which connects to the Salt Master. There can be multiple Salt Masters if the cluster is configured to run in HA (High Availability) mode and in this case each Salt Minion connects to each Salt Master.  Cloudbreak also uses SaltStack to execute user-provided customization scripts called \"recipes\".   Salt Master and Salt Minion logs can be found at the following location:  /var/log/salt", 
            "title": "Saltstack logs"
        }, 
        {
            "location": "/trouble-cb-logs/index.html#ambari-logs", 
            "text": "Cloudbreak uses Ambari to orchestrate the installation of the different HDP components. Each instance in the cluster runs an Ambari agent which connects to the Ambari server. Ambari server is declared by the user during the cluster installation wizard.", 
            "title": "Ambari logs"
        }, 
        {
            "location": "/trouble-cb-logs/index.html#ambari-server-logs", 
            "text": "Ambari server logs can be found on the nodes where Ambari server is installed in the following locations:  /var/log/ambari-server/ambari-server.log  /var/log/ambari-server/ambari-server.out  Both files contain important information about the root cause of a certain issue so it is advised to check both.", 
            "title": "Ambari server logs"
        }, 
        {
            "location": "/trouble-cb-logs/index.html#ambari-agent-logs", 
            "text": "Ambari agent logs can be found on the nodes where Ambari agent is installed in the following locations:  /var/log/ambari-agent/ambari-agent.log", 
            "title": "Ambari agent logs"
        }, 
        {
            "location": "/trouble-cb-logs/index.html#recipe-logs", 
            "text": "Cloudbreak supports \"recipes\" - user-provided customization scripts that can be run prior to or after cluster installation. It is the user\u2019s responsibility to provide an idempotent well tested script. If the execution fails, the recipe logs can be found at  /var/log/recipes  on the nodes on which the recipes were executed.  It is advised, but not required to have an advanced logging mechanism in the script, as Cloudbreak always logs every script that are run. Recipes are often the sources of installation failures as users might try to remove necessary packages or reconfigure services.", 
            "title": "Recipe logs"
        }, 
        {
            "location": "/trouble-cb/index.html", 
            "text": "Troubleshooting Cloudbreak\n\n\nThis section includes common errors and steps to resolve them. \n\n\nInvalid PUBLIC_IP in CBD Profile\n\n\nThe \nPUBLIC_IP\n property must be set in the cbd Profile file or else you won\u2019t be able to log in on the Cloudbreak UI. \n\n\nIf you are migrating your instance, make sure that after the start the IP remains valid. If you need to edit the \nPUBLIC_IP\n property in Profile, make sure to restart Cloudbreak using \ncbd restart\n.\n\n\nCbd cannot get VM's public IP\n\n\nBy default the \ncbd\n tool tries to get the VM's public IP to bind Cloudbreak UI to it. But if \ncbd\n cannot get the IP address during the initialization, you must set it manually. Check your \nProfile\n and if \nPUBLIC_IP\n is not set, add the \nPUBLIC_IP\n variable and set it to the public IP of the VM. For example: \n\n\nexport PUBLIC_IP=192.134.23.10\n\n\n\nPermission or connection problems\n\n\nIf you face permission or connection issues, disable SELinux:\n\n\n\n\nSet \nSELINUX=disabled\n in \n/etc/selinux/config\n.  \n\n\nReboot the machine.  \n\n\n\n\nEnsure the SELinux is not turned on afterwards:", 
            "title": "Troubleshooting Cloudbreak"
        }, 
        {
            "location": "/trouble-cb/index.html#troubleshooting-cloudbreak", 
            "text": "This section includes common errors and steps to resolve them.", 
            "title": "Troubleshooting Cloudbreak"
        }, 
        {
            "location": "/trouble-cb/index.html#invalid-public_ip-in-cbd-profile", 
            "text": "The  PUBLIC_IP  property must be set in the cbd Profile file or else you won\u2019t be able to log in on the Cloudbreak UI.   If you are migrating your instance, make sure that after the start the IP remains valid. If you need to edit the  PUBLIC_IP  property in Profile, make sure to restart Cloudbreak using  cbd restart .", 
            "title": "Invalid PUBLIC_IP in CBD Profile"
        }, 
        {
            "location": "/trouble-cb/index.html#cbd-cannot-get-vms-public-ip", 
            "text": "By default the  cbd  tool tries to get the VM's public IP to bind Cloudbreak UI to it. But if  cbd  cannot get the IP address during the initialization, you must set it manually. Check your  Profile  and if  PUBLIC_IP  is not set, add the  PUBLIC_IP  variable and set it to the public IP of the VM. For example:   export PUBLIC_IP=192.134.23.10", 
            "title": "Cbd cannot get VM's public IP"
        }, 
        {
            "location": "/trouble-cb/index.html#permission-or-connection-problems", 
            "text": "If you face permission or connection issues, disable SELinux:   Set  SELINUX=disabled  in  /etc/selinux/config .    Reboot the machine.     Ensure the SELinux is not turned on afterwards:", 
            "title": "Permission or connection problems"
        }, 
        {
            "location": "/trouble-cluster/index.html", 
            "text": "Troubleshooting cluster creation\n\n\nConfigure communication via private IPs on AWS\n\n\nCloudbreak uses public IP addresses when communicating with cluster nodes. On AWS, you can configure it to use private IPs instead of public IPs by setting the CB_AWS_VPC variable in the Profile. \n\n\n\n\nThis configuration is available for AWS only. Do not use it for other cloud providers. \n\n\n\n\n\n\n\n\nNavigate to the Cloudbreak deployment directory and edit Profile. For example:\n\n\ncd /var/lib/cloudbreak-deployment/\nvi Profile\n\n\n\n\n\n\nAdd the following entry, setting it to the AWS VPC identifier where you have deployed Cloudbreak:\n\n\nexport CB_AWS_VPC=your-VPC-ID\n\n\nFor example:\n\n\nexport CB_AWS_VPC=vpc-e261a185\n\n\n\n\n\n\nRestart Cloudbreak by using \ncbd restart\n.      \n\n\n\n\n\n\nCannot access Oozie web UI\n\n\nExt JS is GPL licensed software and is no longer included in builds of HDP 2.6. Because of this, the Oozie WAR file is not built to include the Ext JS-based user interface unless Ext JS is manually installed on the Oozie server. If you add Oozie using Ambari 2.6.1.0 to an HDP 2.6.4 or greater stack, no Oozie UI will be available by default. Therefore, if you plan to use Oozie web UI with Ambari 2.6.1.0 and HDP 2.6.4 or greater, you you must manually install Ext JS on the Oozie server host.\n\n\nYou can install Ext JS by adding the following PRE-AMBARI-START recipe:\n\n\nexport EXT_JS_VERSION=2.2-1\n     export OS_NAME=centos6\n     wget http://public-repo-1.hortonworks.com/HDP-UTILS-GPL-1.1.0.22/repos/$OS_NAME/extjs/extjs-$EXT_JS_VERSION.noarch.rpm\n     rpm -ivh extjs-$EXT_JS_VERSION.noarch.rpm\n\n\n\nMake the following changes to the script:\n\n\n\n\nChange the EXT_JS_VERSION to the specific ExtJS version that you want to use.  \n\n\nChange the OS_NAME to the name of the operating system. Supported values are: centos6, centos7, centos7-ppc.\n\n\n\n\nThe general steps are:\n\n\n\n\nBe sure to review and agree to the Ext JS license prior to using this recipe.  \n\n\nCreate a PRE-AMBARI-START recipe. For instructions on how to create a recipe, refer to \nAdd recipes\n.   \n\n\nWhen creating a cluster, choose this recipe to be executed on all host groups of the cluster. \n\n\n\n\nRelated links\n\n\nAdd recipes\n\n\nUsing custom scripts (recipes)\n  \n\n\nQuota limitations\n\n\nEach cloud provider has quota limitations on various cloud resources, and these quotas can usually be increased on request. If there is an error message in Cloudbreak stating that there are no more available EIPs (Elastic IP Address) or VPCs, you need to request more of these resources. \n\n\nTo see the limitations visit the cloud provider\u2019s site:\n\n\n\n\nAWS service limits\n \n\n\nAzure subscription and service limits, quotas, and constraints\n\n\nGCP resource quotas\n \n\n\n\n\nConnection timeout when ports are not open\n\n\nIn the cluster installation wizard, you must specify on which node you want to run the Ambari server. Cloudbreak communicates with this node to orchestrate the installation.\n\n\nA common reason for connection timeout is security group misconfiguration. Cloudbreak allows configuring different security groups for the different instance groups; however, there are certain requirements for the Ambari server node. Specifically, the following ports must be open in order to communicate with that node:\n\n\n\n\n22 (SSH)  \n\n\n9443 (two-way-ssl through nginx) \n\n\n\n\nBlueprint errors\n\n\nInvalid services and configurations\n\n\nAmbari blueprints are a declarative definition of a cluster. With a blueprint, you specify a stack, the component layout, and the configurations to materialize a Hadoop cluster instance via a REST API without having to use the Ambari cluster install wizard. \n\n\nCloudbreak supports any type of blueprints, which is a common source of errors. These errors are only visible once the core infrastructure is up and running and Cloudbreak tries to initiate the cluster installation through Ambari. Ambari validates the blueprint and  rejects it if it's invalid. \n\n\nFor example, if there are configurations for a certain service like Hive but Hive as a service is not mapped to any host group, the blueprint is invalid.\n\n\nTo fix these type of issues, edit your blueprint and then reinstall your cluster. Cloudbreak UI has support for this so the infrastructure does not have to be terminated.\n\n\nThere are some cases when Ambari cannot validate your blueprint beforehand. In these cases, the issues are only visible in the Ambari server logs. To troubleshoot, check Ambari server logs.\n\n\nWrong HDP version\n\n\nIn the blueprint, only the major and minor HDP version should be defined (for example, \"2.6\"). If wrong version number is provided, the following error can be found in the logs:\n\n\n5/15/2017 12:23:19 PM testcluster26 - create failed: Cannot use the specified Ambari stack: HDPRepo\n{stack='null'; utils='null'}\n. Error: org.apache.ambari.server.controller.spi.NoSuchResourceException: The specified resource doesn't exist: Stack data, Stack HDP 2.6.0.3 is not found in Ambari metainfo\n\n\n\n\nFor correct blueprint layout, refer to the \nAmbari cwiki\n page.\n\n\nRecipe errors\n\n\nRecipe execution times out\n\n\nIf the scripts are taking too much time to execute, the processes will time out, as the threshold for all recipes is set to 15 minutes. To change this threshold, you must override the default value by adding the following to the cbd Profile file:\n\n\nexport CB_JAVA_OPTS=\u201d -Dcb.max.salt.recipe.execution.retry=90\u201d\n\n\n\n\nThis property indicates the number of tries for checking if the scripts have finished with a sleep time (i.e. the wait time between two polling attempts) of 10 seconds. The default value is 90. To increase the threshold, provide a number greater than 90. You must restart Cloudbreak after changing properties in the Profile file.\n\n\nRecipe execution fails\n\n\nIt often happens that a script cannot be executed successfully because there are typos or errors in the script. To verify this you can check the recipe logs at\n\n/var/log/recipes\n. For each script, there will be a separate log file with the name of the script that you provided on the Cloudbreak UI.", 
            "title": "Troubleshooting cluster creation"
        }, 
        {
            "location": "/trouble-cluster/index.html#troubleshooting-cluster-creation", 
            "text": "", 
            "title": "Troubleshooting cluster creation"
        }, 
        {
            "location": "/trouble-cluster/index.html#configure-communication-via-private-ips-on-aws", 
            "text": "Cloudbreak uses public IP addresses when communicating with cluster nodes. On AWS, you can configure it to use private IPs instead of public IPs by setting the CB_AWS_VPC variable in the Profile.    This configuration is available for AWS only. Do not use it for other cloud providers.      Navigate to the Cloudbreak deployment directory and edit Profile. For example:  cd /var/lib/cloudbreak-deployment/\nvi Profile    Add the following entry, setting it to the AWS VPC identifier where you have deployed Cloudbreak:  export CB_AWS_VPC=your-VPC-ID  For example:  export CB_AWS_VPC=vpc-e261a185    Restart Cloudbreak by using  cbd restart .", 
            "title": "Configure communication via private IPs on AWS"
        }, 
        {
            "location": "/trouble-cluster/index.html#cannot-access-oozie-web-ui", 
            "text": "Ext JS is GPL licensed software and is no longer included in builds of HDP 2.6. Because of this, the Oozie WAR file is not built to include the Ext JS-based user interface unless Ext JS is manually installed on the Oozie server. If you add Oozie using Ambari 2.6.1.0 to an HDP 2.6.4 or greater stack, no Oozie UI will be available by default. Therefore, if you plan to use Oozie web UI with Ambari 2.6.1.0 and HDP 2.6.4 or greater, you you must manually install Ext JS on the Oozie server host.  You can install Ext JS by adding the following PRE-AMBARI-START recipe:  export EXT_JS_VERSION=2.2-1\n     export OS_NAME=centos6\n     wget http://public-repo-1.hortonworks.com/HDP-UTILS-GPL-1.1.0.22/repos/$OS_NAME/extjs/extjs-$EXT_JS_VERSION.noarch.rpm\n     rpm -ivh extjs-$EXT_JS_VERSION.noarch.rpm  Make the following changes to the script:   Change the EXT_JS_VERSION to the specific ExtJS version that you want to use.    Change the OS_NAME to the name of the operating system. Supported values are: centos6, centos7, centos7-ppc.   The general steps are:   Be sure to review and agree to the Ext JS license prior to using this recipe.    Create a PRE-AMBARI-START recipe. For instructions on how to create a recipe, refer to  Add recipes .     When creating a cluster, choose this recipe to be executed on all host groups of the cluster.    Related links  Add recipes  Using custom scripts (recipes)", 
            "title": "Cannot access Oozie web UI"
        }, 
        {
            "location": "/trouble-cluster/index.html#quota-limitations", 
            "text": "Each cloud provider has quota limitations on various cloud resources, and these quotas can usually be increased on request. If there is an error message in Cloudbreak stating that there are no more available EIPs (Elastic IP Address) or VPCs, you need to request more of these resources.   To see the limitations visit the cloud provider\u2019s site:   AWS service limits    Azure subscription and service limits, quotas, and constraints  GCP resource quotas", 
            "title": "Quota limitations"
        }, 
        {
            "location": "/trouble-cluster/index.html#connection-timeout-when-ports-are-not-open", 
            "text": "In the cluster installation wizard, you must specify on which node you want to run the Ambari server. Cloudbreak communicates with this node to orchestrate the installation.  A common reason for connection timeout is security group misconfiguration. Cloudbreak allows configuring different security groups for the different instance groups; however, there are certain requirements for the Ambari server node. Specifically, the following ports must be open in order to communicate with that node:   22 (SSH)    9443 (two-way-ssl through nginx)", 
            "title": "Connection timeout when ports are not open"
        }, 
        {
            "location": "/trouble-cluster/index.html#blueprint-errors", 
            "text": "", 
            "title": "Blueprint errors"
        }, 
        {
            "location": "/trouble-cluster/index.html#invalid-services-and-configurations", 
            "text": "Ambari blueprints are a declarative definition of a cluster. With a blueprint, you specify a stack, the component layout, and the configurations to materialize a Hadoop cluster instance via a REST API without having to use the Ambari cluster install wizard.   Cloudbreak supports any type of blueprints, which is a common source of errors. These errors are only visible once the core infrastructure is up and running and Cloudbreak tries to initiate the cluster installation through Ambari. Ambari validates the blueprint and  rejects it if it's invalid.   For example, if there are configurations for a certain service like Hive but Hive as a service is not mapped to any host group, the blueprint is invalid.  To fix these type of issues, edit your blueprint and then reinstall your cluster. Cloudbreak UI has support for this so the infrastructure does not have to be terminated.  There are some cases when Ambari cannot validate your blueprint beforehand. In these cases, the issues are only visible in the Ambari server logs. To troubleshoot, check Ambari server logs.", 
            "title": "Invalid services and configurations"
        }, 
        {
            "location": "/trouble-cluster/index.html#wrong-hdp-version", 
            "text": "In the blueprint, only the major and minor HDP version should be defined (for example, \"2.6\"). If wrong version number is provided, the following error can be found in the logs:  5/15/2017 12:23:19 PM testcluster26 - create failed: Cannot use the specified Ambari stack: HDPRepo\n{stack='null'; utils='null'}\n. Error: org.apache.ambari.server.controller.spi.NoSuchResourceException: The specified resource doesn't exist: Stack data, Stack HDP 2.6.0.3 is not found in Ambari metainfo  For correct blueprint layout, refer to the  Ambari cwiki  page.", 
            "title": "Wrong HDP version"
        }, 
        {
            "location": "/trouble-cluster/index.html#recipe-errors", 
            "text": "", 
            "title": "Recipe errors"
        }, 
        {
            "location": "/trouble-cluster/index.html#recipe-execution-times-out", 
            "text": "If the scripts are taking too much time to execute, the processes will time out, as the threshold for all recipes is set to 15 minutes. To change this threshold, you must override the default value by adding the following to the cbd Profile file:  export CB_JAVA_OPTS=\u201d -Dcb.max.salt.recipe.execution.retry=90\u201d  This property indicates the number of tries for checking if the scripts have finished with a sleep time (i.e. the wait time between two polling attempts) of 10 seconds. The default value is 90. To increase the threshold, provide a number greater than 90. You must restart Cloudbreak after changing properties in the Profile file.", 
            "title": "Recipe execution times out"
        }, 
        {
            "location": "/trouble-cluster/index.html#recipe-execution-fails", 
            "text": "It often happens that a script cannot be executed successfully because there are typos or errors in the script. To verify this you can check the recipe logs at /var/log/recipes . For each script, there will be a separate log file with the name of the script that you provided on the Cloudbreak UI.", 
            "title": "Recipe execution fails"
        }, 
        {
            "location": "/trouble-aws/index.html", 
            "text": "Troubleshooting Cloudbreak on AWS\n\n\n\n\nCheck out \nHDCloud\n troubleshooting docs.\n\n\n\n\nUnable to create an IAM Role for Cloudbreak\n\n\nMost corporate AWS users are unable to create AWS roles. You may have to contact your AWS admin to create the role(s) for you. \n\n\nCluster fails with a permissions related error\n\n\nMake sure that the policy attached to CredentialRole includes all actions defined in \nCredentialRole\n.", 
            "title": "Troubleshooting AWS"
        }, 
        {
            "location": "/trouble-aws/index.html#troubleshooting-cloudbreak-on-aws", 
            "text": "Check out  HDCloud  troubleshooting docs.", 
            "title": "Troubleshooting Cloudbreak on AWS"
        }, 
        {
            "location": "/trouble-aws/index.html#unable-to-create-an-iam-role-for-cloudbreak", 
            "text": "Most corporate AWS users are unable to create AWS roles. You may have to contact your AWS admin to create the role(s) for you.", 
            "title": "Unable to create an IAM Role for Cloudbreak"
        }, 
        {
            "location": "/trouble-aws/index.html#cluster-fails-with-a-permissions-related-error", 
            "text": "Make sure that the policy attached to CredentialRole includes all actions defined in  CredentialRole .", 
            "title": "Cluster fails with a permissions related error"
        }, 
        {
            "location": "/trouble-azure/index.html", 
            "text": "Troubleshooting Cloudbreak on Azure\n\n\nCloudbreak deployment errors\n\n\nInvalid resource reference\n\n\nExample error message:\n\n\nResource /subscriptions/.../resourceGroups//providers/Microsoft.Network/virtualNetworks/cbdeployerVnet/\n\nsubnets/cbdeployerSubnet referenced by resource /subscriptions/.../resourceGroups/Manulife-ADLS/providers/\n\nMicrosoft.Network/networkInterfaces/cbdeployerNic was not found.\n\nPlease make sure that the referenced resource exists, and that both resources are in the same region.\n\n\nSymptom\n: The most common reason for this error is that you did not provide the Vnet RG Name (last parameter in the template).  \n\n\nSolution\n: When launching Cloudbreak, under \"Vnet RG Name\" provide the name of the resource group in which the selected VNet is located. If using a new VNet, enter the same resource group name as in \"Resource group\". \n\n\nCredential prerequisite errors\n\n\nYou don't have enough permissions to assign roles\n\n\nThis error during the interactive credential creation typically means that you do not have suitable permissions to create an interactive credential. Using an interactive credential currently requires an \"Owner\" role or its equivalent so if you are using a corporate account you are unlikely to have it. Try using the app-based credential. \n\n\nProblems with IAM permissions assignment\n\n\nAfter registering an Azure application you may have to ask your Azure administrator to perform the step of assigning the \"Contributor\" role to it:\n\n\n \n\n\nCredential creation errors\n\n\nRole already exists\n\n\nExample error message: \nRole already exists in Azure with the name: CloudbreakCustom50\n\n\nSymptom\n: You specified that you want to create a new role for Cloudbreak credential, but an existing role with the same name already exists in Azure. \n\n\nSolution\n: You should either rename the role during credential creation or select the \nReuse existing custom role\n option. \n\n\nRole does not exist\n\n\nExample error message: \nRole does not exist in Azure with the name: CloudbreakCustom60\n\n\nSymptom\n: You specified that you want to reuse an existing role for your Cloudbreak credential, but that particular role does not exist in Azure.\n\n\nSolution\n: You should either rename the new role during the credential creation to match the existing role's name or select the \nLet Cloudbreak create a custom role\n option. \n\n\nRole does not have enough privileges\n\n\nExample error message: \nCloudbreakCustom 50 role does not have enough privileges to be used by Cloudbreak!\n\n\n\n\nSymptom\n: You specified that you want to reuse an  existing role for your Cloudbreak credential, but that particular role does not have the necessary privileges for Cloudbreak cluster management.\n\n\nSolution\n: You should either select an existing role with enough privileges or select the \nLet Cloudbreak create a custom role\n option.\n\n\nThe necessary action set for Cloudbreak to be able to manage the clusters includes:\n        \n\"Microsoft.Compute/*\",\n        \"Microsoft.Network/*\",\n        \"Microsoft.Storage/*\",\n        \"Microsoft.Resources/*\"\n\n\nClient does not have authorization\n\n\nExample error message:\n\n\nFailed to verify credential: Status code 403, {\"error\":{\"code\":\"AuthorizationFailed\",\n\n\"message\":\"The client 'X' with object id 'z' does not have authorization to perform action\n\n'Microsoft.Storage/storageAccounts/read' over scope 'subscriptions/...'\"}\n\n\nSymptom\n: Your Azure account does not have sufficient permissions to create a Coudbreak credential. \n\n\nSolution\n: If you get this error during interactive credential creation, please ensure that your Azure account has \nMicrosoft.Authorization/*/Write\n permission. Otherwise contact your Azure administrator to either give your account that permission or create the necessary resources for the app-based credential creation method.  \n\n\nCloud not validate publickey certificate\n\n\nExample error message:\n\n\nCould not validate publickey certificate [certificate: 'fdfdsf'], detailed message: \n\nCorrupt or unknown public key file format\n\n\nSymptom\n: The syntax of your SSH public key is incorrect.\n\n\nSolution\n: You must correct the syntax of your SSH key. For information about the correct syntax, refer to \nthis\n page.", 
            "title": "Troubleshooting Azure"
        }, 
        {
            "location": "/trouble-azure/index.html#troubleshooting-cloudbreak-on-azure", 
            "text": "", 
            "title": "Troubleshooting Cloudbreak on Azure"
        }, 
        {
            "location": "/trouble-azure/index.html#cloudbreak-deployment-errors", 
            "text": "", 
            "title": "Cloudbreak deployment errors"
        }, 
        {
            "location": "/trouble-azure/index.html#invalid-resource-reference", 
            "text": "Example error message:  Resource /subscriptions/.../resourceGroups//providers/Microsoft.Network/virtualNetworks/cbdeployerVnet/ \nsubnets/cbdeployerSubnet referenced by resource /subscriptions/.../resourceGroups/Manulife-ADLS/providers/ \nMicrosoft.Network/networkInterfaces/cbdeployerNic was not found. \nPlease make sure that the referenced resource exists, and that both resources are in the same region.  Symptom : The most common reason for this error is that you did not provide the Vnet RG Name (last parameter in the template).    Solution : When launching Cloudbreak, under \"Vnet RG Name\" provide the name of the resource group in which the selected VNet is located. If using a new VNet, enter the same resource group name as in \"Resource group\".", 
            "title": "Invalid resource reference"
        }, 
        {
            "location": "/trouble-azure/index.html#credential-prerequisite-errors", 
            "text": "", 
            "title": "Credential prerequisite errors"
        }, 
        {
            "location": "/trouble-azure/index.html#you-dont-have-enough-permissions-to-assign-roles", 
            "text": "This error during the interactive credential creation typically means that you do not have suitable permissions to create an interactive credential. Using an interactive credential currently requires an \"Owner\" role or its equivalent so if you are using a corporate account you are unlikely to have it. Try using the app-based credential.", 
            "title": "You don't have enough permissions to assign roles"
        }, 
        {
            "location": "/trouble-azure/index.html#problems-with-iam-permissions-assignment", 
            "text": "After registering an Azure application you may have to ask your Azure administrator to perform the step of assigning the \"Contributor\" role to it:", 
            "title": "Problems with IAM permissions assignment"
        }, 
        {
            "location": "/trouble-azure/index.html#credential-creation-errors", 
            "text": "", 
            "title": "Credential creation errors"
        }, 
        {
            "location": "/trouble-azure/index.html#role-already-exists", 
            "text": "Example error message:  Role already exists in Azure with the name: CloudbreakCustom50  Symptom : You specified that you want to create a new role for Cloudbreak credential, but an existing role with the same name already exists in Azure.   Solution : You should either rename the role during credential creation or select the  Reuse existing custom role  option.", 
            "title": "Role already exists"
        }, 
        {
            "location": "/trouble-azure/index.html#role-does-not-exist", 
            "text": "Example error message:  Role does not exist in Azure with the name: CloudbreakCustom60  Symptom : You specified that you want to reuse an existing role for your Cloudbreak credential, but that particular role does not exist in Azure.  Solution : You should either rename the new role during the credential creation to match the existing role's name or select the  Let Cloudbreak create a custom role  option.", 
            "title": "Role does not exist"
        }, 
        {
            "location": "/trouble-azure/index.html#role-does-not-have-enough-privileges", 
            "text": "Example error message:  CloudbreakCustom 50 role does not have enough privileges to be used by Cloudbreak!   Symptom : You specified that you want to reuse an  existing role for your Cloudbreak credential, but that particular role does not have the necessary privileges for Cloudbreak cluster management.  Solution : You should either select an existing role with enough privileges or select the  Let Cloudbreak create a custom role  option.  The necessary action set for Cloudbreak to be able to manage the clusters includes:\n         \"Microsoft.Compute/*\",\n        \"Microsoft.Network/*\",\n        \"Microsoft.Storage/*\",\n        \"Microsoft.Resources/*\"", 
            "title": "Role does not have enough privileges"
        }, 
        {
            "location": "/trouble-azure/index.html#client-does-not-have-authorization", 
            "text": "Example error message:  Failed to verify credential: Status code 403, {\"error\":{\"code\":\"AuthorizationFailed\", \n\"message\":\"The client 'X' with object id 'z' does not have authorization to perform action \n'Microsoft.Storage/storageAccounts/read' over scope 'subscriptions/...'\"}  Symptom : Your Azure account does not have sufficient permissions to create a Coudbreak credential.   Solution : If you get this error during interactive credential creation, please ensure that your Azure account has  Microsoft.Authorization/*/Write  permission. Otherwise contact your Azure administrator to either give your account that permission or create the necessary resources for the app-based credential creation method.", 
            "title": "Client does not have authorization"
        }, 
        {
            "location": "/trouble-azure/index.html#cloud-not-validate-publickey-certificate", 
            "text": "Example error message:  Could not validate publickey certificate [certificate: 'fdfdsf'], detailed message:  \nCorrupt or unknown public key file format  Symptom : The syntax of your SSH public key is incorrect.  Solution : You must correct the syntax of your SSH key. For information about the correct syntax, refer to  this  page.", 
            "title": "Cloud not validate publickey certificate"
        }, 
        {
            "location": "/trouble-gcp/index.html", 
            "text": "Troubleshooting Cloudbreak on GCP\n\n\nGoogle Cloud create cluster fails with permissions related error\n\n\nIn order to launch clusters on GCP via Cloudbreak, you must have a Service Account that Cloudbreak can use to create resources. In addition, you must also have a P12 key associated with that account.\n\n\nUsually, a user with an \"Owner\" role can assign roles to new and existing service accounts from \nIAM \n Admin \n IAM\n in the Google Cloud console. If you are using your own account, you should be able to perform this step, but if you are using a corporate account, you will likely have to contact your Google Cloud admin.\n\n\nThe roles for the service account are described in \nService account\n.", 
            "title": "Troubleshooting GCP"
        }, 
        {
            "location": "/trouble-gcp/index.html#troubleshooting-cloudbreak-on-gcp", 
            "text": "", 
            "title": "Troubleshooting Cloudbreak on GCP"
        }, 
        {
            "location": "/trouble-gcp/index.html#google-cloud-create-cluster-fails-with-permissions-related-error", 
            "text": "In order to launch clusters on GCP via Cloudbreak, you must have a Service Account that Cloudbreak can use to create resources. In addition, you must also have a P12 key associated with that account.  Usually, a user with an \"Owner\" role can assign roles to new and existing service accounts from  IAM   Admin   IAM  in the Google Cloud console. If you are using your own account, you should be able to perform this step, but if you are using a corporate account, you will likely have to contact your Google Cloud admin.  The roles for the service account are described in  Service account .", 
            "title": "Google Cloud create cluster fails with permissions related error"
        }, 
        {
            "location": "/trouble-cli/index.html", 
            "text": "Troubleshooting Cloudbreak CLI\n\n\nSpecial characters in blueprint name cause an error\n\n\nWhen registering a blueprint via \nblueprint create\n CLI command, if the name of the blueprint includes one or more of the following special characters \n@#$%|:\n*;\n you will get an error similar to:  \n\n\ncb blueprint create from-url --name test@# --url https://myurl.com/myblueprint.bp  \n[1] 7547\n-bash: application.yml: command not found\n-bash: --url: command not found\n ~ \ue0b1 integration-test \ue0b0 1 \ue0b0 time=\"2018-02-01T12:56:44+01:00\" level=\"error\" msg=\"the following parameters are missing: url\\n\"\n\n\n\nSolution:\n\nWhen using special characters in a blueprint name, make sure to use quotes; for example \"test@#\":  \n\n\ncb blueprint create from-url --name \"test@#\" --url https://myurl.com/myblueprint.bp", 
            "title": "Troubleshooting Cloudbreak CLI"
        }, 
        {
            "location": "/trouble-cli/index.html#troubleshooting-cloudbreak-cli", 
            "text": "", 
            "title": "Troubleshooting Cloudbreak CLI"
        }, 
        {
            "location": "/trouble-cli/index.html#special-characters-in-blueprint-name-cause-an-error", 
            "text": "When registering a blueprint via  blueprint create  CLI command, if the name of the blueprint includes one or more of the following special characters  @#$%|: *;  you will get an error similar to:    cb blueprint create from-url --name test@# --url https://myurl.com/myblueprint.bp  \n[1] 7547\n-bash: application.yml: command not found\n-bash: --url: command not found\n ~ \ue0b1 integration-test \ue0b0 1 \ue0b0 time=\"2018-02-01T12:56:44+01:00\" level=\"error\" msg=\"the following parameters are missing: url\\n\"  Solution: \nWhen using special characters in a blueprint name, make sure to use quotes; for example \"test@#\":    cb blueprint create from-url --name \"test@#\" --url https://myurl.com/myblueprint.bp", 
            "title": "Special characters in blueprint name cause an error"
        }, 
        {
            "location": "/cb-upgrade/index.html", 
            "text": "Upgrade Cloudbreak\n\n\nTo upgrade Cloudbreak to the newest version, perform the following steps.\n\n\nWe recommend that you back up Cloudbreak databases before upgrading. Refer to \nBack up Cloudbreak database\n.\n\n\nSteps\n\n\n\n\n\n\nOn the VM where Cloudbreak is running, navigate to the directory where your Profile file is located:\n\n\ncd /var/lib/cloudbreak-deployment/\n\n\n\n\n\n\nStop all of the running Cloudbreak components:\n\n\ncbd kill\n\n\n\n\n\n\nUpdate Cloudbreak deployer:\n\n\ncbd update\n\n\n\n\n\n\nUpdate the \ndocker-compose.yml\n file with new Docker containers needed for the cbd:\n\n\ncbd regenerate\n\n\n\n\n\n\nIf there are no other Cloudbreak instances that still use old Cloudbreak versions, remove the obsolete containers:\n\n\ncbd util cleanup\n\n\n\n\n\n\nCheck the health and version of the updated cbd:\n\n\ncbd doctor\n\n\n\n\n\n\nStart the new version of the cbd:\n\n\ncbd start\n\n\nCloudbreak needs to download updated docker images for the new version, so this step may take a while.", 
            "title": "Upgrade Cloudbreak"
        }, 
        {
            "location": "/cb-upgrade/index.html#upgrade-cloudbreak", 
            "text": "To upgrade Cloudbreak to the newest version, perform the following steps.  We recommend that you back up Cloudbreak databases before upgrading. Refer to  Back up Cloudbreak database .  Steps    On the VM where Cloudbreak is running, navigate to the directory where your Profile file is located:  cd /var/lib/cloudbreak-deployment/    Stop all of the running Cloudbreak components:  cbd kill    Update Cloudbreak deployer:  cbd update    Update the  docker-compose.yml  file with new Docker containers needed for the cbd:  cbd regenerate    If there are no other Cloudbreak instances that still use old Cloudbreak versions, remove the obsolete containers:  cbd util cleanup    Check the health and version of the updated cbd:  cbd doctor    Start the new version of the cbd:  cbd start  Cloudbreak needs to download updated docker images for the new version, so this step may take a while.", 
            "title": "Upgrade Cloudbreak"
        }, 
        {
            "location": "/cb-delete/index.html", 
            "text": "Deleting Cloudbreak\n\n\nYou must terminate all clusters associated with a Cloudbreak before you can terminate the Cloudbreak instance. In general, you should delete clusters from the Cloudbreak UI. If needed, you can also delete the cluster resources manually via the cloud provider tools. \n\n\nDeleting clusters\n\n\nThe proper way to delete clusters is to use the the \nTerminate\n option available in the Cloudbreak UI. If the terminate process fails, try the \nTerminate\n \n \nForce terminate\n option.\n\n\nIf the force termination does not delete all cluster resources, delete the resources manually:\n\n\n\n\nTo find the VMs, click on the links available in the cluster details. \n\n\nTo find the network and subnet, see the \nCluster Information\n in the cluster details. \n\n\nOn Azure, you can delete the cluster manually by deleting the whole resource group created when the cluster was deployed. The name of the resource group, under which the cluster-related resources are organized always includes the name of the cluster, so you should be able to find the group by searching for that name in the \nResource groups\n.\n\n\n\n\nUpon cluster termination, Cloudbreak only terminates the resources that it created. It does not terminate any resources (such as networks, subnets, roles, and so on) which existed prior to cluster creation. \n\n\nDelete Cloudbreak on AWS\n\n\nIf you want to delete the Cloudbreak instance, you can do so by deleting the EC2 instance on which it is running.\n\n\nSteps\n\n\n\n\n\n\nLog in to the AWS Management Console.\n\n\n\n\n\n\nBrowse to the EC2 Management Console.\n\n\n\n\n\n\nnavigate to \nInstances\n.\n\n\n\n\n\n\nSelect the instance that you want to delete and then select \nActions\n \n \nInstance State\n \n \nTerminate\n.\n\n\n\n\n\n\nClick \nYes, Terminate\n to confirm.\n\n\n  \n\n\n\n\n\n\nDelete Cloudbreak on Azure\n\n\nYou can delete Cloudbreak instance from your Azure account by deleting related resources. To delete a Cloudbreak instance:\n\n\n\n\n\n\nIf you deployed Cloudbreak in a new resource group: to delete Cloudbreak, delete the whole related resource group.\n\n\n\n\n\n\nIf you deployed Cloudbreak in an existing resource group: navigate to the group and delete only Cloudbreak related resources such as the VM.\n\n\n\n\n\n\nSteps\n\n\n\n\n\n\nFrom the Microsoft Azure Portal dashboard, select \nResource groups\n.\n\n\n\n\n\n\nFind the resource group that you want to delete.\n\n\n\n\n\n\nIf you deployed Cloudbreak in a new resource group, you can delete the whole resource group. Click on \n...\n and select \nDelete\n:\n\n\n  \n\n\nNext, type the name of the resource group to delete and click \nDelete\n.\n\n\n\n\n\n\nIf you deployed Cloudbreak in an existing resource group, navigate to the details of the resource group and delete only Cloudbreak-related resources such as the VM.    \n\n\n\n\n\n\nDelete Cloudbreak on GCP\n\n\nYou can delete Cloudbreak instance from your Google Cloud account. \n\n\nSteps\n\n\n\n\n\n\nNavigate to your Google Cloud account.\n\n\n\n\n\n\nNavigate to \nCompute Engine\n \n \nVM instances\n.\n\n\n\n\n\n\nSelect the instance that you want to delete:\n\n\n     \n\n\n\n\n\n\nClick on the delete icon and then confirm delete. \n\n\n\n\n\n\nDelete Cloudbreak on OpenStack\n\n\nYou can delete Cloudbreak instance from your OpenStack console. \n\n\nSteps\n\n\n\n\n\n\nNavigate to your OpenStack account.\n\n\n\n\n\n\nNavigate to \nInstances\n.\n\n\n\n\n\n\nSelect the instance to delete, click \nTerminate Instances\n, and confirm.", 
            "title": "Delete Cloudbreak"
        }, 
        {
            "location": "/cb-delete/index.html#deleting-cloudbreak", 
            "text": "You must terminate all clusters associated with a Cloudbreak before you can terminate the Cloudbreak instance. In general, you should delete clusters from the Cloudbreak UI. If needed, you can also delete the cluster resources manually via the cloud provider tools.", 
            "title": "Deleting Cloudbreak"
        }, 
        {
            "location": "/cb-delete/index.html#deleting-clusters", 
            "text": "The proper way to delete clusters is to use the the  Terminate  option available in the Cloudbreak UI. If the terminate process fails, try the  Terminate     Force terminate  option.  If the force termination does not delete all cluster resources, delete the resources manually:   To find the VMs, click on the links available in the cluster details.   To find the network and subnet, see the  Cluster Information  in the cluster details.   On Azure, you can delete the cluster manually by deleting the whole resource group created when the cluster was deployed. The name of the resource group, under which the cluster-related resources are organized always includes the name of the cluster, so you should be able to find the group by searching for that name in the  Resource groups .   Upon cluster termination, Cloudbreak only terminates the resources that it created. It does not terminate any resources (such as networks, subnets, roles, and so on) which existed prior to cluster creation.", 
            "title": "Deleting clusters"
        }, 
        {
            "location": "/cb-delete/index.html#delete-cloudbreak-on-aws", 
            "text": "If you want to delete the Cloudbreak instance, you can do so by deleting the EC2 instance on which it is running.  Steps    Log in to the AWS Management Console.    Browse to the EC2 Management Console.    navigate to  Instances .    Select the instance that you want to delete and then select  Actions     Instance State     Terminate .    Click  Yes, Terminate  to confirm.", 
            "title": "Delete Cloudbreak on AWS"
        }, 
        {
            "location": "/cb-delete/index.html#delete-cloudbreak-on-azure", 
            "text": "You can delete Cloudbreak instance from your Azure account by deleting related resources. To delete a Cloudbreak instance:    If you deployed Cloudbreak in a new resource group: to delete Cloudbreak, delete the whole related resource group.    If you deployed Cloudbreak in an existing resource group: navigate to the group and delete only Cloudbreak related resources such as the VM.    Steps    From the Microsoft Azure Portal dashboard, select  Resource groups .    Find the resource group that you want to delete.    If you deployed Cloudbreak in a new resource group, you can delete the whole resource group. Click on  ...  and select  Delete :      Next, type the name of the resource group to delete and click  Delete .    If you deployed Cloudbreak in an existing resource group, navigate to the details of the resource group and delete only Cloudbreak-related resources such as the VM.", 
            "title": "Delete Cloudbreak on Azure"
        }, 
        {
            "location": "/cb-delete/index.html#delete-cloudbreak-on-gcp", 
            "text": "You can delete Cloudbreak instance from your Google Cloud account.   Steps    Navigate to your Google Cloud account.    Navigate to  Compute Engine     VM instances .    Select the instance that you want to delete:           Click on the delete icon and then confirm delete.", 
            "title": "Delete Cloudbreak on GCP"
        }, 
        {
            "location": "/cb-delete/index.html#delete-cloudbreak-on-openstack", 
            "text": "You can delete Cloudbreak instance from your OpenStack console.   Steps    Navigate to your OpenStack account.    Navigate to  Instances .    Select the instance to delete, click  Terminate Instances , and confirm.", 
            "title": "Delete Cloudbreak on OpenStack"
        }, 
        {
            "location": "/releasenotes/index.html", 
            "text": "Release notes\n\n\n2.7.0\n\n\nCloudbreak 2.7.0 is a general availability release, which is suitable for production deployments.\n\n\n\n\nNew features\n\n\n\n\nTechnical Preview of Shared Services\n\n\nCloudbreak 2.6.0 TP allows you to create a shared services instance and attach it to a cluster.\n\n\n\n\nSorting and Filtering Resource Tables\n\n\nCloudbreak introduces the ability to sort and filter tables listing resources such as clusters, recipes, and blueprints in the Cloudbreak web UI.\n\n\n\n\nCreating Flow Management Clusters\n\n\nCloudbreak introduces the ability to create HDF flow management cluster with Apache NiFi and NiFi Registry. To help you get started, Cloudbreak provides a new built-in \nFlow Management: Apache NiFi\n blueprint. \n\n\nWhen creating a Flow Management cluster from the default blueprint, make sure to do the following:\n\n\n\n\nPlace the Ambari Server on the \"Services\" host group.     \n\n\nWhen creating a cluster, open 9091 TCP port on the NiFi host group. Without it, you will be unable to access the NiFi web UI.   \n\n\nWhen creating a cluster, open port 61443 on the Services host group. This port is used by NiFi Registry.      \n\n\nWhen creating the NiFi Registry controller service in NiFi, the internal hostname has to be used, \ne.g. https://ip-1-2-3-4.us-west-2.compute.internal:61443\n   \n\n\nEnable Kerberos. You can either use your own kerberos or select for Cloudbreak to create a test KDC.  \n\n\nAlthough Cloudbreak allows cluster scaling (including autoscaling), scaling is not supported by NiFi. Downscaling NiFi clusters is not supported - as it can result in data loss when a node is removed that has not yet processed all the data on that node. There is also a known issue related to scaling listed in the \nKnown Issues\n below.  \n\n\n\n\nFor the list of available blueprints, refer to \nDefault Cluster Configurations\n.\n\nTo get started creating NiFi clusters, refer to the following \nHCC post\n.  \n\n\n\n\nCreating HDF Messaging Management Clusters\n\n\nCloudbreak introduces the ability to create HDF Messaging clusters, including Apache Kafka. To help you get started, Cloudbreak provides a new built-in \nHDF Messaging Management: Apache Kafka\n blueprint. \n\n\nWhen creating a Messaging Management cluster from the default blueprint, make sure to do the following:\n\n\n\n\nIf using the default blueprint, place the Ambari Server on the \"Services\" host group.  \n\n\nWhen creating a cluster, open 3000 TCP port on the Services host group for Grafana.     \n\n\n\n\nFor the list of available blueprints, refer to \nDefault Cluster Configurations\n.  \n\n\n\n\nUsing MySQL and Oracle External Databases\n\n\nCloudbreak introduces support for creating external MySQL and Oracle databases, in addition to previously supported Postgres. For more information, refer to \nUsing an external database\n.   \n\n\n\n\nUsing External Databases for Cluster Services\n\n\nYou can register an existing external RDBMS in the Cloudbreak UI or CLI so that it can be used for those cluster components which have support for it. After the RDBMS has been registered with Cloudbreak, it will be available during the cluster create and can be reused with multiple clusters.  \n\n\nOnly Postgres is supported at this time. Refer to component-specific documentation for information on which version of Postgres (if any) is supported.  \n\n\nFor more information, refer to \nRegister an External Database\n.  \n\n\n\n\nUsing External Authentication Sources (LDAP/AD) for Clusters\n\n\nYou can configure an existing LDAP/AD authentication source in the Cloudbreak UI or CLI so that it can later be associated with one or more Cloudbreak-managed clusters. After the authentication source has been registered with Cloudbreak, it will be available during the cluster create and can be reused with multiple clusters.\n\n\nFor more information, refer to \nRegister an Authentication Source\n.   \n\n\n\n\nConfiguring Cloudbreak to Use Existing LDAP/AD\n\n\nYou can configure Cloudbreak to use your existing LDAP/AD so that you can authenticate Cloudbreak users against an existing LDAP/AD server. For more information, refer to \nConfiguring Cloudbreak for LDAP/AD Authentication\n. \n\n\n\n\nLaunching Cloudbreak in Environments with Restricted Internet Access or Required Use of Proxy\n\n\nYou can launch Cloudbreak in environments with limited or restricted internet access and/or required use of a proxy to obtain internet access. For more information, refer to \nConfigure Outbound Internet Access and Proxy\n. \n\n\n\n\nModifying Existing Cloudbreak Credentials\n\n\nCloudbreak allows you to modify existing credentials by using the edit option available in Cloudbreak UI or by using the \ncredential modify\n command in the CLI. For more information, refer to \nModify an Existing Credential\n.\n\n\n\n\nUsing Management Packs\n\n\nCloudbreak 2.6.0 TP introduces support for using management packs, allowing you to register them in Cloudbreak web UI or CLI and then select to install them as part of cluster creation.\n\nFor more information, refer to \nUsing management packs\n.  \n\n\n\n\nBehavioral changes\n\n\n\n\nImage Catalog Option Was Moved to External Sources\n\n\nThe options related to registering a custom image catalog and selecting a default image catalog were removed from the \nSettings\n navigation menu option and are now available under \nExternal Sources \n Image Catalogs\n.\n\n\n\n\nRecipes Option Was Moved to Cluster Extension\n\n\nThe \nRecipes\n navigation menu option was moved under \nCluster Extensions\n, so to find recipe-related settings, select \nCluster Extensions \n Recipes\n from the navigation menu. \n\n\n\n\nAuto-import of HDP/HDF Images on OpenStack\n\n\nWhen using Cloudbreak on OpenStack, you no longer need to import HDP and HDF images manually, because during your first attempt to create a cluster, Cloudbreak automatically imports HDP and HDF images to your OpenStack. Only Cloudbreak image must be imported manually. \n\n\n\n\nImage catalog updates\n\n\n\n\nJune 12, 2018\n\n\nDefault Ambari version 2.6.2.0\n\nDefault HDP version 2.6.5.0 \nDefault HDF version 3.1.1.0-35  \n\n\n\n\nFixed issues\n\n\n\n\n\n\n\n\n\n\nIssue\n\n\nIssue description\n\n\nCategory\n\n\nFix version\n\n\n\n\n\n\n\n\n\n\nBUG-100468\n\n\nImages from a custom image catalog are not listed in the UI after Cloudbreak version changed.\n\n\nStability\n\n\n2.7.0\n\n\n\n\n\n\nBUG-99168\n\n\nAll clusters created on Google Cloud Platform fail.\n\n\nStability\n\n\n2.5.0 TP\n\n\n\n\n\n\nBUG-99400\n\n\nTime-based cluster autoscaling does not work.\n\n\nStability\n\n\n2.5.0 TP\n\n\n\n\n\n\nBUG-99505\n\n\nSync is not working for an AWS instance that was terminated a long time ago.\n\n\nStability\n\n\n2.5.0 TP\n\n\n\n\n\n\nBUG-98277\n\n\nNetwork interface handling in CloudBreak should be improved.\n\n\nStability\n\n\n2.5.0 TP\n\n\n\n\n\n\nBUG-97395\n\n\nNetworks are duplicated on networks tab of the cluster create wizard.\n\n\nStability\n\n\n2.5.0 TP\n\n\n\n\n\n\nBUG-97259\n\n\n\"Update failed\" status after downscale failed, even though cluster was not modified and its status should be \"Running\".\n\n\nStability\n\n\n2.5.0 TP\n\n\n\n\n\n\nBUG-97207\n\n\nChanging lifecycle management on YARN causes NPE.\n\n\nStability\n\n\n2.5.0 TP\n\n\n\n\n\n\nBUG-99189\n\n\nImageCatalog PUT endpoint is not secured.\n\n\nSecurity\n\n\n2.5.0 TP\n\n\n\n\n\n\nBUG-97895\n\n\nLDAP password should be removed from Cloudbreak logs.\n\n\nSecurity\n\n\n2.5.0 TP\n\n\n\n\n\n\nBUG-97300\n\n\nCloudbreak should show proper error messages when the given credential is not valid anymore.\n\n\nUsability\n\n\n2.5.0 TP\n\n\n\n\n\n\nBUG-97296\n\n\nGCP credential creation should validate whether resources are available with the credential.\n\n\nUsability\n\n\n2.5.0 TP\n\n\n\n\n\n\nBUG-97660\n\n\nIgnore repository warnings checkbox are missing after changing base image Ambari or HDP to a custom one.\n\n\nUsability\n\n\n2.5.0 TP\n\n\n\n\n\n\nBUG-97307\n\n\nIgnore repository warnings checkbox is not selectable after change the HDP VDF URL.\n\n\nUsability\n\n\n2.5.0 TP\n\n\n\n\n\n\nBUG-96764\n\n\n\"Failed to remove instance\" error when using the delete icon.\n\n\nUsability\n\n\n2.5.0 TP\n\n\n\n\n\n\nBUG-97390\n\n\nCloudbreak should support longer resource ID-s on AWS.\n\n\nUsability\n\n\n2.5.0 TP\n\n\n\n\n\n\nBUG-99512\n\n\nAzure ES_v3 instances should support premium storage.\n\n\nUsability\n\n\n2.5.0 TP\n\n\n\n\n\n\nBUG-97206\n\n\nBackend should return only images for enabled platforms.\n\n\nUsability\n\n\n2.5.0 TP\n\n\n\n\n\n\n\n\n\n\nKnown issues\n\n\n\n\nKnown issues: Cloudbreak\n\n\n\n\n(BUG-96788) \nAzure Availability Set Option Is Not Available for Instance Count of 1\n\n\nWhen creating a cluster, the Azure availability set feature is not available for host groups with the instance count of 1.\n\n\nWorkaround\n: \n\n\nThis issue will be fixed in a future release. \n\n\nIf you would like to use the Azure availability sets feature now, you must add at least 2 instances to the host group for which you want to use them. The option Azure availability sets is available on the advanced \nHardware and Storage\n page of the create cluster wizard.   \n\n\n\n\n(BUG-92605) \nCluster Creation Fails with ResourceInError\n\n\nCluster creation fails with the following error: \n\n\nInfrastructure creation failed. Reason: Failed to create the stack for CloudContext{id=3689, name='test-exisitngnetwork', platform='StringType{value='OPENSTACK'}', owner='e0307f96-bd7d-4641-8c8f-b95f2667d9c6'} due to: Resource CREATE failed: ResourceInError: resources.ambari_volume_master_0_0: Went to status error due to \"Unknown\"\n\n\nWorkaround\n: \n\n\nThis may mean that the volumes that you requested exceed volumes available on your cloud provider account. When creating a cluster, on the advanced \nHardware and Storage\n page of the create cluster wizard, try reducing the amount of requested storage. If you need more storage, try using a different region or ask your cloud provider admin to increase the resource quota for volumes.  \n\n\n\n\n(BUG-93241) \nError When Scaling Multiple Host Groups\n\n\nScaling of multiple host groups fails with the following error: \n\n\nBatch update returned unexpected row count from update [0]; actual row count: 0; expected: 1; nested exception is org.hibernate.StaleStateException: Batch update returned unexpected row count from update [0]; actual row count: 0; expected: 1\n\n\nWorkaround:\n\n\nScaling multiple host groups at once is not supported. If you would like to scale multiple host groups: scale the first host group and wait until scaling has completed, then scale the second host group, and so on.  \n\n\n\n\n(BUG-97044) \nShow CLI Command Copy JSON Button Does Not Work\n\n\nWhen using the \nShow CLI Command\n \n \nCopy the JSON\n or \nCopy the Command\n button with Firefox, the content does not does not get copied if adblock plugin or other advertise blocker plugins are present.\n\n\nWorkaround:\n  \n\n\nUse a browser without an adblock plugin. \n\n\n\n\n(BUG-93257) \nClusters Are Missing From History When an Exact Day Is Selected\n\n\nOn the History page, when the start date selected is the same as end date, clusters that were running on that date are filtered out. \n\n\n\n\n(BUG-93257) \nClusters Are Missing From History\n\n\nAfter changing the dates on the History page multiple times, the results displayed may sometimes be incorrect. \n\n\nWorkaround:\n\n\nRefresh the page if you think that the history displayed may be incorrect.  \n\n\n\n\n(BUG-101223) \nHardware Status is Incorrect After Stop and Start\n\n\nAfter stopping and starting a cluster, cluster state is incorrectly listed as \"Unhealty\", even though the nodes are healthy.\n\n\n\n\n(BUG-101230) \nThe Command for CLI Download Doesn't Work on Windows\n\n\nThe \ncurl\n command listed on the \nDownload CLI\n page for Windows does not work on Windows. \n\n\n\n\n(BUG-101225) \nCLI: Manual Repair Does Not Work\n\n\ncb cluster repair\n does not work as expected.\n\n\n\n\n(BUG-101204) \nCLI: InstanceProfileStrategy Create Doesn't Work\n\n\nUsing the following parameter in the CLI JSON from creating an instance profile does not work as expected: \n\n\"parameters\": {\n    \"instanceProfileStrategy\": \"CREATE\"\n},\n\n\n\n\nKnown issues: Ambari 2.6.1.3 and HDP 2.6.4.0\n\n\n\n\nThe known issues described here were discovered when testing Cloudbreak with Ambari 2.6.1.3 and HDP 2.6.4.0, which are used by default in Cloudbreak.\n\n\nFor general Ambari 2.6.1.5 and HDP 2.6.4.0 known issues, refer to:\n\n\nAmbari 2.6.1.5 Release Notes\n\n\nHDP 2.6.4.0 Release Notes\n  \n\n\n\n\n\n\n(BUG-96707) \nDruid Overload Does Not Start\n\n\nDruid overload start fails with the following error when using Ambari 2.6.1.3 and HDP 2.6.4.0: \n\n\nERROR [main] io.druid.cli.CliOverlord - Error when starting up.  Failing. com.google.inject.ProvisionException: Unable to provision\n \n\n\n\n\n(BUG-97080) \nAmbari Files In Some Cases When an Mpack is Installed\n\n\nIf you set the following properties then cluster install may fail (in 20-30% of the cases), because of the Ambari agent cache being updated concurrently:\n\n\n/etc/ambari-server/conf/ambari.properties  \nagent.auto.cache.update=true*  \n*/etc/ambari-agent/conf/ambari-agent.ini  \nparallel_execution=1\n\n\n\n\n\n(AMBARI-14149) \nIn Ambari, Cluster Cannot Be Started After Stop\n\n\nWhen using Ambari version 2.5.0.3, after stopping and starting a cluster, Event History shows the following error:\n\n\nAmbari cluster could not be started. Reason: Failed to start Hadoop services.\n2/7/2018, 12:47:05 PM\nStarting Ambari services.\n2/7/2018, 12:47:04 PM\nManual recovery is needed for the following failed nodes:   \n[host-10-0-0-4.openstacklocal, host-10-0-0-3.openstacklocal, host-10-0-0-5.openstacklocal\n\n\n\nAmbari dashboard shows that nodes are not sending heartbeats. \n\n\nWorkaround:\n  \n\n\nThis issue is fixed in Ambari version 2.5.1.0 and newer.  \n\n\n\n\nKnown issues: HDF 3.1.1\n\n\nThe known issues described here were discovered when testing Cloudbreak with Ambari 2.6.1.3 and HDF 3.1.1, which are used by default in Cloudbreak.\n\n\n\n\nFor general HDF 3.1.1 known issues, refer to \nHDF 3.1.1 Release Notes\n\n\n\n\n\n\n(BUG-98865) \nScaling HDF Clusters Does Not Update Configurations on New Nodes\n\n\nBlueprint configuration parameters are not applied when scaling an HDF cluster. \nOne example that affects all users is that after HDF cluster upscale/downscale the \nnifi.web.proxy.host\n blueprint parameter does not get updated to include the new nodes, and as a result the NiFi UI is not reachable from these nodes. \n\n\nWorkaround:\n  \n\n\nConfiguration parameters set in the blueprint are not applied when scaling an HDF cluster. One example that affects all NiFi users is that after HDF cluster upscale the \nnifi.web.proxy.host\n parameter does not get updated to include the new hosts, and as a result the NiFi UI is not reachable from these hosts. \n\n\nHOST1-IP:PORT,HOST2-IP:PORT,HOST3-IP:PORT", 
            "title": "Release Notes"
        }, 
        {
            "location": "/releasenotes/index.html#release-notes", 
            "text": "", 
            "title": "Release notes"
        }, 
        {
            "location": "/releasenotes/index.html#270", 
            "text": "Cloudbreak 2.7.0 is a general availability release, which is suitable for production deployments.", 
            "title": "2.7.0"
        }, 
        {
            "location": "/releasenotes/index.html#new-features", 
            "text": "", 
            "title": "New features"
        }, 
        {
            "location": "/releasenotes/index.html#technical-preview-of-shared-services", 
            "text": "Cloudbreak 2.6.0 TP allows you to create a shared services instance and attach it to a cluster.", 
            "title": "Technical Preview of Shared Services"
        }, 
        {
            "location": "/releasenotes/index.html#sorting-and-filtering-resource-tables", 
            "text": "Cloudbreak introduces the ability to sort and filter tables listing resources such as clusters, recipes, and blueprints in the Cloudbreak web UI.", 
            "title": "Sorting and Filtering Resource Tables"
        }, 
        {
            "location": "/releasenotes/index.html#creating-flow-management-clusters", 
            "text": "Cloudbreak introduces the ability to create HDF flow management cluster with Apache NiFi and NiFi Registry. To help you get started, Cloudbreak provides a new built-in  Flow Management: Apache NiFi  blueprint.   When creating a Flow Management cluster from the default blueprint, make sure to do the following:   Place the Ambari Server on the \"Services\" host group.       When creating a cluster, open 9091 TCP port on the NiFi host group. Without it, you will be unable to access the NiFi web UI.     When creating a cluster, open port 61443 on the Services host group. This port is used by NiFi Registry.        When creating the NiFi Registry controller service in NiFi, the internal hostname has to be used,  e.g. https://ip-1-2-3-4.us-west-2.compute.internal:61443      Enable Kerberos. You can either use your own kerberos or select for Cloudbreak to create a test KDC.    Although Cloudbreak allows cluster scaling (including autoscaling), scaling is not supported by NiFi. Downscaling NiFi clusters is not supported - as it can result in data loss when a node is removed that has not yet processed all the data on that node. There is also a known issue related to scaling listed in the  Known Issues  below.     For the list of available blueprints, refer to  Default Cluster Configurations . \nTo get started creating NiFi clusters, refer to the following  HCC post .", 
            "title": "Creating Flow Management Clusters"
        }, 
        {
            "location": "/releasenotes/index.html#creating-hdf-messaging-management-clusters", 
            "text": "Cloudbreak introduces the ability to create HDF Messaging clusters, including Apache Kafka. To help you get started, Cloudbreak provides a new built-in  HDF Messaging Management: Apache Kafka  blueprint.   When creating a Messaging Management cluster from the default blueprint, make sure to do the following:   If using the default blueprint, place the Ambari Server on the \"Services\" host group.    When creating a cluster, open 3000 TCP port on the Services host group for Grafana.        For the list of available blueprints, refer to  Default Cluster Configurations .", 
            "title": "Creating HDF Messaging Management Clusters"
        }, 
        {
            "location": "/releasenotes/index.html#using-mysql-and-oracle-external-databases", 
            "text": "Cloudbreak introduces support for creating external MySQL and Oracle databases, in addition to previously supported Postgres. For more information, refer to  Using an external database .", 
            "title": "Using MySQL and Oracle External Databases"
        }, 
        {
            "location": "/releasenotes/index.html#using-external-databases-for-cluster-services", 
            "text": "You can register an existing external RDBMS in the Cloudbreak UI or CLI so that it can be used for those cluster components which have support for it. After the RDBMS has been registered with Cloudbreak, it will be available during the cluster create and can be reused with multiple clusters.    Only Postgres is supported at this time. Refer to component-specific documentation for information on which version of Postgres (if any) is supported.    For more information, refer to  Register an External Database .", 
            "title": "Using External Databases for Cluster Services"
        }, 
        {
            "location": "/releasenotes/index.html#using-external-authentication-sources-ldapad-for-clusters", 
            "text": "You can configure an existing LDAP/AD authentication source in the Cloudbreak UI or CLI so that it can later be associated with one or more Cloudbreak-managed clusters. After the authentication source has been registered with Cloudbreak, it will be available during the cluster create and can be reused with multiple clusters.  For more information, refer to  Register an Authentication Source .", 
            "title": "Using External Authentication Sources (LDAP/AD) for Clusters"
        }, 
        {
            "location": "/releasenotes/index.html#configuring-cloudbreak-to-use-existing-ldapad", 
            "text": "You can configure Cloudbreak to use your existing LDAP/AD so that you can authenticate Cloudbreak users against an existing LDAP/AD server. For more information, refer to  Configuring Cloudbreak for LDAP/AD Authentication .", 
            "title": "Configuring Cloudbreak to Use Existing LDAP/AD"
        }, 
        {
            "location": "/releasenotes/index.html#launching-cloudbreak-in-environments-with-restricted-internet-access-or-required-use-of-proxy", 
            "text": "You can launch Cloudbreak in environments with limited or restricted internet access and/or required use of a proxy to obtain internet access. For more information, refer to  Configure Outbound Internet Access and Proxy .", 
            "title": "Launching Cloudbreak in Environments with Restricted Internet Access or Required Use of Proxy"
        }, 
        {
            "location": "/releasenotes/index.html#modifying-existing-cloudbreak-credentials", 
            "text": "Cloudbreak allows you to modify existing credentials by using the edit option available in Cloudbreak UI or by using the  credential modify  command in the CLI. For more information, refer to  Modify an Existing Credential .", 
            "title": "Modifying Existing Cloudbreak Credentials"
        }, 
        {
            "location": "/releasenotes/index.html#using-management-packs", 
            "text": "Cloudbreak 2.6.0 TP introduces support for using management packs, allowing you to register them in Cloudbreak web UI or CLI and then select to install them as part of cluster creation. \nFor more information, refer to  Using management packs .", 
            "title": "Using Management Packs"
        }, 
        {
            "location": "/releasenotes/index.html#behavioral-changes", 
            "text": "", 
            "title": "Behavioral changes"
        }, 
        {
            "location": "/releasenotes/index.html#image-catalog-option-was-moved-to-external-sources", 
            "text": "The options related to registering a custom image catalog and selecting a default image catalog were removed from the  Settings  navigation menu option and are now available under  External Sources   Image Catalogs .", 
            "title": "Image Catalog Option Was Moved to External Sources"
        }, 
        {
            "location": "/releasenotes/index.html#recipes-option-was-moved-to-cluster-extension", 
            "text": "The  Recipes  navigation menu option was moved under  Cluster Extensions , so to find recipe-related settings, select  Cluster Extensions   Recipes  from the navigation menu.", 
            "title": "Recipes Option Was Moved to Cluster Extension"
        }, 
        {
            "location": "/releasenotes/index.html#auto-import-of-hdphdf-images-on-openstack", 
            "text": "When using Cloudbreak on OpenStack, you no longer need to import HDP and HDF images manually, because during your first attempt to create a cluster, Cloudbreak automatically imports HDP and HDF images to your OpenStack. Only Cloudbreak image must be imported manually.", 
            "title": "Auto-import of HDP/HDF Images on OpenStack"
        }, 
        {
            "location": "/releasenotes/index.html#image-catalog-updates", 
            "text": "", 
            "title": "Image catalog updates"
        }, 
        {
            "location": "/releasenotes/index.html#june-12-2018", 
            "text": "Default Ambari version 2.6.2.0 \nDefault HDP version 2.6.5.0 \nDefault HDF version 3.1.1.0-35", 
            "title": "June 12, 2018"
        }, 
        {
            "location": "/releasenotes/index.html#fixed-issues", 
            "text": "Issue  Issue description  Category  Fix version      BUG-100468  Images from a custom image catalog are not listed in the UI after Cloudbreak version changed.  Stability  2.7.0    BUG-99168  All clusters created on Google Cloud Platform fail.  Stability  2.5.0 TP    BUG-99400  Time-based cluster autoscaling does not work.  Stability  2.5.0 TP    BUG-99505  Sync is not working for an AWS instance that was terminated a long time ago.  Stability  2.5.0 TP    BUG-98277  Network interface handling in CloudBreak should be improved.  Stability  2.5.0 TP    BUG-97395  Networks are duplicated on networks tab of the cluster create wizard.  Stability  2.5.0 TP    BUG-97259  \"Update failed\" status after downscale failed, even though cluster was not modified and its status should be \"Running\".  Stability  2.5.0 TP    BUG-97207  Changing lifecycle management on YARN causes NPE.  Stability  2.5.0 TP    BUG-99189  ImageCatalog PUT endpoint is not secured.  Security  2.5.0 TP    BUG-97895  LDAP password should be removed from Cloudbreak logs.  Security  2.5.0 TP    BUG-97300  Cloudbreak should show proper error messages when the given credential is not valid anymore.  Usability  2.5.0 TP    BUG-97296  GCP credential creation should validate whether resources are available with the credential.  Usability  2.5.0 TP    BUG-97660  Ignore repository warnings checkbox are missing after changing base image Ambari or HDP to a custom one.  Usability  2.5.0 TP    BUG-97307  Ignore repository warnings checkbox is not selectable after change the HDP VDF URL.  Usability  2.5.0 TP    BUG-96764  \"Failed to remove instance\" error when using the delete icon.  Usability  2.5.0 TP    BUG-97390  Cloudbreak should support longer resource ID-s on AWS.  Usability  2.5.0 TP    BUG-99512  Azure ES_v3 instances should support premium storage.  Usability  2.5.0 TP    BUG-97206  Backend should return only images for enabled platforms.  Usability  2.5.0 TP", 
            "title": "Fixed issues"
        }, 
        {
            "location": "/releasenotes/index.html#known-issues", 
            "text": "Known issues: Cloudbreak", 
            "title": "Known issues"
        }, 
        {
            "location": "/releasenotes/index.html#bug-96788-azure-availability-set-option-is-not-available-for-instance-count-of-1", 
            "text": "When creating a cluster, the Azure availability set feature is not available for host groups with the instance count of 1.  Workaround :   This issue will be fixed in a future release.   If you would like to use the Azure availability sets feature now, you must add at least 2 instances to the host group for which you want to use them. The option Azure availability sets is available on the advanced  Hardware and Storage  page of the create cluster wizard.", 
            "title": "(BUG-96788) Azure Availability Set Option Is Not Available for Instance Count of 1"
        }, 
        {
            "location": "/releasenotes/index.html#bug-92605-cluster-creation-fails-with-resourceinerror", 
            "text": "Cluster creation fails with the following error:   Infrastructure creation failed. Reason: Failed to create the stack for CloudContext{id=3689, name='test-exisitngnetwork', platform='StringType{value='OPENSTACK'}', owner='e0307f96-bd7d-4641-8c8f-b95f2667d9c6'} due to: Resource CREATE failed: ResourceInError: resources.ambari_volume_master_0_0: Went to status error due to \"Unknown\"  Workaround :   This may mean that the volumes that you requested exceed volumes available on your cloud provider account. When creating a cluster, on the advanced  Hardware and Storage  page of the create cluster wizard, try reducing the amount of requested storage. If you need more storage, try using a different region or ask your cloud provider admin to increase the resource quota for volumes.", 
            "title": "(BUG-92605) Cluster Creation Fails with ResourceInError"
        }, 
        {
            "location": "/releasenotes/index.html#bug-93241-error-when-scaling-multiple-host-groups", 
            "text": "Scaling of multiple host groups fails with the following error:   Batch update returned unexpected row count from update [0]; actual row count: 0; expected: 1; nested exception is org.hibernate.StaleStateException: Batch update returned unexpected row count from update [0]; actual row count: 0; expected: 1  Workaround:  Scaling multiple host groups at once is not supported. If you would like to scale multiple host groups: scale the first host group and wait until scaling has completed, then scale the second host group, and so on.", 
            "title": "(BUG-93241) Error When Scaling Multiple Host Groups"
        }, 
        {
            "location": "/releasenotes/index.html#bug-97044-show-cli-command-copy-json-button-does-not-work", 
            "text": "When using the  Show CLI Command     Copy the JSON  or  Copy the Command  button with Firefox, the content does not does not get copied if adblock plugin or other advertise blocker plugins are present.  Workaround:     Use a browser without an adblock plugin.", 
            "title": "(BUG-97044) Show CLI Command Copy JSON Button Does Not Work"
        }, 
        {
            "location": "/releasenotes/index.html#bug-93257-clusters-are-missing-from-history-when-an-exact-day-is-selected", 
            "text": "On the History page, when the start date selected is the same as end date, clusters that were running on that date are filtered out.", 
            "title": "(BUG-93257) Clusters Are Missing From History When an Exact Day Is Selected"
        }, 
        {
            "location": "/releasenotes/index.html#bug-93257-clusters-are-missing-from-history", 
            "text": "After changing the dates on the History page multiple times, the results displayed may sometimes be incorrect.   Workaround:  Refresh the page if you think that the history displayed may be incorrect.", 
            "title": "(BUG-93257) Clusters Are Missing From History"
        }, 
        {
            "location": "/releasenotes/index.html#bug-101223-hardware-status-is-incorrect-after-stop-and-start", 
            "text": "After stopping and starting a cluster, cluster state is incorrectly listed as \"Unhealty\", even though the nodes are healthy.", 
            "title": "(BUG-101223) Hardware Status is Incorrect After Stop and Start"
        }, 
        {
            "location": "/releasenotes/index.html#bug-101230-the-command-for-cli-download-doesnt-work-on-windows", 
            "text": "The  curl  command listed on the  Download CLI  page for Windows does not work on Windows.", 
            "title": "(BUG-101230) The Command for CLI Download Doesn't Work on Windows"
        }, 
        {
            "location": "/releasenotes/index.html#bug-101225-cli-manual-repair-does-not-work", 
            "text": "cb cluster repair  does not work as expected.", 
            "title": "(BUG-101225) CLI: Manual Repair Does Not Work"
        }, 
        {
            "location": "/releasenotes/index.html#bug-101204-cli-instanceprofilestrategy-create-doesnt-work", 
            "text": "Using the following parameter in the CLI JSON from creating an instance profile does not work as expected:  \"parameters\": {\n    \"instanceProfileStrategy\": \"CREATE\"\n},   Known issues: Ambari 2.6.1.3 and HDP 2.6.4.0   The known issues described here were discovered when testing Cloudbreak with Ambari 2.6.1.3 and HDP 2.6.4.0, which are used by default in Cloudbreak.  For general Ambari 2.6.1.5 and HDP 2.6.4.0 known issues, refer to:  Ambari 2.6.1.5 Release Notes  HDP 2.6.4.0 Release Notes", 
            "title": "(BUG-101204) CLI: InstanceProfileStrategy Create Doesn't Work"
        }, 
        {
            "location": "/releasenotes/index.html#bug-96707-druid-overload-does-not-start", 
            "text": "Druid overload start fails with the following error when using Ambari 2.6.1.3 and HDP 2.6.4.0:   ERROR [main] io.druid.cli.CliOverlord - Error when starting up.  Failing. com.google.inject.ProvisionException: Unable to provision", 
            "title": "(BUG-96707) Druid Overload Does Not Start"
        }, 
        {
            "location": "/releasenotes/index.html#bug-97080-ambari-files-in-some-cases-when-an-mpack-is-installed", 
            "text": "If you set the following properties then cluster install may fail (in 20-30% of the cases), because of the Ambari agent cache being updated concurrently:  /etc/ambari-server/conf/ambari.properties  \nagent.auto.cache.update=true*  \n*/etc/ambari-agent/conf/ambari-agent.ini  \nparallel_execution=1", 
            "title": "(BUG-97080) Ambari Files In Some Cases When an Mpack is Installed"
        }, 
        {
            "location": "/releasenotes/index.html#ambari-14149-in-ambari-cluster-cannot-be-started-after-stop", 
            "text": "When using Ambari version 2.5.0.3, after stopping and starting a cluster, Event History shows the following error:  Ambari cluster could not be started. Reason: Failed to start Hadoop services.\n2/7/2018, 12:47:05 PM\nStarting Ambari services.\n2/7/2018, 12:47:04 PM\nManual recovery is needed for the following failed nodes:   \n[host-10-0-0-4.openstacklocal, host-10-0-0-3.openstacklocal, host-10-0-0-5.openstacklocal  Ambari dashboard shows that nodes are not sending heartbeats.   Workaround:     This issue is fixed in Ambari version 2.5.1.0 and newer.     Known issues: HDF 3.1.1  The known issues described here were discovered when testing Cloudbreak with Ambari 2.6.1.3 and HDF 3.1.1, which are used by default in Cloudbreak.   For general HDF 3.1.1 known issues, refer to  HDF 3.1.1 Release Notes", 
            "title": "(AMBARI-14149) In Ambari, Cluster Cannot Be Started After Stop"
        }, 
        {
            "location": "/releasenotes/index.html#bug-98865-scaling-hdf-clusters-does-not-update-configurations-on-new-nodes", 
            "text": "Blueprint configuration parameters are not applied when scaling an HDF cluster. \nOne example that affects all users is that after HDF cluster upscale/downscale the  nifi.web.proxy.host  blueprint parameter does not get updated to include the new nodes, and as a result the NiFi UI is not reachable from these nodes.   Workaround:     Configuration parameters set in the blueprint are not applied when scaling an HDF cluster. One example that affects all NiFi users is that after HDF cluster upscale the  nifi.web.proxy.host  parameter does not get updated to include the new hosts, and as a result the NiFi UI is not reachable from these hosts.   HOST1-IP:PORT,HOST2-IP:PORT,HOST3-IP:PORT", 
            "title": "(BUG-98865) Scaling HDF Clusters Does Not Update Configurations on New Nodes"
        }, 
        {
            "location": "/faq/index.html", 
            "text": "FAQs\n\n\nHow to...\n\n\nGenerate SSH key pair\n\n\nAll the instances created by Cloudbreak are configured to allow key-based SSH, so you'll need to provide an SSH public key that can be used later to SSH onto the instances in the clusters you'll create with Cloudbreak. You can use one of your existing keys or you can generate a new one.\n\n\nTo generate a new SSH key pair, execute:\n\n\nssh-keygen -t rsa -b 4096 -C \"your_email@example.com\"\n# Creates a new ssh key, using the provided email as a label\n# Generating public/private rsa key pair.\n# Enter file in which to save the key (/Users/you/.ssh/id_rsa): [Press enter]\n\n\n\nYou'll be asked to enter a passphrase, but you can leave it empty:\n\n\n# Enter passphrase (empty for no passphrase): [Type a passphrase]\n# Enter same passphrase again: [Type passphrase again]\n\n\n\nAfter you enter (or not) a passphrase, the key pair is generated. The output should look similar to:\n\n\n# Your identification has been saved in /Users/you/.ssh/id_rsa.\n# Your public key has been saved in /Users/you/.ssh/id_rsa.pub.\n# The key fingerprint is:\n# 01:0f:f4:3b:ca:85:sd:17:sd:7d:sd:68:9d:sd:a2:sd your_email@example.com\n\n\n\nLater you'll need to pass the content of the \n.pub\n file to Cloudbreak and use the private key file to SSH to the instances. \n\n\nRecover public SSH key\n\n\nThe \n-y\n option of \nssh-keygen\n outputs the public key. For example:\n\n\nssh-keygen -y -f ~/.ssh/id_rsa > ~/.ssh/id_rsa.pub\n\n\n\nSSH to the hosts\n\n\nTo connect to a running VM through SSH, you need to know its public IP address and have your private key available. \n\n\nThe private key that you must use to access the VM is the counterpart of the public key that you specified when creating a Cloudbreak credential.\n\n\nYou can find the IP addresses of all the running VMs in the Cloudbreak UI, on the cluster details page. Only key-based authentication is supported. \n\n\nCloudbreak creates a cloudbreak user which can be used to ssh into the box. This user has passwordless sudo rights.\n\n\nFor example:\n\n\nssh -i ~/.ssh/your-private-key.pem cloudbreak@\npublic-ip\n\n\n\n\n\nCheck Cloudbreak version\n\n\nTo check Cloudbreak version, navigate to the Cloudbreak home directory and execute the following command:\n\n\ncbd doctor\n\n\n\n\nCheck available environment variables\n\n\nTo see all available environment variables with their default values, use:\n\n\ncbd env show\n\n\n\n\nAccess Cloudbreak logs\n\n\nRefer to \nTroubleshooting\n.", 
            "title": "FAQs"
        }, 
        {
            "location": "/faq/index.html#faqs", 
            "text": "How to...", 
            "title": "FAQs"
        }, 
        {
            "location": "/faq/index.html#generate-ssh-key-pair", 
            "text": "All the instances created by Cloudbreak are configured to allow key-based SSH, so you'll need to provide an SSH public key that can be used later to SSH onto the instances in the clusters you'll create with Cloudbreak. You can use one of your existing keys or you can generate a new one.  To generate a new SSH key pair, execute:  ssh-keygen -t rsa -b 4096 -C \"your_email@example.com\"\n# Creates a new ssh key, using the provided email as a label\n# Generating public/private rsa key pair.\n# Enter file in which to save the key (/Users/you/.ssh/id_rsa): [Press enter]  You'll be asked to enter a passphrase, but you can leave it empty:  # Enter passphrase (empty for no passphrase): [Type a passphrase]\n# Enter same passphrase again: [Type passphrase again]  After you enter (or not) a passphrase, the key pair is generated. The output should look similar to:  # Your identification has been saved in /Users/you/.ssh/id_rsa.\n# Your public key has been saved in /Users/you/.ssh/id_rsa.pub.\n# The key fingerprint is:\n# 01:0f:f4:3b:ca:85:sd:17:sd:7d:sd:68:9d:sd:a2:sd your_email@example.com  Later you'll need to pass the content of the  .pub  file to Cloudbreak and use the private key file to SSH to the instances.", 
            "title": "Generate SSH key pair"
        }, 
        {
            "location": "/faq/index.html#recover-public-ssh-key", 
            "text": "The  -y  option of  ssh-keygen  outputs the public key. For example:  ssh-keygen -y -f ~/.ssh/id_rsa > ~/.ssh/id_rsa.pub", 
            "title": "Recover public SSH key"
        }, 
        {
            "location": "/faq/index.html#ssh-to-the-hosts", 
            "text": "To connect to a running VM through SSH, you need to know its public IP address and have your private key available.   The private key that you must use to access the VM is the counterpart of the public key that you specified when creating a Cloudbreak credential.  You can find the IP addresses of all the running VMs in the Cloudbreak UI, on the cluster details page. Only key-based authentication is supported.   Cloudbreak creates a cloudbreak user which can be used to ssh into the box. This user has passwordless sudo rights.  For example:  ssh -i ~/.ssh/your-private-key.pem cloudbreak@ public-ip", 
            "title": "SSH to the hosts"
        }, 
        {
            "location": "/faq/index.html#check-cloudbreak-version", 
            "text": "To check Cloudbreak version, navigate to the Cloudbreak home directory and execute the following command:  cbd doctor", 
            "title": "Check Cloudbreak version"
        }, 
        {
            "location": "/faq/index.html#check-available-environment-variables", 
            "text": "To see all available environment variables with their default values, use:  cbd env show", 
            "title": "Check available environment variables"
        }, 
        {
            "location": "/faq/index.html#access-cloudbreak-logs", 
            "text": "Refer to  Troubleshooting .", 
            "title": "Access Cloudbreak logs"
        }, 
        {
            "location": "/dev/index.html", 
            "text": "Developer documentation links\n\n\nThe following table includes links to Cloudbreak developer documentation: \n\n\n\n\n\n\n\n\nDoc Link\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nSet up local development\n\n\nThis documentation will help you set up your local development environment.\n\n\n\n\n\n\nRetrieve OAuth bearer token via Cloudbreak REST API\n\n\nDescribes how to retrieve OAuth bearer token via Cloudbreak REST API.\n\n\n\n\n\n\nSPI reference\n\n\nThis is Cloudbreak SPI reference documentation.\n\n\n\n\n\n\nAPI reference\n\n\nThis is Cloudbreak API reference documentation. Cloudbreak is a RESTful application development platform whose goal is to help developers deploy HDP clusters in various cloud environments. Once Cloudbreak is deployed in your favorite servlet container, it exposes REST APIs, allowing you to spin up Hadoop clusters of any size with your chosen cloud provider.\n\n\n\n\n\n\n\n\nRetrieve OAuth bearer token via Cloudbreak REST API\n\n\nIn order to communicate with Cloudbreak's API, you must retrieve a bearer token. For example: \n\n\nTOKEN=$(curl -k -iX POST -H \"accept: application/x-www-form-urlencoded\" -d 'credentials={\"username\":\"admin@example.com\",\"password\":\"pwd\"}' \"https://192.168.99.100/identity/oauth/authorize?response_type=token\n_id=cloudbreak_shell\n=openid\n=login\n_uri=http://cloudbreak.shell\" | grep location | cut -d'=' -f 3 | cut -d'\n&\n' -f 1)\n\n\n\nCloudbreak service provider interface (SPI)\n\n\nIn addition to supporting multiple cloud platforms, Cloudbreak provides an easy way to integrate a new provider trough its \nService Provider Interface (SPI)\n, a plugin mechanism that enables seamless integration with any cloud provider. \n\n\nThis SPI plugin mechanism has been used to integrate all currently supported providers with Cloudbreak. The following links point to the Cloudbreak SPI implementations for AWS, Azure, Google Cloud, and OpenStack. You can use these implementations as a reference:\n\n\n\n\nThe \ncloud-aws\n module integrates Amazon Web Services\n\n\nThe \ncloud-azure\n module integrates Microsoft Azure\n\n\nThe \ncloud-gcp\n module integrates Google Cloud Platform  \n\n\nThe \ncloud-openstack\n module integrates OpenStack\n\n\n\n\nThe Cloudbreak SPI interface is event-based, scalable, and decoupled from Cloudbreak. The core of Cloudbreak uses \nEventBus\n to communicate with the providers, but the complexity of event handling is hidden from the provider implementation.\n\n\nSupported resource management methods\n\n\nCloud providers support two kinds of deployment and resource management methods:\n\n\n\n\nTemplate based deployments\n\n\nIndividual resource based deployments\n\n\n\n\nCloudbreak's SPI supports both of these methods. It provides a well-defined interface, abstract classes, and helper classes, scheduling and polling of resources to aid the integration and to avoid any boilerplate code in the module of cloud provider.\n\n\nTemplate based deployments\n\n\nProviders with template-based deployments such as \nAWS CloudFormation\n, \nAzure ARM\n or \nOpenStack Heat\n have the ability to create and manage a collection of related cloud resources, provisioning and updating them in an orderly and predictable fashion. \n\n\nWhen working with providers that use template-based deployments, Cloudbreak needs to be provided with a reference to the template, because every change in the infrastructure (for example, creating a new instance or deleting one) is managed through this templating mechanism.\n\n\nIf a provider has templating support, then the provider's \ngradle\n module depends on the \ncloud-api\n module:\n\n\napply plugin: 'java'\n\nsourceCompatibility = 1.7\n\nrepositories {\n    mavenCentral()\n}\n\njar {\n    baseName = 'cloud-new-provider'\n}\n\ndependencies {\n\n    compile project(':cloud-api')\n\n}\n\n\n\n\nThe entry point for the provider is the  \nCloudConnector\n interface and every interface that needs to be implemented is reachable trough this interface.\n\n\nIndividual resource based deployments\n\n\nThere are providers such as GCP that do not support a templating mechanism, and customizable providers such as OpenStack where the Heat Orchestration (templating) component is optional and individual resources need to be handled separately. \n\n\nWhen working with such providers, resources such as networks, discs, and compute instances need to be created and managed with an ordered sequence of API calls, and Cloudbreak needs to provide a solution to manage the collection of related cloud resources as a whole.\n\n\nIf the provider has no templating support, then the provider's \ngradle\n module typically depends on the \ncloud-template\n module, which includes Cloudbreak-defined abstract template. This template is a set of abstract and utility classes to support provisioning and updating related resources in an orderly and predictable manner trough ordered sequences of cloud API calls:\n\n\napply plugin: 'java'\n\nsourceCompatibility = 1.7\n\nrepositories {\n    mavenCentral()\n}\n\njar {\n    baseName = 'cloud-new-provider'\n}\n\ndependencies {\n\n    compile project(':cloud-template')\n\n}\n\n\n\n\nSupport for modularity\n\n\nCloudbreak uses \nvariants\n to deal with highly modular providers such as OpenStack, which allows you to install different components (for volume storage, networking, and so on) and to entirely exclude certain components. For example, Nova or Neutron can be used for networking in OpenStack and some components such as Heat may not installed at all in some deployment scenarios. \n\n\nCloudbreak SPI interface uses variants to support this flexibility: if some part of the cloud provider uses a different component, you don't need to re-implement the complete stack, but just use a different variant and re-implement the part that is different.\n\n\nAn example implementation for this feature can be found in the \ncloud-openstack\n module which supports a HEAT and NATIVE variants. While the HEAT variant utilizes the Heat templating to launch a stack, the NATIVE variant starts the cluster by using a sequence of API calls without Heat to achieve the same result. Both of them use the same authentication and credential management.", 
            "title": "Developer documentation links"
        }, 
        {
            "location": "/dev/index.html#developer-documentation-links", 
            "text": "The following table includes links to Cloudbreak developer documentation:      Doc Link  Description      Set up local development  This documentation will help you set up your local development environment.    Retrieve OAuth bearer token via Cloudbreak REST API  Describes how to retrieve OAuth bearer token via Cloudbreak REST API.    SPI reference  This is Cloudbreak SPI reference documentation.    API reference  This is Cloudbreak API reference documentation. Cloudbreak is a RESTful application development platform whose goal is to help developers deploy HDP clusters in various cloud environments. Once Cloudbreak is deployed in your favorite servlet container, it exposes REST APIs, allowing you to spin up Hadoop clusters of any size with your chosen cloud provider.", 
            "title": "Developer documentation links"
        }, 
        {
            "location": "/dev/index.html#retrieve-oauth-bearer-token-via-cloudbreak-rest-api", 
            "text": "In order to communicate with Cloudbreak's API, you must retrieve a bearer token. For example:   TOKEN=$(curl -k -iX POST -H \"accept: application/x-www-form-urlencoded\" -d 'credentials={\"username\":\"admin@example.com\",\"password\":\"pwd\"}' \"https://192.168.99.100/identity/oauth/authorize?response_type=token _id=cloudbreak_shell =openid =login _uri=http://cloudbreak.shell\" | grep location | cut -d'=' -f 3 | cut -d' & ' -f 1)", 
            "title": "Retrieve OAuth bearer token via Cloudbreak REST API"
        }, 
        {
            "location": "/dev/index.html#cloudbreak-service-provider-interface-spi", 
            "text": "In addition to supporting multiple cloud platforms, Cloudbreak provides an easy way to integrate a new provider trough its  Service Provider Interface (SPI) , a plugin mechanism that enables seamless integration with any cloud provider.   This SPI plugin mechanism has been used to integrate all currently supported providers with Cloudbreak. The following links point to the Cloudbreak SPI implementations for AWS, Azure, Google Cloud, and OpenStack. You can use these implementations as a reference:   The  cloud-aws  module integrates Amazon Web Services  The  cloud-azure  module integrates Microsoft Azure  The  cloud-gcp  module integrates Google Cloud Platform    The  cloud-openstack  module integrates OpenStack   The Cloudbreak SPI interface is event-based, scalable, and decoupled from Cloudbreak. The core of Cloudbreak uses  EventBus  to communicate with the providers, but the complexity of event handling is hidden from the provider implementation.", 
            "title": "Cloudbreak service provider interface (SPI)"
        }, 
        {
            "location": "/dev/index.html#supported-resource-management-methods", 
            "text": "Cloud providers support two kinds of deployment and resource management methods:   Template based deployments  Individual resource based deployments   Cloudbreak's SPI supports both of these methods. It provides a well-defined interface, abstract classes, and helper classes, scheduling and polling of resources to aid the integration and to avoid any boilerplate code in the module of cloud provider.", 
            "title": "Supported resource management methods"
        }, 
        {
            "location": "/dev/index.html#template-based-deployments", 
            "text": "Providers with template-based deployments such as  AWS CloudFormation ,  Azure ARM  or  OpenStack Heat  have the ability to create and manage a collection of related cloud resources, provisioning and updating them in an orderly and predictable fashion.   When working with providers that use template-based deployments, Cloudbreak needs to be provided with a reference to the template, because every change in the infrastructure (for example, creating a new instance or deleting one) is managed through this templating mechanism.  If a provider has templating support, then the provider's  gradle  module depends on the  cloud-api  module:  apply plugin: 'java'\n\nsourceCompatibility = 1.7\n\nrepositories {\n    mavenCentral()\n}\n\njar {\n    baseName = 'cloud-new-provider'\n}\n\ndependencies {\n\n    compile project(':cloud-api')\n\n}  The entry point for the provider is the   CloudConnector  interface and every interface that needs to be implemented is reachable trough this interface.", 
            "title": "Template based deployments"
        }, 
        {
            "location": "/dev/index.html#individual-resource-based-deployments", 
            "text": "There are providers such as GCP that do not support a templating mechanism, and customizable providers such as OpenStack where the Heat Orchestration (templating) component is optional and individual resources need to be handled separately.   When working with such providers, resources such as networks, discs, and compute instances need to be created and managed with an ordered sequence of API calls, and Cloudbreak needs to provide a solution to manage the collection of related cloud resources as a whole.  If the provider has no templating support, then the provider's  gradle  module typically depends on the  cloud-template  module, which includes Cloudbreak-defined abstract template. This template is a set of abstract and utility classes to support provisioning and updating related resources in an orderly and predictable manner trough ordered sequences of cloud API calls:  apply plugin: 'java'\n\nsourceCompatibility = 1.7\n\nrepositories {\n    mavenCentral()\n}\n\njar {\n    baseName = 'cloud-new-provider'\n}\n\ndependencies {\n\n    compile project(':cloud-template')\n\n}", 
            "title": "Individual resource based deployments"
        }, 
        {
            "location": "/dev/index.html#support-for-modularity", 
            "text": "Cloudbreak uses  variants  to deal with highly modular providers such as OpenStack, which allows you to install different components (for volume storage, networking, and so on) and to entirely exclude certain components. For example, Nova or Neutron can be used for networking in OpenStack and some components such as Heat may not installed at all in some deployment scenarios.   Cloudbreak SPI interface uses variants to support this flexibility: if some part of the cloud provider uses a different component, you don't need to re-implement the complete stack, but just use a different variant and re-implement the part that is different.  An example implementation for this feature can be found in the  cloud-openstack  module which supports a HEAT and NATIVE variants. While the HEAT variant utilizes the Heat templating to launch a stack, the NATIVE variant starts the cluster by using a sequence of API calls without Heat to achieve the same result. Both of them use the same authentication and credential management.", 
            "title": "Support for modularity"
        }, 
        {
            "location": "/get-help/index.html", 
            "text": "Getting help\n\n\nIf you need help with Cloudbreak, you have two options:\n\n\n\n\n\n\n\n\nOption\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nHortonworks Community Connection\n\n\nThis is free optional support via Hortonworks Community Connection (HCC).\n\n\n\n\n\n\nHortonworks Flex Support Subscription\n\n\nThis is paid Hortonworks enterprise support.\n\n\n\n\n\n\n\n\nHCC\n\n\nYou can register for optional free community support at \nHortonworks Community Connection\n where you can browse articles and previously answered questions, and ask questions of your own. When posting questions related to Cloudbreak, make sure to use the \"Cloudbreak\" tag.\n\n\nFlex subscription\n\n\nYou can optionally use your existing Hortonworks \nflex support subscription(s)\n to cover the Cloudbreak node and clusters managed by it. \n\n\n\n\nYou must have an existing SmartSense ID and a Flex subscription. For general information about the Hortonworks Flex Support Subscription, visit the Hortonworks Support page at \nhttps://hortonworks.com/services/support/enterprise/\n.\n\n\n\n\nThe general steps are:\n\n\n\n\nConfigure Smart Sense in your \nProfile\n file.   \n\n\nRegister your Flex subscription in the Cloudbreak web UI. You can register and manage multiple Flex subscriptions. For example, you can choose to use your Flex subscription to cover the Cloudbreak node.   \n\n\nWhen creating a cluster, in the \nGeneral Configuration\n \n \nFlex Subscription\n, you can select the Flex subscription that you want to use for the cluster.  \n\n\n\n\nConfiguring SmartSense\n\n\nTo configure SmartSense in Cloudbreak, enable SmartSense and add your SmartSense ID to the \nProfile\n by adding the following variables:\n\n\nexport CB_SMARTSENSE_CONFIGURE=true\nexport CB_SMARTSENSE_ID=YOUR-SMARTSENSE-ID\n\n\n\nFor example:\n\n\nexport CB_SMARTSENSE_CONFIGURE=true\nexport CB_SMARTSENSE_ID=A-00000000-C-00000000\n\n\n\nYou can do this in one of the two ways:\n\n\n\n\nWhen initiating Cloudbreak deployer  \n\n\nAfter you've already initiated Cloudbreak deployer. If you choose this option, you must restart Cloudbreak using \ncbd restart\n.\n\n\n\n\nRegister and manage flex subscriptions\n\n\nOnce you log in to the Cloudbreak web UI, you can manage your Flex subscriptions from the \nSettings\n page \n \nFlex Subscriptions\n:\n\n\n  \n\n\nYou can:\n\n\n\n\nRegister a new Flex subscription    \n\n\nSet a default Flex subscription (\"Default\")  \n\n\nSelect a Flex subscription to be used for the Cloudbreak node (\"Use for controller\")  \n\n\nDelete a Flex subscription    \n\n\n\n\nUse flex subscription for a cluster\n\n\nWhen creating a cluster, on the \nGeneral Configuration\n page you can select the Flex subscription that you want to use for the cluster:\n\n\n  \n\n\nUse flex subscription for Cloudbreak node\n\n\nTo use a Flex subscription for Cloudbreak node, on the \nSettings\n page, in the \nFlex Subscriptions\n section, check the \"Use for controller option\" for the selected Flex ID.  \n\n\nMore Cloudbreak resources\n\n\nCheck out the following documentation to learn more:\n\n\n\n\n Resource \nDescription\n\n\nHortonworks documentation \n\n\nDuring cluster create process, Cloudbreak automatically installs Ambari and sets up a cluster for you. After this deployment is complete, refer to the \nAmbari documentation\n and \nHDP documentation\n for help.\n\n\n\n\n\n\nHortonworks tutorials\n\n\n\n\nUse Hortonworks tutorials to get started with Apache Spark, Apache Hive, Apache Zeppelin, and more.\n\n\nApache documentation\n\n\n\n\n In addition to Hortonworks documentation, refer to the Apache Software Foundation documentation to get information on specific Hadoop services. \n\n\n\n\n\nAmbari blueprints\nLearn about Ambari blueprints. Ambari blueprints are a declarative definition of a Hadoop cluster that Ambari can use to create Hadoop clusters.\n\n\nCloudbreak project\nVisit the Hortonworks website to see Cloudbreak-related news and updates.\n\n\nApache Ambari project\nLearn about the Apache Ambari project. Apache Ambari is an operational platform for provisioning, managing, and monitoring Apache Hadoop clusters. Ambari exposes a robust set of REST APIs and a rich web interface for cluster management.", 
            "title": "Getting help"
        }, 
        {
            "location": "/get-help/index.html#getting-help", 
            "text": "If you need help with Cloudbreak, you have two options:     Option  Description      Hortonworks Community Connection  This is free optional support via Hortonworks Community Connection (HCC).    Hortonworks Flex Support Subscription  This is paid Hortonworks enterprise support.", 
            "title": "Getting help"
        }, 
        {
            "location": "/get-help/index.html#hcc", 
            "text": "You can register for optional free community support at  Hortonworks Community Connection  where you can browse articles and previously answered questions, and ask questions of your own. When posting questions related to Cloudbreak, make sure to use the \"Cloudbreak\" tag.", 
            "title": "HCC"
        }, 
        {
            "location": "/get-help/index.html#flex-subscription", 
            "text": "You can optionally use your existing Hortonworks  flex support subscription(s)  to cover the Cloudbreak node and clusters managed by it.    You must have an existing SmartSense ID and a Flex subscription. For general information about the Hortonworks Flex Support Subscription, visit the Hortonworks Support page at  https://hortonworks.com/services/support/enterprise/ .   The general steps are:   Configure Smart Sense in your  Profile  file.     Register your Flex subscription in the Cloudbreak web UI. You can register and manage multiple Flex subscriptions. For example, you can choose to use your Flex subscription to cover the Cloudbreak node.     When creating a cluster, in the  General Configuration     Flex Subscription , you can select the Flex subscription that you want to use for the cluster.", 
            "title": "Flex subscription"
        }, 
        {
            "location": "/get-help/index.html#configuring-smartsense", 
            "text": "To configure SmartSense in Cloudbreak, enable SmartSense and add your SmartSense ID to the  Profile  by adding the following variables:  export CB_SMARTSENSE_CONFIGURE=true\nexport CB_SMARTSENSE_ID=YOUR-SMARTSENSE-ID  For example:  export CB_SMARTSENSE_CONFIGURE=true\nexport CB_SMARTSENSE_ID=A-00000000-C-00000000  You can do this in one of the two ways:   When initiating Cloudbreak deployer    After you've already initiated Cloudbreak deployer. If you choose this option, you must restart Cloudbreak using  cbd restart .", 
            "title": "Configuring SmartSense"
        }, 
        {
            "location": "/get-help/index.html#register-and-manage-flex-subscriptions", 
            "text": "Once you log in to the Cloudbreak web UI, you can manage your Flex subscriptions from the  Settings  page    Flex Subscriptions :      You can:   Register a new Flex subscription      Set a default Flex subscription (\"Default\")    Select a Flex subscription to be used for the Cloudbreak node (\"Use for controller\")    Delete a Flex subscription", 
            "title": "Register and manage flex subscriptions"
        }, 
        {
            "location": "/get-help/index.html#use-flex-subscription-for-a-cluster", 
            "text": "When creating a cluster, on the  General Configuration  page you can select the Flex subscription that you want to use for the cluster:", 
            "title": "Use flex subscription for a cluster"
        }, 
        {
            "location": "/get-help/index.html#use-flex-subscription-for-cloudbreak-node", 
            "text": "To use a Flex subscription for Cloudbreak node, on the  Settings  page, in the  Flex Subscriptions  section, check the \"Use for controller option\" for the selected Flex ID.", 
            "title": "Use flex subscription for Cloudbreak node"
        }, 
        {
            "location": "/get-help/index.html#more-cloudbreak-resources", 
            "text": "Check out the following documentation to learn more:    Resource  Description  Hortonworks documentation   During cluster create process, Cloudbreak automatically installs Ambari and sets up a cluster for you. After this deployment is complete, refer to the  Ambari documentation  and  HDP documentation  for help.    Hortonworks tutorials   Use Hortonworks tutorials to get started with Apache Spark, Apache Hive, Apache Zeppelin, and more.  Apache documentation    In addition to Hortonworks documentation, refer to the Apache Software Foundation documentation to get information on specific Hadoop services.    Ambari blueprints Learn about Ambari blueprints. Ambari blueprints are a declarative definition of a Hadoop cluster that Ambari can use to create Hadoop clusters.  Cloudbreak project Visit the Hortonworks website to see Cloudbreak-related news and updates.  Apache Ambari project Learn about the Apache Ambari project. Apache Ambari is an operational platform for provisioning, managing, and monitoring Apache Hadoop clusters. Ambari exposes a robust set of REST APIs and a rich web interface for cluster management.", 
            "title": "More Cloudbreak resources"
        }, 
        {
            "location": "/smartsense/index.html", 
            "text": "SmartSense telemetry\n\n\nHelp us make a better product by opting in to automatically send information to Hortonworks. This includes enabling\nHortonworks SmartSense and sending performance and usage info. As you use the product,\nSmartSense measures and collects information and then sends these information bundles to Hortonworks.\n\n\nDisabling SmartSense telemetry\n\n\nDisable bundle upload for Cloudbreak and new clusters\n\n\n\n    \nImportant\n\n    \n\n    Do not perform these steps when you have clusters currently in the process of being deployed.\n    Wait for all clusters to be deployed.\n\n\n\n\n\n\n\n\n\nSSH into the Cloudbreak host.\n\n\n\n\n\n\nEdit \n/var/lib/cloudbreak-deployment/Profile\n.\n\n\n\n\n\n\nChange \nCB_SMARTSENSE_CONFIGURE\n to \nfalse\n:\n\n    \nexport CB_SMARTSENSE_CONFIGURE=false\n\n\n\n\n\n\nRestart the cloud controller:\n\n    \ncd /var/lib/cloudbreak-deployment\ncbd restart\n\n\n\n\n\n\nDisable bundle upload for an existing cluster\n\n\n\n\n\n\nSSH into the master node for the cluster.\n\n\n\n\n\n\nEdit \n/etc/hst/conf/hst-server.ini\n.\n\n\n\n\n\n\nChange \n[gateway]\n configuration to \nfalse\n:\n\n    \n[gateway]\nenabled=false\n\n\n\n\n\n\nRestart the SmartSense Server:\n    \nhst restart\n\n\n\n\n\n\n(Optional) Disable SmartSense daily bundle capture:\n\n\n\n\nSmartSense is scheduled to capture a telemetry bundle daily. With the bundle upload disabled, the bundle will still\nbe captured but just saved locally (i.e. not uploaded).\n\n\nTo disable the bundle capture, execute the following:\n\nhst capture-schedule -a pause\n\n\n\n\n\n\n\n\nRepeat on all existing clusters.", 
            "title": "SmartSense telemetry"
        }, 
        {
            "location": "/smartsense/index.html#smartsense-telemetry", 
            "text": "Help us make a better product by opting in to automatically send information to Hortonworks. This includes enabling\nHortonworks SmartSense and sending performance and usage info. As you use the product,\nSmartSense measures and collects information and then sends these information bundles to Hortonworks.", 
            "title": "SmartSense telemetry"
        }, 
        {
            "location": "/smartsense/index.html#disabling-smartsense-telemetry", 
            "text": "", 
            "title": "Disabling SmartSense telemetry"
        }, 
        {
            "location": "/smartsense/index.html#disable-bundle-upload-for-cloudbreak-and-new-clusters", 
            "text": "Important \n     \n    Do not perform these steps when you have clusters currently in the process of being deployed.\n    Wait for all clusters to be deployed.     SSH into the Cloudbreak host.    Edit  /var/lib/cloudbreak-deployment/Profile .    Change  CB_SMARTSENSE_CONFIGURE  to  false : \n     export CB_SMARTSENSE_CONFIGURE=false    Restart the cloud controller: \n     cd /var/lib/cloudbreak-deployment\ncbd restart", 
            "title": "Disable bundle upload for Cloudbreak and new clusters"
        }, 
        {
            "location": "/smartsense/index.html#disable-bundle-upload-for-an-existing-cluster", 
            "text": "SSH into the master node for the cluster.    Edit  /etc/hst/conf/hst-server.ini .    Change  [gateway]  configuration to  false : \n     [gateway]\nenabled=false    Restart the SmartSense Server:\n     hst restart    (Optional) Disable SmartSense daily bundle capture:   SmartSense is scheduled to capture a telemetry bundle daily. With the bundle upload disabled, the bundle will still\nbe captured but just saved locally (i.e. not uploaded).  To disable the bundle capture, execute the following: hst capture-schedule -a pause     Repeat on all existing clusters.", 
            "title": "Disable bundle upload for an existing cluster"
        }, 
        {
            "location": "/acknowledge/index.html", 
            "text": "Acknowledgements\n\n\nCopyrights and trademarks\n\n\n\u00a9 2011-2018 Hortonworks Inc. All Rights Reserved. Hortonworks and HDP are registered trademarks\nor trademarks of Hortonworks, Inc. in the United States and other jurisdictions.  All other\ntrademarks and trade names are the property of their respective owners.\n\n\nApache, Hadoop, Falcon, Atlas, Tez, Sqoop, Flume, Kafka, Pig, Hive,\nHBase, Accumulo, Storm, Solr, Spark, Ranger, Knox, Ambari, ZooKeeper,\nOozie, Metron, Druid, and the Hadoop elephant logo are either registered trademarks or\ntrademarks of the \nApache Software Foundation\n in\nthe United States or other countries.\n\n\n\"Amazon Web Services\", \"AWS\", \"Amazon EC2\", \"Amazon S3\", \n\"Amazon VPC\", \"Amazon RDS\", the \"Amazon Web Services\" logo, and other\nAWS service names, graphics, and logos are trademarks of Amazon Technologies, Inc. or its affiliates in the United States and/or other countries.\n\n\n\"Microsoft Azure\", \"Windows Azure\", the \"Azure\" logo, and other\nAzure service names, graphics, and logos are trademarks of Microsoft Corporation or its affiliates in the United States and/or other countries.\n\n\n\"Google Cloud\", the \"Google Cloud\" logo, and other\nGoogle Cloud service names, graphics, and logos are trademarks of Google LLC or its affiliates in the United States and/or other countries.\n\n\n\"OpenStack\", the \"OpenStack\" logo, and other\nOpenStack graphics and logos are trademarks of OpenStack LLC or its affiliates in the United States and/or other countries.\n\n\nDocumentation was built with \nMkDocs\n and the\n\nCinder Theme\n, licensed under\nthe \nMIT license\n.\n\n\nContact information\n\n\nHortonworks, Inc.\n\n\n5470 Great America Parkway\n\n\nSanta Clara, CA 95054\n\n\n\nWebsite:\n \nwww.hortonworks.com\n\n\n\n\nCommunity:\n \ncommunity.hortonworks.com", 
            "title": "Acknowledgements"
        }, 
        {
            "location": "/acknowledge/index.html#acknowledgements", 
            "text": "", 
            "title": "Acknowledgements"
        }, 
        {
            "location": "/acknowledge/index.html#copyrights-and-trademarks", 
            "text": "\u00a9 2011-2018 Hortonworks Inc. All Rights Reserved. Hortonworks and HDP are registered trademarks\nor trademarks of Hortonworks, Inc. in the United States and other jurisdictions.  All other\ntrademarks and trade names are the property of their respective owners.  Apache, Hadoop, Falcon, Atlas, Tez, Sqoop, Flume, Kafka, Pig, Hive,\nHBase, Accumulo, Storm, Solr, Spark, Ranger, Knox, Ambari, ZooKeeper,\nOozie, Metron, Druid, and the Hadoop elephant logo are either registered trademarks or\ntrademarks of the  Apache Software Foundation  in\nthe United States or other countries.  \"Amazon Web Services\", \"AWS\", \"Amazon EC2\", \"Amazon S3\", \n\"Amazon VPC\", \"Amazon RDS\", the \"Amazon Web Services\" logo, and other\nAWS service names, graphics, and logos are trademarks of Amazon Technologies, Inc. or its affiliates in the United States and/or other countries.  \"Microsoft Azure\", \"Windows Azure\", the \"Azure\" logo, and other\nAzure service names, graphics, and logos are trademarks of Microsoft Corporation or its affiliates in the United States and/or other countries.  \"Google Cloud\", the \"Google Cloud\" logo, and other\nGoogle Cloud service names, graphics, and logos are trademarks of Google LLC or its affiliates in the United States and/or other countries.  \"OpenStack\", the \"OpenStack\" logo, and other\nOpenStack graphics and logos are trademarks of OpenStack LLC or its affiliates in the United States and/or other countries.  Documentation was built with  MkDocs  and the Cinder Theme , licensed under\nthe  MIT license .", 
            "title": "Copyrights and trademarks"
        }, 
        {
            "location": "/acknowledge/index.html#contact-information", 
            "text": "Hortonworks, Inc. \n5470 Great America Parkway \nSanta Clara, CA 95054  Website:   www.hortonworks.com   Community:   community.hortonworks.com", 
            "title": "Contact information"
        }
    ]
}