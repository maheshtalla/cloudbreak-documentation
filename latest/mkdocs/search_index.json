{
    "docs": [
        {
            "location": "/index.html", 
            "text": "Introduction\n\n\nWelcome to the \nCloudbreak 2.1 Technical Preview\n documentation!\n\n\nCloudbreak simplifies the provisioning, management, and monitoring of on-demand HDP clusters in virtual and cloud environments. It leverages cloud infrastructure to create host instances, and uses Apache Ambari via Ambari blueprints to provision and manage HDP clusters. \n\n\nCloudbreak allows you to create clusters using the Cloudbreak web UI, Cloudbreak Shell, and Cloudbreak REST API. Clusters can be launched on public cloud infrastructure platforms \nMicrosoft Azure\n, \nAmazon Web Services (AWS)\n, and \nGoogle Cloud Platform (GCP)\n, and on the private cloud infrastructure platform \nOpenStack\n.\n\n\n  \n\n\nUse Cases\n\n\nCloudbreak allows you to create, manage, and monitor your clusters on your chosen cloud platform:\n\n\n\n\nQuickly create a cluster using one of the default cluster blueprints and infrastructure settings.  \n\n\nCreate a cluster based on the requirements of your workloads and provision infrastructure based on your IT requirements.\n\n\nSecure your cluster by enabling Kerberos.\n\n\nAutomate cluster creation using the Cloudbreak CLI. \n\n\nDevelop your application using Cloudbreak API.\n\n\n\n\nArchitecture\n\n\nRefer to \nArchitecture\n.\n\n\nGet Started\n\n\nTo get started with Cloudbreak:\n\n\n\n\nSelect the \ncloud platform\n on which you would like to launch Cloudbreak.   \n\n\nSelect the \ndeployment option\n that you would like to use. \n\n\nLaunch Cloudbreak\n. \n\n\n\n\nSelect Cloud Platform\n\n\nYou can deploy and use Cloudbreak on the following cloud platforms:\n\n\n\n\nAmazon Web Services (AWS)\n\n\nMicrosoft Azure\n\n\nGoogle Cloud Platform (GCP)\n\n\nOpenStack\n\n\n\n\nSelect Deployment Option\n\n\nThere are two basic deployment options:\n\n\n\n\n\n\n\n\nDeployment option\n\n\nWhen to use\n\n\n\n\n\n\n\n\n\n\nInstantiate one of the pre-built cloud images\n\n\nThis is the recommended basic deployment option.\n The cloud images include Cloudbreak deployer pre-installed on a CentOS VM.\n\n\n\n\n\n\nInstall the Cloudbreak deployer on your own VM\n\n\nThis is an advanced deployment option.\n \nSelect this option if you have custom VM requirements. The supported operating systems are RHEL, CentOS, and Oracle Linux 7 (64-bit).\n\n\n\n\n\n\n\n\nLaunch Cloudbreak\n\n\n(Option 1) You can launch Cloudbreak from one of the pre-built images:  \n\n\n\n\nLaunch on AWS\n  \n\n\nLaunch on Azure\n  \n\n\nLaunch on GCP\n   \n\n\nLaunch on OpenStack\n    \n\n\n\n\n(Option 2) Or you can launch Cloudbreak \non your own VM\n on one of these cloud platforms. This is an advanced deployment option that you should only use if you have custom VM requirements. \n\n\nIn general, the steps include meeting the prerequisites, launching Cloudbreak on a VM, and creating the Cloudbreak credential. After performing these steps, you can create a cluster based on one of the default blueprints or upload your own blueprint and then create a cluster. \n\n\n\n    \nNote\n\n    \nThe Cloudbreak software runs in your cloud environment. You are responsible for cloud infrastructure related charges while running Cloudbreak and the clusters being managed by Cloudbreak.", 
            "title": "Introduction"
        }, 
        {
            "location": "/index.html#introduction", 
            "text": "Welcome to the  Cloudbreak 2.1 Technical Preview  documentation!  Cloudbreak simplifies the provisioning, management, and monitoring of on-demand HDP clusters in virtual and cloud environments. It leverages cloud infrastructure to create host instances, and uses Apache Ambari via Ambari blueprints to provision and manage HDP clusters.   Cloudbreak allows you to create clusters using the Cloudbreak web UI, Cloudbreak Shell, and Cloudbreak REST API. Clusters can be launched on public cloud infrastructure platforms  Microsoft Azure ,  Amazon Web Services (AWS) , and  Google Cloud Platform (GCP) , and on the private cloud infrastructure platform  OpenStack .", 
            "title": "Introduction"
        }, 
        {
            "location": "/index.html#use-cases", 
            "text": "Cloudbreak allows you to create, manage, and monitor your clusters on your chosen cloud platform:   Quickly create a cluster using one of the default cluster blueprints and infrastructure settings.    Create a cluster based on the requirements of your workloads and provision infrastructure based on your IT requirements.  Secure your cluster by enabling Kerberos.  Automate cluster creation using the Cloudbreak CLI.   Develop your application using Cloudbreak API.", 
            "title": "Use Cases"
        }, 
        {
            "location": "/index.html#architecture", 
            "text": "Refer to  Architecture .", 
            "title": "Architecture"
        }, 
        {
            "location": "/index.html#get-started", 
            "text": "To get started with Cloudbreak:   Select the  cloud platform  on which you would like to launch Cloudbreak.     Select the  deployment option  that you would like to use.   Launch Cloudbreak .", 
            "title": "Get Started"
        }, 
        {
            "location": "/index.html#select-cloud-platform", 
            "text": "You can deploy and use Cloudbreak on the following cloud platforms:   Amazon Web Services (AWS)  Microsoft Azure  Google Cloud Platform (GCP)  OpenStack", 
            "title": "Select Cloud Platform"
        }, 
        {
            "location": "/index.html#select-deployment-option", 
            "text": "There are two basic deployment options:     Deployment option  When to use      Instantiate one of the pre-built cloud images  This is the recommended basic deployment option.  The cloud images include Cloudbreak deployer pre-installed on a CentOS VM.    Install the Cloudbreak deployer on your own VM  This is an advanced deployment option.   Select this option if you have custom VM requirements. The supported operating systems are RHEL, CentOS, and Oracle Linux 7 (64-bit).", 
            "title": "Select Deployment Option"
        }, 
        {
            "location": "/index.html#launch-cloudbreak", 
            "text": "(Option 1) You can launch Cloudbreak from one of the pre-built images:     Launch on AWS     Launch on Azure     Launch on GCP      Launch on OpenStack        (Option 2) Or you can launch Cloudbreak  on your own VM  on one of these cloud platforms. This is an advanced deployment option that you should only use if you have custom VM requirements.   In general, the steps include meeting the prerequisites, launching Cloudbreak on a VM, and creating the Cloudbreak credential. After performing these steps, you can create a cluster based on one of the default blueprints or upload your own blueprint and then create a cluster.   \n     Note \n     The Cloudbreak software runs in your cloud environment. You are responsible for cloud infrastructure related charges while running Cloudbreak and the clusters being managed by Cloudbreak.", 
            "title": "Launch Cloudbreak"
        }, 
        {
            "location": "/architecture/index.html", 
            "text": "Architecture\n\n\nCloudbreak deployer\n installs Cloudbreak components on a VM. Once these components are deployed, you can use \nCloudbreak application\n or Cloudbreak CLI to create, manage, and monitor clusters. \n\n\nCloudbreak Deployer Architecture\n\n\nCloudbreak deployer\n installs Cloudbreak components on a VM. It includes the following components:\n\n\n\n\n\n\n\n\nComponent\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nCloudbreak Application\n\n\nCloudbreak application is built on the foundation of cloud provider APIs and Apache Ambari.\n\n\n\n\n\n\nUluwatu\n\n\nThis is Cloudbreak web UI, which can be used to create, manage, and monitor clusters.\n\n\n\n\n\n\nCloudbreak CLI\n\n\nThis is Cloudbreak's command line tool, which can be used to create, manage, and monitor clusters.\n\n\n\n\n\n\nIdentity\n\n\nThis is Cloudbreak's OAuth identity server implementation, which utilizes UAA.\n\n\n\n\n\n\nSultans\n\n\nThis is Cloudbreak's user management system.\n\n\n\n\n\n\nPeriscope\n\n\nThis is Cloudbreak's autoscaling application, which is responsible for automatically increasing or decreasing the capacity of the cluster when your pre-defined conditions are met.\n\n\n\n\n\n\n\n\n\n\nThese component names are used in Cloudbreak logs, so for troubleshooting purposes it is useful to know what they refer to.\n\n\n\n\nCloudbreak Application Architecture\n\n\nThe Cloudbreak application is a web application that communicates with the cloud provider account to create cloud resources on your behalf. Once the cloud resources are in place, Cloudbreak uses Apache Ambari to deploy and configure the cluster on cloud VMs. Once your cluster is deployed, you can use Cloudbreak to scale the cluster.\n\n\nCloudbreak application is built on the foundation of cloud provider APIs and Apache Ambari:\n\n\n\n\n\n\nCloudbreak uses \nApache Ambari\n to provision, manage, and monitor HDP clusters. \n\n\nAmbari \nblueprints\n are a declarative definition of a cluster. With a blueprint, you can specify stack, component layout, and configurations to materialize an HDP cluster instance via Ambari REST API, without having to use the Ambari cluster install wizard. \n\n\n\n\n\n\nCloudbreak uses \ncloud provider APIs\n to create cloud resources required for the HDP clusters. \n\n\nYou can define these resources (networks, security groups, VMs and storage, and so on) in the create a cluster wizard in the Cloudbreak web UI. Resources are only provisioned once you create the cluster.  \n\n\n\n\n\n\nAmbari Blueprints\n\n\nAmbari blueprints\n are a declarative definition of a cluster. With a blueprint, you can specify stack, component layout, and configurations to materialize an HDP cluster instance via Ambari REST API, without having to use the Ambari cluster install wizard.  \n\n\nAmbari blueprints are specified in JSON format. After you provide the blueprint to Cloudbreak, the host groups in the JSON will be mapped to a set of instances when starting the cluster, and the specified services and components will be installed on the corresponding nodes.\n\n\nCloudbreak includes a few default blueprints and allows you to upload your own blueprints.\n\n\nRelated Links\n\n\nBlueprints\n\n\nApache documentation\n (External)  \n\n\nCloudbreak Credential\n\n\nCloudbreak credential\n allows Cloudbreak to authenticate with the cloud provider and create resources on your behalf. This is typically done via assigning a specific IAM role to Cloudbreak which allows Cloudbreak to perform certain actions within your cloud provider account.\n\n\nAfter launching Cloudbreak, you must create a Cloudbreak credential. Only after you have completed that step you can start creating clusters.\n\n\nRelated Links\n\n\nIdentity Management", 
            "title": "Architecture"
        }, 
        {
            "location": "/architecture/index.html#architecture", 
            "text": "Cloudbreak deployer  installs Cloudbreak components on a VM. Once these components are deployed, you can use  Cloudbreak application  or Cloudbreak CLI to create, manage, and monitor clusters.", 
            "title": "Architecture"
        }, 
        {
            "location": "/architecture/index.html#cloudbreak-deployer-architecture", 
            "text": "Cloudbreak deployer  installs Cloudbreak components on a VM. It includes the following components:     Component  Description      Cloudbreak Application  Cloudbreak application is built on the foundation of cloud provider APIs and Apache Ambari.    Uluwatu  This is Cloudbreak web UI, which can be used to create, manage, and monitor clusters.    Cloudbreak CLI  This is Cloudbreak's command line tool, which can be used to create, manage, and monitor clusters.    Identity  This is Cloudbreak's OAuth identity server implementation, which utilizes UAA.    Sultans  This is Cloudbreak's user management system.    Periscope  This is Cloudbreak's autoscaling application, which is responsible for automatically increasing or decreasing the capacity of the cluster when your pre-defined conditions are met.      These component names are used in Cloudbreak logs, so for troubleshooting purposes it is useful to know what they refer to.", 
            "title": "Cloudbreak Deployer Architecture"
        }, 
        {
            "location": "/architecture/index.html#cloudbreak-application-architecture", 
            "text": "The Cloudbreak application is a web application that communicates with the cloud provider account to create cloud resources on your behalf. Once the cloud resources are in place, Cloudbreak uses Apache Ambari to deploy and configure the cluster on cloud VMs. Once your cluster is deployed, you can use Cloudbreak to scale the cluster.  Cloudbreak application is built on the foundation of cloud provider APIs and Apache Ambari:    Cloudbreak uses  Apache Ambari  to provision, manage, and monitor HDP clusters.   Ambari  blueprints  are a declarative definition of a cluster. With a blueprint, you can specify stack, component layout, and configurations to materialize an HDP cluster instance via Ambari REST API, without having to use the Ambari cluster install wizard.     Cloudbreak uses  cloud provider APIs  to create cloud resources required for the HDP clusters.   You can define these resources (networks, security groups, VMs and storage, and so on) in the create a cluster wizard in the Cloudbreak web UI. Resources are only provisioned once you create the cluster.", 
            "title": "Cloudbreak Application Architecture"
        }, 
        {
            "location": "/architecture/index.html#ambari-blueprints", 
            "text": "Ambari blueprints  are a declarative definition of a cluster. With a blueprint, you can specify stack, component layout, and configurations to materialize an HDP cluster instance via Ambari REST API, without having to use the Ambari cluster install wizard.    Ambari blueprints are specified in JSON format. After you provide the blueprint to Cloudbreak, the host groups in the JSON will be mapped to a set of instances when starting the cluster, and the specified services and components will be installed on the corresponding nodes.  Cloudbreak includes a few default blueprints and allows you to upload your own blueprints.  Related Links  Blueprints  Apache documentation  (External)", 
            "title": "Ambari Blueprints"
        }, 
        {
            "location": "/architecture/index.html#cloudbreak-credential", 
            "text": "Cloudbreak credential  allows Cloudbreak to authenticate with the cloud provider and create resources on your behalf. This is typically done via assigning a specific IAM role to Cloudbreak which allows Cloudbreak to perform certain actions within your cloud provider account.  After launching Cloudbreak, you must create a Cloudbreak credential. Only after you have completed that step you can start creating clusters.  Related Links  Identity Management", 
            "title": "Cloudbreak Credential"
        }, 
        {
            "location": "/aws-launch/index.html", 
            "text": "Launch Cloudbreak on AWS\n\n\nBefore launching Cloudbreak on AWS, review and meet the prerequisites. Next, launch a VM using a Cloudbreak Amazon Machine Image, access the VM, and then start Cloudbreak. Once Cloudbreak is running, log in to the Cloudbreak UI and create a Cloudbreak credential. \n\n\nMeet the Prerequisites\n\n\nBefore launching Cloudbreak on AWS, you must meet the following prerequisites.\n\n\nAWS Account\n\n\nIn order to launch Cloudbreak on Azure, you must log in to your AWS account. If you don't have an account, you can create one at \nhttps://aws.amazon.com/\n.\n\n\nAWS Region\n\n\nDecide in which AWS region you would like to launch Cloudbreak. The following AWS regions are supported: \n\n\n\n\n\n\n\n\nRegion Name\n\n\nRegion\n\n\n\n\n\n\n\n\n\n\nEU (Ireland)\n\n\neu-west-1\n\n\n\n\n\n\nEU (Frankfurt)\n\n\neu-central-1\n\n\n\n\n\n\nUS East (N. Virginia)\n\n\nus-east-1\n\n\n\n\n\n\nUS West (N. California)\n\n\nus-west-1\n\n\n\n\n\n\nUS West (Oregon)\n\n\nus-west-2\n\n\n\n\n\n\nSouth America (S\u00e3o Paulo)\n\n\nsa-east-1\n\n\n\n\n\n\nAsia Pacific (Tokyo)\n\n\nap-northeast-1\n\n\n\n\n\n\nAsia Pacific (Singapore)\n\n\nap-southeast-1\n\n\n\n\n\n\nAsia Pacific (Sydney)\n\n\nap-southeast-2\n\n\n\n\n\n\n\n\nClusters created via Cloudbreak can be in the same or different region as Cloudbreak; when you launch a cluster, you select the region in which to launch it.\n\n\nRelated Links\n\n\nAWS Regions and Endpoints\n (External)   \n\n\nSSH Key Pair\n\n\nImport an existing key pair or generate a new key pair in the AWS region which you are planning to use for launching Cloudbreak and clusters. You can do this using the following steps.\n\n\nSteps\n \n\n\n\n\nNavigate to the Amazon EC2 console at https://console.aws.amazon.com/ec2/.  \n\n\nCheck the region listed in the top right corner to make sure that you are in the correct region.  \n\n\nIn the left pane, find \nNETWORK AND SECURITY\n and click \nKey Pairs\n.   \n\n\nDo one of the following:\n\n\nClick \nCreate Key Pair\n to create a new key pair. Your private key file will be automatically downloaded onto your computer. Make sure to save it in a secure location. You will need it to SSH to the cluster nodes. You may want to change access settings for the file using \nchmod 400 my-key-pair.pem\n.  \n\n\nClick \nImport Key Pair\n to upload an existing public key and then select it and click \nImport\n. Make sure that you have access to its corresponding private key.    \n\n\n\n\n\n\n\n\nYou need this SSH key pair to SSH to the Cloudbreak instance and start Cloudbreak. \n\n\nRelated Links\n\n\nCreating a Key Pair Using Amazon EC2\n (External)  \n\n\nAuthentication\n\n\nBefore you can start using Cloudbreak for provisioning clusters, you must select a way for Cloudbreak to authenticate with your AWS account and create resources on your behalf. There are two ways to do this: \n\n\n\n\n\n\nKey-based\n: This is a simpler option which does not require additional configuration at this point. It requires that you provide your AWS access key and secret key pair in the Cloudbreak web UI later. All you need to do now is check your AWS account and ensure that you can access this key pair.\n\n\n\n\n\n\nRole-based\n: This requires that you or your AWS admin create an IAM role to allow Cloudbreak to assume AWS roles (the \"AssumeRole\" policy).\n\n\n\n\n\n\nOption 1: Key-based Authentication\n\n\nIf you are using key-based authentication for Cloudbreak on AWS, you must be able to provide your AWS access key and secret key pair. Cloudbreak will use these keys to launch the resources. You must provide the access and secret keys later in the Cloudbreak web UI later when creating a credential. \n\n\nIf you choose this option, all you need to do at this point is check your AWS account and make sure that you can access this key pair. You can generate new access and secret keys from the \nIAM Console\n \n \nUsers\n. Next, select a user and click on the \nSecurity credentials\n tab:\n\n\n \n\n\nIf you choose this option, you can proceed to \nLaunch the VM\n.\n\n\nOption 2: Role-based Authentication\n\n\nIf you are using role-based authentication for Cloudbreak on AWS, you must create two IAM roles: one to grant Cloudbreak access to allow Cloudbreak to assume AWS roles (using the \"AssumeRole\" policy) and the second one to provide Cloudbreak with the capabilities required for cluster creation (using the \"cb-policy\" policy).\n\n\nThe following table provides contextual information about the two roles required: \n\n\n\n\n\n\n\n\nRole\n\n\nPurpose\n\n\nOverview of Steps\n\n\nConfiguration\n\n\n\n\n\n\n\n\n\n\nCloudbreakRole\n\n\nAllows Cloudbreak to assume other IAM roles - specifically the CredentialRole.\n\n\nCreate a role called \"CloudbreakRole\" and attach the \"AssumeRole\" policy. The \"AssumeRole\" policy definition and steps for creating the CloudbreakRole are provided below.\n\n\nWhen launching your Cloudbreak VM, during \nStep 3: Configure Instance Details\n \n \nIAM\n, you will attach the \"CloudbreakRole\" IAM role to the VM.\n\n\n\n\n\n\nCredentialRole\n\n\nAllows Cloudbreak to create AWS resources required for clusters.\n\n\nCreate a new IAM role called \"CredentialRole\" and attach the \"cb-policy\" policy to it. The \"cb-policy\" policy definition and steps for creating the CredentialRole are provided below.\n When creating this role using the AWS Console, make sure that that it is a role for cross-account access and that the trust-relation is set up as follows: 'Account ID' is your own 12-digit AWS account ID and 'External ID' is \u201cprovision-ambari\u201d. See steps below.\n\n\nOnce you log in to the Cloudbreak UI and are ready to create clusters, you will use this role to create the Cloudbreak credential.\n\n\n\n\n\n\n\n\n\n\nThe following alternative steps are available for the \nCloudbreakRole\n:\n\n\nAlternatively, you can generate the \"CredentialRole\" role later once your Cloudbreak VM is running and once you have \nconfigured AWS CLI\n on your VM, by running the \ncbd aws generate-role\n command. This command creates a role with the name \"cbreak-deployer\" (equivalent to the \"CredentialRole\"). To customize the name of the role, add \nexport AWS_ROLE_NAME=my-cloudbreak-role-name\n (where \"my-cloudbreak-role-name\" is your custom role name) as a new line to your Profile.\n\n\nAlternatively, instead of attaching the \"CloudbreakRole\" role during the VM launch, you can assign the \"CloudbreakRole\" to an IAM user and then add the access and security key of that user to your 'Profile'.\n\n\n\n\nYou can create these roles in the \nIAM console\n, on the \nRoles\n page via the \nCreate Role\n option. Detailed steps are provided below. \n\n\nCreate CloudbreakRole\n\n\nUse these steps to create CloudbreakRole. \n\n\nUse the following \"AssumeRole\" policy definition: \n\n\n{\n  \"Version\": \"2012-10-17\",\n  \"Statement\": {\n    \"Sid\": \"Stmt1400068149000\",\n    \"Effect\": \"Allow\",\n    \"Action\": [\"sts:AssumeRole\"],\n    \"Resource\": \"*\"\n  }\n}\n\n\n\n\nSteps\n\n\n\n\n\n\nNavigate to the \nIAM console\n \n \nRoles\n and click \nCreate Role\n.\n\n\n \n\n\n\n\n\n\nIn the \"Create Role\" wizard, select \nAWS service\n role type and then select any service. \n\n\n \n\n\n\n\n\n\nWhen done, click \nNext: Permissions\n to navigate to the next page in the wizard.\n\n\n\n\n\n\nClick \nCreate policy\n.\n\n\n\n\n\n\n\n\nClick \nSelect\n next to \"Create Your Own Policy\".\n\n\n  \n\n\n\n\n\n\nIn the \nPolicy Name\n field, enter \"AssumeRole\" and in the \nPolicy Document\n paste the policy definition provided above. You can also download and copy it from \nhere\n.\n\n\n  \n\n\n\n\n\n\nWhen done, click \nCreate Policy\n.\n\n\n\n\n\n\nClick \nRefresh\n. Next, find the \"AsumeRole\" policy that you just created and select it by checking the box.\n\n\n \n\n\n\n\n\n\nWhen done, click \nNext: Review\n.\n\n\n\n\n\n\nIn the \nRoles name\n field, enter role name, for example \"CloudbreakRole\". \n\n\n \n\n\n\n\n\n\nWhen done, click \nCreate role\n to finish the role creation process.\n\n\n\n\n\n\nCreate CredentialRole\n\n\nUse these steps to create CredentialRole.\n\n\nUse the following \"cb-policy\" policy definition: \n\n\n{\n  \"Version\": \"2012-10-17\",\n  \"Statement\": [\n    {\n      \"Effect\": \"Allow\",\n      \"Action\": [ \"cloudformation:*\" ],\n      \"Resource\": [ \"*\" ]\n    },\n    {\n      \"Effect\": \"Allow\",\n      \"Action\": [ \"ec2:*\" ],\n      \"Resource\": [ \"*\" ]\n    },\n    {\n      \"Effect\": \"Allow\",\n      \"Action\": [ \"iam:PassRole\" ],\n      \"Resource\": [ \"*\" ]\n    },\n    {\n      \"Effect\": \"Allow\",\n      \"Action\": [ \"autoscaling:*\" ],\n      \"Resource\": [ \"*\" ]\n    }\n  ]\n}\n\n\n\nSteps\n\n\n\n\n\n\nNavigate to the \nIAM console\n \n \nRoles\n and click \nCreate Role\n.\n\n\n \n\n\n\n\n\n\nIn the \"Create Role\" wizard, select \nAnother AWS account\n role type. Next, provide the following:\n\n\n\n\nIn the \nAccount ID\n field, enter your AWS account ID.\n\n\nUnder \nOptions\n, check \nRequire external ID\n.\n\n\nIn the \nExternal ID\n, enter \"provision-ambari\".\n\n\n\n\n \n\n\n\n\n\n\nWhen done, click \nNext: Permissions\n to navigate to the next page in the wizard.\n\n\n\n\n\n\nClick \nCreate policy\n.\n\n\n\n\n\n\n\n\nClick \nSelect\n next to \"Create Your Own Policy\".\n\n\n \n\n\n\n\n\n\nCopy the \"cb-policy\" policy definition (see below).   \n\n\n\n\n\n\nIn the \nPolicy Name\n field, enter \"cb-policy\" and in the \nPolicy Document\n paste the policy definition provided above. You can also download and copy it from \nhere\n.\n\n\n  \n\n\n\n\n\n\nWhen done, click \nCreate Policy\n.\n\n\n\n\n\n\nClick \nRefresh\n. Next, find the \"cb-policy\" that you just created and select it by checking the box.\n\n\n \n\n\n\n\n\n\nWhen done, click \nNext: Review\n.\n\n\n\n\n\n\nIn the \nRoles name\n field, enter role name, for example \"CredentialRole\". \n\n\n \n\n\n\n\n\n\nWhen done, click \nCreate role\n to finish the role creation process. \n\n\n\n\n\n\nOnce you are done, you can proceed to \nLaunch the VM\n.  \n\n\nRelated Links\n\n\nUsing Instance Profiles\n (External)\n\n\nUsing an IAM Role to Grant Permissions to Applications\n (External)   \n\n\nLaunch the VM\n\n\nNow that you've met the prerequisites, you can launch the Cloudbreak deployer VM available as a Community AMI.\n\n\nSteps\n\n\n\n\n\n\nIn the AWS Management Console, navigate to the EC2 Console.  \n\n\n\n\n\n\nIn the top right corner, select the region in which you want to launch Cloudbreak.  \n\n\n \n\n\n\n\n\n\nFrom the left pane, select \nINSTANCES\n \n \nInstances\n.  \n\n\n\n\n\n\nClick on \nLaunch Instance\n.\n\n\n\n\n\n\nIn \nStep 1: Choose an Amazon Machine Image (AMI)\n page, from the left pane, select \nCommunity AMIs\n. \n\n\n \n\n\n\n\n\n\nIn the search box, enter the image name. The following Cloudbreak deployer images are available:\n\n\n\n\n\n\n\n\nRegion\n\n\nImage Name\n\n\n\n\n\n\n\n\n\n\neu-west-1\n\n\nami-79c63f00\n\n\n\n\n\n\nsa-east-1\n\n\nami-1af18176\n\n\n\n\n\n\nus-east-1\n\n\nami-e9bdbc92\n\n\n\n\n\n\nus-west-1\n\n\nami-2dd1e44d\n\n\n\n\n\n\nus-west-2\n\n\nami-ae6c85d6\n\n\n\n\n\n\neu-central-1\n\n\nami-bc18b3d3\n\n\n\n\n\n\nap-northeast-1\n\n\nami-f2fb0494\n\n\n\n\n\n\nap-southeast-1\n\n\nami-47036324\n\n\n\n\n\n\nap-southeast-2\n\n\nami-370b1154\n\n\n\n\n\n\n\n\n\n\n\n\nClick \nSelect\n.  \n\n\n\n\nThe steps listed below only mention required parameters. You may optionally review and adjust additional parameters. \n\n\n\n\n\n\n\n\nIn \nStep2: Choose Instance Type\n, choose an instance type. The minimum instance type which is suitable for Cloudbreak is \nm3.large\n. Minimum requirements are 8GB RAM, 10GB disk, 2 cores. Click \nNext\n.\n\n\n   \n\n\n\n\n\n\n(Perform this step only if you are using role-based authorization) In \nStep 3: Configure Instance Details\n \n \nIAM\n, select the \"CloudbreakRole\" IAM role which you \ncreated earlier\n.\n\n\n\n\n\n\nIn \nStep 6: Configure Security Group\n, open the following ports: 22 (for access via SSH) and 443 (for access via HTTPS). Click \nReview and Launch\n.\n\n\n \n\n\n\n\n\n\nIn \nStep 7: Review Instance Launch\n, review the information carefully and then click \nLaunch\n. \n\n\n\n\n\n\nWhen prompted select an existing key pair or create a new one. Next, acknowledge that you have access to the private key file and click \nLaunch Instance\n. \n\n\n  \n\n\n\n\n\n\nClick on the instance ID to navigate to the \nInstances\n view in your EC2 console. \n\n\n  \n\n\n\n\n\n\nSSH to the VM\n\n\nNow that your VM is ready, access it via SSH: \n\n\n\n\nUse the private key from the key pair that you selected when launching the instance. \n\n\nThe SSH user is called \"cloudbreak\".\n\n\nYou can obtain the host IP from the EC2 console \n \nInstances\n view by selecting the instance, selecting the \nDescription\n tab, and copying the value of the \nPublic DNS (IPv4)\n\n or \nIPv4 Public IP\n parameter.\n\n\n\n\nOn Mac OS X, you can SSH to the VM by running the following from the Terminal app: \nssh -i \"your-private-key.pem\" cloudnreak@instance_IP\n where \"your-private-key.pem\" points to the location of your private key and \"instance_IP\" is the public IP address of the VM.\n\n\nOn Windows, you can use \nPuTTy\n.\n\n\nLaunch Cloudbreak Deployer\n\n\nAfter accessing the VM via SSH, launch Cloudbreak deployer using the following steps.\n\n\nSteps\n \n\n\n\n\n\n\nNavigate to the cloudbreak-deployment directory:\n\n\ncd /var/lib/cloudbreak-deployment/\n\n\nThis directory contains configuration files and the supporting binaries for Cloudbreak deployer.\n\n\n\n\n\n\nInitialize your profile by creating a new file called \nProfile\n and adding the following content:\n\n\nexport UAA_DEFAULT_SECRET=MY-SECRET\nexport UAA_DEFAULT_USER_PW=MY-PASSWORD\n  \n\n\nFor example: \n\n\nexport UAA_DEFAULT_SECRET=MySecret123\nexport UAA_DEFAULT_USER_PW=MySecurePassword123\n \n\n\n\n\nYou will need to provide the password when logging in to the Cloudbreak web UI and when using the Cloudbreak Shell. The secret will be used by Cloudbreak for authentication.  \n\n\n\n\n\n\n\n\nStart the Cloudbreak application by using the following command:\n\n\ncbd start\n\n\nThis will start the Docker containers and initialize the application. The first time you start the Coudbreak app, the process will take longer than usual due to the download of all the necessary docker images.\n\n\nThe \ncbd start\n command includes the \ncbd generate\n command which applies the following steps:\n\n\n\n\nCreates the \ndocker-compose.yml\n file, which describes the configuration of all the Docker containers needed for the Cloudbreak deployment.\n\n\nCreates the \nuaa.yml\n file, which holds the configuration of the identity server used to authenticate users with Cloudbreak.\n\n\n\n\n\n\nOnce the \ncbd start\n has finished, it returns the \"Uluwatu (Cloudbreak UI) url\" which you can later paste in your browser and log in to Cloudbreak web UI. \n\n\n\n\n\n\n\n\nCheck Cloudbreak deployer version and health: \n\n\ncbd doctor\n\n\n\n\n\n\nNext, check Cloudbreak Application logs: \n\n\ncbd logs cloudbreak\n\n\nYou should see a message like this in the log: \nStarted CloudbreakApplication in 36.823 seconds.\n Cloudbreak normally takes less than a minute to start.\n\n\n\n\n\n\nAccess Cloudbreak UI\n\n\nLog in to the Cloudbreak UI using the following steps.\n\n\nSteps\n\n\n\n\n\n\nYou can log into the Cloudbreak application at \nhttps://IPv4_Public_IP\n/\n or \nhttps://Public_DNS\n. For example \nhttps://34.212.141.253\n or \nhttps://ec2-34-212-141-253.us-west-2.compute.amazonaws.com\n. \n\n\n\n\n\n\nConfirm the security exception to proceed to the Cloudbreak web UI.\n\n\nThe first time you access Cloudbreak UI, Cloudbreak will automatically generate a self-signed certificate, due to which your browser will warn you about an untrusted connection and will ask you to confirm a security exception.\n\n\n\n\n\n\nThe login page is displayed:\n\n\n  \n\n\n\n\n\n\nLog in to the Cloudbreak web UI: \n\n\n\n\nThe default username is \nadmin@example.com\n but you should sign up with your own email address.    \n\n\nThe password is the value of the \nUAA_DEFAULT_USER_PW\n variable that you configured in your \nProfile\n file when \nlaunching Cloudbreak deployer\n.\n\n\n\n\n\n\n\n\nUpon a successful login, you are redirected to the dashboard:\n\n\n  \n\n\n\n\n\n\nCreate Cloudbreak Credential\n\n\nBefore you can start creating clusters, you must first create a \nCloudbreak credential\n. Without this credential, you will not be able to create clusters via Cloudbreak. \n\n\nAs part of the \nprerequisites\n, you had two options to allow Cloudbreak to authenticate with AWS and create resources on your behalf: key-based or role-based authentication. \n\n\nDepending on your choice, you must configure a key-based or role-based credential: \n\n\nCreate Key-Based Credential\n\n\nCreate Role-Based Credential\n\n\nCreate Key-Based Credential\n\n\nTo perform these steps, you must know your access and secret key. If needed, you or your AWS administrator can generate new access and secret keys from the \nIAM Console\n \n \nUsers\n \n select a user \n \nSecurity credentials\n. \n\n\nSteps\n\n\n\n\n\n\nIn the Cloudbreak web UI, select \nCredentials\n from the navigation pane. \n\n\n\n\n\n\nClick \nCreate Credential\n. \n\n\n\n\n\n\nUnder \nCloud provider\n, select \"Amazon Web Services\":\n\n\n  \n\n\n\n\n\n\nProvide the following information:\n\n\n\n\n\n\n\n\nParameter\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nSelect Credential Type\n\n\nSelect \nKey Based\n.\n\n\n\n\n\n\nName\n\n\nEnter a name for your credential.\n\n\n\n\n\n\nDescription\n\n\n(Optional) Enter a description.\n\n\n\n\n\n\nAccess Key\n\n\nPaste your access key.\n\n\n\n\n\n\nSecret Access Key\n\n\nPaste your secret key.\n\n\n\n\n\n\n\n\n\n\n\n\nClick \nCreate\n.\n\n\n\n\n\n\nYour credential should now be displayed in the \nCredentials\n pane.\n\n\nCongratulations! You've successfully launched Cloudbreak and create a Cloudbreak credential. Now it's time to \ncreate a cluster\n. \n\n\n\n\n\n\nCreate Role-Based Credential\n\n\nTo perform these steps, you must know the \nIAM Role ARN\n corresponding to the \"CredentialRole\" (configured as a \nprerequisite\n).  \n\n\nSteps\n\n\n\n\n\n\nIn the Cloudbreak web UI, select \nCredentials\n from the navigation pane. \n\n\n\n\n\n\nClick \nCreate Credential\n. \n\n\n\n\n\n\nUnder \nCloud provider\n, select \"Amazon Web Services\":\n\n\n  \n\n\n\n\n\n\nProvide the following information:\n\n\n\n\n\n\n\n\nParameter\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nSelect Credential Type\n\n\nSelect \nRole Based\n (default value).\n\n\n\n\n\n\nName\n\n\nEnter a name for your credential.\n\n\n\n\n\n\nDescription\n\n\n(Optional) Enter a description.\n\n\n\n\n\n\nIAM Role ARN\n\n\nPaste the IAM Role ARN corresponding to the \"CredentialRole\" that you created earlier. For example \narn:aws:iam::315627065446:role/CredentialRole\n is a valid IAM Role ARN.\n\n\n\n\n\n\n\n\n\n\n\n\nClick \nCreate\n.\n\n\n\n\n\n\nYour credential should now be displayed in the \nCredentials\n pane.\n\n\nCongratulations! You've successfully launched Cloudbreak and create a Cloudbreak credential. Now it's time to \ncreate a cluster\n. \n\n\n\n\n\n\n\n\nNext: Create a Cluster", 
            "title": "Launch on AWS"
        }, 
        {
            "location": "/aws-launch/index.html#launch-cloudbreak-on-aws", 
            "text": "Before launching Cloudbreak on AWS, review and meet the prerequisites. Next, launch a VM using a Cloudbreak Amazon Machine Image, access the VM, and then start Cloudbreak. Once Cloudbreak is running, log in to the Cloudbreak UI and create a Cloudbreak credential.", 
            "title": "Launch Cloudbreak on AWS"
        }, 
        {
            "location": "/aws-launch/index.html#meet-the-prerequisites", 
            "text": "Before launching Cloudbreak on AWS, you must meet the following prerequisites.", 
            "title": "Meet the Prerequisites"
        }, 
        {
            "location": "/aws-launch/index.html#aws-account", 
            "text": "In order to launch Cloudbreak on Azure, you must log in to your AWS account. If you don't have an account, you can create one at  https://aws.amazon.com/ .", 
            "title": "AWS Account"
        }, 
        {
            "location": "/aws-launch/index.html#aws-region", 
            "text": "Decide in which AWS region you would like to launch Cloudbreak. The following AWS regions are supported:      Region Name  Region      EU (Ireland)  eu-west-1    EU (Frankfurt)  eu-central-1    US East (N. Virginia)  us-east-1    US West (N. California)  us-west-1    US West (Oregon)  us-west-2    South America (S\u00e3o Paulo)  sa-east-1    Asia Pacific (Tokyo)  ap-northeast-1    Asia Pacific (Singapore)  ap-southeast-1    Asia Pacific (Sydney)  ap-southeast-2     Clusters created via Cloudbreak can be in the same or different region as Cloudbreak; when you launch a cluster, you select the region in which to launch it.  Related Links  AWS Regions and Endpoints  (External)", 
            "title": "AWS Region"
        }, 
        {
            "location": "/aws-launch/index.html#ssh-key-pair", 
            "text": "Import an existing key pair or generate a new key pair in the AWS region which you are planning to use for launching Cloudbreak and clusters. You can do this using the following steps.  Steps     Navigate to the Amazon EC2 console at https://console.aws.amazon.com/ec2/.    Check the region listed in the top right corner to make sure that you are in the correct region.    In the left pane, find  NETWORK AND SECURITY  and click  Key Pairs .     Do one of the following:  Click  Create Key Pair  to create a new key pair. Your private key file will be automatically downloaded onto your computer. Make sure to save it in a secure location. You will need it to SSH to the cluster nodes. You may want to change access settings for the file using  chmod 400 my-key-pair.pem .    Click  Import Key Pair  to upload an existing public key and then select it and click  Import . Make sure that you have access to its corresponding private key.         You need this SSH key pair to SSH to the Cloudbreak instance and start Cloudbreak.   Related Links  Creating a Key Pair Using Amazon EC2  (External)", 
            "title": "SSH Key Pair"
        }, 
        {
            "location": "/aws-launch/index.html#authentication", 
            "text": "Before you can start using Cloudbreak for provisioning clusters, you must select a way for Cloudbreak to authenticate with your AWS account and create resources on your behalf. There are two ways to do this:     Key-based : This is a simpler option which does not require additional configuration at this point. It requires that you provide your AWS access key and secret key pair in the Cloudbreak web UI later. All you need to do now is check your AWS account and ensure that you can access this key pair.    Role-based : This requires that you or your AWS admin create an IAM role to allow Cloudbreak to assume AWS roles (the \"AssumeRole\" policy).", 
            "title": "Authentication"
        }, 
        {
            "location": "/aws-launch/index.html#option-1-key-based-authentication", 
            "text": "If you are using key-based authentication for Cloudbreak on AWS, you must be able to provide your AWS access key and secret key pair. Cloudbreak will use these keys to launch the resources. You must provide the access and secret keys later in the Cloudbreak web UI later when creating a credential.   If you choose this option, all you need to do at this point is check your AWS account and make sure that you can access this key pair. You can generate new access and secret keys from the  IAM Console     Users . Next, select a user and click on the  Security credentials  tab:     If you choose this option, you can proceed to  Launch the VM .", 
            "title": "Option 1: Key-based Authentication"
        }, 
        {
            "location": "/aws-launch/index.html#option-2-role-based-authentication", 
            "text": "If you are using role-based authentication for Cloudbreak on AWS, you must create two IAM roles: one to grant Cloudbreak access to allow Cloudbreak to assume AWS roles (using the \"AssumeRole\" policy) and the second one to provide Cloudbreak with the capabilities required for cluster creation (using the \"cb-policy\" policy).  The following table provides contextual information about the two roles required:      Role  Purpose  Overview of Steps  Configuration      CloudbreakRole  Allows Cloudbreak to assume other IAM roles - specifically the CredentialRole.  Create a role called \"CloudbreakRole\" and attach the \"AssumeRole\" policy. The \"AssumeRole\" policy definition and steps for creating the CloudbreakRole are provided below.  When launching your Cloudbreak VM, during  Step 3: Configure Instance Details     IAM , you will attach the \"CloudbreakRole\" IAM role to the VM.    CredentialRole  Allows Cloudbreak to create AWS resources required for clusters.  Create a new IAM role called \"CredentialRole\" and attach the \"cb-policy\" policy to it. The \"cb-policy\" policy definition and steps for creating the CredentialRole are provided below.  When creating this role using the AWS Console, make sure that that it is a role for cross-account access and that the trust-relation is set up as follows: 'Account ID' is your own 12-digit AWS account ID and 'External ID' is \u201cprovision-ambari\u201d. See steps below.  Once you log in to the Cloudbreak UI and are ready to create clusters, you will use this role to create the Cloudbreak credential.      The following alternative steps are available for the  CloudbreakRole :  Alternatively, you can generate the \"CredentialRole\" role later once your Cloudbreak VM is running and once you have  configured AWS CLI  on your VM, by running the  cbd aws generate-role  command. This command creates a role with the name \"cbreak-deployer\" (equivalent to the \"CredentialRole\"). To customize the name of the role, add  export AWS_ROLE_NAME=my-cloudbreak-role-name  (where \"my-cloudbreak-role-name\" is your custom role name) as a new line to your Profile.  Alternatively, instead of attaching the \"CloudbreakRole\" role during the VM launch, you can assign the \"CloudbreakRole\" to an IAM user and then add the access and security key of that user to your 'Profile'.   You can create these roles in the  IAM console , on the  Roles  page via the  Create Role  option. Detailed steps are provided below.   Create CloudbreakRole  Use these steps to create CloudbreakRole.   Use the following \"AssumeRole\" policy definition:   {\n  \"Version\": \"2012-10-17\",\n  \"Statement\": {\n    \"Sid\": \"Stmt1400068149000\",\n    \"Effect\": \"Allow\",\n    \"Action\": [\"sts:AssumeRole\"],\n    \"Resource\": \"*\"\n  }\n}  Steps    Navigate to the  IAM console     Roles  and click  Create Role .       In the \"Create Role\" wizard, select  AWS service  role type and then select any service.        When done, click  Next: Permissions  to navigate to the next page in the wizard.    Click  Create policy .     Click  Select  next to \"Create Your Own Policy\".        In the  Policy Name  field, enter \"AssumeRole\" and in the  Policy Document  paste the policy definition provided above. You can also download and copy it from  here .        When done, click  Create Policy .    Click  Refresh . Next, find the \"AsumeRole\" policy that you just created and select it by checking the box.       When done, click  Next: Review .    In the  Roles name  field, enter role name, for example \"CloudbreakRole\".        When done, click  Create role  to finish the role creation process.    Create CredentialRole  Use these steps to create CredentialRole.  Use the following \"cb-policy\" policy definition:   {\n  \"Version\": \"2012-10-17\",\n  \"Statement\": [\n    {\n      \"Effect\": \"Allow\",\n      \"Action\": [ \"cloudformation:*\" ],\n      \"Resource\": [ \"*\" ]\n    },\n    {\n      \"Effect\": \"Allow\",\n      \"Action\": [ \"ec2:*\" ],\n      \"Resource\": [ \"*\" ]\n    },\n    {\n      \"Effect\": \"Allow\",\n      \"Action\": [ \"iam:PassRole\" ],\n      \"Resource\": [ \"*\" ]\n    },\n    {\n      \"Effect\": \"Allow\",\n      \"Action\": [ \"autoscaling:*\" ],\n      \"Resource\": [ \"*\" ]\n    }\n  ]\n}  Steps    Navigate to the  IAM console     Roles  and click  Create Role .       In the \"Create Role\" wizard, select  Another AWS account  role type. Next, provide the following:   In the  Account ID  field, enter your AWS account ID.  Under  Options , check  Require external ID .  In the  External ID , enter \"provision-ambari\".        When done, click  Next: Permissions  to navigate to the next page in the wizard.    Click  Create policy .     Click  Select  next to \"Create Your Own Policy\".       Copy the \"cb-policy\" policy definition (see below).       In the  Policy Name  field, enter \"cb-policy\" and in the  Policy Document  paste the policy definition provided above. You can also download and copy it from  here .        When done, click  Create Policy .    Click  Refresh . Next, find the \"cb-policy\" that you just created and select it by checking the box.       When done, click  Next: Review .    In the  Roles name  field, enter role name, for example \"CredentialRole\".        When done, click  Create role  to finish the role creation process.     Once you are done, you can proceed to  Launch the VM .    Related Links  Using Instance Profiles  (External)  Using an IAM Role to Grant Permissions to Applications  (External)", 
            "title": "Option 2: Role-based Authentication"
        }, 
        {
            "location": "/aws-launch/index.html#launch-the-vm", 
            "text": "Now that you've met the prerequisites, you can launch the Cloudbreak deployer VM available as a Community AMI.  Steps    In the AWS Management Console, navigate to the EC2 Console.      In the top right corner, select the region in which you want to launch Cloudbreak.         From the left pane, select  INSTANCES     Instances .      Click on  Launch Instance .    In  Step 1: Choose an Amazon Machine Image (AMI)  page, from the left pane, select  Community AMIs .        In the search box, enter the image name. The following Cloudbreak deployer images are available:     Region  Image Name      eu-west-1  ami-79c63f00    sa-east-1  ami-1af18176    us-east-1  ami-e9bdbc92    us-west-1  ami-2dd1e44d    us-west-2  ami-ae6c85d6    eu-central-1  ami-bc18b3d3    ap-northeast-1  ami-f2fb0494    ap-southeast-1  ami-47036324    ap-southeast-2  ami-370b1154       Click  Select .     The steps listed below only mention required parameters. You may optionally review and adjust additional parameters.      In  Step2: Choose Instance Type , choose an instance type. The minimum instance type which is suitable for Cloudbreak is  m3.large . Minimum requirements are 8GB RAM, 10GB disk, 2 cores. Click  Next .         (Perform this step only if you are using role-based authorization) In  Step 3: Configure Instance Details     IAM , select the \"CloudbreakRole\" IAM role which you  created earlier .    In  Step 6: Configure Security Group , open the following ports: 22 (for access via SSH) and 443 (for access via HTTPS). Click  Review and Launch .       In  Step 7: Review Instance Launch , review the information carefully and then click  Launch .     When prompted select an existing key pair or create a new one. Next, acknowledge that you have access to the private key file and click  Launch Instance .         Click on the instance ID to navigate to the  Instances  view in your EC2 console.", 
            "title": "Launch the VM"
        }, 
        {
            "location": "/aws-launch/index.html#ssh-to-the-vm", 
            "text": "Now that your VM is ready, access it via SSH:    Use the private key from the key pair that you selected when launching the instance.   The SSH user is called \"cloudbreak\".  You can obtain the host IP from the EC2 console    Instances  view by selecting the instance, selecting the  Description  tab, and copying the value of the  Public DNS (IPv4)  or  IPv4 Public IP  parameter.   On Mac OS X, you can SSH to the VM by running the following from the Terminal app:  ssh -i \"your-private-key.pem\" cloudnreak@instance_IP  where \"your-private-key.pem\" points to the location of your private key and \"instance_IP\" is the public IP address of the VM.  On Windows, you can use  PuTTy .", 
            "title": "SSH to the VM"
        }, 
        {
            "location": "/aws-launch/index.html#launch-cloudbreak-deployer", 
            "text": "After accessing the VM via SSH, launch Cloudbreak deployer using the following steps.  Steps      Navigate to the cloudbreak-deployment directory:  cd /var/lib/cloudbreak-deployment/  This directory contains configuration files and the supporting binaries for Cloudbreak deployer.    Initialize your profile by creating a new file called  Profile  and adding the following content:  export UAA_DEFAULT_SECRET=MY-SECRET\nexport UAA_DEFAULT_USER_PW=MY-PASSWORD     For example:   export UAA_DEFAULT_SECRET=MySecret123\nexport UAA_DEFAULT_USER_PW=MySecurePassword123     You will need to provide the password when logging in to the Cloudbreak web UI and when using the Cloudbreak Shell. The secret will be used by Cloudbreak for authentication.       Start the Cloudbreak application by using the following command:  cbd start  This will start the Docker containers and initialize the application. The first time you start the Coudbreak app, the process will take longer than usual due to the download of all the necessary docker images.  The  cbd start  command includes the  cbd generate  command which applies the following steps:   Creates the  docker-compose.yml  file, which describes the configuration of all the Docker containers needed for the Cloudbreak deployment.  Creates the  uaa.yml  file, which holds the configuration of the identity server used to authenticate users with Cloudbreak.    Once the  cbd start  has finished, it returns the \"Uluwatu (Cloudbreak UI) url\" which you can later paste in your browser and log in to Cloudbreak web UI.      Check Cloudbreak deployer version and health:   cbd doctor    Next, check Cloudbreak Application logs:   cbd logs cloudbreak  You should see a message like this in the log:  Started CloudbreakApplication in 36.823 seconds.  Cloudbreak normally takes less than a minute to start.", 
            "title": "Launch Cloudbreak Deployer"
        }, 
        {
            "location": "/aws-launch/index.html#access-cloudbreak-ui", 
            "text": "Log in to the Cloudbreak UI using the following steps.  Steps    You can log into the Cloudbreak application at  https://IPv4_Public_IP /  or  https://Public_DNS . For example  https://34.212.141.253  or  https://ec2-34-212-141-253.us-west-2.compute.amazonaws.com .     Confirm the security exception to proceed to the Cloudbreak web UI.  The first time you access Cloudbreak UI, Cloudbreak will automatically generate a self-signed certificate, due to which your browser will warn you about an untrusted connection and will ask you to confirm a security exception.    The login page is displayed:        Log in to the Cloudbreak web UI:    The default username is  admin@example.com  but you should sign up with your own email address.      The password is the value of the  UAA_DEFAULT_USER_PW  variable that you configured in your  Profile  file when  launching Cloudbreak deployer .     Upon a successful login, you are redirected to the dashboard:", 
            "title": "Access Cloudbreak UI"
        }, 
        {
            "location": "/aws-launch/index.html#create-cloudbreak-credential", 
            "text": "Before you can start creating clusters, you must first create a  Cloudbreak credential . Without this credential, you will not be able to create clusters via Cloudbreak.   As part of the  prerequisites , you had two options to allow Cloudbreak to authenticate with AWS and create resources on your behalf: key-based or role-based authentication.   Depending on your choice, you must configure a key-based or role-based credential:   Create Key-Based Credential  Create Role-Based Credential", 
            "title": "Create Cloudbreak Credential"
        }, 
        {
            "location": "/aws-launch/index.html#create-key-based-credential", 
            "text": "To perform these steps, you must know your access and secret key. If needed, you or your AWS administrator can generate new access and secret keys from the  IAM Console     Users    select a user    Security credentials .   Steps    In the Cloudbreak web UI, select  Credentials  from the navigation pane.     Click  Create Credential .     Under  Cloud provider , select \"Amazon Web Services\":        Provide the following information:     Parameter  Description      Select Credential Type  Select  Key Based .    Name  Enter a name for your credential.    Description  (Optional) Enter a description.    Access Key  Paste your access key.    Secret Access Key  Paste your secret key.       Click  Create .    Your credential should now be displayed in the  Credentials  pane.  Congratulations! You've successfully launched Cloudbreak and create a Cloudbreak credential. Now it's time to  create a cluster .", 
            "title": "Create Key-Based Credential"
        }, 
        {
            "location": "/aws-launch/index.html#create-role-based-credential", 
            "text": "To perform these steps, you must know the  IAM Role ARN  corresponding to the \"CredentialRole\" (configured as a  prerequisite ).    Steps    In the Cloudbreak web UI, select  Credentials  from the navigation pane.     Click  Create Credential .     Under  Cloud provider , select \"Amazon Web Services\":        Provide the following information:     Parameter  Description      Select Credential Type  Select  Role Based  (default value).    Name  Enter a name for your credential.    Description  (Optional) Enter a description.    IAM Role ARN  Paste the IAM Role ARN corresponding to the \"CredentialRole\" that you created earlier. For example  arn:aws:iam::315627065446:role/CredentialRole  is a valid IAM Role ARN.       Click  Create .    Your credential should now be displayed in the  Credentials  pane.  Congratulations! You've successfully launched Cloudbreak and create a Cloudbreak credential. Now it's time to  create a cluster .      Next: Create a Cluster", 
            "title": "Create Role-Based Credential"
        }, 
        {
            "location": "/aws-create/index.html", 
            "text": "Create a Cluster on AWS\n\n\nUse these steps to create a cluster.\n\n\nSteps\n\n\n\n\n\n\nLog in to the Cloudbreak UI.\n\n\n\n\n\n\nClick \nCreate Cluster\n and the \nCreate Cluster\n form is displayed.\n\n\nTo view advanced options, click \nAdvanced\n. To learn about advanced options, refer to \nAdvanced Options\n.\n\n\n \n\n\n\n\n\n\nOn the \nGeneral Configuration\n page, specify the following general parameters for your cluster:\n\n\n\n\n\n\n\n\nParameter\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nSelect Credential\n\n\nChoose a previously created credential.\n\n\n\n\n\n\nCluster Name\n\n\nEnter a name for your cluster. The name must be between 5 and 40 characters, must start with a letter, and must only include lowercase letters, numbers, and hyphens.\n\n\n\n\n\n\nRegion\n\n\nSelect the region in which you would like to launch your cluster. For information on AWS regions, refer to \nAWS documentation\n.\n\n\n\n\n\n\nPlatform Version\n\n\nChoose the HDP version to use for this cluster.\n\n\n\n\n\n\nCluster Type\n\n\nChoose one of default cluster configurations, or, if you have defined your own cluster configuration via Ambari blueprint, you can choose it here. For more information on default and custom blueprints, refer to \nBlueprints\n.\n\n\n\n\n\n\n\n\n\n\n\n\nOn the \nHardware and Storage\n page, for each host group provide the following information to define your cluster nodes and attached storage:\n\n\n\n\n\n\n\n\nParameter\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nInstance Type\n\n\nSelect an instance type. For more information about instance types on AWS refer to \nAWS documentation\n.\n\n\n\n\n\n\nInstance Count\n\n\nEnter the number of instances of a given type. Default is 1.\n\n\n\n\n\n\nAmbari Server\n\n\nYou must select one node for Ambari Server. The \"Group Size\" for that host group must be set to \"1\".\n\n\n\n\n\n\n\n\n\n\n\n\nOn the \nNetwork\n page, provide the following to specify the networking resources that will be used for your cluster:\n\n\n\n\n\n\n\n\nParameter\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nSelect Network\n\n\nSelect the virtual network in which you would like your cluster to be provisioned. You can select an existing network or create a new network.\n\n\n\n\n\n\nSelect Subnet\n\n\nSelect the subnet in which you would like your cluster to be provisioned. You can select an existing subnet or create a new subnet.\n\n\n\n\n\n\nSubnet (CIDR)\n\n\nIf you selected to create a new subnet, you must define a valid \nCIDR\n for the subnet. Default is 10.0.0.0/16.\n\n\n\n\n\n\n\n\n\n\n\n\nOn the \nSecurity\n page, provide the following parameters:\n\n\n\n\n\n\n\n\nParameter\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nCluster User\n\n\nYou can log in to the Ambari UI using this username. By default, this is set to \nadmin\n.\n\n\n\n\n\n\nPassword\n\n\nYou can log in to the Ambari UI using this password.\n\n\n\n\n\n\nConfirm Password\n\n\nConfirm the password.\n\n\n\n\n\n\nSSH Key Pair\n\n\nSelect an existing public key. You will use the matching private key to access your cluster nodes via SSH.\n\n\n\n\n\n\nSpecify new SSH public key pair\n\n\nCheck this option to specify a new public key and then enter the public key. You will use the matching private key to access your cluster nodes via SSH.\n\n\n\n\n\n\n\n\n\n\n\n\nClick on \nCreate Cluster\n to create a cluster.\n\n\n\n\n\n\nYou will be redirected to the Cloudbreak dashboard, and a new tile representing your cluster will appear at the top of the page.\n\n\n\n\n\n\nRelated Links\n\n\nAmazon EC2 Instance Types\n (External) \n\n\nAWS Regions and Endpoints\n (External)   \n\n\nCIDR\n (External)   \n\n\nAdvanced Options\n\n\nClick on \nAdvanced\n to view and enter additional configuration options.\n\n\nAvailability Zone\n\n\nChoose one of the availability zones within the selected region. \n\n\nEnable Lifetime Management\n\n\nCheck this option if you would like your cluster to be automatically terminated after a specific amount of time (defined as \"Time to Live\" in minutes) has passed. \n\n\nTags\n\n\nYou can optionally add tags, which will help you find your cluster-related resources, such as VMs, in your cloud provider account. refer to \nResource Tagging\n.\n\n\nRelated Links\n  \n\n\nResource Tagging\n  \n\n\nStorage\n\n\nYou can optionally specify the following storage options for your cluster:\n\n\n\n\n\n\n\n\nParameter\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nStorage Type\n\n\nSelect the volume type. The options are:\nMagnetic\nEphemeral\nGeneral Purpose (SSD)\nThroughput Optimized HDD\nFor more information about these options refer to \nAWS documentation\n.\n\n\n\n\n\n\nAttached Volumes Per Instance\n\n\nEnter the number of volumes attached per instance. Default is 1.\n\n\n\n\n\n\nVolume Size (GB)\n\n\nEnter the size in GBs for each volume. Default is 100.\n\n\n\n\n\n\n\n\nRelated Links\n\n\nAmazon EC2 Instance Store\n (External)  \n\n\nRecipes\n\n\nThis option allows you to select previously uploaded recipes (scripts that will be run pre- or post- cluster deployment) for each host group. For more information on default and custom blueprints, refer to \nRecipes\n. \n\n\nRelated Links\n \n\n\nRecipes\n \n\n\nSecurity Groups\n\n\nFor each host group, select one of the options:\n\n\n\n\nCreate new security group  \n\n\nDo not use security group  \n\n\nSelect an existing security group\n\n\n\n\nIf you choose to create a new security group, the \nNew Security Group\n wizard will open.\n\n\n\n\nTCP ports 22, 443, and 9443 will be open by default. These ports must be open on every security group; otherwise Cloudbreak will not be able to communicate with your provisioned cluster.  \n\n\nYou may open additional ports by defining the \nCIDR\n, \nPort\n, and \nProtocol\n for each and clicking \nAdd Rule\n. \n\n\nOnce done, click \nSAVE\n to save your security group settings.\n\n\nOnce you define a custom security group for one host group, you can reuse this definition for other node groups.\n\n\n\n\nEnable Kerberos Security\n\n\nSelect this option to enable Kerberos for your cluster. You will have an option to create a new kerberos or use an existing one. For more information refer to \nKerberos\n documentation. \n\n\nRelated Links\n \n\n\nKerberos\n\n\n\n\nNext: Access Cluster", 
            "title": "Create a Cluster"
        }, 
        {
            "location": "/aws-create/index.html#create-a-cluster-on-aws", 
            "text": "Use these steps to create a cluster.  Steps    Log in to the Cloudbreak UI.    Click  Create Cluster  and the  Create Cluster  form is displayed.  To view advanced options, click  Advanced . To learn about advanced options, refer to  Advanced Options .       On the  General Configuration  page, specify the following general parameters for your cluster:     Parameter  Description      Select Credential  Choose a previously created credential.    Cluster Name  Enter a name for your cluster. The name must be between 5 and 40 characters, must start with a letter, and must only include lowercase letters, numbers, and hyphens.    Region  Select the region in which you would like to launch your cluster. For information on AWS regions, refer to  AWS documentation .    Platform Version  Choose the HDP version to use for this cluster.    Cluster Type  Choose one of default cluster configurations, or, if you have defined your own cluster configuration via Ambari blueprint, you can choose it here. For more information on default and custom blueprints, refer to  Blueprints .       On the  Hardware and Storage  page, for each host group provide the following information to define your cluster nodes and attached storage:     Parameter  Description      Instance Type  Select an instance type. For more information about instance types on AWS refer to  AWS documentation .    Instance Count  Enter the number of instances of a given type. Default is 1.    Ambari Server  You must select one node for Ambari Server. The \"Group Size\" for that host group must be set to \"1\".       On the  Network  page, provide the following to specify the networking resources that will be used for your cluster:     Parameter  Description      Select Network  Select the virtual network in which you would like your cluster to be provisioned. You can select an existing network or create a new network.    Select Subnet  Select the subnet in which you would like your cluster to be provisioned. You can select an existing subnet or create a new subnet.    Subnet (CIDR)  If you selected to create a new subnet, you must define a valid  CIDR  for the subnet. Default is 10.0.0.0/16.       On the  Security  page, provide the following parameters:     Parameter  Description      Cluster User  You can log in to the Ambari UI using this username. By default, this is set to  admin .    Password  You can log in to the Ambari UI using this password.    Confirm Password  Confirm the password.    SSH Key Pair  Select an existing public key. You will use the matching private key to access your cluster nodes via SSH.    Specify new SSH public key pair  Check this option to specify a new public key and then enter the public key. You will use the matching private key to access your cluster nodes via SSH.       Click on  Create Cluster  to create a cluster.    You will be redirected to the Cloudbreak dashboard, and a new tile representing your cluster will appear at the top of the page.    Related Links  Amazon EC2 Instance Types  (External)   AWS Regions and Endpoints  (External)     CIDR  (External)", 
            "title": "Create a Cluster on AWS"
        }, 
        {
            "location": "/aws-create/index.html#advanced-options", 
            "text": "Click on  Advanced  to view and enter additional configuration options.", 
            "title": "Advanced Options"
        }, 
        {
            "location": "/aws-create/index.html#availability-zone", 
            "text": "Choose one of the availability zones within the selected region.", 
            "title": "Availability Zone"
        }, 
        {
            "location": "/aws-create/index.html#enable-lifetime-management", 
            "text": "Check this option if you would like your cluster to be automatically terminated after a specific amount of time (defined as \"Time to Live\" in minutes) has passed.", 
            "title": "Enable Lifetime Management"
        }, 
        {
            "location": "/aws-create/index.html#tags", 
            "text": "You can optionally add tags, which will help you find your cluster-related resources, such as VMs, in your cloud provider account. refer to  Resource Tagging .  Related Links     Resource Tagging", 
            "title": "Tags"
        }, 
        {
            "location": "/aws-create/index.html#storage", 
            "text": "You can optionally specify the following storage options for your cluster:     Parameter  Description      Storage Type  Select the volume type. The options are: Magnetic Ephemeral General Purpose (SSD) Throughput Optimized HDD For more information about these options refer to  AWS documentation .    Attached Volumes Per Instance  Enter the number of volumes attached per instance. Default is 1.    Volume Size (GB)  Enter the size in GBs for each volume. Default is 100.     Related Links  Amazon EC2 Instance Store  (External)", 
            "title": "Storage"
        }, 
        {
            "location": "/aws-create/index.html#recipes", 
            "text": "This option allows you to select previously uploaded recipes (scripts that will be run pre- or post- cluster deployment) for each host group. For more information on default and custom blueprints, refer to  Recipes .   Related Links    Recipes", 
            "title": "Recipes"
        }, 
        {
            "location": "/aws-create/index.html#security-groups", 
            "text": "For each host group, select one of the options:   Create new security group    Do not use security group    Select an existing security group   If you choose to create a new security group, the  New Security Group  wizard will open.   TCP ports 22, 443, and 9443 will be open by default. These ports must be open on every security group; otherwise Cloudbreak will not be able to communicate with your provisioned cluster.    You may open additional ports by defining the  CIDR ,  Port , and  Protocol  for each and clicking  Add Rule .   Once done, click  SAVE  to save your security group settings.  Once you define a custom security group for one host group, you can reuse this definition for other node groups.", 
            "title": "Security Groups"
        }, 
        {
            "location": "/aws-create/index.html#enable-kerberos-security", 
            "text": "Select this option to enable Kerberos for your cluster. You will have an option to create a new kerberos or use an existing one. For more information refer to  Kerberos  documentation.   Related Links    Kerberos   Next: Access Cluster", 
            "title": "Enable Kerberos Security"
        }, 
        {
            "location": "/aws-clusters-access/index.html", 
            "text": "Access Your Cluster\n\n\nThe following section describes how to access the various services in the cluster.\n\n\nFinding Cluster Information in the UI\n\n\nOnce your cluster is up and running, click on the tile representing your cluster in the Cloudbreak UI to access information related the cluster and access cluster actions. \n\n\n \n\n\nThe information presented includes:\n\n\n\n\nCluster Summary\n\n\nCluster Information\n \n\n\nHardware\n\n\nTags\n \n\n\nEvent History\n \n\n\n\n\n\n  \nTips\n\n  \n\n  \n Access cluster actions such as resize and sync by clicking on \nACTIONS\n.\n\n  \n Access Ambari web UI by clicking on the link in the \nCLUSTER INFORMATION\n section.\n\n\n View public IP addresses for all cluster instances in the \nHARDWARE\n section. Click on the links to view the instances in the cloud console.\n\n\n The SSH user that you must use when accessing cluster VMs is \"cloudbreak\".\n \n\n\n\n\n\n\n\n\nCluster Summary\n\n\nThe summary bar includes the following information about your cluster:\n\n\n\n\n\n\n\n\nItem\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nCloud Provider\n\n\nThe logo of the cloud provider on which the cluster is running.\n\n\n\n\n\n\nCredential\n\n\nThe name of the credential used to create the cluster.\n\n\n\n\n\n\nStatus\n\n\nCurrent status. When a cluster is healthy, the status is \nRunning\n.\n\n\n\n\n\n\nNodes\n\n\nThe current number of cluster nodes, including the master node.\n\n\n\n\n\n\nUptime\n\n\nThe amount of time (HH:MM) that the cluster has been in the running state.\n\n\n\n\n\n\nCreated\n\n\nThe date when the cluster was created. The date format is Mon DD, YYYY. For exampple: Oct 27, 2017.\n\n\n\n\n\n\n\n\nCluster Information\n\n\n\n\n\n\n\n\nItem\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nNetwork\n\n\nThe name of the network in which the cluster is running.\n\n\n\n\n\n\nSubnet\n\n\nThe name of the subnet in which the cluster is running.\n\n\n\n\n\n\nCluster User\n\n\nThe name of the cluster user that you created when creating the cluster.\n\n\n\n\n\n\nSSH Username\n\n\nThe SSH user which you must use when accessing cluster VMs via SSH. The SSH user is always \"cloudbreak\".\n\n\n\n\n\n\nAmbari URL\n\n\nLink to the Ambari web UI.\n\n\n\n\n\n\nRegion\n\n\nThe region in which the cluster is running in the cloud provider infrastructure.\n\n\n\n\n\n\nAvailability Zone\n\n\nThe availability zone within the region in which the cluster is running.\n\n\n\n\n\n\nBlueprint\n\n\nThe name of the blueprint selected under \"Cluster Type\" to create this cluster.\n\n\n\n\n\n\nStarted With\n\n\nThe version of Cloubdreak used to create this cluster.\n\n\n\n\n\n\nAmbari Version\n\n\nThe Ambari version which this cluster is currently running.\n\n\n\n\n\n\nHDP Version\n\n\nThe HDP version which this cluster is currently running.\n\n\n\n\n\n\n\n\nHardware\n\n\nThis section includes information about your cluster nodes: instance names, instance IDs (with links to the cloud provider console), and public IPs.\n\n\nTags\n\n\nThis section lists user-defined tags, in the same order as you added them.\n\n\nEvent History\n\n\nThe Event History section shows you events logged for the cluster, with the most recent event at the top. For example, after your cluster has been created, the following messages will be written to the log:\n\n\n\nAmbari cluster built; Ambari ip:34.215.103.66\n10/26/2017, 9:41:58 AM\nBuilding Ambari cluster; Ambari ip:34.215.103.66\n10/26/2017, 9:30:20 AM\nStarting Ambari cluster services\n10/26/2017, 9:27:12 AM\nSetting up infrastructure metadata\n10/26/2017, 9:27:11 AM\nBootstrapping infrastructure cluster\n10/26/2017, 9:26:38 AM\nInfrastructure successfully provisioned\n10/26/2017, 9:26:37 AM\nBilling started, Infrastructure successfully provisioned\n10/26/2017, 9:26:37 AM\nInfrastructure metadata collection finished\n10/26/2017, 9:25:39 AM\nInfrastructure creation took 194 seconds\n10/26/2017, 9:25:37 AM\nCreating infrastructure\n10/26/2017, 9:22:22 AM\nSetting up HDP image\n10/26/2017, 9:22:21 AM\n\n\n\nAccess Cluster via SSH\n\n\nIf you plan to access the cluster via the command line clients, SSH into the master node instance in the cluster. \n\n\n\n\nIn order to use SSH, you must generate an SSH key pair or use an existing SSH keypair.  \n\n\nYou can find the cluster instance public IP addresses on the cluster details page.  \n\n\nWhen accessing instances via SSH use the \ncloudbreak\n user. \n\n\n\n\nOn Mac OS, you can use the following syntax to SSH to the VM:\n\nssh -i \"privatekey.pem\" cloudbreak@publicIP\n\nFor example:\n\nssh -i \"dominika-kp.pem\" cloudbreak@p52.25.169.132\n\n\nOn Windows, you can SSH using an SSH client such as PuTTY.\n\n\nAccess Ambari\n\n\nYou can access Ambari web UI by clicking on the links provided in the \nCluster Information\n \n \nAmbari URL\n.\n\n\nSteps\n\n\n\n\n\n\nFrom the cluster dashboard, click on the tile representing your cluster to navigate to cluster details.\n\n\n\n\n\n\nFind the Ambairi URL in the \nCluster Information\n section. This URL is available once the Ambari cluster creation process has completed.  \n\n\n\n\n\n\nClick on the \nAmbari URL\n link.\n\n\n\n\n\n\nThe first time you access the server, your browser will attempt to confirm that the SSL Certificate is valid. Since Cloudbreak automatically generates a self-signed certificate, your browser will warn you about an Untrusted Connection and ask you to confirm a Security Exception. Depending on your browser, perform the steps below to proceed.\n\n\n\n\n\n\n\n\nBrowser\n\n\nSteps\n\n\n\n\n\n\n\n\n\n\nFirefox\n\n\nClick \nAdvanced\n \n Click \nAdd Exception...\n \n Click \nConfirm Security Exception\n\n\n\n\n\n\nSafari\n\n\nClick \nContinue\n\n\n\n\n\n\nChrome\n\n\nClick \nAdvanced\n \n Click \nProceed...\n\n\n\n\n\n\n\n\n\n\n\n\nUser Accounts\n\n\nThe following table describes what credentials to use to access Cloudbreak and Cloudbreak-managed clusters:\n\n\n\n\n\n\n\n\nComponent\n\n\nMethod\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nCloudbreak\n\n\nWeb UI, CLI\n\n\nAccess with the username and password provided when launching Cloudbreak on the cloud provider.\n\n\n\n\n\n\nCloudbreak\n\n\nSSH to VM\n\n\nAccess as the \"cloudbreak\" user with the SSH key provided when launching Cloudbreak on the cloud provider.\n\n\n\n\n\n\nCluster\n\n\nSSH to VMs\n\n\nAccess as the \"cloudbreak\" user with the SSH key provided during cluster creation.\n\n\n\n\n\n\nCluster\n\n\nAmbari UI\n\n\nAccess with the credentials provided in the \u201cCluster User\u201d parameter during cluster creation.\n\n\n\n\n\n\n\n\n\n\nNext: Manage and Monitor Clusters", 
            "title": "Access Cluster"
        }, 
        {
            "location": "/aws-clusters-access/index.html#access-your-cluster", 
            "text": "The following section describes how to access the various services in the cluster.", 
            "title": "Access Your Cluster"
        }, 
        {
            "location": "/aws-clusters-access/index.html#finding-cluster-information-in-the-ui", 
            "text": "Once your cluster is up and running, click on the tile representing your cluster in the Cloudbreak UI to access information related the cluster and access cluster actions.      The information presented includes:   Cluster Summary  Cluster Information    Hardware  Tags    Event History     \n   Tips \n   \n    Access cluster actions such as resize and sync by clicking on  ACTIONS . \n    Access Ambari web UI by clicking on the link in the  CLUSTER INFORMATION  section.   View public IP addresses for all cluster instances in the  HARDWARE  section. Click on the links to view the instances in the cloud console.   The SSH user that you must use when accessing cluster VMs is \"cloudbreak\".", 
            "title": "Finding Cluster Information in the UI"
        }, 
        {
            "location": "/aws-clusters-access/index.html#cluster-summary", 
            "text": "The summary bar includes the following information about your cluster:     Item  Description      Cloud Provider  The logo of the cloud provider on which the cluster is running.    Credential  The name of the credential used to create the cluster.    Status  Current status. When a cluster is healthy, the status is  Running .    Nodes  The current number of cluster nodes, including the master node.    Uptime  The amount of time (HH:MM) that the cluster has been in the running state.    Created  The date when the cluster was created. The date format is Mon DD, YYYY. For exampple: Oct 27, 2017.", 
            "title": "Cluster Summary"
        }, 
        {
            "location": "/aws-clusters-access/index.html#cluster-information", 
            "text": "Item  Description      Network  The name of the network in which the cluster is running.    Subnet  The name of the subnet in which the cluster is running.    Cluster User  The name of the cluster user that you created when creating the cluster.    SSH Username  The SSH user which you must use when accessing cluster VMs via SSH. The SSH user is always \"cloudbreak\".    Ambari URL  Link to the Ambari web UI.    Region  The region in which the cluster is running in the cloud provider infrastructure.    Availability Zone  The availability zone within the region in which the cluster is running.    Blueprint  The name of the blueprint selected under \"Cluster Type\" to create this cluster.    Started With  The version of Cloubdreak used to create this cluster.    Ambari Version  The Ambari version which this cluster is currently running.    HDP Version  The HDP version which this cluster is currently running.", 
            "title": "Cluster Information"
        }, 
        {
            "location": "/aws-clusters-access/index.html#hardware", 
            "text": "This section includes information about your cluster nodes: instance names, instance IDs (with links to the cloud provider console), and public IPs.", 
            "title": "Hardware"
        }, 
        {
            "location": "/aws-clusters-access/index.html#tags", 
            "text": "This section lists user-defined tags, in the same order as you added them.", 
            "title": "Tags"
        }, 
        {
            "location": "/aws-clusters-access/index.html#event-history", 
            "text": "The Event History section shows you events logged for the cluster, with the most recent event at the top. For example, after your cluster has been created, the following messages will be written to the log:  \nAmbari cluster built; Ambari ip:34.215.103.66\n10/26/2017, 9:41:58 AM\nBuilding Ambari cluster; Ambari ip:34.215.103.66\n10/26/2017, 9:30:20 AM\nStarting Ambari cluster services\n10/26/2017, 9:27:12 AM\nSetting up infrastructure metadata\n10/26/2017, 9:27:11 AM\nBootstrapping infrastructure cluster\n10/26/2017, 9:26:38 AM\nInfrastructure successfully provisioned\n10/26/2017, 9:26:37 AM\nBilling started, Infrastructure successfully provisioned\n10/26/2017, 9:26:37 AM\nInfrastructure metadata collection finished\n10/26/2017, 9:25:39 AM\nInfrastructure creation took 194 seconds\n10/26/2017, 9:25:37 AM\nCreating infrastructure\n10/26/2017, 9:22:22 AM\nSetting up HDP image\n10/26/2017, 9:22:21 AM", 
            "title": "Event History"
        }, 
        {
            "location": "/aws-clusters-access/index.html#access-cluster-via-ssh", 
            "text": "If you plan to access the cluster via the command line clients, SSH into the master node instance in the cluster.    In order to use SSH, you must generate an SSH key pair or use an existing SSH keypair.    You can find the cluster instance public IP addresses on the cluster details page.    When accessing instances via SSH use the  cloudbreak  user.    On Mac OS, you can use the following syntax to SSH to the VM: ssh -i \"privatekey.pem\" cloudbreak@publicIP \nFor example: ssh -i \"dominika-kp.pem\" cloudbreak@p52.25.169.132  On Windows, you can SSH using an SSH client such as PuTTY.", 
            "title": "Access Cluster via SSH"
        }, 
        {
            "location": "/aws-clusters-access/index.html#access-ambari", 
            "text": "You can access Ambari web UI by clicking on the links provided in the  Cluster Information     Ambari URL .  Steps    From the cluster dashboard, click on the tile representing your cluster to navigate to cluster details.    Find the Ambairi URL in the  Cluster Information  section. This URL is available once the Ambari cluster creation process has completed.      Click on the  Ambari URL  link.    The first time you access the server, your browser will attempt to confirm that the SSL Certificate is valid. Since Cloudbreak automatically generates a self-signed certificate, your browser will warn you about an Untrusted Connection and ask you to confirm a Security Exception. Depending on your browser, perform the steps below to proceed.     Browser  Steps      Firefox  Click  Advanced    Click  Add Exception...    Click  Confirm Security Exception    Safari  Click  Continue    Chrome  Click  Advanced    Click  Proceed...", 
            "title": "Access Ambari"
        }, 
        {
            "location": "/aws-clusters-access/index.html#user-accounts", 
            "text": "The following table describes what credentials to use to access Cloudbreak and Cloudbreak-managed clusters:     Component  Method  Description      Cloudbreak  Web UI, CLI  Access with the username and password provided when launching Cloudbreak on the cloud provider.    Cloudbreak  SSH to VM  Access as the \"cloudbreak\" user with the SSH key provided when launching Cloudbreak on the cloud provider.    Cluster  SSH to VMs  Access as the \"cloudbreak\" user with the SSH key provided during cluster creation.    Cluster  Ambari UI  Access with the credentials provided in the \u201cCluster User\u201d parameter during cluster creation.      Next: Manage and Monitor Clusters", 
            "title": "User Accounts"
        }, 
        {
            "location": "/aws-clusters-manage/index.html", 
            "text": "Manage and Monitor Clusters\n\n\nYou can manage monitor your clusters from the Cloudbreak UI. To do that, click on the tile representing the cluster that you want to access. The actions available for your cluster are listed in the top right corner: \n\n\n \n\n\n\n  \nTips\n\n  \n\n  \nTo add or remove nodes from your cluster click \nACTIONS>Resize\n.\n\n  \nTo synchronize your cluster with the cloud provider account click \nACTIONS>Sync\n.\n\n  \nTo temporarily stop your cluster click \nSTOP\n.\n\n  \nTo terminate your cluster click \nTERMINATE\n.\n\n\n\n\n\n\n\n\n\nResize Cluster\n\n\nTo resize a cluster, follow these steps.\n\n\nSteps\n\n\n\n\n\n\nBrowse to the cluster details.\n\n\n\n\n\n\nClick \nActions\n and select \nResize\n. The cluster resize dialog is displayed.\n\n\n\n\n\n\nUsing the +/- controls, adjust how many nodes to add or remove from each host group.  \n\n\n\n\n\n\nClick \nYes\n to confirm the scale-up/scale-down.\n\n\nWhile nodes are being added or removed, cluster status changes to \"Update In Progress\". Once the operation has completed, cluster status changes back to \"Running\". Messages similar to the following are written to the \"Event History\", : \n\n\nAmbari cluster scaled up\n11/6/2017, 12:33:40 PM\nScaling up the Ambari cluster\n11/6/2017, 12:26:59 PM\nStack successfully upscaled\n11/6/2017, 12:26:54 PM\nBootstrapping new nodes\n11/6/2017, 12:26:16 PM\nInfrastructure metadata extension finished\n11/6/2017, 12:26:10 PM\nBilling changed due to upscaling of cluster infrastructure\n11/6/2017, 12:26:10 PM\nAdding 1 new instances to the infrastructure\n11/6/2017, 12:25:23 PM\n\n\n\n\n\n\nSynchronize with Cloud Provider\n\n\nIf you have just made changes on your cloud provider side (for example, deleted cluster VMs) and you would like to synchronize Cloudbreak with the cloud provider, use the \nsync\n option. \n\n\nTo synchronize your cluster with the cloud provider, follow these steps. \n\n\nSteps\n\n\n\n\n\n\nBrowse to the cluster details.\n\n\n\n\n\n\nClick \nActions\n and select \nSync\n.\n\n\n\n\n\n\nClick \nYes\n to confirm.\n\n\nYour cluster infrastructure is synchronized based on changes on the cloud provider. The updates are written to the \"Event History\". \n\n\n\n\n\n\nStop Cluster\n\n\nCloudbreak supports stopping and restarting clusters. To stop and restart a cluster managed by Cloudbreak, use the options available from the Coudbreak UI. \n\n\nSteps\n\n\n\n\n\n\nBrowse to the cluster details.\n\n\n\n\n\n\nClick \nStop\n to stop a currently running cluster.  \n\n\n\n\n\n\nClick \nYes\n to confirm. \n\n\nYour cluster status changes to \"Stopping in progress\" and then to \"Stopped\". You should see the following messages in the \"Event History\":\n\n\nBilling stopped, the cluster and its infrastructure have been terminated\n11/6/2017, 12:46:48 PM\nInfrastructure successfully stopped\n11/6/2017, 12:46:47 PM\nInfrastructure is now stopping\n11/6/2017, 12:43:58 PM\nAmbari cluster stopped\n11/6/2017, 12:43:52 PM\nAmbari services have been stopped.\n11/6/2017, 12:43:49 PM\nStopping Ambari services.\n11/6/2017, 12:42:14 PM\nStopping Ambari cluster\n11/6/2017, 12:42:10 PM\nCluster infrastructure stop requested\n11/6/2017, 12:42:06 PM\n\n\n\n\n\n\nOnce stopping the infrastructure has completed, you will see a \nStart\n option to restart your cluster. \n\n\nRestart Cluster\n\n\nIf your cluster is in the \"Stopped\" state, you can restart the cluster by follow these steps.\n\n\nSteps\n\n\n\n\n\n\nclick \nStart\n. This option is only available when the cluster has been stopped. \n\n\n\n\n\n\nClick \nYes\n to confirm.\n\n\nYour cluster status changes to \"Start in progress\" and then to \"Running\". You should see the following messages in the \"Event History\":\n\n\nAmbari cluster started; Ambari ip:35.203.149.236\n11/6/2017, 1:02:13 PM\nAmbari services have been started.\n11/6/2017, 1:02:08 PM\nStarting Ambari services.\n11/6/2017, 12:54:37 PM\nStarting Ambari cluster\n11/6/2017, 12:52:57 PM\nBilling started, the cluster and its infrastructure have successfully been started\n11/6/2017, 12:52:48 PM\nInfrastructure successfully started\n11/6/2017, 12:52:48 PM\nInfrastructure is now starting\n11/6/2017, 12:52:26 PM\nAmbari cluster start requested\n11/6/2017, 12:52:24 PM\n\n\n\n\n\n\nTerminate Cluster\n\n\nTo terminate a cluster managed by Cloudbreak, use the option available from the Coudbreak UI. \n\n\nSteps\n\n\n\n\n\n\nBrowse to the cluster details.\n\n\n\n\n\n\nClick \nTerminate\n. \n\n\n\n\n\n\nClick \nYes\n to confirm.\n\n\nAll cluster-related resources will be deleted, unless the resources (such as networks and subnets) existed prior to cluster creation or are used by other VMs in which case they will be preserved. \n\n\n\n\n\n\nForce Terminate\n\n\nCluster deletion may fail if Cloudbreak is unable to delete one or more of the cloud resources that were part of your cluster infrastructure. In such as case, you can use the \nTerminate\n \n \nForce terminate\n option to remove the cluster entry from the Cloudbreak web UI, but you must also check your cloud provider account to see if there are any resources that must be deleted manually.\n\n\nSteps\n\n\n\n\n\n\nBrowse to the cluster details.\n\n\n\n\n\n\nClick \nTerminate\n. \n\n\n\n\n\n\nCheck  \nForce terminate\n.\n\n\n\n\n\n\nClick \nYes\n to confirm. \n\n\n\n\n\n\nThis deletes the cluster tile from the UI.  \n\n\n\n\n\n\nLog in to your cloud provider account and \nmanually delete\n any resources that failed to be deleted.\n\n\n\n\n\n\nView Cluster History\n\n\nFrom the navigation menu in the Cloudbreak UI, you can access the History page that allows you to generate a report showing basic information related to the clusters that were running within the specified range of dates.\n\n\nTo generate a report, follow these steps.\n\n\nSteps\n\n\n\n\n\n\nFrom the Cloudbreak UI navigation menu, select \nHistory\n.\n\n\n\n\n\n\nOn the History page, select the range of dates and click \nShow History\n to generate a report for the selected period.\n\n\n\n\n\n\nHistory Report Content\n\n\nEach entry in the report represents one cluster instance group. For each entry, the report includes the following information:\n\n\n\n\nCreated\n - The date when your cluster was created (YYYY-MM-DD).\n\n\nProvider\n - The name of the cloud provider (AWS, Azure, Google, or OpenStack) on which the cluster instances are/were running.\n\n\nCluster Name\n - The name that you selected for the cluster.\n\n\nWorker Count\n - The number of worker nodes in the cluster. This number may be a decimal if a cluster has been resized.\n\n\nInstance Type\n - Provider-specific VM type of the cluster instances.\n\n\nInstance Group\n - The name of the instance group.  \n\n\nRegion\n - The AWS region in which your cluster is running.\n\n\nRunning Time (hours)\n - The sum of the running times for all the nodes in the instance group.\n\n\n\n\nThe \nAGGREGATE RUNNING TIME\n is the sum of the Running Times, adjusted for the selected time range.\n\n\nTo learn about how your cloud provider bills you for the VMs, refer to their documentation:\n\n\n\n\nAWS\n      \n\n\nAzure\n     \n\n\nGCP\n   \n\n\n\n\n\n\nNext: Access Data", 
            "title": "Manage and Monitor Clusters"
        }, 
        {
            "location": "/aws-clusters-manage/index.html#manage-and-monitor-clusters", 
            "text": "You can manage monitor your clusters from the Cloudbreak UI. To do that, click on the tile representing the cluster that you want to access. The actions available for your cluster are listed in the top right corner:      \n   Tips \n   \n   To add or remove nodes from your cluster click  ACTIONS>Resize . \n   To synchronize your cluster with the cloud provider account click  ACTIONS>Sync . \n   To temporarily stop your cluster click  STOP . \n   To terminate your cluster click  TERMINATE .", 
            "title": "Manage and Monitor Clusters"
        }, 
        {
            "location": "/aws-clusters-manage/index.html#resize-cluster", 
            "text": "To resize a cluster, follow these steps.  Steps    Browse to the cluster details.    Click  Actions  and select  Resize . The cluster resize dialog is displayed.    Using the +/- controls, adjust how many nodes to add or remove from each host group.      Click  Yes  to confirm the scale-up/scale-down.  While nodes are being added or removed, cluster status changes to \"Update In Progress\". Once the operation has completed, cluster status changes back to \"Running\". Messages similar to the following are written to the \"Event History\", :   Ambari cluster scaled up\n11/6/2017, 12:33:40 PM\nScaling up the Ambari cluster\n11/6/2017, 12:26:59 PM\nStack successfully upscaled\n11/6/2017, 12:26:54 PM\nBootstrapping new nodes\n11/6/2017, 12:26:16 PM\nInfrastructure metadata extension finished\n11/6/2017, 12:26:10 PM\nBilling changed due to upscaling of cluster infrastructure\n11/6/2017, 12:26:10 PM\nAdding 1 new instances to the infrastructure\n11/6/2017, 12:25:23 PM", 
            "title": "Resize Cluster"
        }, 
        {
            "location": "/aws-clusters-manage/index.html#synchronize-with-cloud-provider", 
            "text": "If you have just made changes on your cloud provider side (for example, deleted cluster VMs) and you would like to synchronize Cloudbreak with the cloud provider, use the  sync  option.   To synchronize your cluster with the cloud provider, follow these steps.   Steps    Browse to the cluster details.    Click  Actions  and select  Sync .    Click  Yes  to confirm.  Your cluster infrastructure is synchronized based on changes on the cloud provider. The updates are written to the \"Event History\".", 
            "title": "Synchronize with Cloud Provider"
        }, 
        {
            "location": "/aws-clusters-manage/index.html#stop-cluster", 
            "text": "Cloudbreak supports stopping and restarting clusters. To stop and restart a cluster managed by Cloudbreak, use the options available from the Coudbreak UI.   Steps    Browse to the cluster details.    Click  Stop  to stop a currently running cluster.      Click  Yes  to confirm.   Your cluster status changes to \"Stopping in progress\" and then to \"Stopped\". You should see the following messages in the \"Event History\":  Billing stopped, the cluster and its infrastructure have been terminated\n11/6/2017, 12:46:48 PM\nInfrastructure successfully stopped\n11/6/2017, 12:46:47 PM\nInfrastructure is now stopping\n11/6/2017, 12:43:58 PM\nAmbari cluster stopped\n11/6/2017, 12:43:52 PM\nAmbari services have been stopped.\n11/6/2017, 12:43:49 PM\nStopping Ambari services.\n11/6/2017, 12:42:14 PM\nStopping Ambari cluster\n11/6/2017, 12:42:10 PM\nCluster infrastructure stop requested\n11/6/2017, 12:42:06 PM    Once stopping the infrastructure has completed, you will see a  Start  option to restart your cluster.", 
            "title": "Stop Cluster"
        }, 
        {
            "location": "/aws-clusters-manage/index.html#restart-cluster", 
            "text": "If your cluster is in the \"Stopped\" state, you can restart the cluster by follow these steps.  Steps    click  Start . This option is only available when the cluster has been stopped.     Click  Yes  to confirm.  Your cluster status changes to \"Start in progress\" and then to \"Running\". You should see the following messages in the \"Event History\":  Ambari cluster started; Ambari ip:35.203.149.236\n11/6/2017, 1:02:13 PM\nAmbari services have been started.\n11/6/2017, 1:02:08 PM\nStarting Ambari services.\n11/6/2017, 12:54:37 PM\nStarting Ambari cluster\n11/6/2017, 12:52:57 PM\nBilling started, the cluster and its infrastructure have successfully been started\n11/6/2017, 12:52:48 PM\nInfrastructure successfully started\n11/6/2017, 12:52:48 PM\nInfrastructure is now starting\n11/6/2017, 12:52:26 PM\nAmbari cluster start requested\n11/6/2017, 12:52:24 PM", 
            "title": "Restart Cluster"
        }, 
        {
            "location": "/aws-clusters-manage/index.html#terminate-cluster", 
            "text": "To terminate a cluster managed by Cloudbreak, use the option available from the Coudbreak UI.   Steps    Browse to the cluster details.    Click  Terminate .     Click  Yes  to confirm.  All cluster-related resources will be deleted, unless the resources (such as networks and subnets) existed prior to cluster creation or are used by other VMs in which case they will be preserved.", 
            "title": "Terminate Cluster"
        }, 
        {
            "location": "/aws-clusters-manage/index.html#force-terminate", 
            "text": "Cluster deletion may fail if Cloudbreak is unable to delete one or more of the cloud resources that were part of your cluster infrastructure. In such as case, you can use the  Terminate     Force terminate  option to remove the cluster entry from the Cloudbreak web UI, but you must also check your cloud provider account to see if there are any resources that must be deleted manually.  Steps    Browse to the cluster details.    Click  Terminate .     Check   Force terminate .    Click  Yes  to confirm.     This deletes the cluster tile from the UI.      Log in to your cloud provider account and  manually delete  any resources that failed to be deleted.", 
            "title": "Force Terminate"
        }, 
        {
            "location": "/aws-clusters-manage/index.html#view-cluster-history", 
            "text": "From the navigation menu in the Cloudbreak UI, you can access the History page that allows you to generate a report showing basic information related to the clusters that were running within the specified range of dates.  To generate a report, follow these steps.  Steps    From the Cloudbreak UI navigation menu, select  History .    On the History page, select the range of dates and click  Show History  to generate a report for the selected period.", 
            "title": "View Cluster History"
        }, 
        {
            "location": "/aws-clusters-manage/index.html#history-report-content", 
            "text": "Each entry in the report represents one cluster instance group. For each entry, the report includes the following information:   Created  - The date when your cluster was created (YYYY-MM-DD).  Provider  - The name of the cloud provider (AWS, Azure, Google, or OpenStack) on which the cluster instances are/were running.  Cluster Name  - The name that you selected for the cluster.  Worker Count  - The number of worker nodes in the cluster. This number may be a decimal if a cluster has been resized.  Instance Type  - Provider-specific VM type of the cluster instances.  Instance Group  - The name of the instance group.    Region  - The AWS region in which your cluster is running.  Running Time (hours)  - The sum of the running times for all the nodes in the instance group.   The  AGGREGATE RUNNING TIME  is the sum of the Running Times, adjusted for the selected time range.  To learn about how your cloud provider bills you for the VMs, refer to their documentation:   AWS         Azure        GCP        Next: Access Data", 
            "title": "History Report Content"
        }, 
        {
            "location": "/aws-data/index.html", 
            "text": "Access Data on S3\n\n\nPrerequisites\n\n\nTo use S3 storage, you must have one or more S3 buckets on your AWS account. For instructions on how to create a bucket on S3, refer to \nAWS documentation\n.\n\n\nRelated Links\n\n\nCreate a Bucket\n (External)    \n\n\nConfiguring Access to S3\n\n\nAmazon S3 is not supported as a default file system, but access to data in Amazon S3 via the s3a connector. To configure access to Amazon S3 from a cluster crated via Cloudbreak, declare your access key and secret key in a configuration file such as \ncore-site.xml\n:\n\n\nproperty\n\n  \nname\nfs.s3a.access.key\n/name\n\n  \nvalue\nACCESS-KEY\n/value\n\n\n/property\n\n\n\nproperty\n\n  \nname\nfs.s3a.secret.key\n/name\n\n  \nvalue\nSECRET-KEY\n/value\n\n\n/property\n\n\n\n\n\nTesting Access to S3\n\n\nTo tests access to S3, SSH to a cluster node and run a few hadoop fs shell commands against your existing S3 bucket.\n\n\nAmazon S3 access path syntax is:\n\n\ns3a://bucket/dir/file\n\n\n\nFor example, to access a file called \"mytestfile\" in a directory called \"mytestdir\", which is stored in a bucket called \"mytestbucket\", the URL is:\n\n\ns3a://mytestbucket/mytestdir/mytestfile\n\n\n\nThe following FileSystem shell commands demonstrate access to a bucket named \"mytestbucket\": \n\n\nhadoop fs -ls s3a://mytestbucket/\n\nhadoop fs -mkdir s3a://mytestbucket/testDir\n\nhadoop fs -put testFile s3a://mytestbucket/testFile\n\nhadoop fs -cat s3a://mytestbucket/testFile\ntest file content\n\n\n\nLearn More\n\n\nFor more information about configuring the S3 connector and working with data stored on S3, refer to \nCloud Data Access\n documentation.\n\n\nRelated Links\n\n\nCloud Data Access\n (Hortonworks)", 
            "title": "Access Data on S3"
        }, 
        {
            "location": "/aws-data/index.html#access-data-on-s3", 
            "text": "", 
            "title": "Access Data on S3"
        }, 
        {
            "location": "/aws-data/index.html#prerequisites", 
            "text": "To use S3 storage, you must have one or more S3 buckets on your AWS account. For instructions on how to create a bucket on S3, refer to  AWS documentation .  Related Links  Create a Bucket  (External)", 
            "title": "Prerequisites"
        }, 
        {
            "location": "/aws-data/index.html#configuring-access-to-s3", 
            "text": "Amazon S3 is not supported as a default file system, but access to data in Amazon S3 via the s3a connector. To configure access to Amazon S3 from a cluster crated via Cloudbreak, declare your access key and secret key in a configuration file such as  core-site.xml :  property \n   name fs.s3a.access.key /name \n   value ACCESS-KEY /value  /property  property \n   name fs.s3a.secret.key /name \n   value SECRET-KEY /value  /property", 
            "title": "Configuring Access to S3"
        }, 
        {
            "location": "/aws-data/index.html#testing-access-to-s3", 
            "text": "To tests access to S3, SSH to a cluster node and run a few hadoop fs shell commands against your existing S3 bucket.  Amazon S3 access path syntax is:  s3a://bucket/dir/file  For example, to access a file called \"mytestfile\" in a directory called \"mytestdir\", which is stored in a bucket called \"mytestbucket\", the URL is:  s3a://mytestbucket/mytestdir/mytestfile  The following FileSystem shell commands demonstrate access to a bucket named \"mytestbucket\":   hadoop fs -ls s3a://mytestbucket/\n\nhadoop fs -mkdir s3a://mytestbucket/testDir\n\nhadoop fs -put testFile s3a://mytestbucket/testFile\n\nhadoop fs -cat s3a://mytestbucket/testFile\ntest file content", 
            "title": "Testing Access to S3"
        }, 
        {
            "location": "/aws-data/index.html#learn-more", 
            "text": "For more information about configuring the S3 connector and working with data stored on S3, refer to  Cloud Data Access  documentation.  Related Links  Cloud Data Access  (Hortonworks)", 
            "title": "Learn More"
        }, 
        {
            "location": "/azure-launch/index.html", 
            "text": "Launch Cloudbreak on Azure\n\n\nBefore launching Cloudbreak on Azure, review and meet the prerequisites. Next, launch Cloudbreak using one of the two available methods. Once Cloudbreak is running, log in to the Cloudbreak UI and create a Cloudbreak credential. \n\n\nMeet the Prerequisites\n\n\nBefore launching Cloudbreak on Azure, you must meet the following prerequisites.\n\n\nAzure Account\n\n\nIn order to launch Cloudbreak on the Azure, log in to your existing Microsoft Azure account. If you don't have an account, you can set it up at \nhttps://azure.microsoft.com\n.\n\n\nAzure Roles\n\n\nIn order to provision clusters on Azure, Cloudbreak must be able to assume a sufficient Azure role (\"Owner\" or \"Contributor\") via Cloudbreak credential: \n\n\n\n\n\n\nYour account must have the \"\nOwner\n\" role (or a role with equivalent permissions) in the subscription in order to \ncreate a Cloudbreak credential\n using the interactive credential method. \n\n\n\n\n\n\nYour account must have the \"\nContributor\n\" role (or a role with equivalent permissions) in the subscription in order to \ncreate a Cloudbreak credential\n using the app-based credential method. The role must be assigned to the app that you register in Cloudbreak.\n\n\n\n\n\n\nTo check the roles in your subscription, log in to your Azure account and navigate to \nSubscriptions\n. \n\n\nRelated Links\n\n\nBuilt-in Roles: Owner\n (External)\n\n\nBuilt-in Roles: Contributor\n (External)\n\n\nAzure Region\n\n\nDecide in which Azure region you would like to launch Cloudbreak. You can launch Cloudbreak and provision your clusters in all regions \nsupported by Microsoft Azure\n. \n\n\nClusters created via Cloudbreak can be in the same or different region as Cloudbreak; when you launch a cluster, you select the region in which to launch it.\n\n\nRelated Links\n\n\nAzure Regions\n (External) \n\n\nSSH Key Pair\n\n\nWhen launching Cloudbreak, you will be required to provide your public SSH key. If needed, you can generate a new SSH keypair:\n\n\n\n\nOn MacOS X and Linux using \nssh-keygen -t rsa -b 4096 -C \"your_email@example.com\"\n  \n\n\nOn Windows using \nPuTTygen\n\n\n\n\nLaunch Cloudbreak\n\n\nLaunch Cloudbreak deployer using the following steps.\n\n\nSteps\n\n\n\n\n\n\nLog in to your \nAzure Portal\n.\n\n\n\n\n\n\nClick here to get started with Cloudbreak installation using the Azure Resource Manager template:\n\n\n \n\n\n\n\n\n\nThe template for installing Cloudbreak will appear. On the \nBasics\n page, provide the following basic parameters:   \n\n\nBASICS\n\n\n\n\n\n\n\n\nParameter\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nSubscription\n\n\n(Required) Select which existing subscription you want to use.\n\n\n\n\n\n\nResource group\n\n\n(Required) Select an existing resource group or create a new one by selecting \nCreate new\n and entering a name for your new resource group. Cloudbreak resources will later be accessible in that chosen resource group.\n\n\n\n\n\n\nLocation\n\n\n(Required) Select an Azure region in which you want to deploy Cloudbreak.\n\n\n\n\n\n\n\n\nSETTINGS\n\n\n\n\n\n\n\n\nParameter\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nBase Url\n\n\nThis is the URL to the page where the template is stored.\n\n\n\n\n\n\nLocation\n\n\nThis is an internal parameter. Do not change it.\n\n\n\n\n\n\nVM Size\n\n\nSelect virtual machine instance type to use for the Cloudbreak controller. The minimum instance type suitable for Cloudbreak is \nD2\n.\n\n\n\n\n\n\nAdmin Username\n\n\nCreate an admin login that you will use to log in to the Cloudbreak UI. Must be a valid email address.\n\n\n\n\n\n\nAdmin User Password\n\n\n(Required) Password for the admin login. Must be at least 8 characters containing letters, numbers, and symbols.\n\n\n\n\n\n\nUsername\n\n\nEnter an admin username for the virtual machine. You will use it to SSH to the VM.\n\n\n\n\n\n\nSmartSense\n\n\nSelect whether you want to use SmartSense telemetry. Default is \"false\" (not using SmartSense telemetry).\n\n\n\n\n\n\nRemote Location\n\n\nEnter a valid \nCIDR IP\n or use one of the default tags. Default value is \nInternet\n which allows access from all IP addresses. Examples: \n10.0.0.0/24 will allow access from 10.0.0.0 through 10.0.0.255\n'Internet' will allow access from all. This is not a secure option but you can use it it you are just getting started and are not planning to have the instance on for a longer period. \n(Advanced) 'VirtualNetwork' will allow access from the address space of the Virtual Network.\n (Advanced) 'AzureLoadBalancer' will allow access from the address space of the load balancer.\nFor more information, refer to the \nAzure documentation\n.\n\n\n\n\n\n\nSsh Key\n\n\n(Required) Paste your SSH public key.\nYou can use \npbcopy\n to quickly copy it. For example: \npbcopy \n /Users/homedir/.ssh/id_rsa.pub\n\n\n\n\n\n\nVnet New Or Existing\n\n\nBy default, Cloudbreak is launched in a new VNet called \ncbdeployerVnet\n and a new subnet called \ncbdeployerSubnet\n; if needed, you can customize the settings for the new VNet using available VNet and Subnet parameters.\n\n\n\n\n\n\nVnet Name\n\n\nProvide the name for a new Vnet. Default is \n`cbdeployerVnet\n.\n\n\n\n\n\n\nVnet Subnet Name\n\n\nProvide a name for a new subnet. Default is \ncbdeployerSubnet\n.\n\n\n\n\n\n\nVnet Address Prefix\n\n\nProvide a CIDR for the virtual network. Default is \n10.0.0.0/16\n.\n\n\n\n\n\n\nVnet Subnet Address Prefix\n\n\nProvide a CIDR for the subnet. Default is \n10.0.0.0/24\n.\n\n\n\n\n\n\nVnet RG Name\n\n\nThe name of the resource group in which the Vnet is located. If creating a new Vnet, enter the same resource group name as provided in the \nResource group\n field in the \nBASICS\n section.\n\n\n\n\n\n\n\n\n\n\n\n\nReview terms of use and check \"I agree to the terms and conditions stated above\". \n\n\n\n\n\n\nClick \nPurchase\n.\n\n\n\n\n\n\nProceed to the next step: \nExplore Newly Created Resources\n.\n\n\n\n\n\n\nRelated Links\n\n\nCIDR IP\n (External) \n\n\nFilter Network Traffic with Network Security Groups\n (External)  \n\n\nExplore Newly Created Resources\n\n\n\n\nThis step is optional.  \n\n\n\n\nWhile the deployment is in progress, you can optionally navigate to the newly created resource group and see what Azure resources are being created.\n\n\nSteps\n\n\n\n\n\n\nFrom the left pane, select \nResource groups\n.\n\n\n\n\n\n\nFind the the resource group that you just created and select it to view details.\n\n\n\n\n\n\nThe following resources should have been created in your resource group:\n\n\n\n\n\n\nIf you chose to use an existing virtual network, the virtual network will not be added to the resource group. \n\n\n\n\n\n\nVirtual network\n (VNet) securely connects Azure resources to each other.    \n\n\nNetwork security group\n (NSG) defines inbound and outbound security rules, which control network traffic flow.  \n\n\nVirtual machine\n runs Cloudbreak.   \n\n\nPublic IP address\n is assigned to your VM so that it can communicate with other Azure resources.  \n\n\nNetwork interface\n (NIC) attached to the VM provides the interconnection between the VM and the underlying software network.    \n\n\nBlob storage container\n is created to store Cloudbreak Deployer OS disk's data.  \n\n\n\n\n\n\n\n\nYou can click on each entry to view details of the resource. For example, click on \ncbdeployerVM\n to view details, including Cloudbreak IP address.\n\n\n\n\n\n\nOnce your deployment is ready, the status will change from \"Deploying\" to \"Success\".\n\n\n\n\n\n\nAccess Cloudbreak UI\n\n\nLog in to the Cloudbreak UI using the following steps.\n\n\nSteps\n\n\n\n\n\n\nWhen your deployment succeeds, you will receive a notification in the top-right corner. You can click on the link provided to navigate to the resource group created earlier.\n\n\n\n\nThis only works right after deployment. At other times, you can find your resource group by selecting \nResource Groups\n from the service menu and then finding your resource group by name.\n\n\n\n\n\n\n\n\nOnce you've navigated to your resource group, click on \nDeployments\n and then click on \nhortonworks.cloudbreal-for-hortonworks-data-platf-...\n:\n\n\n \n\n\n\n\n\n\nFrom \nOutputs\n, you can copy the link by clicking on the \n icon:\n\n\n   \n\n\n\n\n\n\nPaste the link in your browser's address bar.\n\n\nThe first time you access Cloudbreak UI, Cloudbreak will automatically generate a self-signed certificate, due to which your browser will warn you about an untrusted connection and will ask you to confirm a security exception. You can safely proceed to the website. \n\n\n\n\n\n\n\n\nBrowser\n\n\nSteps\n\n\n\n\n\n\n\n\n\n\nFirefox\n\n\nClick \nAdvanced\n \n Click \nAdd Exception...\n \n Click \nConfirm Security Exception\n\n\n\n\n\n\nSafari\n\n\nClick \nContinue\n\n\n\n\n\n\nChrome\n\n\nClick \nAdvanced\n \n Click \nProceed...\n\n\n\n\n\n\n\n\n\n\n\n\nNow you should be able to access Cloudbreak UI and log in with the \nAdmin email address\n and \nAdmin password\n that you created when launching Cloudbreak:\n\n\n\n\n\n\n\n\nUpon a successful login, you are redirected to the dashboard:\n\n\n  \n\n\n\n\n\n\nThe last task that you need to perform before you can use Cloudbreak is to \ncreate a cloudbreak credential\n.         \n\n\nCreate Cloudbreak Credential\n\n\nBefore you can start creating clusters, you must first create a \nCloudbreak credential\n. Without this credential, you will not be able to create clusters via Cloudbreak. Cloudbreak works by connecting your Azure account through this credential, and then uses it to create resources on your behalf.\n\n\nThere are two methods for creating a Cloudbreak credential:\n\n\n\n\n\n\n\n\nMethod\n\n\nDescription\n\n\nPrerequisite\n\n\nSteps\n\n\n\n\n\n\n\n\n\n\nInteractive\n\n\nThe advantage of using this method is that the app and service principal creation and role assignment are fully automated, so the only input that you need to provide is the Subscription ID and Directory ID. During the interactive credential creation, you must log in to your Azure account.\n\n\nYour account must have the \"Owner\" role (or its equivalent) in the subscription.\n\n\nTo configure an interactive credential, refer to \nCreate an Interactive Credential\n.\n\n\n\n\n\n\nApp-based\n\n\nThe advantage of the app-based credential creation is that it allows you to create a credential without logging in to the Azure account, as long as you have been given all the information. In addition to providing your Subscription ID and  Directory ID, you must provide information for your previously created Azure AD application (its ID and key which allows access to it).\n\n\nYour account must have the \"Contributor\" role (or equivalent) in the subscription. The role must be assigned to the app that you register in Cloudbreak.\n\n\nTo configure an app based credential, refer to \nCreate an App Based Credential\n.\n\n\n\n\n\n\n\n\nCreate an Interactive Credential\n\n\nFollow these steps to create an interactive Cloudbreak credential.\n\n\nSteps\n\n\n\n\n\n\nIn the Cloudbreak web UI, select \nCredentials\n from the navigation pane. \n\n\n\n\n\n\nClick \nCreate Credential\n. \n\n\n\n\n\n\nUnder \nCloud provider\n, select \"Microsoft Azure\". \n\n\n\n\n\n\nSelect \nInteractive Login\n:\n\n\n     \n\n\n\n\n\n\nProvide the following information:\n\n\n\n\n\n\n\n\nParameter\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nName\n\n\nEnter a name for your credential.\n\n\n\n\n\n\nDescription\n\n\n(Optional) Enter a description.\n\n\n\n\n\n\nSubscription Id\n\n\nCopy and paste the Subscription ID from your \nSubscriptions\n.\n\n\n\n\n\n\nTenant Id\n\n\nCopy and paste your Directory ID from your \nActive Directory\n \n \nProperties\n.\n\n\n\n\n\n\nAzure role type\n\n\nYou have the following options:\n\"Use existing Contributor role\" (default): If you select this option, Cloudbreak will use the \"\nContributor\n\" role to create resources. This requires no further input.\n\"Reuse existing custom role\": If you select this option and enter the name of an existing role, Cloudbreak will use this role to create resources.\n\"Let Cloudbreak create a custom role\": If you select this option and enter a name for the new role, the role will be created. When choosing role name, make sure that there is no existing role with the name chosen. For information on creating custom roles, refer to \nAzure\n documentation. \nIf using a custom role, make sure that it includes the necessary Action set for Cloudbreak to be able to manage clusters: \nMicrosoft.Compute/*\n, \nMicrosoft.Network/*\n, \nMicrosoft.Storage/*\n, \nMicrosoft.Resources/*\n.\n\n\n\n\n\n\n\n\nTo obtain the \nSubscription Id\n: \n\n\n   \n\n\nTo obtain the \nTenant ID\n (actually \nDirectory Id\n): \n\n\n    \n\n\n\n\n\n\nAfter providing the parameters, click \nInteractive Login\n.\n\n\n\n\n\n\nCopy the code provided in the UI:\n\n\n     \n\n\n\n\n\n\nClick \nAzure login\n and a new \nDevice login\n page will open in a new browser tab:\n\n\n  \n\n\n\n\n\n\nNext, paste the code in field on the  \nDevice login\n page and click \nContinue\n.\n\n\n\n\n\n\nConfirm your account by selecting it:\n\n\n\n\n\n\n\n\nA confirmation page will appear, confirming that you have signed in to the Microsoft Azure Cross-platform Command Line Interface application on your device. You may now close this window.\n\n\nCongratulations! You've successfully launched and configured Cloudbreak. Now you can use Cloudbreak to \ncreate clusters\n.\n\n\n\n\n\n\nCreate an App Based Credential\n\n\nFollow these steps to create an app based Cloudbreak credential.\n\n\nSteps\n\n\n\n\n\n\nOn Azure Portal, navigate to the \nActive Directory\n \n \nApp Registrations\n and register a new application. For more information, refer to \nCreate an Azure AD Application\n.\n\n\n\n\nAa an alternative to the steps listed below for creating an application registration, you use a utility called \nazure-cli-tools\n. The utility supports app creation and role assignment. It is available at \nhttps://github.com/sequenceiq/azure-cli-tools/blob/master/cli_tools\n.\n\n\n\n\n  \n\n\n\n\n\n\nNavigate to the \nSubscriptions\n, choose \nAccess control (IAM)\n. Click \nAdd\n and then assign the \"Contributor\" role to your newly created application by selecting \"Contributor\" under \nRole\n and your app name under \nSelect\n:\n\n\n   \n\n\n\n\n\n\nIn the Cloudbreak web UI, select \nCredentials\n from the navigation pane. \n\n\n\n\n\n\nClick \nCreate Credential\n. \n\n\n\n\n\n\nUnder \nCloud provider\n, select \"Microsoft Azure\". \n\n\n\n\n\n\nSelect \nApp based Login\n:\n\n\n \n\n\n\n\n\n\nOn the \nConfigure credential\n page, provide the following parameters:\n\n\n\n\n\n\n\n\nParameter\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nSelect Credential Type\n\n\nSelect \nApp based\n.\n\n\n\n\n\n\nName\n\n\nEnter a name for your credential.\n\n\n\n\n\n\nDescription\n\n\n(Optional) Enter a description.\n\n\n\n\n\n\nSubscription Id\n\n\nCopy and paste the Subscription ID from your \nSubscriptions\n.\n\n\n\n\n\n\nTenant Id\n\n\nCopy and paste your Directory ID from your \nActive Directory\n \n \nProperties\n.\n\n\n\n\n\n\nApp Id\n\n\nCopy and paste the Application ID from your \nAzure Active Directory\n \n \nApp Registrations\n \n your app registration's \nSettings\n \n \nProperties\n.\n\n\n\n\n\n\nPassword\n\n\nThis is your application key. You can generate it from your from your \nAzure Active Directory\n app registration's \nSettings\n \n \nKeys\n.\n\n\n\n\n\n\n\n\nTo obtain the \nSubscription Id\n from Subscriptions: \n\n\n   \n\n\nTo obtain the \nApp ID\n (actually \nApplication ID\n) and an application key from Azure Active Directory: \n\n\n  \n\n\n      \n\n\nTo obtain the \nTenant ID\n (actually \nDirectory Id\n) from Azure Active Directory: \n\n\n  \n\n\n\n\n\n\nClick \nCreate\n.\n\n\nCongratulations! You've successfully launched and configured Cloudbreak. Now you can use Cloudbreak to \ncreate clusters\n.\n\n\n\n\n\n\nRelated Links\n\n\nCLI Tools\n (Hortonworks)  \n\n\nUse Portal to Create an Azure Active Directory Application\n (External)     \n\n\n\n\nNext: Create a Cluster", 
            "title": "Launch on Azure"
        }, 
        {
            "location": "/azure-launch/index.html#launch-cloudbreak-on-azure", 
            "text": "Before launching Cloudbreak on Azure, review and meet the prerequisites. Next, launch Cloudbreak using one of the two available methods. Once Cloudbreak is running, log in to the Cloudbreak UI and create a Cloudbreak credential.", 
            "title": "Launch Cloudbreak on Azure"
        }, 
        {
            "location": "/azure-launch/index.html#meet-the-prerequisites", 
            "text": "Before launching Cloudbreak on Azure, you must meet the following prerequisites.", 
            "title": "Meet the Prerequisites"
        }, 
        {
            "location": "/azure-launch/index.html#azure-account", 
            "text": "In order to launch Cloudbreak on the Azure, log in to your existing Microsoft Azure account. If you don't have an account, you can set it up at  https://azure.microsoft.com .", 
            "title": "Azure Account"
        }, 
        {
            "location": "/azure-launch/index.html#azure-roles", 
            "text": "In order to provision clusters on Azure, Cloudbreak must be able to assume a sufficient Azure role (\"Owner\" or \"Contributor\") via Cloudbreak credential:     Your account must have the \" Owner \" role (or a role with equivalent permissions) in the subscription in order to  create a Cloudbreak credential  using the interactive credential method.     Your account must have the \" Contributor \" role (or a role with equivalent permissions) in the subscription in order to  create a Cloudbreak credential  using the app-based credential method. The role must be assigned to the app that you register in Cloudbreak.    To check the roles in your subscription, log in to your Azure account and navigate to  Subscriptions .   Related Links  Built-in Roles: Owner  (External)  Built-in Roles: Contributor  (External)", 
            "title": "Azure Roles"
        }, 
        {
            "location": "/azure-launch/index.html#azure-region", 
            "text": "Decide in which Azure region you would like to launch Cloudbreak. You can launch Cloudbreak and provision your clusters in all regions  supported by Microsoft Azure .   Clusters created via Cloudbreak can be in the same or different region as Cloudbreak; when you launch a cluster, you select the region in which to launch it.  Related Links  Azure Regions  (External)", 
            "title": "Azure Region"
        }, 
        {
            "location": "/azure-launch/index.html#ssh-key-pair", 
            "text": "When launching Cloudbreak, you will be required to provide your public SSH key. If needed, you can generate a new SSH keypair:   On MacOS X and Linux using  ssh-keygen -t rsa -b 4096 -C \"your_email@example.com\"     On Windows using  PuTTygen", 
            "title": "SSH Key Pair"
        }, 
        {
            "location": "/azure-launch/index.html#launch-cloudbreak", 
            "text": "Launch Cloudbreak deployer using the following steps.  Steps    Log in to your  Azure Portal .    Click here to get started with Cloudbreak installation using the Azure Resource Manager template:       The template for installing Cloudbreak will appear. On the  Basics  page, provide the following basic parameters:     BASICS     Parameter  Description      Subscription  (Required) Select which existing subscription you want to use.    Resource group  (Required) Select an existing resource group or create a new one by selecting  Create new  and entering a name for your new resource group. Cloudbreak resources will later be accessible in that chosen resource group.    Location  (Required) Select an Azure region in which you want to deploy Cloudbreak.     SETTINGS     Parameter  Description      Base Url  This is the URL to the page where the template is stored.    Location  This is an internal parameter. Do not change it.    VM Size  Select virtual machine instance type to use for the Cloudbreak controller. The minimum instance type suitable for Cloudbreak is  D2 .    Admin Username  Create an admin login that you will use to log in to the Cloudbreak UI. Must be a valid email address.    Admin User Password  (Required) Password for the admin login. Must be at least 8 characters containing letters, numbers, and symbols.    Username  Enter an admin username for the virtual machine. You will use it to SSH to the VM.    SmartSense  Select whether you want to use SmartSense telemetry. Default is \"false\" (not using SmartSense telemetry).    Remote Location  Enter a valid  CIDR IP  or use one of the default tags. Default value is  Internet  which allows access from all IP addresses. Examples:  10.0.0.0/24 will allow access from 10.0.0.0 through 10.0.0.255 'Internet' will allow access from all. This is not a secure option but you can use it it you are just getting started and are not planning to have the instance on for a longer period.  (Advanced) 'VirtualNetwork' will allow access from the address space of the Virtual Network.  (Advanced) 'AzureLoadBalancer' will allow access from the address space of the load balancer. For more information, refer to the  Azure documentation .    Ssh Key  (Required) Paste your SSH public key. You can use  pbcopy  to quickly copy it. For example:  pbcopy   /Users/homedir/.ssh/id_rsa.pub    Vnet New Or Existing  By default, Cloudbreak is launched in a new VNet called  cbdeployerVnet  and a new subnet called  cbdeployerSubnet ; if needed, you can customize the settings for the new VNet using available VNet and Subnet parameters.    Vnet Name  Provide the name for a new Vnet. Default is  `cbdeployerVnet .    Vnet Subnet Name  Provide a name for a new subnet. Default is  cbdeployerSubnet .    Vnet Address Prefix  Provide a CIDR for the virtual network. Default is  10.0.0.0/16 .    Vnet Subnet Address Prefix  Provide a CIDR for the subnet. Default is  10.0.0.0/24 .    Vnet RG Name  The name of the resource group in which the Vnet is located. If creating a new Vnet, enter the same resource group name as provided in the  Resource group  field in the  BASICS  section.       Review terms of use and check \"I agree to the terms and conditions stated above\".     Click  Purchase .    Proceed to the next step:  Explore Newly Created Resources .    Related Links  CIDR IP  (External)   Filter Network Traffic with Network Security Groups  (External)", 
            "title": "Launch Cloudbreak"
        }, 
        {
            "location": "/azure-launch/index.html#explore-newly-created-resources", 
            "text": "This step is optional.     While the deployment is in progress, you can optionally navigate to the newly created resource group and see what Azure resources are being created.  Steps    From the left pane, select  Resource groups .    Find the the resource group that you just created and select it to view details.    The following resources should have been created in your resource group:    If you chose to use an existing virtual network, the virtual network will not be added to the resource group.     Virtual network  (VNet) securely connects Azure resources to each other.      Network security group  (NSG) defines inbound and outbound security rules, which control network traffic flow.    Virtual machine  runs Cloudbreak.     Public IP address  is assigned to your VM so that it can communicate with other Azure resources.    Network interface  (NIC) attached to the VM provides the interconnection between the VM and the underlying software network.      Blob storage container  is created to store Cloudbreak Deployer OS disk's data.       You can click on each entry to view details of the resource. For example, click on  cbdeployerVM  to view details, including Cloudbreak IP address.    Once your deployment is ready, the status will change from \"Deploying\" to \"Success\".", 
            "title": "Explore Newly Created Resources"
        }, 
        {
            "location": "/azure-launch/index.html#access-cloudbreak-ui", 
            "text": "Log in to the Cloudbreak UI using the following steps.  Steps    When your deployment succeeds, you will receive a notification in the top-right corner. You can click on the link provided to navigate to the resource group created earlier.   This only works right after deployment. At other times, you can find your resource group by selecting  Resource Groups  from the service menu and then finding your resource group by name.     Once you've navigated to your resource group, click on  Deployments  and then click on  hortonworks.cloudbreal-for-hortonworks-data-platf-... :       From  Outputs , you can copy the link by clicking on the   icon:         Paste the link in your browser's address bar.  The first time you access Cloudbreak UI, Cloudbreak will automatically generate a self-signed certificate, due to which your browser will warn you about an untrusted connection and will ask you to confirm a security exception. You can safely proceed to the website.      Browser  Steps      Firefox  Click  Advanced    Click  Add Exception...    Click  Confirm Security Exception    Safari  Click  Continue    Chrome  Click  Advanced    Click  Proceed...       Now you should be able to access Cloudbreak UI and log in with the  Admin email address  and  Admin password  that you created when launching Cloudbreak:     Upon a successful login, you are redirected to the dashboard:        The last task that you need to perform before you can use Cloudbreak is to  create a cloudbreak credential .", 
            "title": "Access Cloudbreak UI"
        }, 
        {
            "location": "/azure-launch/index.html#create-cloudbreak-credential", 
            "text": "Before you can start creating clusters, you must first create a  Cloudbreak credential . Without this credential, you will not be able to create clusters via Cloudbreak. Cloudbreak works by connecting your Azure account through this credential, and then uses it to create resources on your behalf.  There are two methods for creating a Cloudbreak credential:     Method  Description  Prerequisite  Steps      Interactive  The advantage of using this method is that the app and service principal creation and role assignment are fully automated, so the only input that you need to provide is the Subscription ID and Directory ID. During the interactive credential creation, you must log in to your Azure account.  Your account must have the \"Owner\" role (or its equivalent) in the subscription.  To configure an interactive credential, refer to  Create an Interactive Credential .    App-based  The advantage of the app-based credential creation is that it allows you to create a credential without logging in to the Azure account, as long as you have been given all the information. In addition to providing your Subscription ID and  Directory ID, you must provide information for your previously created Azure AD application (its ID and key which allows access to it).  Your account must have the \"Contributor\" role (or equivalent) in the subscription. The role must be assigned to the app that you register in Cloudbreak.  To configure an app based credential, refer to  Create an App Based Credential .", 
            "title": "Create Cloudbreak Credential"
        }, 
        {
            "location": "/azure-launch/index.html#create-an-interactive-credential", 
            "text": "Follow these steps to create an interactive Cloudbreak credential.  Steps    In the Cloudbreak web UI, select  Credentials  from the navigation pane.     Click  Create Credential .     Under  Cloud provider , select \"Microsoft Azure\".     Select  Interactive Login :           Provide the following information:     Parameter  Description      Name  Enter a name for your credential.    Description  (Optional) Enter a description.    Subscription Id  Copy and paste the Subscription ID from your  Subscriptions .    Tenant Id  Copy and paste your Directory ID from your  Active Directory     Properties .    Azure role type  You have the following options: \"Use existing Contributor role\" (default): If you select this option, Cloudbreak will use the \" Contributor \" role to create resources. This requires no further input. \"Reuse existing custom role\": If you select this option and enter the name of an existing role, Cloudbreak will use this role to create resources. \"Let Cloudbreak create a custom role\": If you select this option and enter a name for the new role, the role will be created. When choosing role name, make sure that there is no existing role with the name chosen. For information on creating custom roles, refer to  Azure  documentation.  If using a custom role, make sure that it includes the necessary Action set for Cloudbreak to be able to manage clusters:  Microsoft.Compute/* ,  Microsoft.Network/* ,  Microsoft.Storage/* ,  Microsoft.Resources/* .     To obtain the  Subscription Id :        To obtain the  Tenant ID  (actually  Directory Id ):           After providing the parameters, click  Interactive Login .    Copy the code provided in the UI:           Click  Azure login  and a new  Device login  page will open in a new browser tab:        Next, paste the code in field on the   Device login  page and click  Continue .    Confirm your account by selecting it:     A confirmation page will appear, confirming that you have signed in to the Microsoft Azure Cross-platform Command Line Interface application on your device. You may now close this window.  Congratulations! You've successfully launched and configured Cloudbreak. Now you can use Cloudbreak to  create clusters .", 
            "title": "Create an Interactive Credential"
        }, 
        {
            "location": "/azure-launch/index.html#create-an-app-based-credential", 
            "text": "Follow these steps to create an app based Cloudbreak credential.  Steps    On Azure Portal, navigate to the  Active Directory     App Registrations  and register a new application. For more information, refer to  Create an Azure AD Application .   Aa an alternative to the steps listed below for creating an application registration, you use a utility called  azure-cli-tools . The utility supports app creation and role assignment. It is available at  https://github.com/sequenceiq/azure-cli-tools/blob/master/cli_tools .         Navigate to the  Subscriptions , choose  Access control (IAM) . Click  Add  and then assign the \"Contributor\" role to your newly created application by selecting \"Contributor\" under  Role  and your app name under  Select :         In the Cloudbreak web UI, select  Credentials  from the navigation pane.     Click  Create Credential .     Under  Cloud provider , select \"Microsoft Azure\".     Select  App based Login :       On the  Configure credential  page, provide the following parameters:     Parameter  Description      Select Credential Type  Select  App based .    Name  Enter a name for your credential.    Description  (Optional) Enter a description.    Subscription Id  Copy and paste the Subscription ID from your  Subscriptions .    Tenant Id  Copy and paste your Directory ID from your  Active Directory     Properties .    App Id  Copy and paste the Application ID from your  Azure Active Directory     App Registrations    your app registration's  Settings     Properties .    Password  This is your application key. You can generate it from your from your  Azure Active Directory  app registration's  Settings     Keys .     To obtain the  Subscription Id  from Subscriptions:        To obtain the  App ID  (actually  Application ID ) and an application key from Azure Active Directory:               To obtain the  Tenant ID  (actually  Directory Id ) from Azure Active Directory:         Click  Create .  Congratulations! You've successfully launched and configured Cloudbreak. Now you can use Cloudbreak to  create clusters .    Related Links  CLI Tools  (Hortonworks)    Use Portal to Create an Azure Active Directory Application  (External)        Next: Create a Cluster", 
            "title": "Create an App Based Credential"
        }, 
        {
            "location": "/azure-create/index.html", 
            "text": "Create a Cluster on Azure\n\n\nUse these steps to create a cluster.\n\n\nSteps\n\n\n\n\n\n\nLog in to the Cloudbreak UI.\n\n\n\n\n\n\nClick \nCreate Cluster\n and the \nCreate Cluster\n form is displayed.\n\n\nTo view advanced options, click \nAdvanced\n. To learn about advanced options, refer to \nAdvanced Options\n.\n\n\n \n\n\n\n\n\n\nOn the \nGeneral Configuration\n page, specify the following general parameters for your cluster:\n\n\n\n\n\n\n\n\nParameter\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nSelect Credential\n\n\nChoose a previously created credential.\n\n\n\n\n\n\nCluster Name\n\n\nEnter a name for your cluster. The name must be between 5 and 40 characters, must start with a letter, and must only include lowercase letters, numbers, and hyphens.\n\n\n\n\n\n\nRegion\n\n\nSelect the region in which you would like to launch your cluster.\n\n\n\n\n\n\nPlatform Version\n\n\nChoose the HDP version to use for this cluster.\n\n\n\n\n\n\nCluster Type\n\n\nChoose one of default cluster configurations, or, if you have defined your own cluster configuration via Ambari blueprint, you can choose it here. For more information, refer to \nBlueprints\n.\n\n\n\n\n\n\n\n\n\n\n\n\nOn the \nHardware and Storage\n page, for each host group  provide the following information to define your cluster nodes and attached storage:\n\n\n\n\n\n\n\n\nParameter\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nInstance Type\n\n\nSelect an instance type. For more information about instance types on Azure refer to \nAzure documentation\n.\n\n\n\n\n\n\nInstance Count\n\n\nEnter the number of instances of a given type. Default is 1.\n\n\n\n\n\n\nAmbari Server\n\n\nYou must select one node for Ambari Server. The \"Group Size\" for that host group must be set to \"1\".\n\n\n\n\n\n\n\n\n\n\n\n\nOn the \nNetwork\n page, provide the following to specify the networking resources that will be used for your cluster:\n\n\n\n\n\n\n\n\nParameter\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nSelect Network\n\n\nSelect the virtual network in which you would like your cluster to be provisioned. You can select an existing network or create a new network.\n\n\n\n\n\n\nSelect Subnet\n\n\nSelect the subnet in which you would like your cluster to be provisioned. You can select an existing subnet or create a new subnet.\n\n\n\n\n\n\nSubnet (CIDR)\n\n\nIf you selected to create a new subnet, you must define a valid \nCIDR\n for the subnet. Default is 10.0.0.0/16.\n\n\n\n\n\n\n\n\n\n\n\n\nOn the \nSecurity\n page, provide the following parameters:\n\n\n\n\n\n\n\n\nParameter\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nCluster User\n\n\nYou can log in to the Ambari UI using this username. By default, this is set to \nadmin\n.\n\n\n\n\n\n\nPassword\n\n\nYou can log in to the Ambari UI using this password.\n\n\n\n\n\n\nConfirm Password\n\n\nConfirm the password.\n\n\n\n\n\n\nSSH Key Pair\n\n\nSpecify a public SSH key. You will use the matching private key to access your cluster nodes via SSH.\n\n\n\n\n\n\n\n\n\n\n\n\nClick on \nCreate Cluster\n to create a cluster.\n\n\n\n\n\n\nYou will be redirected to the Cloudbreak dashboard, and a new tile representing your cluster will appear at the top of the page.\n\n\n\n\n\n\nRelated Links\n\n\nBlueprints\n\n\nCIDR\n (External)\n\n\nGeneral Purpose Linux VM Sizes\n (External)  \n\n\nAdvanced Options\n\n\nClick on \nAdvanced\n to view and enter additional configuration options.\n\n\nEnable Lifetime Management\n\n\nCheck this option if you would like your cluster to be automatically terminated after a specific amount of time (defined as \"Time to Live\" in minutes) has passed. \n\n\nTags\n\n\nYou can optionally add tags, which will help you find your cluster-related resources, such as VMs, in your cloud provider account. refer to \nResource Tagging\n.\n\n\nRelated Links\n\n\nResource Tagging\n  \n\n\nStorage\n\n\nYou can optionally specify the following storage options for your cluster: \n\n\n\n\n\n\n\n\nParameter\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nStorage Type\n\n\nSelect the volume type. The options are:\nLocally-redundant storage\nGeo-redundant storage\nPremium locally-redundant storage\n For more information about these options refer to \nAzure documentation\n.\n\n\n\n\n\n\nAttached Volumes Per Instance\n\n\nEnter the number of volumes attached per instance. Default is 1.\n\n\n\n\n\n\nVolume Size (GB)\n\n\nEnter the size in GBs for each volume. Default is 100.\n\n\n\n\n\n\n\n\nRelated Links\n\n\nIntroduction to Microsoft Azure Storage\n (External)  \n\n\nRecipes\n\n\nThis option allows you to select previously uploaded recipes (scripts that will be run pre- or post- cluster deployment) for each host group. For more information on default and custom blueprints, refer to \nRecipes\n.\n\n\nRelated Links\n\n\nRecipes\n\n\nSecurity Groups\n\n\nFor each host group, select one of the options:\n\n\n\n\nCreate new security group  \n\n\nDo not use security group  \n\n\nSelect an existing security group  \n\n\n\n\nIf you choose to create a new security group, the \nNew Security Group\n wizard will open.\n\n\n\n\nTCP ports 22, 443, and 9443 will be open by default. These ports must be open on every security group; otherwise Cloudbreak will not be able to communicate with your provisioned cluster.  \n\n\nYou may open additional ports by defining the \nCIDR\n, \nPort\n, and \nProtocol\n for each and clicking \nAdd Rule\n. \n\n\nOnce done, click \nSAVE\n to save your security group settings.\n\n\nOnce you define a custom security group for one host group, you can reuse this definition for other node groups.\n\n\n\n\nEnable Kerberos Security\n\n\nSelect this option to enable Kerberos for your cluster. You will have an option to create a new kerberos or use an existing one. For more information refer to \nKerberos\n documentation. |\n\n\n\n\nNext: Access Cluster", 
            "title": "Create a Cluster"
        }, 
        {
            "location": "/azure-create/index.html#create-a-cluster-on-azure", 
            "text": "Use these steps to create a cluster.  Steps    Log in to the Cloudbreak UI.    Click  Create Cluster  and the  Create Cluster  form is displayed.  To view advanced options, click  Advanced . To learn about advanced options, refer to  Advanced Options .       On the  General Configuration  page, specify the following general parameters for your cluster:     Parameter  Description      Select Credential  Choose a previously created credential.    Cluster Name  Enter a name for your cluster. The name must be between 5 and 40 characters, must start with a letter, and must only include lowercase letters, numbers, and hyphens.    Region  Select the region in which you would like to launch your cluster.    Platform Version  Choose the HDP version to use for this cluster.    Cluster Type  Choose one of default cluster configurations, or, if you have defined your own cluster configuration via Ambari blueprint, you can choose it here. For more information, refer to  Blueprints .       On the  Hardware and Storage  page, for each host group  provide the following information to define your cluster nodes and attached storage:     Parameter  Description      Instance Type  Select an instance type. For more information about instance types on Azure refer to  Azure documentation .    Instance Count  Enter the number of instances of a given type. Default is 1.    Ambari Server  You must select one node for Ambari Server. The \"Group Size\" for that host group must be set to \"1\".       On the  Network  page, provide the following to specify the networking resources that will be used for your cluster:     Parameter  Description      Select Network  Select the virtual network in which you would like your cluster to be provisioned. You can select an existing network or create a new network.    Select Subnet  Select the subnet in which you would like your cluster to be provisioned. You can select an existing subnet or create a new subnet.    Subnet (CIDR)  If you selected to create a new subnet, you must define a valid  CIDR  for the subnet. Default is 10.0.0.0/16.       On the  Security  page, provide the following parameters:     Parameter  Description      Cluster User  You can log in to the Ambari UI using this username. By default, this is set to  admin .    Password  You can log in to the Ambari UI using this password.    Confirm Password  Confirm the password.    SSH Key Pair  Specify a public SSH key. You will use the matching private key to access your cluster nodes via SSH.       Click on  Create Cluster  to create a cluster.    You will be redirected to the Cloudbreak dashboard, and a new tile representing your cluster will appear at the top of the page.    Related Links  Blueprints  CIDR  (External)  General Purpose Linux VM Sizes  (External)", 
            "title": "Create a Cluster on Azure"
        }, 
        {
            "location": "/azure-create/index.html#advanced-options", 
            "text": "Click on  Advanced  to view and enter additional configuration options.", 
            "title": "Advanced Options"
        }, 
        {
            "location": "/azure-create/index.html#enable-lifetime-management", 
            "text": "Check this option if you would like your cluster to be automatically terminated after a specific amount of time (defined as \"Time to Live\" in minutes) has passed.", 
            "title": "Enable Lifetime Management"
        }, 
        {
            "location": "/azure-create/index.html#tags", 
            "text": "You can optionally add tags, which will help you find your cluster-related resources, such as VMs, in your cloud provider account. refer to  Resource Tagging .  Related Links  Resource Tagging", 
            "title": "Tags"
        }, 
        {
            "location": "/azure-create/index.html#storage", 
            "text": "You can optionally specify the following storage options for your cluster:      Parameter  Description      Storage Type  Select the volume type. The options are: Locally-redundant storage Geo-redundant storage Premium locally-redundant storage  For more information about these options refer to  Azure documentation .    Attached Volumes Per Instance  Enter the number of volumes attached per instance. Default is 1.    Volume Size (GB)  Enter the size in GBs for each volume. Default is 100.     Related Links  Introduction to Microsoft Azure Storage  (External)", 
            "title": "Storage"
        }, 
        {
            "location": "/azure-create/index.html#recipes", 
            "text": "This option allows you to select previously uploaded recipes (scripts that will be run pre- or post- cluster deployment) for each host group. For more information on default and custom blueprints, refer to  Recipes .  Related Links  Recipes", 
            "title": "Recipes"
        }, 
        {
            "location": "/azure-create/index.html#security-groups", 
            "text": "For each host group, select one of the options:   Create new security group    Do not use security group    Select an existing security group     If you choose to create a new security group, the  New Security Group  wizard will open.   TCP ports 22, 443, and 9443 will be open by default. These ports must be open on every security group; otherwise Cloudbreak will not be able to communicate with your provisioned cluster.    You may open additional ports by defining the  CIDR ,  Port , and  Protocol  for each and clicking  Add Rule .   Once done, click  SAVE  to save your security group settings.  Once you define a custom security group for one host group, you can reuse this definition for other node groups.", 
            "title": "Security Groups"
        }, 
        {
            "location": "/azure-create/index.html#enable-kerberos-security", 
            "text": "Select this option to enable Kerberos for your cluster. You will have an option to create a new kerberos or use an existing one. For more information refer to  Kerberos  documentation. |   Next: Access Cluster", 
            "title": "Enable Kerberos Security"
        }, 
        {
            "location": "/azure-clusters-access/index.html", 
            "text": "Access Your Cluster\n\n\nThe following section describes how to access the various services in the cluster.\n\n\nFinding Cluster Information in the UI\n\n\nOnce your cluster is up and running, click on the tile representing your cluster in the Cloudbreak UI to access information related the cluster and access cluster actions. \n\n\n \n\n\nThe information presented includes:\n\n\n\n\nCluster Summary\n\n\nCluster Information\n \n\n\nHardware\n\n\nTags\n \n\n\nEvent History\n \n\n\n\n\n\n  \nTips\n\n  \n\n  \n Access cluster actions such as resize and sync by clicking on \nACTIONS\n.\n\n  \n Access Ambari web UI by clicking on the link in the \nCLUSTER INFORMATION\n section.\n\n\n View public IP addresses for all cluster instances in the \nHARDWARE\n section. Click on the links to view the instances in the cloud console.\n\n\n The SSH user that you must use when accessing cluster VMs is \"cloudbreak\".\n \n\n\n\n\n\n\n\n\nCluster Summary\n\n\nThe summary bar includes the following information about your cluster:\n\n\n\n\n\n\n\n\nItem\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nCloud Provider\n\n\nThe logo of the cloud provider on which the cluster is running.\n\n\n\n\n\n\nCredential\n\n\nThe name of the credential used to create the cluster.\n\n\n\n\n\n\nStatus\n\n\nCurrent status. When a cluster is healthy, the status is \nRunning\n.\n\n\n\n\n\n\nNodes\n\n\nThe current number of cluster nodes, including the master node.\n\n\n\n\n\n\nUptime\n\n\nThe amount of time (HH:MM) that the cluster has been in the running state.\n\n\n\n\n\n\nCreated\n\n\nThe date when the cluster was created. The date format is Mon DD, YYYY. For exampple: Oct 27, 2017.\n\n\n\n\n\n\n\n\nCluster Information\n\n\n\n\n\n\n\n\nItem\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nNetwork\n\n\nThe name of the network in which the cluster is running.\n\n\n\n\n\n\nSubnet\n\n\nThe name of the subnet in which the cluster is running.\n\n\n\n\n\n\nCluster User\n\n\nThe name of the cluster user that you created when creating the cluster.\n\n\n\n\n\n\nSSH Username\n\n\nThe SSH user which you must use when accessing cluster VMs via SSH. The SSH user is always \"cloudbreak\".\n\n\n\n\n\n\nAmbari URL\n\n\nLink to the Ambari web UI.\n\n\n\n\n\n\nRegion\n\n\nThe region in which the cluster is running in the cloud provider infrastructure.\n\n\n\n\n\n\nAvailability Zone\n\n\nThe availability zone within the region in which the cluster is running.\n\n\n\n\n\n\nBlueprint\n\n\nThe name of the blueprint selected under \"Cluster Type\" to create this cluster.\n\n\n\n\n\n\nStarted With\n\n\nThe version of Cloubdreak used to create this cluster.\n\n\n\n\n\n\nAmbari Version\n\n\nThe Ambari version which this cluster is currently running.\n\n\n\n\n\n\nHDP Version\n\n\nThe HDP version which this cluster is currently running.\n\n\n\n\n\n\n\n\nHardware\n\n\nThis section includes information about your cluster nodes: instance names, instance IDs (with links to the cloud provider console), and public IPs.\n\n\nTags\n\n\nThis section lists user-defined tags, in the same order as you added them.\n\n\nEvent History\n\n\nThe Event History section shows you events logged for the cluster, with the most recent event at the top. For example, after your cluster has been created, the following messages will be written to the log:\n\n\n\nAmbari cluster built; Ambari ip:34.215.103.66\n10/26/2017, 9:41:58 AM\nBuilding Ambari cluster; Ambari ip:34.215.103.66\n10/26/2017, 9:30:20 AM\nStarting Ambari cluster services\n10/26/2017, 9:27:12 AM\nSetting up infrastructure metadata\n10/26/2017, 9:27:11 AM\nBootstrapping infrastructure cluster\n10/26/2017, 9:26:38 AM\nInfrastructure successfully provisioned\n10/26/2017, 9:26:37 AM\nBilling started, Infrastructure successfully provisioned\n10/26/2017, 9:26:37 AM\nInfrastructure metadata collection finished\n10/26/2017, 9:25:39 AM\nInfrastructure creation took 194 seconds\n10/26/2017, 9:25:37 AM\nCreating infrastructure\n10/26/2017, 9:22:22 AM\nSetting up HDP image\n10/26/2017, 9:22:21 AM\n\n\n\nAccess Cluster via SSH\n\n\nIf you plan to access the cluster via the command line clients, SSH into the master node instance in the cluster. \n\n\n\n\nIn order to use SSH, you must generate an SSH key pair or use an existing SSH keypair.  \n\n\nYou can find the cluster instance public IP addresses on the cluster details page.  \n\n\nWhen accessing instances via SSH use the \ncloudbreak\n user. \n\n\n\n\nOn Mac OS, you can use the following syntax to SSH to the VM:\n\nssh -i \"privatekey.pem\" cloudbreak@publicIP\n\nFor example:\n\nssh -i \"dominika-kp.pem\" cloudbreak@p52.25.169.132\n\n\nOn Windows, you can SSH using an SSH client such as PuTTY.\n\n\nAccess Ambari\n\n\nYou can access Ambari web UI by clicking on the links provided in the \nCluster Information\n \n \nAmbari URL\n.\n\n\nSteps\n\n\n\n\n\n\nFrom the cluster dashboard, click on the tile representing your cluster to navigate to cluster details.\n\n\n\n\n\n\nFind the Ambairi URL in the \nCluster Information\n section. This URL is available once the Ambari cluster creation process has completed.  \n\n\n\n\n\n\nClick on the \nAmbari URL\n link.\n\n\n\n\n\n\nThe first time you access the server, your browser will attempt to confirm that the SSL Certificate is valid. Since Cloudbreak automatically generates a self-signed certificate, your browser will warn you about an Untrusted Connection and ask you to confirm a Security Exception. Depending on your browser, perform the steps below to proceed.\n\n\n\n\n\n\n\n\nBrowser\n\n\nSteps\n\n\n\n\n\n\n\n\n\n\nFirefox\n\n\nClick \nAdvanced\n \n Click \nAdd Exception...\n \n Click \nConfirm Security Exception\n\n\n\n\n\n\nSafari\n\n\nClick \nContinue\n\n\n\n\n\n\nChrome\n\n\nClick \nAdvanced\n \n Click \nProceed...\n\n\n\n\n\n\n\n\n\n\n\n\nUser Accounts\n\n\nThe following table describes what credentials to use to access Cloudbreak and Cloudbreak-managed clusters:\n\n\n\n\n\n\n\n\nComponent\n\n\nMethod\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nCloudbreak\n\n\nWeb UI, CLI\n\n\nAccess with the username and password provided when launching Cloudbreak on the cloud provider.\n\n\n\n\n\n\nCloudbreak\n\n\nSSH to VM\n\n\nAccess as the \"cloudbreak\" user with the SSH key provided when launching Cloudbreak on the cloud provider.\n\n\n\n\n\n\nCluster\n\n\nSSH to VMs\n\n\nAccess as the \"cloudbreak\" user with the SSH key provided during cluster creation.\n\n\n\n\n\n\nCluster\n\n\nAmbari UI\n\n\nAccess with the credentials provided in the \u201cCluster User\u201d parameter during cluster creation.\n\n\n\n\n\n\n\n\n\n\nNext: Manage and Monitor Clusters", 
            "title": "Access Cluster"
        }, 
        {
            "location": "/azure-clusters-access/index.html#access-your-cluster", 
            "text": "The following section describes how to access the various services in the cluster.", 
            "title": "Access Your Cluster"
        }, 
        {
            "location": "/azure-clusters-access/index.html#finding-cluster-information-in-the-ui", 
            "text": "Once your cluster is up and running, click on the tile representing your cluster in the Cloudbreak UI to access information related the cluster and access cluster actions.      The information presented includes:   Cluster Summary  Cluster Information    Hardware  Tags    Event History     \n   Tips \n   \n    Access cluster actions such as resize and sync by clicking on  ACTIONS . \n    Access Ambari web UI by clicking on the link in the  CLUSTER INFORMATION  section.   View public IP addresses for all cluster instances in the  HARDWARE  section. Click on the links to view the instances in the cloud console.   The SSH user that you must use when accessing cluster VMs is \"cloudbreak\".", 
            "title": "Finding Cluster Information in the UI"
        }, 
        {
            "location": "/azure-clusters-access/index.html#cluster-summary", 
            "text": "The summary bar includes the following information about your cluster:     Item  Description      Cloud Provider  The logo of the cloud provider on which the cluster is running.    Credential  The name of the credential used to create the cluster.    Status  Current status. When a cluster is healthy, the status is  Running .    Nodes  The current number of cluster nodes, including the master node.    Uptime  The amount of time (HH:MM) that the cluster has been in the running state.    Created  The date when the cluster was created. The date format is Mon DD, YYYY. For exampple: Oct 27, 2017.", 
            "title": "Cluster Summary"
        }, 
        {
            "location": "/azure-clusters-access/index.html#cluster-information", 
            "text": "Item  Description      Network  The name of the network in which the cluster is running.    Subnet  The name of the subnet in which the cluster is running.    Cluster User  The name of the cluster user that you created when creating the cluster.    SSH Username  The SSH user which you must use when accessing cluster VMs via SSH. The SSH user is always \"cloudbreak\".    Ambari URL  Link to the Ambari web UI.    Region  The region in which the cluster is running in the cloud provider infrastructure.    Availability Zone  The availability zone within the region in which the cluster is running.    Blueprint  The name of the blueprint selected under \"Cluster Type\" to create this cluster.    Started With  The version of Cloubdreak used to create this cluster.    Ambari Version  The Ambari version which this cluster is currently running.    HDP Version  The HDP version which this cluster is currently running.", 
            "title": "Cluster Information"
        }, 
        {
            "location": "/azure-clusters-access/index.html#hardware", 
            "text": "This section includes information about your cluster nodes: instance names, instance IDs (with links to the cloud provider console), and public IPs.", 
            "title": "Hardware"
        }, 
        {
            "location": "/azure-clusters-access/index.html#tags", 
            "text": "This section lists user-defined tags, in the same order as you added them.", 
            "title": "Tags"
        }, 
        {
            "location": "/azure-clusters-access/index.html#event-history", 
            "text": "The Event History section shows you events logged for the cluster, with the most recent event at the top. For example, after your cluster has been created, the following messages will be written to the log:  \nAmbari cluster built; Ambari ip:34.215.103.66\n10/26/2017, 9:41:58 AM\nBuilding Ambari cluster; Ambari ip:34.215.103.66\n10/26/2017, 9:30:20 AM\nStarting Ambari cluster services\n10/26/2017, 9:27:12 AM\nSetting up infrastructure metadata\n10/26/2017, 9:27:11 AM\nBootstrapping infrastructure cluster\n10/26/2017, 9:26:38 AM\nInfrastructure successfully provisioned\n10/26/2017, 9:26:37 AM\nBilling started, Infrastructure successfully provisioned\n10/26/2017, 9:26:37 AM\nInfrastructure metadata collection finished\n10/26/2017, 9:25:39 AM\nInfrastructure creation took 194 seconds\n10/26/2017, 9:25:37 AM\nCreating infrastructure\n10/26/2017, 9:22:22 AM\nSetting up HDP image\n10/26/2017, 9:22:21 AM", 
            "title": "Event History"
        }, 
        {
            "location": "/azure-clusters-access/index.html#access-cluster-via-ssh", 
            "text": "If you plan to access the cluster via the command line clients, SSH into the master node instance in the cluster.    In order to use SSH, you must generate an SSH key pair or use an existing SSH keypair.    You can find the cluster instance public IP addresses on the cluster details page.    When accessing instances via SSH use the  cloudbreak  user.    On Mac OS, you can use the following syntax to SSH to the VM: ssh -i \"privatekey.pem\" cloudbreak@publicIP \nFor example: ssh -i \"dominika-kp.pem\" cloudbreak@p52.25.169.132  On Windows, you can SSH using an SSH client such as PuTTY.", 
            "title": "Access Cluster via SSH"
        }, 
        {
            "location": "/azure-clusters-access/index.html#access-ambari", 
            "text": "You can access Ambari web UI by clicking on the links provided in the  Cluster Information     Ambari URL .  Steps    From the cluster dashboard, click on the tile representing your cluster to navigate to cluster details.    Find the Ambairi URL in the  Cluster Information  section. This URL is available once the Ambari cluster creation process has completed.      Click on the  Ambari URL  link.    The first time you access the server, your browser will attempt to confirm that the SSL Certificate is valid. Since Cloudbreak automatically generates a self-signed certificate, your browser will warn you about an Untrusted Connection and ask you to confirm a Security Exception. Depending on your browser, perform the steps below to proceed.     Browser  Steps      Firefox  Click  Advanced    Click  Add Exception...    Click  Confirm Security Exception    Safari  Click  Continue    Chrome  Click  Advanced    Click  Proceed...", 
            "title": "Access Ambari"
        }, 
        {
            "location": "/azure-clusters-access/index.html#user-accounts", 
            "text": "The following table describes what credentials to use to access Cloudbreak and Cloudbreak-managed clusters:     Component  Method  Description      Cloudbreak  Web UI, CLI  Access with the username and password provided when launching Cloudbreak on the cloud provider.    Cloudbreak  SSH to VM  Access as the \"cloudbreak\" user with the SSH key provided when launching Cloudbreak on the cloud provider.    Cluster  SSH to VMs  Access as the \"cloudbreak\" user with the SSH key provided during cluster creation.    Cluster  Ambari UI  Access with the credentials provided in the \u201cCluster User\u201d parameter during cluster creation.      Next: Manage and Monitor Clusters", 
            "title": "User Accounts"
        }, 
        {
            "location": "/azure-clusters-manage/index.html", 
            "text": "Manage and Monitor Clusters\n\n\nYou can manage monitor your clusters from the Cloudbreak UI. To do that, click on the tile representing the cluster that you want to access. The actions available for your cluster are listed in the top right corner: \n\n\n \n\n\n\n  \nTips\n\n  \n\n  \nTo add or remove nodes from your cluster click \nACTIONS>Resize\n.\n\n  \nTo synchronize your cluster with the cloud provider account click \nACTIONS>Sync\n.\n\n  \nTo temporarily stop your cluster click \nSTOP\n.\n\n  \nTo terminate your cluster click \nTERMINATE\n.\n\n\n\n\n\n\n\n\n\nResize Cluster\n\n\nTo resize a cluster, follow these steps.\n\n\nSteps\n\n\n\n\n\n\nBrowse to the cluster details.\n\n\n\n\n\n\nClick \nActions\n and select \nResize\n. The cluster resize dialog is displayed.\n\n\n\n\n\n\nUsing the +/- controls, adjust how many nodes to add or remove from each host group.  \n\n\n\n\n\n\nClick \nYes\n to confirm the scale-up/scale-down.\n\n\nWhile nodes are being added or removed, cluster status changes to \"Update In Progress\". Once the operation has completed, cluster status changes back to \"Running\". Messages similar to the following are written to the \"Event History\", : \n\n\nAmbari cluster scaled up\n11/6/2017, 12:33:40 PM\nScaling up the Ambari cluster\n11/6/2017, 12:26:59 PM\nStack successfully upscaled\n11/6/2017, 12:26:54 PM\nBootstrapping new nodes\n11/6/2017, 12:26:16 PM\nInfrastructure metadata extension finished\n11/6/2017, 12:26:10 PM\nBilling changed due to upscaling of cluster infrastructure\n11/6/2017, 12:26:10 PM\nAdding 1 new instances to the infrastructure\n11/6/2017, 12:25:23 PM\n\n\n\n\n\n\nSynchronize with Cloud Provider\n\n\nIf you have just made changes on your cloud provider side (for example, deleted cluster VMs) and you would like to synchronize Cloudbreak with the cloud provider, use the \nsync\n option. \n\n\nTo synchronize your cluster with the cloud provider, follow these steps. \n\n\nSteps\n\n\n\n\n\n\nBrowse to the cluster details.\n\n\n\n\n\n\nClick \nActions\n and select \nSync\n.\n\n\n\n\n\n\nClick \nYes\n to confirm.\n\n\nYour cluster infrastructure is synchronized based on changes on the cloud provider. The updates are written to the \"Event History\". \n\n\n\n\n\n\nStop Cluster\n\n\nCloudbreak supports stopping and restarting clusters. To stop and restart a cluster managed by Cloudbreak, use the options available from the Coudbreak UI. \n\n\nSteps\n\n\n\n\n\n\nBrowse to the cluster details.\n\n\n\n\n\n\nClick \nStop\n to stop a currently running cluster.  \n\n\n\n\n\n\nClick \nYes\n to confirm. \n\n\nYour cluster status changes to \"Stopping in progress\" and then to \"Stopped\". You should see the following messages in the \"Event History\":\n\n\nBilling stopped, the cluster and its infrastructure have been terminated\n11/6/2017, 12:46:48 PM\nInfrastructure successfully stopped\n11/6/2017, 12:46:47 PM\nInfrastructure is now stopping\n11/6/2017, 12:43:58 PM\nAmbari cluster stopped\n11/6/2017, 12:43:52 PM\nAmbari services have been stopped.\n11/6/2017, 12:43:49 PM\nStopping Ambari services.\n11/6/2017, 12:42:14 PM\nStopping Ambari cluster\n11/6/2017, 12:42:10 PM\nCluster infrastructure stop requested\n11/6/2017, 12:42:06 PM\n\n\n\n\n\n\nOnce stopping the infrastructure has completed, you will see a \nStart\n option to restart your cluster. \n\n\nRestart Cluster\n\n\nIf your cluster is in the \"Stopped\" state, you can restart the cluster by follow these steps.\n\n\nSteps\n\n\n\n\n\n\nclick \nStart\n. This option is only available when the cluster has been stopped. \n\n\n\n\n\n\nClick \nYes\n to confirm.\n\n\nYour cluster status changes to \"Start in progress\" and then to \"Running\". You should see the following messages in the \"Event History\":\n\n\nAmbari cluster started; Ambari ip:35.203.149.236\n11/6/2017, 1:02:13 PM\nAmbari services have been started.\n11/6/2017, 1:02:08 PM\nStarting Ambari services.\n11/6/2017, 12:54:37 PM\nStarting Ambari cluster\n11/6/2017, 12:52:57 PM\nBilling started, the cluster and its infrastructure have successfully been started\n11/6/2017, 12:52:48 PM\nInfrastructure successfully started\n11/6/2017, 12:52:48 PM\nInfrastructure is now starting\n11/6/2017, 12:52:26 PM\nAmbari cluster start requested\n11/6/2017, 12:52:24 PM\n\n\n\n\n\n\nTerminate Cluster\n\n\nTo terminate a cluster managed by Cloudbreak, use the option available from the Coudbreak UI. \n\n\nSteps\n\n\n\n\n\n\nBrowse to the cluster details.\n\n\n\n\n\n\nClick \nTerminate\n. \n\n\n\n\n\n\nClick \nYes\n to confirm.\n\n\nAll cluster-related resources will be deleted, unless the resources (such as networks and subnets) existed prior to cluster creation or are used by other VMs in which case they will be preserved. \n\n\n\n\n\n\nForce Terminate\n\n\nCluster deletion may fail if Cloudbreak is unable to delete one or more of the cloud resources that were part of your cluster infrastructure. In such as case, you can use the \nTerminate\n \n \nForce terminate\n option to remove the cluster entry from the Cloudbreak web UI, but you must also check your cloud provider account to see if there are any resources that must be deleted manually.\n\n\nSteps\n\n\n\n\n\n\nBrowse to the cluster details.\n\n\n\n\n\n\nClick \nTerminate\n. \n\n\n\n\n\n\nCheck  \nForce terminate\n.\n\n\n\n\n\n\nClick \nYes\n to confirm. \n\n\n\n\n\n\nThis deletes the cluster tile from the UI.  \n\n\n\n\n\n\nLog in to your cloud provider account and \nmanually delete\n any resources that failed to be deleted.\n\n\n\n\n\n\nView Cluster History\n\n\nFrom the navigation menu in the Cloudbreak UI, you can access the History page that allows you to generate a report showing basic information related to the clusters that were running within the specified range of dates.\n\n\nTo generate a report, follow these steps.\n\n\nSteps\n\n\n\n\n\n\nFrom the Cloudbreak UI navigation menu, select \nHistory\n.\n\n\n\n\n\n\nOn the History page, select the range of dates and click \nShow History\n to generate a report for the selected period.\n\n\n\n\n\n\nHistory Report Content\n\n\nEach entry in the report represents one cluster instance group. For each entry, the report includes the following information:\n\n\n\n\nCreated\n - The date when your cluster was created (YYYY-MM-DD).\n\n\nProvider\n - The name of the cloud provider (AWS, Azure, Google, or OpenStack) on which the cluster instances are/were running.\n\n\nCluster Name\n - The name that you selected for the cluster.\n\n\nWorker Count\n - The number of worker nodes in the cluster. This number may be a decimal if a cluster has been resized.\n\n\nInstance Type\n - Provider-specific VM type of the cluster instances.\n\n\nInstance Group\n - The name of the instance group.  \n\n\nRegion\n - The AWS region in which your cluster is running.\n\n\nRunning Time (hours)\n - The sum of the running times for all the nodes in the instance group.\n\n\n\n\nThe \nAGGREGATE RUNNING TIME\n is the sum of the Running Times, adjusted for the selected time range.\n\n\nTo learn about how your cloud provider bills you for the VMs, refer to their documentation:\n\n\n\n\nAWS\n      \n\n\nAzure\n     \n\n\nGCP\n   \n\n\n\n\n\n\nNext: Access Data", 
            "title": "Manage and Monitor Clusters"
        }, 
        {
            "location": "/azure-clusters-manage/index.html#manage-and-monitor-clusters", 
            "text": "You can manage monitor your clusters from the Cloudbreak UI. To do that, click on the tile representing the cluster that you want to access. The actions available for your cluster are listed in the top right corner:      \n   Tips \n   \n   To add or remove nodes from your cluster click  ACTIONS>Resize . \n   To synchronize your cluster with the cloud provider account click  ACTIONS>Sync . \n   To temporarily stop your cluster click  STOP . \n   To terminate your cluster click  TERMINATE .", 
            "title": "Manage and Monitor Clusters"
        }, 
        {
            "location": "/azure-clusters-manage/index.html#resize-cluster", 
            "text": "To resize a cluster, follow these steps.  Steps    Browse to the cluster details.    Click  Actions  and select  Resize . The cluster resize dialog is displayed.    Using the +/- controls, adjust how many nodes to add or remove from each host group.      Click  Yes  to confirm the scale-up/scale-down.  While nodes are being added or removed, cluster status changes to \"Update In Progress\". Once the operation has completed, cluster status changes back to \"Running\". Messages similar to the following are written to the \"Event History\", :   Ambari cluster scaled up\n11/6/2017, 12:33:40 PM\nScaling up the Ambari cluster\n11/6/2017, 12:26:59 PM\nStack successfully upscaled\n11/6/2017, 12:26:54 PM\nBootstrapping new nodes\n11/6/2017, 12:26:16 PM\nInfrastructure metadata extension finished\n11/6/2017, 12:26:10 PM\nBilling changed due to upscaling of cluster infrastructure\n11/6/2017, 12:26:10 PM\nAdding 1 new instances to the infrastructure\n11/6/2017, 12:25:23 PM", 
            "title": "Resize Cluster"
        }, 
        {
            "location": "/azure-clusters-manage/index.html#synchronize-with-cloud-provider", 
            "text": "If you have just made changes on your cloud provider side (for example, deleted cluster VMs) and you would like to synchronize Cloudbreak with the cloud provider, use the  sync  option.   To synchronize your cluster with the cloud provider, follow these steps.   Steps    Browse to the cluster details.    Click  Actions  and select  Sync .    Click  Yes  to confirm.  Your cluster infrastructure is synchronized based on changes on the cloud provider. The updates are written to the \"Event History\".", 
            "title": "Synchronize with Cloud Provider"
        }, 
        {
            "location": "/azure-clusters-manage/index.html#stop-cluster", 
            "text": "Cloudbreak supports stopping and restarting clusters. To stop and restart a cluster managed by Cloudbreak, use the options available from the Coudbreak UI.   Steps    Browse to the cluster details.    Click  Stop  to stop a currently running cluster.      Click  Yes  to confirm.   Your cluster status changes to \"Stopping in progress\" and then to \"Stopped\". You should see the following messages in the \"Event History\":  Billing stopped, the cluster and its infrastructure have been terminated\n11/6/2017, 12:46:48 PM\nInfrastructure successfully stopped\n11/6/2017, 12:46:47 PM\nInfrastructure is now stopping\n11/6/2017, 12:43:58 PM\nAmbari cluster stopped\n11/6/2017, 12:43:52 PM\nAmbari services have been stopped.\n11/6/2017, 12:43:49 PM\nStopping Ambari services.\n11/6/2017, 12:42:14 PM\nStopping Ambari cluster\n11/6/2017, 12:42:10 PM\nCluster infrastructure stop requested\n11/6/2017, 12:42:06 PM    Once stopping the infrastructure has completed, you will see a  Start  option to restart your cluster.", 
            "title": "Stop Cluster"
        }, 
        {
            "location": "/azure-clusters-manage/index.html#restart-cluster", 
            "text": "If your cluster is in the \"Stopped\" state, you can restart the cluster by follow these steps.  Steps    click  Start . This option is only available when the cluster has been stopped.     Click  Yes  to confirm.  Your cluster status changes to \"Start in progress\" and then to \"Running\". You should see the following messages in the \"Event History\":  Ambari cluster started; Ambari ip:35.203.149.236\n11/6/2017, 1:02:13 PM\nAmbari services have been started.\n11/6/2017, 1:02:08 PM\nStarting Ambari services.\n11/6/2017, 12:54:37 PM\nStarting Ambari cluster\n11/6/2017, 12:52:57 PM\nBilling started, the cluster and its infrastructure have successfully been started\n11/6/2017, 12:52:48 PM\nInfrastructure successfully started\n11/6/2017, 12:52:48 PM\nInfrastructure is now starting\n11/6/2017, 12:52:26 PM\nAmbari cluster start requested\n11/6/2017, 12:52:24 PM", 
            "title": "Restart Cluster"
        }, 
        {
            "location": "/azure-clusters-manage/index.html#terminate-cluster", 
            "text": "To terminate a cluster managed by Cloudbreak, use the option available from the Coudbreak UI.   Steps    Browse to the cluster details.    Click  Terminate .     Click  Yes  to confirm.  All cluster-related resources will be deleted, unless the resources (such as networks and subnets) existed prior to cluster creation or are used by other VMs in which case they will be preserved.", 
            "title": "Terminate Cluster"
        }, 
        {
            "location": "/azure-clusters-manage/index.html#force-terminate", 
            "text": "Cluster deletion may fail if Cloudbreak is unable to delete one or more of the cloud resources that were part of your cluster infrastructure. In such as case, you can use the  Terminate     Force terminate  option to remove the cluster entry from the Cloudbreak web UI, but you must also check your cloud provider account to see if there are any resources that must be deleted manually.  Steps    Browse to the cluster details.    Click  Terminate .     Check   Force terminate .    Click  Yes  to confirm.     This deletes the cluster tile from the UI.      Log in to your cloud provider account and  manually delete  any resources that failed to be deleted.", 
            "title": "Force Terminate"
        }, 
        {
            "location": "/azure-clusters-manage/index.html#view-cluster-history", 
            "text": "From the navigation menu in the Cloudbreak UI, you can access the History page that allows you to generate a report showing basic information related to the clusters that were running within the specified range of dates.  To generate a report, follow these steps.  Steps    From the Cloudbreak UI navigation menu, select  History .    On the History page, select the range of dates and click  Show History  to generate a report for the selected period.", 
            "title": "View Cluster History"
        }, 
        {
            "location": "/azure-clusters-manage/index.html#history-report-content", 
            "text": "Each entry in the report represents one cluster instance group. For each entry, the report includes the following information:   Created  - The date when your cluster was created (YYYY-MM-DD).  Provider  - The name of the cloud provider (AWS, Azure, Google, or OpenStack) on which the cluster instances are/were running.  Cluster Name  - The name that you selected for the cluster.  Worker Count  - The number of worker nodes in the cluster. This number may be a decimal if a cluster has been resized.  Instance Type  - Provider-specific VM type of the cluster instances.  Instance Group  - The name of the instance group.    Region  - The AWS region in which your cluster is running.  Running Time (hours)  - The sum of the running times for all the nodes in the instance group.   The  AGGREGATE RUNNING TIME  is the sum of the Running Times, adjusted for the selected time range.  To learn about how your cloud provider bills you for the VMs, refer to their documentation:   AWS         Azure        GCP        Next: Access Data", 
            "title": "History Report Content"
        }, 
        {
            "location": "/azure-data/index.html", 
            "text": "Access Data on Azure\n\n\nHortonworks Data Platform (HDP) supports reading and writing both block blobs and page blobs\nfrom/to \nWindows Azure Storage Blob (WASB)\n object store, as well as reading and writing files stored in an\n\nAzure Data Lake Storage (ADLS)\n account. This allows you to:\n\n\n\n\nPersist data using cloud storage services beyond the lifetime of your HDP clusters.  \n\n\nLoad data in Hadoop ecosystem applications directly from Azure storage services, without first importing or uploading data from external resources to HDFS.  \n\n\nUse other applications (not necessarily in your Hadoop ecosystem) to manipulate the data stored in Azure storage services beyond the lifetime of your HDP clusters.  \n\n\nShare data between multiple HDP clusters fast and easily by pointing to the same Azure data sets. \n\n\nMove or copy data between different Azure storage services or between Azure storage services and HDFS to facilitate different scenarios for big data analytics workloads.  \n\n\nBack up unlimited archive data at any scale from HDP cluster to fully managed, durable, and highly available Azure storage services.   \n\n\n\n\nAccessing Data in ADLS\n\n\nAzure Data Lake Store (ADLS)\n is an enterprise-wide hyper-scale repository for big data analytic workloads.\n\n\nPrerequisites\n\n\nIf you want to use ADLS to store your data, you must enable Azure subscription for Data Lake Store, and then create an Azure Data Lake Store \nstorage account\n.\n\n\nConfiguring Access to ADLS\n\n\nADLS is not supported as a default file system, but access to data in ADLS via the adl connector. To configure access to ADLS from a cluster managed via Cloudbreak use the steps described in \nHow to Configure Authentication with ADLS\n.\n\n\nTesting Access to ADLS\n\n\nTo tests access to ADLS, SSH to a cluster node and run a few hadoop fs shell commands against your existing ADLS account.\n\n\nADLS access path syntax is:\n\n\nadl://\naccount_name\n.azuredatalakestore.net/\ndir/file\n\n\n\nFor example, the following Hadoop FileSystem shell commands demonstrate access to a storage account named \"myaccount\":\n\n\nhadoop fs -mkdir adl://myaccount.azuredatalakestore.net/testdir\n\n\n\nhadoop fs -put testfile adl://myaccount.azuredatalakestore.net/testdir/testfile\n\n\n\nTo use DistCp against ADLS, use the following syntax:\n\nhadoop distcp\n    [-D hadoop.security.credential.provider.path=localjceks://file/home/user/adls.jceks]\n    hdfs://\nnamenode_hostname\n:9001/user/foo/007020615\n    adl://\nmyaccount\n.azuredatalakestore.net/testDir/\n\n\nRelated Links\n\n\nAzure Data Lake Store\n (External)  \n\n\nGet started with Azure Data Lake Store\n (External)\n\n\nHow to Configure Authentication with ADLS\n (Hortonworks)\n\n\nAccessing Data in WASB\n\n\nWindows Azure Storage Blob (WASB) is an object store service available on Azure.\n\n\nPrerequisites\n\n\nIf you want to use Windows Azure Storage Blob to store your data, you must enable Azure subscription for Blob Storage, and then create a \nstorage account\n.  \n\n\nConfiguring Access to WASB\n\n\nIn order to access data stored in your Azure blob storage account, you must configure your storage account access key in \ncore-site.xml\n. The configuration property that you must use is \nfs.azure.account.key.\naccount name\n.blob.core.windows.net\n and the value is the access key. \n\n\nFor example the following property should be used for a storage account called \"testaccount\": \n\n\nproperty\n\n  \nname\nfs.azure.account.key.testaccount.blob.core.windows.net\n/name\n\n  \nvalue\nTESTACCOUNT-ACCESS-KEY\n/value\n\n\n/property\n\n\n\n\n\nYou can obtain your access key from the Access keys in your storage account settings.\n\n\nTesting Access to WASB\n\n\nTo tests access to WASB, SSH to a cluster node and run a few hadoop fs shell commands against your existing WASB account.\n\n\nWASB access path syntax is:\n\n\nwasb://\ncontainer_name\n@\nstorage_account_name\n.blob.core.windows.net/\ndir/file\n\n\n\nFor example, to access a file called \"testfile\" located in a directory called \"testdir\", stored in the container called \"testcontainer\" on the account called \"hortonworks\", the URL is:\n\n\nwasb://testcontainer@hortonworks.blob.core.windows.net/testdir/testfile\n\n\n\nYou can also use \"wasbs\" prefix to utilize SSL-encrypted HTTPS access:\n\n\nwasbs://\n@\n.blob.core.windows.net/dir/file\n\n\n\nThe following Hadoop FileSystem shell commands demonstrate access to a storage account named \"myaccount\" and a container named \"mycontainer\":\n\n\nhadoop fs -ls wasb://mycontainer@myaccount.blob.core.windows.net/\n\nhadoop fs -mkdir wasb://mycontainer@myaccount.blob.core.windows.net/testDir\n\nhadoop fs -put testFile wasb://mycontainer@myaccount.blob.core.windows.net/testDir/testFile\n\nhadoop fs -cat wasb://mycontainer@myaccount.blob.core.windows.net/testDir/testFile\ntest file content\n\n\n\nLearn More\n\n\nFor more information about configuring the ADLS and WASB connectors and working with data stored in ADLS and WASB, refer to \nCloud Data Access\n documentation.\n\n\nRelated Links\n \n\n\nCloud Data Access\n (Hortonworks) \n\n\nCreate a Storage Account\n (External)", 
            "title": "Access Data on Azure"
        }, 
        {
            "location": "/azure-data/index.html#access-data-on-azure", 
            "text": "Hortonworks Data Platform (HDP) supports reading and writing both block blobs and page blobs\nfrom/to  Windows Azure Storage Blob (WASB)  object store, as well as reading and writing files stored in an Azure Data Lake Storage (ADLS)  account. This allows you to:   Persist data using cloud storage services beyond the lifetime of your HDP clusters.    Load data in Hadoop ecosystem applications directly from Azure storage services, without first importing or uploading data from external resources to HDFS.    Use other applications (not necessarily in your Hadoop ecosystem) to manipulate the data stored in Azure storage services beyond the lifetime of your HDP clusters.    Share data between multiple HDP clusters fast and easily by pointing to the same Azure data sets.   Move or copy data between different Azure storage services or between Azure storage services and HDFS to facilitate different scenarios for big data analytics workloads.    Back up unlimited archive data at any scale from HDP cluster to fully managed, durable, and highly available Azure storage services.", 
            "title": "Access Data on Azure"
        }, 
        {
            "location": "/azure-data/index.html#accessing-data-in-adls", 
            "text": "Azure Data Lake Store (ADLS)  is an enterprise-wide hyper-scale repository for big data analytic workloads.", 
            "title": "Accessing Data in ADLS"
        }, 
        {
            "location": "/azure-data/index.html#prerequisites", 
            "text": "If you want to use ADLS to store your data, you must enable Azure subscription for Data Lake Store, and then create an Azure Data Lake Store  storage account .", 
            "title": "Prerequisites"
        }, 
        {
            "location": "/azure-data/index.html#configuring-access-to-adls", 
            "text": "ADLS is not supported as a default file system, but access to data in ADLS via the adl connector. To configure access to ADLS from a cluster managed via Cloudbreak use the steps described in  How to Configure Authentication with ADLS .", 
            "title": "Configuring Access to ADLS"
        }, 
        {
            "location": "/azure-data/index.html#testing-access-to-adls", 
            "text": "To tests access to ADLS, SSH to a cluster node and run a few hadoop fs shell commands against your existing ADLS account.  ADLS access path syntax is:  adl:// account_name .azuredatalakestore.net/ dir/file  For example, the following Hadoop FileSystem shell commands demonstrate access to a storage account named \"myaccount\":  hadoop fs -mkdir adl://myaccount.azuredatalakestore.net/testdir  hadoop fs -put testfile adl://myaccount.azuredatalakestore.net/testdir/testfile  To use DistCp against ADLS, use the following syntax: hadoop distcp\n    [-D hadoop.security.credential.provider.path=localjceks://file/home/user/adls.jceks]\n    hdfs:// namenode_hostname :9001/user/foo/007020615\n    adl:// myaccount .azuredatalakestore.net/testDir/  Related Links  Azure Data Lake Store  (External)    Get started with Azure Data Lake Store  (External)  How to Configure Authentication with ADLS  (Hortonworks)", 
            "title": "Testing Access to ADLS"
        }, 
        {
            "location": "/azure-data/index.html#accessing-data-in-wasb", 
            "text": "Windows Azure Storage Blob (WASB) is an object store service available on Azure.", 
            "title": "Accessing Data in WASB"
        }, 
        {
            "location": "/azure-data/index.html#prerequisites_1", 
            "text": "If you want to use Windows Azure Storage Blob to store your data, you must enable Azure subscription for Blob Storage, and then create a  storage account .", 
            "title": "Prerequisites"
        }, 
        {
            "location": "/azure-data/index.html#configuring-access-to-wasb", 
            "text": "In order to access data stored in your Azure blob storage account, you must configure your storage account access key in  core-site.xml . The configuration property that you must use is  fs.azure.account.key. account name .blob.core.windows.net  and the value is the access key.   For example the following property should be used for a storage account called \"testaccount\":   property \n   name fs.azure.account.key.testaccount.blob.core.windows.net /name \n   value TESTACCOUNT-ACCESS-KEY /value  /property   You can obtain your access key from the Access keys in your storage account settings.", 
            "title": "Configuring Access to WASB"
        }, 
        {
            "location": "/azure-data/index.html#testing-access-to-wasb", 
            "text": "To tests access to WASB, SSH to a cluster node and run a few hadoop fs shell commands against your existing WASB account.  WASB access path syntax is:  wasb:// container_name @ storage_account_name .blob.core.windows.net/ dir/file  For example, to access a file called \"testfile\" located in a directory called \"testdir\", stored in the container called \"testcontainer\" on the account called \"hortonworks\", the URL is:  wasb://testcontainer@hortonworks.blob.core.windows.net/testdir/testfile  You can also use \"wasbs\" prefix to utilize SSL-encrypted HTTPS access:  wasbs:// @ .blob.core.windows.net/dir/file  The following Hadoop FileSystem shell commands demonstrate access to a storage account named \"myaccount\" and a container named \"mycontainer\":  hadoop fs -ls wasb://mycontainer@myaccount.blob.core.windows.net/\n\nhadoop fs -mkdir wasb://mycontainer@myaccount.blob.core.windows.net/testDir\n\nhadoop fs -put testFile wasb://mycontainer@myaccount.blob.core.windows.net/testDir/testFile\n\nhadoop fs -cat wasb://mycontainer@myaccount.blob.core.windows.net/testDir/testFile\ntest file content", 
            "title": "Testing Access to WASB"
        }, 
        {
            "location": "/azure-data/index.html#learn-more", 
            "text": "For more information about configuring the ADLS and WASB connectors and working with data stored in ADLS and WASB, refer to  Cloud Data Access  documentation.  Related Links    Cloud Data Access  (Hortonworks)   Create a Storage Account  (External)", 
            "title": "Learn More"
        }, 
        {
            "location": "/gcp-launch/index.html", 
            "text": "Launch Cloudbreak on GCP\n\n\nBefore launching Cloudbreak on Google Cloud, review and meet the prerequisites. Next, import Cloudbreak image, launch a VM, SSH to the VM, and start Cloudbreak. Once Cloudbreak is running, log in to the Cloudbreak UI and create a Cloudbreak credential. \n\n\nMeet the Prerequisites\n\n\nBefore launching Cloudbreak on GCP, you must meet the following prerequisites.\n\n\nGCP Account\n\n\nIn order to launch Cloudbreak on GCP, you must log in to your GCP account. If you don't have an account, you can create one at \nhttps://console.cloud.google.com\n.\n\n\nOnce you log in to your GCP account, you must either create a project or use an existing project. \n\n\nService Account\n\n\nIn order to launch clusters on GCP via Cloudbreak, you must have a Service Account that Cloudbreak can use to create resources. In addition, you must also have a P12 key associated with the account. If you need to create these, refer to \nGCP documentation\n on how to create a service account and generate a P12 key. \n\n\nOnce you have the service account that you want to use for Cloudbreak, make sure that your service account fulfills one of the following APIs are enabled for your service account:\n\n\n\n\nCompute Image User   \n\n\nCompute Instance Admin (v1)  \n\n\nCompute Network Admin  \n\n\nCompute Security Admin  \n\n\n\n\nA user with an \"Owner\" role can assign roles or access rules to service accounts from \nIAM \n Admin\n \n \nIAM\n. For example:\n\n\n \n\n\nRelated Links\n\n\nService Account Credentials\n (External)  \n\n\nVPC Network\n\n\nWhen launching Cloudbreak, you will be required to select an existing network in which Cloudbreak can be placed. The following ports must be open on the security group: 22 (SSH) and 443 (HTTPS). You may use the \ndefault\n network as long as the aforementioned ports are open. \n\n\nYou can manage networks under \nNetworking\n \n \nVPC Networks\n. To edit ports, click on the network name and then click on \nAdd firewall rules\n.\n\n\nRegion and Zone\n\n\nDecide in which region and zone you would like to launch Cloudbreak. You can launch Cloudbreak and provision your clusters in all regions \nsupported by GCP\n.  \n\n\nClusters created via Cloudbreak can be in the same or different region as Cloudbreak; when you launch a cluster, you select the region in which to launch it. \n\n\nRelated Links\n\n\nRegions and Zones\n (External)  \n\n\nLaunch the VM\n\n\nSteps\n\n\n\n\n\n\nLog in to Google Cloud Platform.\n\n\n\n\n\n\nOpen the \nGoogle Cloud Shell\n by clicking on the  \n icon in the top-right corner:\n\n\n \n\n\n\n\n\n\nImport the Cloudbreak deployer image by executing the following command: \n\n\ngcloud compute images create cloudbreak-deployer-1164-2017-08-29 --source-uri gs://sequenceiqimage/cloudbreak-deployer-1164-2017-08-29.tar.gz\n\n\n\n\n\n\nIn the GCP UI, from the \nProducts and services\n menu, select \nCompute Engine\n \n \nImages\n.\n\n\n\n\n\n\nIn the search bar, type the name of the Cloudbreak deployer image that you imported earlier.\n\n\n\n\n\n\nSelect the image and then select \nCreate Instance\n:  \n\n\n  \n\n\n\n\n\n\nYou will be redirected to \nVM instances\n \n \nCreate an instance\n form. Provide the following parameters for your VM:\n\n\n  \n\n\n\n\n\n\n\n\nParameter\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nName\n\n\nEnter a name for the VM.\n\n\n\n\n\n\nZone\n\n\nSelect the zone in which to launch the VM.\n\n\n\n\n\n\nMachine type\n\n\nThe minimum instance type suitable for Cloudbreak is \nn1-standard-2\n. The minimum requirements are 4GB RAM, 10GB disk, 2 cores.\n\n\n\n\n\n\nBoot disk\n\n\nVerify that the Cloudbreak deployer disk which you imported earlier is pre-selected.\n\n\n\n\n\n\n\n\n\n\n\n\nClick on \nManagement, disks, networking, SSH keys\n to view the options.\n\n\n\n\n\n\nUnder \nNetworking\n \n \nNetwork interfaces\n, select the network in which you want to launch Cloudbreak. \n\n\n\n\n\n\nUnder \nSSH Keys\n, check \nBlock project-wise SSH keys\n and paste your public SSH key.\n\n\n\n\n\n\nClick \nCreate\n. \n\n\n\n\n\n\nSSH to the VM\n\n\nNow that your VM is ready, access it via SSH: \n\n\n\n\nUse a private key matching the public key that you added to your  project.\n\n\nThe SSH user is called \"cloudbreak\".\n\n\nYou can obtain the VM's IP address from \nCompute Engine\n \n \nVM Instances\n, the \nExternal IP\n column.\n\n\n\n\nOn Mac OS X, you can SSH to the VM by running the following from the Terminal app: \nssh -i \"your-private-key.pem\" cloudnreak@instance_IP\n where \"your-private-key.pem\" points to the location of your private key and \"instance_IP\" is the public IP address of the VM.\n\n\nOn Windows, you can use \nPuTTy\n.\n\n\nLaunch Cloudbreak Deployer\n\n\nAfter accessing the VM via SSH, launch Cloudbreak deployer using the following steps.\n\n\nSteps\n \n\n\n\n\n\n\nNavigate to the cloudbreak-deployment directory:\n\n\ncd /var/lib/cloudbreak-deployment/\n\n\nThis directory contains configuration files and the supporting binaries for Cloudbreak deployer.\n\n\n\n\n\n\nInitialize your profile by creating a new file called \nProfile\n and adding the following content:\n\n\nexport UAA_DEFAULT_SECRET=MY-SECRET\nexport UAA_DEFAULT_USER_PW=MY-PASSWORDP\n  \n\n\nFor example: \n\n\nexport UAA_DEFAULT_SECRET=MySecret123\nexport UAA_DEFAULT_USER_PW=MySecurePassword123\n \n\n\n\n\nYou will need to provide the password when logging in to the Cloudbreak web UI and when using the Cloudbreak Shell. The secret will be used by Cloudbreak for authentication.  \n\n\n\n\n\n\n\n\nStart the Cloudbreak application by using the following command:\n\n\ncbd start\n\n\nThis will start the Docker containers and initialize the application. The first time you start the Coudbreak app, the process will take longer than usual due to the download of all the necessary docker images.\n\n\nThe \ncbd start\n command includes the \ncbd generate\n command which applies the following steps:\n\n\n\n\nCreates the \ndocker-compose.yml\n file, which describes the configuration of all the Docker containers needed for the Cloudbreak deployment.\n\n\nCreates the \nuaa.yml\n file, which holds the configuration of the identity server used to authenticate users with Cloudbreak.\n\n\n\n\n\n\nOnce the \ncbd start\n has finished, it returns the \"Uluwatu (Cloudbreak UI) url\" which you can later paste in your browser and log in to Cloudbreak web UI.\n\n\n\n\n\n\n\n\nCheck Cloudbreak deployer version and health: \n\n\ncbd doctor\n\n\n\n\n\n\nNext, check Cloudbreak Application logs: \n\n\ncbd logs cloudbreak\n\n\nYou should see a message like this in the log: \nStarted CloudbreakApplication in 36.823 seconds.\n Cloudbreak normally takes less than a minute to start.\n\n\n\n\n\n\nAccess Cloudbreak UI\n\n\nLog in to the Cloudbreak UI using the following steps.\n\n\nSteps\n\n\n\n\n\n\nYou can log into the Cloudbreak application at  \nhttps://IP_Address\n. For example \nhttps://34.212.141.253\n.  You can obtain the VM's IP address from \nCompute Engine\n \n \nVM Instances\n, the \nExternal IP\n column.\n\n\n\n\n\n\nConfirm the security exception to proceed to the Cloudbreak web UI.\n\n\nThe first time you access Cloudbreak UI, Cloudbreak will automatically generate a self-signed certificate, due to which your browser will warn you about an untrusted connection and will ask you to confirm a security exception.\n\n\n\n\n\n\nThe login page is displayed:\n\n\n  \n\n\n\n\n\n\nLog in to the Cloudbreak web UI: \n\n\n\n\nThe default username is \nadmin@example.com\n but you should sign up with your own email address.    \n\n\nThe password is the value of the \nUAA_DEFAULT_USER_PW\n variable that you configured in your \nProfile\n file when \nlaunching Cloudbreak deployer\n.\n\n\n\n\n\n\n\n\nUpon a successful login, you are redirected to the dashboard:\n\n\n  \n\n\n\n\n\n\nCreate Cloudbreak Credential\n\n\nCloudbreak works by connecting your GCP account through this credential, and then uses it to create resources on your behalf. Before you can start provisioning cluster using Cloudbreak, you must create a Cloudbreak credential.\n\n\nSteps\n\n\n\n\n\n\nIn the Cloudbreak web UI, select \nCredentials\n from the navigation pane. \n\n\n\n\n\n\nClick \nCreate Credential\n. \n\n\n\n\n\n\nUnder \nCloud provider\n, select \"Google Cloud Platform\":\n\n\n  \n\n\n\n\n\n\nProvide the following information:\n\n\n\n\n\n\n\n\nParameter\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nName\n\n\nEnter a name for your credential.\n\n\n\n\n\n\nDescription\n\n\n(Optional) Enter a description.\n\n\n\n\n\n\nProject Id\n\n\nEnter the project ID. You can obtain it from your GCP account by clicking on the name of your project at the top of the page and copying the \nID\n.\n\n\n\n\n\n\nService Account Email Address\n\n\n\"Service account ID\" value for your service account created in prerequisites. You can find it on GCP at \nIAM \n Admin\n \n \nService accounts\n.\n\n\n\n\n\n\nService Account Private (p12) Key\n\n\nUpload the P12 key that you created in the prerequisites when creating a service account.\n\n\n\n\n\n\n\n\n\n\n\n\nClick \nCreate\n.\n\n\n\n\n\n\nYour credential should now be displayed in the \nCredentials\n pane.\n\n\nCongratulations! You've successfully launched and configured Cloudbreak. Now you can use Cloudbreak to \ncreate clusters\n. \n\n\n\n\n\n\n\n\nNext: Create a Cluster", 
            "title": "Launch on GCP"
        }, 
        {
            "location": "/gcp-launch/index.html#launch-cloudbreak-on-gcp", 
            "text": "Before launching Cloudbreak on Google Cloud, review and meet the prerequisites. Next, import Cloudbreak image, launch a VM, SSH to the VM, and start Cloudbreak. Once Cloudbreak is running, log in to the Cloudbreak UI and create a Cloudbreak credential.", 
            "title": "Launch Cloudbreak on GCP"
        }, 
        {
            "location": "/gcp-launch/index.html#meet-the-prerequisites", 
            "text": "Before launching Cloudbreak on GCP, you must meet the following prerequisites.", 
            "title": "Meet the Prerequisites"
        }, 
        {
            "location": "/gcp-launch/index.html#gcp-account", 
            "text": "In order to launch Cloudbreak on GCP, you must log in to your GCP account. If you don't have an account, you can create one at  https://console.cloud.google.com .  Once you log in to your GCP account, you must either create a project or use an existing project.", 
            "title": "GCP Account"
        }, 
        {
            "location": "/gcp-launch/index.html#service-account", 
            "text": "In order to launch clusters on GCP via Cloudbreak, you must have a Service Account that Cloudbreak can use to create resources. In addition, you must also have a P12 key associated with the account. If you need to create these, refer to  GCP documentation  on how to create a service account and generate a P12 key.   Once you have the service account that you want to use for Cloudbreak, make sure that your service account fulfills one of the following APIs are enabled for your service account:   Compute Image User     Compute Instance Admin (v1)    Compute Network Admin    Compute Security Admin     A user with an \"Owner\" role can assign roles or access rules to service accounts from  IAM   Admin     IAM . For example:     Related Links  Service Account Credentials  (External)", 
            "title": "Service Account"
        }, 
        {
            "location": "/gcp-launch/index.html#vpc-network", 
            "text": "When launching Cloudbreak, you will be required to select an existing network in which Cloudbreak can be placed. The following ports must be open on the security group: 22 (SSH) and 443 (HTTPS). You may use the  default  network as long as the aforementioned ports are open.   You can manage networks under  Networking     VPC Networks . To edit ports, click on the network name and then click on  Add firewall rules .", 
            "title": "VPC Network"
        }, 
        {
            "location": "/gcp-launch/index.html#region-and-zone", 
            "text": "Decide in which region and zone you would like to launch Cloudbreak. You can launch Cloudbreak and provision your clusters in all regions  supported by GCP .    Clusters created via Cloudbreak can be in the same or different region as Cloudbreak; when you launch a cluster, you select the region in which to launch it.   Related Links  Regions and Zones  (External)", 
            "title": "Region and Zone"
        }, 
        {
            "location": "/gcp-launch/index.html#launch-the-vm", 
            "text": "Steps    Log in to Google Cloud Platform.    Open the  Google Cloud Shell  by clicking on the    icon in the top-right corner:       Import the Cloudbreak deployer image by executing the following command:   gcloud compute images create cloudbreak-deployer-1164-2017-08-29 --source-uri gs://sequenceiqimage/cloudbreak-deployer-1164-2017-08-29.tar.gz    In the GCP UI, from the  Products and services  menu, select  Compute Engine     Images .    In the search bar, type the name of the Cloudbreak deployer image that you imported earlier.    Select the image and then select  Create Instance :          You will be redirected to  VM instances     Create an instance  form. Provide the following parameters for your VM:         Parameter  Description      Name  Enter a name for the VM.    Zone  Select the zone in which to launch the VM.    Machine type  The minimum instance type suitable for Cloudbreak is  n1-standard-2 . The minimum requirements are 4GB RAM, 10GB disk, 2 cores.    Boot disk  Verify that the Cloudbreak deployer disk which you imported earlier is pre-selected.       Click on  Management, disks, networking, SSH keys  to view the options.    Under  Networking     Network interfaces , select the network in which you want to launch Cloudbreak.     Under  SSH Keys , check  Block project-wise SSH keys  and paste your public SSH key.    Click  Create .", 
            "title": "Launch the VM"
        }, 
        {
            "location": "/gcp-launch/index.html#ssh-to-the-vm", 
            "text": "Now that your VM is ready, access it via SSH:    Use a private key matching the public key that you added to your  project.  The SSH user is called \"cloudbreak\".  You can obtain the VM's IP address from  Compute Engine     VM Instances , the  External IP  column.   On Mac OS X, you can SSH to the VM by running the following from the Terminal app:  ssh -i \"your-private-key.pem\" cloudnreak@instance_IP  where \"your-private-key.pem\" points to the location of your private key and \"instance_IP\" is the public IP address of the VM.  On Windows, you can use  PuTTy .", 
            "title": "SSH to the VM"
        }, 
        {
            "location": "/gcp-launch/index.html#launch-cloudbreak-deployer", 
            "text": "After accessing the VM via SSH, launch Cloudbreak deployer using the following steps.  Steps      Navigate to the cloudbreak-deployment directory:  cd /var/lib/cloudbreak-deployment/  This directory contains configuration files and the supporting binaries for Cloudbreak deployer.    Initialize your profile by creating a new file called  Profile  and adding the following content:  export UAA_DEFAULT_SECRET=MY-SECRET\nexport UAA_DEFAULT_USER_PW=MY-PASSWORDP     For example:   export UAA_DEFAULT_SECRET=MySecret123\nexport UAA_DEFAULT_USER_PW=MySecurePassword123     You will need to provide the password when logging in to the Cloudbreak web UI and when using the Cloudbreak Shell. The secret will be used by Cloudbreak for authentication.       Start the Cloudbreak application by using the following command:  cbd start  This will start the Docker containers and initialize the application. The first time you start the Coudbreak app, the process will take longer than usual due to the download of all the necessary docker images.  The  cbd start  command includes the  cbd generate  command which applies the following steps:   Creates the  docker-compose.yml  file, which describes the configuration of all the Docker containers needed for the Cloudbreak deployment.  Creates the  uaa.yml  file, which holds the configuration of the identity server used to authenticate users with Cloudbreak.    Once the  cbd start  has finished, it returns the \"Uluwatu (Cloudbreak UI) url\" which you can later paste in your browser and log in to Cloudbreak web UI.     Check Cloudbreak deployer version and health:   cbd doctor    Next, check Cloudbreak Application logs:   cbd logs cloudbreak  You should see a message like this in the log:  Started CloudbreakApplication in 36.823 seconds.  Cloudbreak normally takes less than a minute to start.", 
            "title": "Launch Cloudbreak Deployer"
        }, 
        {
            "location": "/gcp-launch/index.html#access-cloudbreak-ui", 
            "text": "Log in to the Cloudbreak UI using the following steps.  Steps    You can log into the Cloudbreak application at   https://IP_Address . For example  https://34.212.141.253 .  You can obtain the VM's IP address from  Compute Engine     VM Instances , the  External IP  column.    Confirm the security exception to proceed to the Cloudbreak web UI.  The first time you access Cloudbreak UI, Cloudbreak will automatically generate a self-signed certificate, due to which your browser will warn you about an untrusted connection and will ask you to confirm a security exception.    The login page is displayed:        Log in to the Cloudbreak web UI:    The default username is  admin@example.com  but you should sign up with your own email address.      The password is the value of the  UAA_DEFAULT_USER_PW  variable that you configured in your  Profile  file when  launching Cloudbreak deployer .     Upon a successful login, you are redirected to the dashboard:", 
            "title": "Access Cloudbreak UI"
        }, 
        {
            "location": "/gcp-launch/index.html#create-cloudbreak-credential", 
            "text": "Cloudbreak works by connecting your GCP account through this credential, and then uses it to create resources on your behalf. Before you can start provisioning cluster using Cloudbreak, you must create a Cloudbreak credential.  Steps    In the Cloudbreak web UI, select  Credentials  from the navigation pane.     Click  Create Credential .     Under  Cloud provider , select \"Google Cloud Platform\":        Provide the following information:     Parameter  Description      Name  Enter a name for your credential.    Description  (Optional) Enter a description.    Project Id  Enter the project ID. You can obtain it from your GCP account by clicking on the name of your project at the top of the page and copying the  ID .    Service Account Email Address  \"Service account ID\" value for your service account created in prerequisites. You can find it on GCP at  IAM   Admin     Service accounts .    Service Account Private (p12) Key  Upload the P12 key that you created in the prerequisites when creating a service account.       Click  Create .    Your credential should now be displayed in the  Credentials  pane.  Congratulations! You've successfully launched and configured Cloudbreak. Now you can use Cloudbreak to  create clusters .      Next: Create a Cluster", 
            "title": "Create Cloudbreak Credential"
        }, 
        {
            "location": "/gcp-create/index.html", 
            "text": "Create a Cluster on GCP\n\n\nUse these steps to create a cluster.\n\n\nSteps\n\n\n\n\n\n\nLog in to the Cloudbreak UI.\n\n\n\n\n\n\nClick \nCreate Cluster\n and the \nCreate Cluster\n form is displayed.\n\n\nTo view advanced options, click \nAdvanced\n. To learn about advanced options, refer to \nAdvanced Options\n.\n\n\n \n\n\n\n\n\n\nOn the \nGeneral Configuration\n page, specify the following general parameters for your cluster:\n\n\n\n\n\n\n\n\nParameter\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nSelect Credential\n\n\nChoose a previously created credential.\n\n\n\n\n\n\nCluster Name\n\n\nEnter a name for your cluster. The name must be between 5 and 40 characters, must start with a letter, and must only include lowercase letters, numbers, and hyphens.\n\n\n\n\n\n\nRegion\n\n\nSelect the region in which you would like to launch your cluster.\n\n\n\n\n\n\nPlatform Version\n\n\nChoose the HDP version to use for this cluster.\n\n\n\n\n\n\nCluster Type\n\n\nChoose one of default cluster configurations, or, if you have defined your own cluster configuration via Ambari blueprint, you can choose it here. For more information, refer to \nBlueprints\n.\n\n\n\n\n\n\n\n\n\n\n\n\nOn the \nHardware and Storage\n page, for each host group provide the following information to define your cluster nodes and attached storage:\n\n\n\n\n\n\n\n\nParameter\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nInstance Type\n\n\nSelect a VM instance type. For more information about instance types on Azure refer to \nGCP documentation\n.\n\n\n\n\n\n\nInstance Count\n\n\nEnter the number of instances of a given type. Default is 1.\n\n\n\n\n\n\nAmbari Server\n\n\nYou must select one node for Ambari Server. The \"Group Size\" for that host group must be set to \"1\".\n\n\n\n\n\n\n\n\n\n\n\n\nOn the \nNetwork\n page, provide the following to specify the networking resources that will be used for your cluster:\n\n\n\n\n\n\n\n\nParameter\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nSelect Network\n\n\nSelect the virtual network in which you would like your cluster to be provisioned. You can select an existing network or create a new network.\n\n\n\n\n\n\nSelect Subnet\n\n\nSelect the subnet in which you would like your cluster to be provisioned. You must create a new subnet.\n\n\n\n\n\n\nSubnet (CIDR)\n\n\nIf you selected to create a new subnet, you must define a valid \nCIDR\n for the subnet. Default is 10.0.0.0/16.\n\n\n\n\n\n\n\n\n\n\n\n\nOn the \nSecurity\n page, provide the following parameters:\n\n\n\n\n\n\n\n\nParameter\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nCluster User\n\n\nYou can log in to the Ambari UI using this username. By default, this is set to \nadmin\n.\n\n\n\n\n\n\nPassword\n\n\nYou can log in to the Ambari UI using this password.\n\n\n\n\n\n\nConfirm Password\n\n\nConfirm the password.\n\n\n\n\n\n\nSSH Key Pair\n\n\nSelect an existing public key or specify a new public key. You will use the matching private key to access your cluster nodes via SSH.\n\n\n\n\n\n\n\n\n\n\n\n\nClick on \nCreate Cluster\n to create a cluster.\n\n\n\n\n\n\nYou will be redirected to the Cloudbreak dashboard, and a new tile representing your cluster will appear at the top of the page.\n\n\n\n\n\n\nRelated Links\n\n\nBlueprints\n \n\n\nCIDR\n (External) \n\n\nMachine Types\n (External)  \n\n\nAdvanced Options\n\n\nClick on \nAdvanced\n to view and enter additional configuration options.\n\n\nAvailability Zone\n\n\nChoose one of the availability zones within the selected region. \n\n\nEnable Lifetime Management\n\n\nCheck this option if you would like your cluster to be automatically terminated after a specific amount of time (defined as \"Time to Live\" in minutes) has passed. \n\n\nTags\n\n\nYou can optionally add tags, which will help you find your cluster-related resources, such as VMs, in your cloud provider account. refer to \nResource Tagging\n.\n\n\nRelated Links\n\n\nResource Tagging\n  \n\n\nStorage\n\n\nYou can optionally specify the following storage options for your cluster:\n\n\n\n\n\n\n\n\nParameter\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nStorage Type\n\n\nSelect the volume type. The options are:\nStandard persistent disks (HDD)\nSolid-state persistent disks (SSD)\n For more information about these options refer to \nGCP documentation\n.\n\n\n\n\n\n\nAttached Volumes Per Instance\n\n\nEnter the number of volumes attached per instance. Default is 1.\n\n\n\n\n\n\nVolume Size (GB)\n\n\nEnter the size in GBs for each volume. Default is 100.\n\n\n\n\n\n\n\n\nRelated Links\n\n\nStorage Options\n (External)   \n\n\nRecipes\n\n\nThis option allows you to select previously uploaded recipes (scripts that will be run pre- or post- cluster deployment) for each host group. For more information on default and custom blueprints, refer to \nRecipes\n.\n\n\nRelated Links\n\n\nRecipes\n \n\n\nSecurity Groups\n\n\nFor each host group, select one of the options:\n\n\n\n\nCreate new security group  \n\n\nDo not use security group  \n\n\nSelect an existing security group  \n\n\n\n\nIf you choose to create a new security group, the \nNew Security Group\n wizard will open.\n\n\n\n\nTCP ports 22, 443, and 9443 will be open by default. These ports must be open on every security group; otherwise Cloudbreak will not be able to communicate with your provisioned cluster.  \n\n\nYou may open additional ports by defining the \nCIDR\n, \nPort\n, and \nProtocol\n for each and clicking \nAdd Rule\n. \n\n\nOnce done, click \nSAVE\n to save your security group settings.\n\n\nOnce you define a custom security group for one host group, you can reuse this definition for other node groups.\n\n\n\n\nEnable Kerberos Security\n\n\nSelect this option to enable Kerberos for your cluster. You will have an option to create a new kerberos or use an existing one. For more information refer to \nKerberos\n documentation. \n\n\nRelated Links\n \n\n\nKerberos\n  \n\n\n\n\nNext: Access Cluster", 
            "title": "Create a Cluster"
        }, 
        {
            "location": "/gcp-create/index.html#create-a-cluster-on-gcp", 
            "text": "Use these steps to create a cluster.  Steps    Log in to the Cloudbreak UI.    Click  Create Cluster  and the  Create Cluster  form is displayed.  To view advanced options, click  Advanced . To learn about advanced options, refer to  Advanced Options .       On the  General Configuration  page, specify the following general parameters for your cluster:     Parameter  Description      Select Credential  Choose a previously created credential.    Cluster Name  Enter a name for your cluster. The name must be between 5 and 40 characters, must start with a letter, and must only include lowercase letters, numbers, and hyphens.    Region  Select the region in which you would like to launch your cluster.    Platform Version  Choose the HDP version to use for this cluster.    Cluster Type  Choose one of default cluster configurations, or, if you have defined your own cluster configuration via Ambari blueprint, you can choose it here. For more information, refer to  Blueprints .       On the  Hardware and Storage  page, for each host group provide the following information to define your cluster nodes and attached storage:     Parameter  Description      Instance Type  Select a VM instance type. For more information about instance types on Azure refer to  GCP documentation .    Instance Count  Enter the number of instances of a given type. Default is 1.    Ambari Server  You must select one node for Ambari Server. The \"Group Size\" for that host group must be set to \"1\".       On the  Network  page, provide the following to specify the networking resources that will be used for your cluster:     Parameter  Description      Select Network  Select the virtual network in which you would like your cluster to be provisioned. You can select an existing network or create a new network.    Select Subnet  Select the subnet in which you would like your cluster to be provisioned. You must create a new subnet.    Subnet (CIDR)  If you selected to create a new subnet, you must define a valid  CIDR  for the subnet. Default is 10.0.0.0/16.       On the  Security  page, provide the following parameters:     Parameter  Description      Cluster User  You can log in to the Ambari UI using this username. By default, this is set to  admin .    Password  You can log in to the Ambari UI using this password.    Confirm Password  Confirm the password.    SSH Key Pair  Select an existing public key or specify a new public key. You will use the matching private key to access your cluster nodes via SSH.       Click on  Create Cluster  to create a cluster.    You will be redirected to the Cloudbreak dashboard, and a new tile representing your cluster will appear at the top of the page.    Related Links  Blueprints    CIDR  (External)   Machine Types  (External)", 
            "title": "Create a Cluster on GCP"
        }, 
        {
            "location": "/gcp-create/index.html#advanced-options", 
            "text": "Click on  Advanced  to view and enter additional configuration options.", 
            "title": "Advanced Options"
        }, 
        {
            "location": "/gcp-create/index.html#availability-zone", 
            "text": "Choose one of the availability zones within the selected region.", 
            "title": "Availability Zone"
        }, 
        {
            "location": "/gcp-create/index.html#enable-lifetime-management", 
            "text": "Check this option if you would like your cluster to be automatically terminated after a specific amount of time (defined as \"Time to Live\" in minutes) has passed.", 
            "title": "Enable Lifetime Management"
        }, 
        {
            "location": "/gcp-create/index.html#tags", 
            "text": "You can optionally add tags, which will help you find your cluster-related resources, such as VMs, in your cloud provider account. refer to  Resource Tagging .  Related Links  Resource Tagging", 
            "title": "Tags"
        }, 
        {
            "location": "/gcp-create/index.html#storage", 
            "text": "You can optionally specify the following storage options for your cluster:     Parameter  Description      Storage Type  Select the volume type. The options are: Standard persistent disks (HDD) Solid-state persistent disks (SSD)  For more information about these options refer to  GCP documentation .    Attached Volumes Per Instance  Enter the number of volumes attached per instance. Default is 1.    Volume Size (GB)  Enter the size in GBs for each volume. Default is 100.     Related Links  Storage Options  (External)", 
            "title": "Storage"
        }, 
        {
            "location": "/gcp-create/index.html#recipes", 
            "text": "This option allows you to select previously uploaded recipes (scripts that will be run pre- or post- cluster deployment) for each host group. For more information on default and custom blueprints, refer to  Recipes .  Related Links  Recipes", 
            "title": "Recipes"
        }, 
        {
            "location": "/gcp-create/index.html#security-groups", 
            "text": "For each host group, select one of the options:   Create new security group    Do not use security group    Select an existing security group     If you choose to create a new security group, the  New Security Group  wizard will open.   TCP ports 22, 443, and 9443 will be open by default. These ports must be open on every security group; otherwise Cloudbreak will not be able to communicate with your provisioned cluster.    You may open additional ports by defining the  CIDR ,  Port , and  Protocol  for each and clicking  Add Rule .   Once done, click  SAVE  to save your security group settings.  Once you define a custom security group for one host group, you can reuse this definition for other node groups.", 
            "title": "Security Groups"
        }, 
        {
            "location": "/gcp-create/index.html#enable-kerberos-security", 
            "text": "Select this option to enable Kerberos for your cluster. You will have an option to create a new kerberos or use an existing one. For more information refer to  Kerberos  documentation.   Related Links    Kerberos      Next: Access Cluster", 
            "title": "Enable Kerberos Security"
        }, 
        {
            "location": "/gcp-clusters-access/index.html", 
            "text": "Access Your Cluster\n\n\nThe following section describes how to access the various services in the cluster.\n\n\nFinding Cluster Information in the UI\n\n\nOnce your cluster is up and running, click on the tile representing your cluster in the Cloudbreak UI to access information related the cluster and access cluster actions. \n\n\n \n\n\nThe information presented includes:\n\n\n\n\nCluster Summary\n\n\nCluster Information\n \n\n\nHardware\n\n\nTags\n \n\n\nEvent History\n \n\n\n\n\n\n  \nTips\n\n  \n\n  \n Access cluster actions such as resize and sync by clicking on \nACTIONS\n.\n\n  \n Access Ambari web UI by clicking on the link in the \nCLUSTER INFORMATION\n section.\n\n\n View public IP addresses for all cluster instances in the \nHARDWARE\n section. Click on the links to view the instances in the cloud console.\n\n\n The SSH user that you must use when accessing cluster VMs is \"cloudbreak\".\n \n\n\n\n\n\n\n\n\nCluster Summary\n\n\nThe summary bar includes the following information about your cluster:\n\n\n\n\n\n\n\n\nItem\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nCloud Provider\n\n\nThe logo of the cloud provider on which the cluster is running.\n\n\n\n\n\n\nCredential\n\n\nThe name of the credential used to create the cluster.\n\n\n\n\n\n\nStatus\n\n\nCurrent status. When a cluster is healthy, the status is \nRunning\n.\n\n\n\n\n\n\nNodes\n\n\nThe current number of cluster nodes, including the master node.\n\n\n\n\n\n\nUptime\n\n\nThe amount of time (HH:MM) that the cluster has been in the running state.\n\n\n\n\n\n\nCreated\n\n\nThe date when the cluster was created. The date format is Mon DD, YYYY. For exampple: Oct 27, 2017.\n\n\n\n\n\n\n\n\nCluster Information\n\n\n\n\n\n\n\n\nItem\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nNetwork\n\n\nThe name of the network in which the cluster is running.\n\n\n\n\n\n\nSubnet\n\n\nThe name of the subnet in which the cluster is running.\n\n\n\n\n\n\nCluster User\n\n\nThe name of the cluster user that you created when creating the cluster.\n\n\n\n\n\n\nSSH Username\n\n\nThe SSH user which you must use when accessing cluster VMs via SSH. The SSH user is always \"cloudbreak\".\n\n\n\n\n\n\nAmbari URL\n\n\nLink to the Ambari web UI.\n\n\n\n\n\n\nRegion\n\n\nThe region in which the cluster is running in the cloud provider infrastructure.\n\n\n\n\n\n\nAvailability Zone\n\n\nThe availability zone within the region in which the cluster is running.\n\n\n\n\n\n\nBlueprint\n\n\nThe name of the blueprint selected under \"Cluster Type\" to create this cluster.\n\n\n\n\n\n\nStarted With\n\n\nThe version of Cloubdreak used to create this cluster.\n\n\n\n\n\n\nAmbari Version\n\n\nThe Ambari version which this cluster is currently running.\n\n\n\n\n\n\nHDP Version\n\n\nThe HDP version which this cluster is currently running.\n\n\n\n\n\n\n\n\nHardware\n\n\nThis section includes information about your cluster nodes: instance names, instance IDs (with links to the cloud provider console), and public IPs.\n\n\nTags\n\n\nThis section lists user-defined tags, in the same order as you added them.\n\n\nEvent History\n\n\nThe Event History section shows you events logged for the cluster, with the most recent event at the top. For example, after your cluster has been created, the following messages will be written to the log:\n\n\n\nAmbari cluster built; Ambari ip:34.215.103.66\n10/26/2017, 9:41:58 AM\nBuilding Ambari cluster; Ambari ip:34.215.103.66\n10/26/2017, 9:30:20 AM\nStarting Ambari cluster services\n10/26/2017, 9:27:12 AM\nSetting up infrastructure metadata\n10/26/2017, 9:27:11 AM\nBootstrapping infrastructure cluster\n10/26/2017, 9:26:38 AM\nInfrastructure successfully provisioned\n10/26/2017, 9:26:37 AM\nBilling started, Infrastructure successfully provisioned\n10/26/2017, 9:26:37 AM\nInfrastructure metadata collection finished\n10/26/2017, 9:25:39 AM\nInfrastructure creation took 194 seconds\n10/26/2017, 9:25:37 AM\nCreating infrastructure\n10/26/2017, 9:22:22 AM\nSetting up HDP image\n10/26/2017, 9:22:21 AM\n\n\n\nAccess Cluster via SSH\n\n\nIf you plan to access the cluster via the command line clients, SSH into the master node instance in the cluster. \n\n\n\n\nIn order to use SSH, you must generate an SSH key pair or use an existing SSH keypair.  \n\n\nYou can find the cluster instance public IP addresses on the cluster details page.  \n\n\nWhen accessing instances via SSH use the \ncloudbreak\n user. \n\n\n\n\nOn Mac OS, you can use the following syntax to SSH to the VM:\n\nssh -i \"privatekey.pem\" cloudbreak@publicIP\n\nFor example:\n\nssh -i \"dominika-kp.pem\" cloudbreak@p52.25.169.132\n\n\nOn Windows, you can SSH using an SSH client such as PuTTY.\n\n\nAccess Ambari\n\n\nYou can access Ambari web UI by clicking on the links provided in the \nCluster Information\n \n \nAmbari URL\n.\n\n\nSteps\n\n\n\n\n\n\nFrom the cluster dashboard, click on the tile representing your cluster to navigate to cluster details.\n\n\n\n\n\n\nFind the Ambairi URL in the \nCluster Information\n section. This URL is available once the Ambari cluster creation process has completed.  \n\n\n\n\n\n\nClick on the \nAmbari URL\n link.\n\n\n\n\n\n\nThe first time you access the server, your browser will attempt to confirm that the SSL Certificate is valid. Since Cloudbreak automatically generates a self-signed certificate, your browser will warn you about an Untrusted Connection and ask you to confirm a Security Exception. Depending on your browser, perform the steps below to proceed.\n\n\n\n\n\n\n\n\nBrowser\n\n\nSteps\n\n\n\n\n\n\n\n\n\n\nFirefox\n\n\nClick \nAdvanced\n \n Click \nAdd Exception...\n \n Click \nConfirm Security Exception\n\n\n\n\n\n\nSafari\n\n\nClick \nContinue\n\n\n\n\n\n\nChrome\n\n\nClick \nAdvanced\n \n Click \nProceed...\n\n\n\n\n\n\n\n\n\n\n\n\nUser Accounts\n\n\nThe following table describes what credentials to use to access Cloudbreak and Cloudbreak-managed clusters:\n\n\n\n\n\n\n\n\nComponent\n\n\nMethod\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nCloudbreak\n\n\nWeb UI, CLI\n\n\nAccess with the username and password provided when launching Cloudbreak on the cloud provider.\n\n\n\n\n\n\nCloudbreak\n\n\nSSH to VM\n\n\nAccess as the \"cloudbreak\" user with the SSH key provided when launching Cloudbreak on the cloud provider.\n\n\n\n\n\n\nCluster\n\n\nSSH to VMs\n\n\nAccess as the \"cloudbreak\" user with the SSH key provided during cluster creation.\n\n\n\n\n\n\nCluster\n\n\nAmbari UI\n\n\nAccess with the credentials provided in the \u201cCluster User\u201d parameter during cluster creation.\n\n\n\n\n\n\n\n\n\n\nNext: Manage and Monitor Clusters", 
            "title": "Access Cluster"
        }, 
        {
            "location": "/gcp-clusters-access/index.html#access-your-cluster", 
            "text": "The following section describes how to access the various services in the cluster.", 
            "title": "Access Your Cluster"
        }, 
        {
            "location": "/gcp-clusters-access/index.html#finding-cluster-information-in-the-ui", 
            "text": "Once your cluster is up and running, click on the tile representing your cluster in the Cloudbreak UI to access information related the cluster and access cluster actions.      The information presented includes:   Cluster Summary  Cluster Information    Hardware  Tags    Event History     \n   Tips \n   \n    Access cluster actions such as resize and sync by clicking on  ACTIONS . \n    Access Ambari web UI by clicking on the link in the  CLUSTER INFORMATION  section.   View public IP addresses for all cluster instances in the  HARDWARE  section. Click on the links to view the instances in the cloud console.   The SSH user that you must use when accessing cluster VMs is \"cloudbreak\".", 
            "title": "Finding Cluster Information in the UI"
        }, 
        {
            "location": "/gcp-clusters-access/index.html#cluster-summary", 
            "text": "The summary bar includes the following information about your cluster:     Item  Description      Cloud Provider  The logo of the cloud provider on which the cluster is running.    Credential  The name of the credential used to create the cluster.    Status  Current status. When a cluster is healthy, the status is  Running .    Nodes  The current number of cluster nodes, including the master node.    Uptime  The amount of time (HH:MM) that the cluster has been in the running state.    Created  The date when the cluster was created. The date format is Mon DD, YYYY. For exampple: Oct 27, 2017.", 
            "title": "Cluster Summary"
        }, 
        {
            "location": "/gcp-clusters-access/index.html#cluster-information", 
            "text": "Item  Description      Network  The name of the network in which the cluster is running.    Subnet  The name of the subnet in which the cluster is running.    Cluster User  The name of the cluster user that you created when creating the cluster.    SSH Username  The SSH user which you must use when accessing cluster VMs via SSH. The SSH user is always \"cloudbreak\".    Ambari URL  Link to the Ambari web UI.    Region  The region in which the cluster is running in the cloud provider infrastructure.    Availability Zone  The availability zone within the region in which the cluster is running.    Blueprint  The name of the blueprint selected under \"Cluster Type\" to create this cluster.    Started With  The version of Cloubdreak used to create this cluster.    Ambari Version  The Ambari version which this cluster is currently running.    HDP Version  The HDP version which this cluster is currently running.", 
            "title": "Cluster Information"
        }, 
        {
            "location": "/gcp-clusters-access/index.html#hardware", 
            "text": "This section includes information about your cluster nodes: instance names, instance IDs (with links to the cloud provider console), and public IPs.", 
            "title": "Hardware"
        }, 
        {
            "location": "/gcp-clusters-access/index.html#tags", 
            "text": "This section lists user-defined tags, in the same order as you added them.", 
            "title": "Tags"
        }, 
        {
            "location": "/gcp-clusters-access/index.html#event-history", 
            "text": "The Event History section shows you events logged for the cluster, with the most recent event at the top. For example, after your cluster has been created, the following messages will be written to the log:  \nAmbari cluster built; Ambari ip:34.215.103.66\n10/26/2017, 9:41:58 AM\nBuilding Ambari cluster; Ambari ip:34.215.103.66\n10/26/2017, 9:30:20 AM\nStarting Ambari cluster services\n10/26/2017, 9:27:12 AM\nSetting up infrastructure metadata\n10/26/2017, 9:27:11 AM\nBootstrapping infrastructure cluster\n10/26/2017, 9:26:38 AM\nInfrastructure successfully provisioned\n10/26/2017, 9:26:37 AM\nBilling started, Infrastructure successfully provisioned\n10/26/2017, 9:26:37 AM\nInfrastructure metadata collection finished\n10/26/2017, 9:25:39 AM\nInfrastructure creation took 194 seconds\n10/26/2017, 9:25:37 AM\nCreating infrastructure\n10/26/2017, 9:22:22 AM\nSetting up HDP image\n10/26/2017, 9:22:21 AM", 
            "title": "Event History"
        }, 
        {
            "location": "/gcp-clusters-access/index.html#access-cluster-via-ssh", 
            "text": "If you plan to access the cluster via the command line clients, SSH into the master node instance in the cluster.    In order to use SSH, you must generate an SSH key pair or use an existing SSH keypair.    You can find the cluster instance public IP addresses on the cluster details page.    When accessing instances via SSH use the  cloudbreak  user.    On Mac OS, you can use the following syntax to SSH to the VM: ssh -i \"privatekey.pem\" cloudbreak@publicIP \nFor example: ssh -i \"dominika-kp.pem\" cloudbreak@p52.25.169.132  On Windows, you can SSH using an SSH client such as PuTTY.", 
            "title": "Access Cluster via SSH"
        }, 
        {
            "location": "/gcp-clusters-access/index.html#access-ambari", 
            "text": "You can access Ambari web UI by clicking on the links provided in the  Cluster Information     Ambari URL .  Steps    From the cluster dashboard, click on the tile representing your cluster to navigate to cluster details.    Find the Ambairi URL in the  Cluster Information  section. This URL is available once the Ambari cluster creation process has completed.      Click on the  Ambari URL  link.    The first time you access the server, your browser will attempt to confirm that the SSL Certificate is valid. Since Cloudbreak automatically generates a self-signed certificate, your browser will warn you about an Untrusted Connection and ask you to confirm a Security Exception. Depending on your browser, perform the steps below to proceed.     Browser  Steps      Firefox  Click  Advanced    Click  Add Exception...    Click  Confirm Security Exception    Safari  Click  Continue    Chrome  Click  Advanced    Click  Proceed...", 
            "title": "Access Ambari"
        }, 
        {
            "location": "/gcp-clusters-access/index.html#user-accounts", 
            "text": "The following table describes what credentials to use to access Cloudbreak and Cloudbreak-managed clusters:     Component  Method  Description      Cloudbreak  Web UI, CLI  Access with the username and password provided when launching Cloudbreak on the cloud provider.    Cloudbreak  SSH to VM  Access as the \"cloudbreak\" user with the SSH key provided when launching Cloudbreak on the cloud provider.    Cluster  SSH to VMs  Access as the \"cloudbreak\" user with the SSH key provided during cluster creation.    Cluster  Ambari UI  Access with the credentials provided in the \u201cCluster User\u201d parameter during cluster creation.      Next: Manage and Monitor Clusters", 
            "title": "User Accounts"
        }, 
        {
            "location": "/gcp-clusters-manage/index.html", 
            "text": "Manage and Monitor Clusters\n\n\nYou can manage monitor your clusters from the Cloudbreak UI. To do that, click on the tile representing the cluster that you want to access. The actions available for your cluster are listed in the top right corner: \n\n\n \n\n\n\n  \nTips\n\n  \n\n  \nTo add or remove nodes from your cluster click \nACTIONS>Resize\n.\n\n  \nTo synchronize your cluster with the cloud provider account click \nACTIONS>Sync\n.\n\n  \nTo temporarily stop your cluster click \nSTOP\n.\n\n  \nTo terminate your cluster click \nTERMINATE\n.\n\n\n\n\n\n\n\n\n\nResize Cluster\n\n\nTo resize a cluster, follow these steps.\n\n\nSteps\n\n\n\n\n\n\nBrowse to the cluster details.\n\n\n\n\n\n\nClick \nActions\n and select \nResize\n. The cluster resize dialog is displayed.\n\n\n\n\n\n\nUsing the +/- controls, adjust how many nodes to add or remove from each host group.  \n\n\n\n\n\n\nClick \nYes\n to confirm the scale-up/scale-down.\n\n\nWhile nodes are being added or removed, cluster status changes to \"Update In Progress\". Once the operation has completed, cluster status changes back to \"Running\". Messages similar to the following are written to the \"Event History\", : \n\n\nAmbari cluster scaled up\n11/6/2017, 12:33:40 PM\nScaling up the Ambari cluster\n11/6/2017, 12:26:59 PM\nStack successfully upscaled\n11/6/2017, 12:26:54 PM\nBootstrapping new nodes\n11/6/2017, 12:26:16 PM\nInfrastructure metadata extension finished\n11/6/2017, 12:26:10 PM\nBilling changed due to upscaling of cluster infrastructure\n11/6/2017, 12:26:10 PM\nAdding 1 new instances to the infrastructure\n11/6/2017, 12:25:23 PM\n\n\n\n\n\n\nSynchronize with Cloud Provider\n\n\nIf you have just made changes on your cloud provider side (for example, deleted cluster VMs) and you would like to synchronize Cloudbreak with the cloud provider, use the \nsync\n option. \n\n\nTo synchronize your cluster with the cloud provider, follow these steps. \n\n\nSteps\n\n\n\n\n\n\nBrowse to the cluster details.\n\n\n\n\n\n\nClick \nActions\n and select \nSync\n.\n\n\n\n\n\n\nClick \nYes\n to confirm.\n\n\nYour cluster infrastructure is synchronized based on changes on the cloud provider. The updates are written to the \"Event History\". \n\n\n\n\n\n\nStop Cluster\n\n\nCloudbreak supports stopping and restarting clusters. To stop and restart a cluster managed by Cloudbreak, use the options available from the Coudbreak UI. \n\n\nSteps\n\n\n\n\n\n\nBrowse to the cluster details.\n\n\n\n\n\n\nClick \nStop\n to stop a currently running cluster.  \n\n\n\n\n\n\nClick \nYes\n to confirm. \n\n\nYour cluster status changes to \"Stopping in progress\" and then to \"Stopped\". You should see the following messages in the \"Event History\":\n\n\nBilling stopped, the cluster and its infrastructure have been terminated\n11/6/2017, 12:46:48 PM\nInfrastructure successfully stopped\n11/6/2017, 12:46:47 PM\nInfrastructure is now stopping\n11/6/2017, 12:43:58 PM\nAmbari cluster stopped\n11/6/2017, 12:43:52 PM\nAmbari services have been stopped.\n11/6/2017, 12:43:49 PM\nStopping Ambari services.\n11/6/2017, 12:42:14 PM\nStopping Ambari cluster\n11/6/2017, 12:42:10 PM\nCluster infrastructure stop requested\n11/6/2017, 12:42:06 PM\n\n\n\n\n\n\nOnce stopping the infrastructure has completed, you will see a \nStart\n option to restart your cluster. \n\n\nRestart Cluster\n\n\nIf your cluster is in the \"Stopped\" state, you can restart the cluster by follow these steps.\n\n\nSteps\n\n\n\n\n\n\nclick \nStart\n. This option is only available when the cluster has been stopped. \n\n\n\n\n\n\nClick \nYes\n to confirm.\n\n\nYour cluster status changes to \"Start in progress\" and then to \"Running\". You should see the following messages in the \"Event History\":\n\n\nAmbari cluster started; Ambari ip:35.203.149.236\n11/6/2017, 1:02:13 PM\nAmbari services have been started.\n11/6/2017, 1:02:08 PM\nStarting Ambari services.\n11/6/2017, 12:54:37 PM\nStarting Ambari cluster\n11/6/2017, 12:52:57 PM\nBilling started, the cluster and its infrastructure have successfully been started\n11/6/2017, 12:52:48 PM\nInfrastructure successfully started\n11/6/2017, 12:52:48 PM\nInfrastructure is now starting\n11/6/2017, 12:52:26 PM\nAmbari cluster start requested\n11/6/2017, 12:52:24 PM\n\n\n\n\n\n\nTerminate Cluster\n\n\nTo terminate a cluster managed by Cloudbreak, use the option available from the Coudbreak UI. \n\n\nSteps\n\n\n\n\n\n\nBrowse to the cluster details.\n\n\n\n\n\n\nClick \nTerminate\n. \n\n\n\n\n\n\nClick \nYes\n to confirm.\n\n\nAll cluster-related resources will be deleted, unless the resources (such as networks and subnets) existed prior to cluster creation or are used by other VMs in which case they will be preserved. \n\n\n\n\n\n\nForce Terminate\n\n\nCluster deletion may fail if Cloudbreak is unable to delete one or more of the cloud resources that were part of your cluster infrastructure. In such as case, you can use the \nTerminate\n \n \nForce terminate\n option to remove the cluster entry from the Cloudbreak web UI, but you must also check your cloud provider account to see if there are any resources that must be deleted manually.\n\n\nSteps\n\n\n\n\n\n\nBrowse to the cluster details.\n\n\n\n\n\n\nClick \nTerminate\n. \n\n\n\n\n\n\nCheck  \nForce terminate\n.\n\n\n\n\n\n\nClick \nYes\n to confirm. \n\n\n\n\n\n\nThis deletes the cluster tile from the UI.  \n\n\n\n\n\n\nLog in to your cloud provider account and \nmanually delete\n any resources that failed to be deleted.\n\n\n\n\n\n\nView Cluster History\n\n\nFrom the navigation menu in the Cloudbreak UI, you can access the History page that allows you to generate a report showing basic information related to the clusters that were running within the specified range of dates.\n\n\nTo generate a report, follow these steps.\n\n\nSteps\n\n\n\n\n\n\nFrom the Cloudbreak UI navigation menu, select \nHistory\n.\n\n\n\n\n\n\nOn the History page, select the range of dates and click \nShow History\n to generate a report for the selected period.\n\n\n\n\n\n\nHistory Report Content\n\n\nEach entry in the report represents one cluster instance group. For each entry, the report includes the following information:\n\n\n\n\nCreated\n - The date when your cluster was created (YYYY-MM-DD).\n\n\nProvider\n - The name of the cloud provider (AWS, Azure, Google, or OpenStack) on which the cluster instances are/were running.\n\n\nCluster Name\n - The name that you selected for the cluster.\n\n\nWorker Count\n - The number of worker nodes in the cluster. This number may be a decimal if a cluster has been resized.\n\n\nInstance Type\n - Provider-specific VM type of the cluster instances.\n\n\nInstance Group\n - The name of the instance group.  \n\n\nRegion\n - The AWS region in which your cluster is running.\n\n\nRunning Time (hours)\n - The sum of the running times for all the nodes in the instance group.\n\n\n\n\nThe \nAGGREGATE RUNNING TIME\n is the sum of the Running Times, adjusted for the selected time range.\n\n\nTo learn about how your cloud provider bills you for the VMs, refer to their documentation:\n\n\n\n\nAWS\n      \n\n\nAzure\n     \n\n\nGCP", 
            "title": "Manage and Monitor Clusters"
        }, 
        {
            "location": "/gcp-clusters-manage/index.html#manage-and-monitor-clusters", 
            "text": "You can manage monitor your clusters from the Cloudbreak UI. To do that, click on the tile representing the cluster that you want to access. The actions available for your cluster are listed in the top right corner:      \n   Tips \n   \n   To add or remove nodes from your cluster click  ACTIONS>Resize . \n   To synchronize your cluster with the cloud provider account click  ACTIONS>Sync . \n   To temporarily stop your cluster click  STOP . \n   To terminate your cluster click  TERMINATE .", 
            "title": "Manage and Monitor Clusters"
        }, 
        {
            "location": "/gcp-clusters-manage/index.html#resize-cluster", 
            "text": "To resize a cluster, follow these steps.  Steps    Browse to the cluster details.    Click  Actions  and select  Resize . The cluster resize dialog is displayed.    Using the +/- controls, adjust how many nodes to add or remove from each host group.      Click  Yes  to confirm the scale-up/scale-down.  While nodes are being added or removed, cluster status changes to \"Update In Progress\". Once the operation has completed, cluster status changes back to \"Running\". Messages similar to the following are written to the \"Event History\", :   Ambari cluster scaled up\n11/6/2017, 12:33:40 PM\nScaling up the Ambari cluster\n11/6/2017, 12:26:59 PM\nStack successfully upscaled\n11/6/2017, 12:26:54 PM\nBootstrapping new nodes\n11/6/2017, 12:26:16 PM\nInfrastructure metadata extension finished\n11/6/2017, 12:26:10 PM\nBilling changed due to upscaling of cluster infrastructure\n11/6/2017, 12:26:10 PM\nAdding 1 new instances to the infrastructure\n11/6/2017, 12:25:23 PM", 
            "title": "Resize Cluster"
        }, 
        {
            "location": "/gcp-clusters-manage/index.html#synchronize-with-cloud-provider", 
            "text": "If you have just made changes on your cloud provider side (for example, deleted cluster VMs) and you would like to synchronize Cloudbreak with the cloud provider, use the  sync  option.   To synchronize your cluster with the cloud provider, follow these steps.   Steps    Browse to the cluster details.    Click  Actions  and select  Sync .    Click  Yes  to confirm.  Your cluster infrastructure is synchronized based on changes on the cloud provider. The updates are written to the \"Event History\".", 
            "title": "Synchronize with Cloud Provider"
        }, 
        {
            "location": "/gcp-clusters-manage/index.html#stop-cluster", 
            "text": "Cloudbreak supports stopping and restarting clusters. To stop and restart a cluster managed by Cloudbreak, use the options available from the Coudbreak UI.   Steps    Browse to the cluster details.    Click  Stop  to stop a currently running cluster.      Click  Yes  to confirm.   Your cluster status changes to \"Stopping in progress\" and then to \"Stopped\". You should see the following messages in the \"Event History\":  Billing stopped, the cluster and its infrastructure have been terminated\n11/6/2017, 12:46:48 PM\nInfrastructure successfully stopped\n11/6/2017, 12:46:47 PM\nInfrastructure is now stopping\n11/6/2017, 12:43:58 PM\nAmbari cluster stopped\n11/6/2017, 12:43:52 PM\nAmbari services have been stopped.\n11/6/2017, 12:43:49 PM\nStopping Ambari services.\n11/6/2017, 12:42:14 PM\nStopping Ambari cluster\n11/6/2017, 12:42:10 PM\nCluster infrastructure stop requested\n11/6/2017, 12:42:06 PM    Once stopping the infrastructure has completed, you will see a  Start  option to restart your cluster.", 
            "title": "Stop Cluster"
        }, 
        {
            "location": "/gcp-clusters-manage/index.html#restart-cluster", 
            "text": "If your cluster is in the \"Stopped\" state, you can restart the cluster by follow these steps.  Steps    click  Start . This option is only available when the cluster has been stopped.     Click  Yes  to confirm.  Your cluster status changes to \"Start in progress\" and then to \"Running\". You should see the following messages in the \"Event History\":  Ambari cluster started; Ambari ip:35.203.149.236\n11/6/2017, 1:02:13 PM\nAmbari services have been started.\n11/6/2017, 1:02:08 PM\nStarting Ambari services.\n11/6/2017, 12:54:37 PM\nStarting Ambari cluster\n11/6/2017, 12:52:57 PM\nBilling started, the cluster and its infrastructure have successfully been started\n11/6/2017, 12:52:48 PM\nInfrastructure successfully started\n11/6/2017, 12:52:48 PM\nInfrastructure is now starting\n11/6/2017, 12:52:26 PM\nAmbari cluster start requested\n11/6/2017, 12:52:24 PM", 
            "title": "Restart Cluster"
        }, 
        {
            "location": "/gcp-clusters-manage/index.html#terminate-cluster", 
            "text": "To terminate a cluster managed by Cloudbreak, use the option available from the Coudbreak UI.   Steps    Browse to the cluster details.    Click  Terminate .     Click  Yes  to confirm.  All cluster-related resources will be deleted, unless the resources (such as networks and subnets) existed prior to cluster creation or are used by other VMs in which case they will be preserved.", 
            "title": "Terminate Cluster"
        }, 
        {
            "location": "/gcp-clusters-manage/index.html#force-terminate", 
            "text": "Cluster deletion may fail if Cloudbreak is unable to delete one or more of the cloud resources that were part of your cluster infrastructure. In such as case, you can use the  Terminate     Force terminate  option to remove the cluster entry from the Cloudbreak web UI, but you must also check your cloud provider account to see if there are any resources that must be deleted manually.  Steps    Browse to the cluster details.    Click  Terminate .     Check   Force terminate .    Click  Yes  to confirm.     This deletes the cluster tile from the UI.      Log in to your cloud provider account and  manually delete  any resources that failed to be deleted.", 
            "title": "Force Terminate"
        }, 
        {
            "location": "/gcp-clusters-manage/index.html#view-cluster-history", 
            "text": "From the navigation menu in the Cloudbreak UI, you can access the History page that allows you to generate a report showing basic information related to the clusters that were running within the specified range of dates.  To generate a report, follow these steps.  Steps    From the Cloudbreak UI navigation menu, select  History .    On the History page, select the range of dates and click  Show History  to generate a report for the selected period.", 
            "title": "View Cluster History"
        }, 
        {
            "location": "/gcp-clusters-manage/index.html#history-report-content", 
            "text": "Each entry in the report represents one cluster instance group. For each entry, the report includes the following information:   Created  - The date when your cluster was created (YYYY-MM-DD).  Provider  - The name of the cloud provider (AWS, Azure, Google, or OpenStack) on which the cluster instances are/were running.  Cluster Name  - The name that you selected for the cluster.  Worker Count  - The number of worker nodes in the cluster. This number may be a decimal if a cluster has been resized.  Instance Type  - Provider-specific VM type of the cluster instances.  Instance Group  - The name of the instance group.    Region  - The AWS region in which your cluster is running.  Running Time (hours)  - The sum of the running times for all the nodes in the instance group.   The  AGGREGATE RUNNING TIME  is the sum of the Running Times, adjusted for the selected time range.  To learn about how your cloud provider bills you for the VMs, refer to their documentation:   AWS         Azure        GCP", 
            "title": "History Report Content"
        }, 
        {
            "location": "/os-launch/index.html", 
            "text": "Launch Cloudbreak on OpenStack\n\n\nBefore launching Cloudbreak on OpenStack, review and meet the prerequisites. Next, import Cloudbreak image, launch a VM, SSH to the VM, and start Cloudbreak. Once Cloudbreak is running, log in to the Cloudbreak UI and create a Cloudbreak credential. \n\n\nMeet Minimum System Requirements\n\n\nBefore launching Cloudbreak on your OpenStack, make sure that your OpenStack deployment fulfills the following requirements.\n\n\nSupported Linux Distributions\n\n\nThe following versions of the \nRed Hat Distribution of OpenStack\n (RDO) are supported:\n\n\n\n\nJuno\n\n\nKilo\n\n\nLiberty\n\n\nMitaka\n\n\n\n\nStandard Modules\n\n\nCloudbreak requires that the following standard modules are installed and configured on OpenStack:\n\n\n\n\nKeystone V2 or Keystone V3  \n\n\nNeutron (Self-service and provider networking)  \n\n\nNova (KVM or Xen hypervisor)  \n\n\nGlance  \n\n\nCinder (Optional)  \n\n\nHeat (Optional but highly recommended, since provisioning through native API calls will be deprecated in the future)  \n\n\n\n\nRelated Links\n\n\nRed Hat Distribution of OpenStack\n (External)   \n\n\nMeet the Prerequisites\n\n\nBefore launching Cloudbreak on OpenStack, you must meet the following prerequisites.\n\n\nSSH Key Pair\n\n\nCreate a new SSH key pair or import an existing SSH key pair. \n\n\nSecurity Group\n\n\nIn order to launch Cloudbreak, you must have an existing security group with the following ports open: 22 (SSH) and 443 (HTTPS). \n\n\nFor information about OpenStack security groups, refer to the \nOpenStack Administrator Guide\n.\n\n\nRelated Links\n \n\nOpenStack Administrator Guide\n (External)    \n\n\nImport Images to OpenStack\n\n\nAn OpenStack administrator must perform these steps to add the Cloudbreak deployer and HDP images to your OpenStack deployment.\n\n\nImport Cloudbreak Deployer Image\n\n\nImport Cloudbreak deployer image using the following steps.\n\n\nSteps\n\n\n\n\n\n\nDownload the latest Cloudbreak deployer image to your local machine: \n\n\ncurl -O https://public-repo-1.hortonworks.com/HDP/cloudbreak/cloudbreak-deployer-1164-2017-08-25.img\n\n\n\n\n\n\nSet the following environment variables for the OpenStack image import: \n\n\nexport CBD_LATEST_IMAGE=cloudbreak-deployer-1164-2017-08-25.img\nexport OS_IMAGE_NAME=cloudbreak-deployer-1161-2017-06-15.img\nexport OS_USERNAME=your_os_user_name\nexport OS_AUTH_URL=your_authentication_url\nexport OS_TENANT_NAME=your_os_tenant_name\n\n\n\n\n\n\nImport the new image into your OpenStack:\n\n\nglance image-create --name \"$OS_IMAGE_NAME\" --file \"$CBD_LATEST_IMAGE\" --disk-format qcow2 --container-format bare --progress\n \n\n\n\n\n\n\nAfter performing the import, you should be able to see the Cloudbreak deployer image among your other OpenStack images. \n\n\nImport HDP Image\n\n\nImport HDP image using the following steps.\n\n\nSteps\n\n\n\n\n\n\nDownload the latest HDP image to your local machine: \n\n\ncurl -O https://public-repo-1.hortonworks.com/HDP/cloudbreak/hdc-hdp--1706141444.img\n\n\n\n\n\n\nSet the following environment variables for the OpenStack image import: \n\n\nexport CB_LATEST_IMAGE=hdc-hdp--1706141444.img \nexport CB_LATEST_IMAGE_NAME=hdc-hdp--1705081316.img\nexport OS_USERNAME=your_os_user_name\nexport OS_AUTH_URL=your_authentication_url\nexport OS_TENANT_NAME=your_os_tenant_name\n\n\n\n\n\n\nImport the new image into your OpenStack:\n\n\nglance image-create --name \"$CB_LATEST_IMAGE_NAME\" --file \"$CB_LATEST_IMAGE\" --disk-format qcow2 --container-format bare --progress\n\n\n\n\n\n\nAfter performing the import, you should be able to see the Cloudbreak image among your OpenStack images. \n\n\nLaunch the VM\n\n\nIn your OpenStack, launch and instance providing the following parameters:\n\n\n\n\nSelect a VM flavor which meets the following minimum requirements: 4GB RAM, 10GB disk, 2 cores.  \n\n\nSelect the Cloudbreak deployer image that you imported earlier and launch an instance using that image. \n\n\nSelect your SSH key pair.  \n\n\nSelect the security group which has the following ports open: 22 (SSH) and 443 (HTTPS). \n\n\nSelect your preconfigured network.  \n\n\n\n\nSSH to the VM\n\n\nNow that your VM is ready, access it via SSH: \n\n\n\n\nUse a private key matching the public key that you added to your OpenStack project.\n\n\nThe SSH user is called \"cloudbreak\".\n\n\nYou can obtain the VM's IP address from the details of your instance.\n\n\n\n\nOn Mac OS X, you can SSH to the VM by running the following from the Terminal app: \nssh -i \"your-private-key.pem\" cloudnreak@instance_IP\n where \"your-private-key.pem\" points to the location of your private key and \"instance_IP\" is the public IP address of the VM.\n\n\nOn Windows, you can use \nPuTTy\n.\n\n\nInitialize Your Profile\n\n\nAfter accessing the VM via SSH, you must initialize your Profile.\n\n\nSteps\n \n\n\n\n\n\n\nNavigate to the cloudbreak-deployment directory:\n\n\ncd /var/lib/cloudbreak-deployment/\n\n\nThis directory contains configuration files and the supporting binaries for Cloudbreak deployer.\n\n\n\n\n\n\nInitialize your profile by creating a new file called \nProfile\n and adding the following content:\n\n\nexport UAA_DEFAULT_SECRET=MY-SECRET\nexport UAA_DEFAULT_USER_PW=MY-PASSWORD\nexport PUBLIC_IP=VM-PUBLIC-IP\n  \n\n\nFor example: \n\n\nexport UAA_DEFAULT_SECRET=MySecret123\nexport UAA_DEFAULT_USER_PW=MySecurePassword123\nexport PUBLIC_IP=34.212.141.253\n \n\n\n\n\nYou will need to provide the password when logging in to the Cloudbreak web UI and when using the Cloudbreak Shell. The secret will be used by Cloudbreak for authentication.  \n\n\n\n\n\n\n\n\nPerform Optional Configurations\n\n\n\n\nThese configurations are optional. \n\n\n\n\nConfiguring a Self-Signed Certificate\n\n\nIf your OpenStack is secured with a self-signed certificate, you need to import that certificate into Cloudbreak, or else Cloudbreak won't be able to communicate with your OpenStack. \n\n\nTo import the certificate, place the certificate file in the \n/certs/trusted/\n directory, follow these steps.\n\n\nSteps\n\n\n\n\nNavigate to the \ncerts\n directory (automatically generated).\n\n\nCreate the \ntrusted\n directory.\n\n\nCopy the certificate to the \ntrusted\n directory. \n\n\n\n\nCloudbreak will automatically pick up the certificate and import it into its truststore upon start.\n\n\nConfiguring Availability Zone and Region\n\n\nBy default, Cloudbreak uses \nRegionOne\n region with \nnova\n availability zone, but you can customize Cloudbreak deployment and enable multiple regions and availability zones by creating an \nopenstack-zone.json\n file in the \netc\n directory of Cloudbreak deployment (that is\n/var/lib/cloudbreak-deployment/etc/openstack-zone.json\n). If the etc directory does not exist in the Cloudbreak deployment directory, then create it. \n\n\nThe following is an example of \nopenstack-zone.json\n containing two regions and four availability zones:\n\n\n{\n  \"items\": [\n    {\n      \"name\": \"MyRegionOne\",\n      \"zones\": [ \"az1\", \"az2\", \"az3\"]\n    },\n    {\n      \"name\": \"MyRegionTwo\",\n      \"zones\": [ \"myaz\"]\n    }\n  ]\n}\n\n\n\n\n\n\nIf you are performing this after you have started cbd, perform \ncbd restart\n.  \n\n\n\n\nLaunch Cloudbreak Deployer\n\n\nLaunch Cloudbreak deployer using the following steps.\n\n\nSteps\n\n\n\n\n\n\nStart the Cloudbreak application by using the following command:\n\n\ncbd start\n\n\nThis will start the Docker containers and initialize the application. The first time you start the Coudbreak app, the process will take longer than usual due to the download of all the necessary docker images.\n\n\nThe \ncbd start\n command includes the \ncbd generate\n command which applies the following steps:\n\n\n\n\nCreates the \ndocker-compose.yml\n file, which describes the configuration of all the Docker containers needed for the Cloudbreak deployment.\n\n\nCreates the \nuaa.yml\n file, which holds the configuration of the identity server used to authenticate users with Cloudbreak.\n\n\n\n\n\n\nOnce the \ncbd start\n has finished, it returns the \"Uluwatu (Cloudbreak UI) url\" which you can later paste in your browser and log in to Cloudbreak web UI.\n\n\n\n\n\n\n\n\nCheck Cloudbreak deployer version and health: \n\n\ncbd doctor\n\n\n\n\n\n\nNext, check Cloudbreak Application logs: \n\n\ncbd logs cloudbreak\n\n\nYou should see a message like this in the log: \nStarted CloudbreakApplication in 36.823 seconds.\n Cloudbreak normally takes less than a minute to start.\n\n\n\n\n\n\nAccess Cloudbreak UI\n\n\nLog in to the Cloudbreak UI using the following steps.\n\n\nSteps\n\n\n\n\n\n\nYou can log into the Cloudbreak application at \nhttps://IP_Address\n where \"IP_Address\" if the public IP of your OpenStack VM. For example \nhttps://34.212.141.253\n.\n\n\n\n\n\n\nConfirm the security exception to proceed to the Cloudbreak web UI.\n\n\nThe first time you access Cloudbreak UI, Cloudbreak will automatically generate a self-signed certificate, due to which your browser will warn you about an untrusted connection and will ask you to confirm a security exception.\n\n\n\n\n\n\nThe login page is displayed:\n\n\n  \n\n\n\n\n\n\nLog in to the Cloudbreak web UI: \n\n\n\n\nThe default username is \nadmin@example.com\n but you should sign up with your own email address.    \n\n\nThe password is the value of the \nUAA_DEFAULT_USER_PW\n variable that you configured in your \nProfile\n file when \nlaunching Cloudbreak deployer\n.\n\n\n\n\n\n\n\n\nUpon a successful login, you are redirected to the dashboard:\n\n\n  \n\n\n\n\n\n\nCreate Cloudbreak Credential\n\n\nCloudbreak works by connecting your OpenStack account through this credential, and then uses it to create resources on your behalf. Before you can start provisioning cluster using Cloudbreak, you must create a Cloudbreak credential.\n\n\nSteps\n\n\n\n\n\n\nIn the Cloudbreak web UI, select \nCredentials\n from the navigation pane. \n\n\n\n\n\n\nClick \nCreate Credential\n. \n\n\n\n\n\n\nUnder \nCloud provider\n, select \"Google Cloud Platform\".\n\n\n \n\n\n\n\n\n\nSelect the keystone version.\n\n\n\n\n\n\nProvide the  following information:\n\n\nFor Keystone v2:\n\n\n\n\n\n\n\n\nParameter\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nName\n\n\nEnter a name for your credential.\n\n\n\n\n\n\nDescription\n\n\n(Optional) Enter a description.\n\n\n\n\n\n\nUser\n\n\nEnter your OpenStack user name.\n\n\n\n\n\n\nPassword\n\n\nEnter your OpenStack password.\n\n\n\n\n\n\nTenant Name\n\n\nEnter the OpenStack tenant name.\n\n\n\n\n\n\nEndpoint\n\n\nEnter the OpenStack endpoint.\n\n\n\n\n\n\nAPI Facing\n\n\n(Optional) Select \npublic\n, \nprivate\n, or \ninternal\n.\n\n\n\n\n\n\n\n\nFor Keystone v3:\n\n\n\n\n\n\n\n\nParameter\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nKeystone scope\n\n\nSelect the scope: default, domain, or project.\n\n\n\n\n\n\nName\n\n\nEnter a name for your credential.\n\n\n\n\n\n\nDescription\n\n\n(Optional) Enter a description.\n\n\n\n\n\n\nUser\n\n\nEnter your OpenStack user name.\n\n\n\n\n\n\nPassword\n\n\nEnter your OpenStack password.\n\n\n\n\n\n\nUser Domain\n\n\nEnter your OpenStack user domain.\n\n\n\n\n\n\nEndpoint\n\n\nEnter the OpenStack endpoint.\n\n\n\n\n\n\nAPI Facing\n\n\n(Optional) Select \npublic\n, \nprivate\n, or \ninternal\n.\n\n\n\n\n\n\n\n\n\n\n\n\nClick \nCreate\n.\n\n\n\n\n\n\nYour credential should now be displayed in the \nCredentials\n pane.\n\n\nCongratulations! You've successfully launched and configured Cloudbreak. Now you can use Cloudbreak to \ncreate clusters\n. \n\n\n\n\n\n\n\n\nNext: Create a Cluster", 
            "title": "Launch on Open Stack"
        }, 
        {
            "location": "/os-launch/index.html#launch-cloudbreak-on-openstack", 
            "text": "Before launching Cloudbreak on OpenStack, review and meet the prerequisites. Next, import Cloudbreak image, launch a VM, SSH to the VM, and start Cloudbreak. Once Cloudbreak is running, log in to the Cloudbreak UI and create a Cloudbreak credential.", 
            "title": "Launch Cloudbreak on OpenStack"
        }, 
        {
            "location": "/os-launch/index.html#meet-minimum-system-requirements", 
            "text": "Before launching Cloudbreak on your OpenStack, make sure that your OpenStack deployment fulfills the following requirements.", 
            "title": "Meet Minimum System Requirements"
        }, 
        {
            "location": "/os-launch/index.html#supported-linux-distributions", 
            "text": "The following versions of the  Red Hat Distribution of OpenStack  (RDO) are supported:   Juno  Kilo  Liberty  Mitaka", 
            "title": "Supported Linux Distributions"
        }, 
        {
            "location": "/os-launch/index.html#standard-modules", 
            "text": "Cloudbreak requires that the following standard modules are installed and configured on OpenStack:   Keystone V2 or Keystone V3    Neutron (Self-service and provider networking)    Nova (KVM or Xen hypervisor)    Glance    Cinder (Optional)    Heat (Optional but highly recommended, since provisioning through native API calls will be deprecated in the future)     Related Links  Red Hat Distribution of OpenStack  (External)", 
            "title": "Standard Modules"
        }, 
        {
            "location": "/os-launch/index.html#meet-the-prerequisites", 
            "text": "Before launching Cloudbreak on OpenStack, you must meet the following prerequisites.", 
            "title": "Meet the Prerequisites"
        }, 
        {
            "location": "/os-launch/index.html#ssh-key-pair", 
            "text": "Create a new SSH key pair or import an existing SSH key pair.", 
            "title": "SSH Key Pair"
        }, 
        {
            "location": "/os-launch/index.html#security-group", 
            "text": "In order to launch Cloudbreak, you must have an existing security group with the following ports open: 22 (SSH) and 443 (HTTPS).   For information about OpenStack security groups, refer to the  OpenStack Administrator Guide .  Related Links   OpenStack Administrator Guide  (External)", 
            "title": "Security Group"
        }, 
        {
            "location": "/os-launch/index.html#import-images-to-openstack", 
            "text": "An OpenStack administrator must perform these steps to add the Cloudbreak deployer and HDP images to your OpenStack deployment.", 
            "title": "Import Images to OpenStack"
        }, 
        {
            "location": "/os-launch/index.html#import-cloudbreak-deployer-image", 
            "text": "Import Cloudbreak deployer image using the following steps.  Steps    Download the latest Cloudbreak deployer image to your local machine:   curl -O https://public-repo-1.hortonworks.com/HDP/cloudbreak/cloudbreak-deployer-1164-2017-08-25.img    Set the following environment variables for the OpenStack image import:   export CBD_LATEST_IMAGE=cloudbreak-deployer-1164-2017-08-25.img\nexport OS_IMAGE_NAME=cloudbreak-deployer-1161-2017-06-15.img\nexport OS_USERNAME=your_os_user_name\nexport OS_AUTH_URL=your_authentication_url\nexport OS_TENANT_NAME=your_os_tenant_name    Import the new image into your OpenStack:  glance image-create --name \"$OS_IMAGE_NAME\" --file \"$CBD_LATEST_IMAGE\" --disk-format qcow2 --container-format bare --progress      After performing the import, you should be able to see the Cloudbreak deployer image among your other OpenStack images.", 
            "title": "Import Cloudbreak Deployer Image"
        }, 
        {
            "location": "/os-launch/index.html#import-hdp-image", 
            "text": "Import HDP image using the following steps.  Steps    Download the latest HDP image to your local machine:   curl -O https://public-repo-1.hortonworks.com/HDP/cloudbreak/hdc-hdp--1706141444.img    Set the following environment variables for the OpenStack image import:   export CB_LATEST_IMAGE=hdc-hdp--1706141444.img \nexport CB_LATEST_IMAGE_NAME=hdc-hdp--1705081316.img\nexport OS_USERNAME=your_os_user_name\nexport OS_AUTH_URL=your_authentication_url\nexport OS_TENANT_NAME=your_os_tenant_name    Import the new image into your OpenStack:  glance image-create --name \"$CB_LATEST_IMAGE_NAME\" --file \"$CB_LATEST_IMAGE\" --disk-format qcow2 --container-format bare --progress    After performing the import, you should be able to see the Cloudbreak image among your OpenStack images.", 
            "title": "Import HDP Image"
        }, 
        {
            "location": "/os-launch/index.html#launch-the-vm", 
            "text": "In your OpenStack, launch and instance providing the following parameters:   Select a VM flavor which meets the following minimum requirements: 4GB RAM, 10GB disk, 2 cores.    Select the Cloudbreak deployer image that you imported earlier and launch an instance using that image.   Select your SSH key pair.    Select the security group which has the following ports open: 22 (SSH) and 443 (HTTPS).   Select your preconfigured network.", 
            "title": "Launch the VM"
        }, 
        {
            "location": "/os-launch/index.html#ssh-to-the-vm", 
            "text": "Now that your VM is ready, access it via SSH:    Use a private key matching the public key that you added to your OpenStack project.  The SSH user is called \"cloudbreak\".  You can obtain the VM's IP address from the details of your instance.   On Mac OS X, you can SSH to the VM by running the following from the Terminal app:  ssh -i \"your-private-key.pem\" cloudnreak@instance_IP  where \"your-private-key.pem\" points to the location of your private key and \"instance_IP\" is the public IP address of the VM.  On Windows, you can use  PuTTy .", 
            "title": "SSH to the VM"
        }, 
        {
            "location": "/os-launch/index.html#initialize-your-profile", 
            "text": "After accessing the VM via SSH, you must initialize your Profile.  Steps      Navigate to the cloudbreak-deployment directory:  cd /var/lib/cloudbreak-deployment/  This directory contains configuration files and the supporting binaries for Cloudbreak deployer.    Initialize your profile by creating a new file called  Profile  and adding the following content:  export UAA_DEFAULT_SECRET=MY-SECRET\nexport UAA_DEFAULT_USER_PW=MY-PASSWORD\nexport PUBLIC_IP=VM-PUBLIC-IP     For example:   export UAA_DEFAULT_SECRET=MySecret123\nexport UAA_DEFAULT_USER_PW=MySecurePassword123\nexport PUBLIC_IP=34.212.141.253     You will need to provide the password when logging in to the Cloudbreak web UI and when using the Cloudbreak Shell. The secret will be used by Cloudbreak for authentication.", 
            "title": "Initialize Your Profile"
        }, 
        {
            "location": "/os-launch/index.html#perform-optional-configurations", 
            "text": "These configurations are optional.", 
            "title": "Perform Optional Configurations"
        }, 
        {
            "location": "/os-launch/index.html#configuring-a-self-signed-certificate", 
            "text": "If your OpenStack is secured with a self-signed certificate, you need to import that certificate into Cloudbreak, or else Cloudbreak won't be able to communicate with your OpenStack.   To import the certificate, place the certificate file in the  /certs/trusted/  directory, follow these steps.  Steps   Navigate to the  certs  directory (automatically generated).  Create the  trusted  directory.  Copy the certificate to the  trusted  directory.    Cloudbreak will automatically pick up the certificate and import it into its truststore upon start.", 
            "title": "Configuring a Self-Signed Certificate"
        }, 
        {
            "location": "/os-launch/index.html#configuring-availability-zone-and-region", 
            "text": "By default, Cloudbreak uses  RegionOne  region with  nova  availability zone, but you can customize Cloudbreak deployment and enable multiple regions and availability zones by creating an  openstack-zone.json  file in the  etc  directory of Cloudbreak deployment (that is /var/lib/cloudbreak-deployment/etc/openstack-zone.json ). If the etc directory does not exist in the Cloudbreak deployment directory, then create it.   The following is an example of  openstack-zone.json  containing two regions and four availability zones:  {\n  \"items\": [\n    {\n      \"name\": \"MyRegionOne\",\n      \"zones\": [ \"az1\", \"az2\", \"az3\"]\n    },\n    {\n      \"name\": \"MyRegionTwo\",\n      \"zones\": [ \"myaz\"]\n    }\n  ]\n}   If you are performing this after you have started cbd, perform  cbd restart .", 
            "title": "Configuring Availability Zone and Region"
        }, 
        {
            "location": "/os-launch/index.html#launch-cloudbreak-deployer", 
            "text": "Launch Cloudbreak deployer using the following steps.  Steps    Start the Cloudbreak application by using the following command:  cbd start  This will start the Docker containers and initialize the application. The first time you start the Coudbreak app, the process will take longer than usual due to the download of all the necessary docker images.  The  cbd start  command includes the  cbd generate  command which applies the following steps:   Creates the  docker-compose.yml  file, which describes the configuration of all the Docker containers needed for the Cloudbreak deployment.  Creates the  uaa.yml  file, which holds the configuration of the identity server used to authenticate users with Cloudbreak.    Once the  cbd start  has finished, it returns the \"Uluwatu (Cloudbreak UI) url\" which you can later paste in your browser and log in to Cloudbreak web UI.     Check Cloudbreak deployer version and health:   cbd doctor    Next, check Cloudbreak Application logs:   cbd logs cloudbreak  You should see a message like this in the log:  Started CloudbreakApplication in 36.823 seconds.  Cloudbreak normally takes less than a minute to start.", 
            "title": "Launch Cloudbreak Deployer"
        }, 
        {
            "location": "/os-launch/index.html#access-cloudbreak-ui", 
            "text": "Log in to the Cloudbreak UI using the following steps.  Steps    You can log into the Cloudbreak application at  https://IP_Address  where \"IP_Address\" if the public IP of your OpenStack VM. For example  https://34.212.141.253 .    Confirm the security exception to proceed to the Cloudbreak web UI.  The first time you access Cloudbreak UI, Cloudbreak will automatically generate a self-signed certificate, due to which your browser will warn you about an untrusted connection and will ask you to confirm a security exception.    The login page is displayed:        Log in to the Cloudbreak web UI:    The default username is  admin@example.com  but you should sign up with your own email address.      The password is the value of the  UAA_DEFAULT_USER_PW  variable that you configured in your  Profile  file when  launching Cloudbreak deployer .     Upon a successful login, you are redirected to the dashboard:", 
            "title": "Access Cloudbreak UI"
        }, 
        {
            "location": "/os-launch/index.html#create-cloudbreak-credential", 
            "text": "Cloudbreak works by connecting your OpenStack account through this credential, and then uses it to create resources on your behalf. Before you can start provisioning cluster using Cloudbreak, you must create a Cloudbreak credential.  Steps    In the Cloudbreak web UI, select  Credentials  from the navigation pane.     Click  Create Credential .     Under  Cloud provider , select \"Google Cloud Platform\".       Select the keystone version.    Provide the  following information:  For Keystone v2:     Parameter  Description      Name  Enter a name for your credential.    Description  (Optional) Enter a description.    User  Enter your OpenStack user name.    Password  Enter your OpenStack password.    Tenant Name  Enter the OpenStack tenant name.    Endpoint  Enter the OpenStack endpoint.    API Facing  (Optional) Select  public ,  private , or  internal .     For Keystone v3:     Parameter  Description      Keystone scope  Select the scope: default, domain, or project.    Name  Enter a name for your credential.    Description  (Optional) Enter a description.    User  Enter your OpenStack user name.    Password  Enter your OpenStack password.    User Domain  Enter your OpenStack user domain.    Endpoint  Enter the OpenStack endpoint.    API Facing  (Optional) Select  public ,  private , or  internal .       Click  Create .    Your credential should now be displayed in the  Credentials  pane.  Congratulations! You've successfully launched and configured Cloudbreak. Now you can use Cloudbreak to  create clusters .      Next: Create a Cluster", 
            "title": "Create Cloudbreak Credential"
        }, 
        {
            "location": "/os-create/index.html", 
            "text": "Create a Cluster on OpenStack\n\n\nUse these steps to create a cluster.\n\n\nSteps\n\n\n\n\n\n\nLog in to the Cloudbreak UI.\n\n\n\n\n\n\nClick \nCreate Cluster\n and the \nCreate Cluster\n form is displayed.\n\n\nTo view advanced options, click \nAdvanced\n. To learn about advanced options, refer to \nAdvanced Options\n.\n\n\n \n\n\n\n\n\n\nOn the \nGeneral Configuration\n page, specify the following general parameters for your cluster:\n\n\n\n\n\n\n\n\nParameter\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nSelect Credential\n\n\nChoose a previously created credential.\n\n\n\n\n\n\nCluster Name\n\n\nEnter a name for your cluster. The name must be between 5 and 40 characters, must start with a letter, and must only include lowercase letters, numbers, and hyphens.\n\n\n\n\n\n\nRegion\n\n\nSelect the region in which you would like to launch your cluster.\n\n\n\n\n\n\nPlatform Version\n\n\nChoose the HDP version to use for this cluster.\n\n\n\n\n\n\nCluster Type\n\n\nChoose one of default cluster configurations, or, if you have defined your own cluster configuration via Ambari blueprint, you can choose it here. For more information, refer to \nBlueprints\n.\n\n\n\n\n\n\n\n\n\n\n\n\nOn the \nHardware and Storage\n page, for each host group provide the following information to define your cluster nodes and attached storage:\n\n\n\n\n\n\n\n\nParameter\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nInstance Type\n\n\nSelect an instance type.\n\n\n\n\n\n\nInstance Count\n\n\nEnter the number of instances of a given type. Default is 1.\n\n\n\n\n\n\nAmbari Server\n\n\nYou must select one node for Ambari Server. The \"Group Size\" for that host group must be set to \"1\".\n\n\n\n\n\n\n\n\n\n\n\n\nOn the \nNetwork\n page, provide the following to specify the networking resources that will be used for your cluster:\n\n\n\n\n\n\n\n\nParameter\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nSelect Network\n\n\nSelect the virtual network in which you would like your cluster to be provisioned. You can select an existing network or create a new network.\n\n\n\n\n\n\nSelect Subnet\n\n\nSelect the subnet in which you would like your cluster to be provisioned. You must create a new subnet.\n\n\n\n\n\n\nSubnet (CIDR)\n\n\nIf you selected to create a new subnet, you must define a valid \nCIDR\n for the subnet. Default is 10.0.0.0/16.\n\n\n\n\n\n\n\n\n\n\n\n\nOn the \nSecurity\n page, provide the following parameters:\n\n\n\n\n\n\n\n\nParameter\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nCluster User\n\n\nYou can log in to the Ambari UI using this username. By default, this is set to \nadmin\n.\n\n\n\n\n\n\nPassword\n\n\nYou can log in to the Ambari UI using this password.\n\n\n\n\n\n\nConfirm Password\n\n\nConfirm the password.\n\n\n\n\n\n\nSSH Key Pair\n\n\nSelect an existing public key or specify a new public key. You will use the matching private key to access your cluster nodes via SSH.\n\n\n\n\n\n\n\n\n\n\n\n\nClick on \nCreate Cluster\n to create a cluster.\n\n\n\n\n\n\nYou will be redirected to the Cloudbreak dashboard, and a new tile representing your cluster will appear at the top of the page.\n\n\n\n\n\n\nRelated Links\n\n\nBlueprints\n \n\n\nCIDR\n (External)    \n\n\nAdvanced Options\n\n\nClick on \nAdvanced\n to view and enter additional configuration options.\n\n\nAvailability Zone\n\n\nChoose one of the availability zones within the selected region. \n\n\nEnable Lifetime Management\n\n\nCheck this option if you would like your cluster to be automatically terminated after a specific amount of time (defined as \"Time to Live\" in minutes) has passed. \n\n\nTags\n\n\nYou can optionally add tags, which will help you find your cluster-related resources, such as VMs, in your cloud provider account. refer to \nResource Tagging\n.\n\n\nRelated Links\n\n\nResource Tagging\n \n\n\nStorage\n\n\nYou can optionally specify the following storage options for your cluster:\n\n\n\n\n\n\n\n\nParameter\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nStorage Type\n\n\nSelect the volume type. The options are:\nMagnetic\nEphemeral\nGeneral Purpose (SSD)\nThroughput Optimized HDD\nFor more information about these options refer to \nAWS documentation\n.\n\n\n\n\n\n\nAttached Volumes Per Instance\n\n\nEnter the number of volumes attached per instance. Default is 1.\n\n\n\n\n\n\nVolume Size (GB)\n\n\nEnter the size in GBs for each volume. Default is 100.\n\n\n\n\n\n\n\n\nRecipes\n\n\nThis option allows you to select previously uploaded recipes (scripts that will be run pre- or post- cluster deployment) for each host group. For more information on default and custom blueprints, refer to \nRecipes\n.\n\n\nRelated Links\n\n\nRecipes\n  \n\n\nSecurity Groups\n\n\nFor each host group, select one of the options:\n\n\n\n\nCreate new security group  \n\n\nDo not use security group  \n\n\nSelect an existing security group\n\n\n\n\nIf you choose to create a new security group, the \nNew Security Group\n wizard will open.\n\n\n\n\nTCP ports 22, 443, and 9443 will be open by default. These ports must be open on every security group; otherwise Cloudbreak will not be able to communicate with your provisioned cluster.  \n\n\nYou may open additional ports by defining the \nCIDR\n, \nPort\n, and \nProtocol\n for each and clicking \nAdd Rule\n.  \n\n\nOnce done, click \nSAVE\n to save your security group settings. \n\n\nOnce you define a custom security group for one host group, you can reuse this definition for other node groups. \n\n\n\n\nEnable Kerberos Security\n\n\nSelect this option to enable Kerberos for your cluster. You will have an option to create a new kerberos or use an existing one. For more information refer to \nKerberos\n documentation. \n\n\nRelated Links\n\n\nKerberos\n\n\n\n\nNext: Access Cluster", 
            "title": "Create a Cluster"
        }, 
        {
            "location": "/os-create/index.html#create-a-cluster-on-openstack", 
            "text": "Use these steps to create a cluster.  Steps    Log in to the Cloudbreak UI.    Click  Create Cluster  and the  Create Cluster  form is displayed.  To view advanced options, click  Advanced . To learn about advanced options, refer to  Advanced Options .       On the  General Configuration  page, specify the following general parameters for your cluster:     Parameter  Description      Select Credential  Choose a previously created credential.    Cluster Name  Enter a name for your cluster. The name must be between 5 and 40 characters, must start with a letter, and must only include lowercase letters, numbers, and hyphens.    Region  Select the region in which you would like to launch your cluster.    Platform Version  Choose the HDP version to use for this cluster.    Cluster Type  Choose one of default cluster configurations, or, if you have defined your own cluster configuration via Ambari blueprint, you can choose it here. For more information, refer to  Blueprints .       On the  Hardware and Storage  page, for each host group provide the following information to define your cluster nodes and attached storage:     Parameter  Description      Instance Type  Select an instance type.    Instance Count  Enter the number of instances of a given type. Default is 1.    Ambari Server  You must select one node for Ambari Server. The \"Group Size\" for that host group must be set to \"1\".       On the  Network  page, provide the following to specify the networking resources that will be used for your cluster:     Parameter  Description      Select Network  Select the virtual network in which you would like your cluster to be provisioned. You can select an existing network or create a new network.    Select Subnet  Select the subnet in which you would like your cluster to be provisioned. You must create a new subnet.    Subnet (CIDR)  If you selected to create a new subnet, you must define a valid  CIDR  for the subnet. Default is 10.0.0.0/16.       On the  Security  page, provide the following parameters:     Parameter  Description      Cluster User  You can log in to the Ambari UI using this username. By default, this is set to  admin .    Password  You can log in to the Ambari UI using this password.    Confirm Password  Confirm the password.    SSH Key Pair  Select an existing public key or specify a new public key. You will use the matching private key to access your cluster nodes via SSH.       Click on  Create Cluster  to create a cluster.    You will be redirected to the Cloudbreak dashboard, and a new tile representing your cluster will appear at the top of the page.    Related Links  Blueprints    CIDR  (External)", 
            "title": "Create a Cluster on OpenStack"
        }, 
        {
            "location": "/os-create/index.html#advanced-options", 
            "text": "Click on  Advanced  to view and enter additional configuration options.", 
            "title": "Advanced Options"
        }, 
        {
            "location": "/os-create/index.html#availability-zone", 
            "text": "Choose one of the availability zones within the selected region.", 
            "title": "Availability Zone"
        }, 
        {
            "location": "/os-create/index.html#enable-lifetime-management", 
            "text": "Check this option if you would like your cluster to be automatically terminated after a specific amount of time (defined as \"Time to Live\" in minutes) has passed.", 
            "title": "Enable Lifetime Management"
        }, 
        {
            "location": "/os-create/index.html#tags", 
            "text": "You can optionally add tags, which will help you find your cluster-related resources, such as VMs, in your cloud provider account. refer to  Resource Tagging .  Related Links  Resource Tagging", 
            "title": "Tags"
        }, 
        {
            "location": "/os-create/index.html#storage", 
            "text": "You can optionally specify the following storage options for your cluster:     Parameter  Description      Storage Type  Select the volume type. The options are: Magnetic Ephemeral General Purpose (SSD) Throughput Optimized HDD For more information about these options refer to  AWS documentation .    Attached Volumes Per Instance  Enter the number of volumes attached per instance. Default is 1.    Volume Size (GB)  Enter the size in GBs for each volume. Default is 100.", 
            "title": "Storage"
        }, 
        {
            "location": "/os-create/index.html#recipes", 
            "text": "This option allows you to select previously uploaded recipes (scripts that will be run pre- or post- cluster deployment) for each host group. For more information on default and custom blueprints, refer to  Recipes .  Related Links  Recipes", 
            "title": "Recipes"
        }, 
        {
            "location": "/os-create/index.html#security-groups", 
            "text": "For each host group, select one of the options:   Create new security group    Do not use security group    Select an existing security group   If you choose to create a new security group, the  New Security Group  wizard will open.   TCP ports 22, 443, and 9443 will be open by default. These ports must be open on every security group; otherwise Cloudbreak will not be able to communicate with your provisioned cluster.    You may open additional ports by defining the  CIDR ,  Port , and  Protocol  for each and clicking  Add Rule .    Once done, click  SAVE  to save your security group settings.   Once you define a custom security group for one host group, you can reuse this definition for other node groups.", 
            "title": "Security Groups"
        }, 
        {
            "location": "/os-create/index.html#enable-kerberos-security", 
            "text": "Select this option to enable Kerberos for your cluster. You will have an option to create a new kerberos or use an existing one. For more information refer to  Kerberos  documentation.   Related Links  Kerberos   Next: Access Cluster", 
            "title": "Enable Kerberos Security"
        }, 
        {
            "location": "/os-clusters-access/index.html", 
            "text": "Access Your Cluster\n\n\nThe following section describes how to access the various services in the cluster.\n\n\nFinding Cluster Information in the UI\n\n\nOnce your cluster is up and running, click on the tile representing your cluster in the Cloudbreak UI to access information related the cluster and access cluster actions. \n\n\n \n\n\nThe information presented includes:\n\n\n\n\nCluster Summary\n\n\nCluster Information\n \n\n\nHardware\n\n\nTags\n \n\n\nEvent History\n \n\n\n\n\n\n  \nTips\n\n  \n\n  \n Access cluster actions such as resize and sync by clicking on \nACTIONS\n.\n\n  \n Access Ambari web UI by clicking on the link in the \nCLUSTER INFORMATION\n section.\n\n\n View public IP addresses for all cluster instances in the \nHARDWARE\n section. Click on the links to view the instances in the cloud console.\n\n\n The SSH user that you must use when accessing cluster VMs is \"cloudbreak\".\n \n\n\n\n\n\n\n\n\nCluster Summary\n\n\nThe summary bar includes the following information about your cluster:\n\n\n\n\n\n\n\n\nItem\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nCloud Provider\n\n\nThe logo of the cloud provider on which the cluster is running.\n\n\n\n\n\n\nCredential\n\n\nThe name of the credential used to create the cluster.\n\n\n\n\n\n\nStatus\n\n\nCurrent status. When a cluster is healthy, the status is \nRunning\n.\n\n\n\n\n\n\nNodes\n\n\nThe current number of cluster nodes, including the master node.\n\n\n\n\n\n\nUptime\n\n\nThe amount of time (HH:MM) that the cluster has been in the running state.\n\n\n\n\n\n\nCreated\n\n\nThe date when the cluster was created. The date format is Mon DD, YYYY. For exampple: Oct 27, 2017.\n\n\n\n\n\n\n\n\nCluster Information\n\n\n\n\n\n\n\n\nItem\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nNetwork\n\n\nThe name of the network in which the cluster is running.\n\n\n\n\n\n\nSubnet\n\n\nThe name of the subnet in which the cluster is running.\n\n\n\n\n\n\nCluster User\n\n\nThe name of the cluster user that you created when creating the cluster.\n\n\n\n\n\n\nSSH Username\n\n\nThe SSH user which you must use when accessing cluster VMs via SSH. The SSH user is always \"cloudbreak\".\n\n\n\n\n\n\nAmbari URL\n\n\nLink to the Ambari web UI.\n\n\n\n\n\n\nRegion\n\n\nThe region in which the cluster is running in the cloud provider infrastructure.\n\n\n\n\n\n\nAvailability Zone\n\n\nThe availability zone within the region in which the cluster is running.\n\n\n\n\n\n\nBlueprint\n\n\nThe name of the blueprint selected under \"Cluster Type\" to create this cluster.\n\n\n\n\n\n\nStarted With\n\n\nThe version of Cloubdreak used to create this cluster.\n\n\n\n\n\n\nAmbari Version\n\n\nThe Ambari version which this cluster is currently running.\n\n\n\n\n\n\nHDP Version\n\n\nThe HDP version which this cluster is currently running.\n\n\n\n\n\n\n\n\nHardware\n\n\nThis section includes information about your cluster nodes: instance names, instance IDs (with links to the cloud provider console), and public IPs.\n\n\nTags\n\n\nThis section lists user-defined tags, in the same order as you added them.\n\n\nEvent History\n\n\nThe Event History section shows you events logged for the cluster, with the most recent event at the top. For example, after your cluster has been created, the following messages will be written to the log:\n\n\n\nAmbari cluster built; Ambari ip:34.215.103.66\n10/26/2017, 9:41:58 AM\nBuilding Ambari cluster; Ambari ip:34.215.103.66\n10/26/2017, 9:30:20 AM\nStarting Ambari cluster services\n10/26/2017, 9:27:12 AM\nSetting up infrastructure metadata\n10/26/2017, 9:27:11 AM\nBootstrapping infrastructure cluster\n10/26/2017, 9:26:38 AM\nInfrastructure successfully provisioned\n10/26/2017, 9:26:37 AM\nBilling started, Infrastructure successfully provisioned\n10/26/2017, 9:26:37 AM\nInfrastructure metadata collection finished\n10/26/2017, 9:25:39 AM\nInfrastructure creation took 194 seconds\n10/26/2017, 9:25:37 AM\nCreating infrastructure\n10/26/2017, 9:22:22 AM\nSetting up HDP image\n10/26/2017, 9:22:21 AM\n\n\n\nAccess Cluster via SSH\n\n\nIf you plan to access the cluster via the command line clients, SSH into the master node instance in the cluster. \n\n\n\n\nIn order to use SSH, you must generate an SSH key pair or use an existing SSH keypair.  \n\n\nYou can find the cluster instance public IP addresses on the cluster details page.  \n\n\nWhen accessing instances via SSH use the \ncloudbreak\n user. \n\n\n\n\nOn Mac OS, you can use the following syntax to SSH to the VM:\n\nssh -i \"privatekey.pem\" cloudbreak@publicIP\n\nFor example:\n\nssh -i \"dominika-kp.pem\" cloudbreak@p52.25.169.132\n\n\nOn Windows, you can SSH using an SSH client such as PuTTY.\n\n\nAccess Ambari\n\n\nYou can access Ambari web UI by clicking on the links provided in the \nCluster Information\n \n \nAmbari URL\n.\n\n\nSteps\n\n\n\n\n\n\nFrom the cluster dashboard, click on the tile representing your cluster to navigate to cluster details.\n\n\n\n\n\n\nFind the Ambairi URL in the \nCluster Information\n section. This URL is available once the Ambari cluster creation process has completed.  \n\n\n\n\n\n\nClick on the \nAmbari URL\n link.\n\n\n\n\n\n\nThe first time you access the server, your browser will attempt to confirm that the SSL Certificate is valid. Since Cloudbreak automatically generates a self-signed certificate, your browser will warn you about an Untrusted Connection and ask you to confirm a Security Exception. Depending on your browser, perform the steps below to proceed.\n\n\n\n\n\n\n\n\nBrowser\n\n\nSteps\n\n\n\n\n\n\n\n\n\n\nFirefox\n\n\nClick \nAdvanced\n \n Click \nAdd Exception...\n \n Click \nConfirm Security Exception\n\n\n\n\n\n\nSafari\n\n\nClick \nContinue\n\n\n\n\n\n\nChrome\n\n\nClick \nAdvanced\n \n Click \nProceed...\n\n\n\n\n\n\n\n\n\n\n\n\nUser Accounts\n\n\nThe following table describes what credentials to use to access Cloudbreak and Cloudbreak-managed clusters:\n\n\n\n\n\n\n\n\nComponent\n\n\nMethod\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nCloudbreak\n\n\nWeb UI, CLI\n\n\nAccess with the username and password provided when launching Cloudbreak on the cloud provider.\n\n\n\n\n\n\nCloudbreak\n\n\nSSH to VM\n\n\nAccess as the \"cloudbreak\" user with the SSH key provided when launching Cloudbreak on the cloud provider.\n\n\n\n\n\n\nCluster\n\n\nSSH to VMs\n\n\nAccess as the \"cloudbreak\" user with the SSH key provided during cluster creation.\n\n\n\n\n\n\nCluster\n\n\nAmbari UI\n\n\nAccess with the credentials provided in the \u201cCluster User\u201d parameter during cluster creation.\n\n\n\n\n\n\n\n\n\n\nNext: Manage and Monitor Clusters", 
            "title": "Access Cluster"
        }, 
        {
            "location": "/os-clusters-access/index.html#access-your-cluster", 
            "text": "The following section describes how to access the various services in the cluster.", 
            "title": "Access Your Cluster"
        }, 
        {
            "location": "/os-clusters-access/index.html#finding-cluster-information-in-the-ui", 
            "text": "Once your cluster is up and running, click on the tile representing your cluster in the Cloudbreak UI to access information related the cluster and access cluster actions.      The information presented includes:   Cluster Summary  Cluster Information    Hardware  Tags    Event History     \n   Tips \n   \n    Access cluster actions such as resize and sync by clicking on  ACTIONS . \n    Access Ambari web UI by clicking on the link in the  CLUSTER INFORMATION  section.   View public IP addresses for all cluster instances in the  HARDWARE  section. Click on the links to view the instances in the cloud console.   The SSH user that you must use when accessing cluster VMs is \"cloudbreak\".", 
            "title": "Finding Cluster Information in the UI"
        }, 
        {
            "location": "/os-clusters-access/index.html#cluster-summary", 
            "text": "The summary bar includes the following information about your cluster:     Item  Description      Cloud Provider  The logo of the cloud provider on which the cluster is running.    Credential  The name of the credential used to create the cluster.    Status  Current status. When a cluster is healthy, the status is  Running .    Nodes  The current number of cluster nodes, including the master node.    Uptime  The amount of time (HH:MM) that the cluster has been in the running state.    Created  The date when the cluster was created. The date format is Mon DD, YYYY. For exampple: Oct 27, 2017.", 
            "title": "Cluster Summary"
        }, 
        {
            "location": "/os-clusters-access/index.html#cluster-information", 
            "text": "Item  Description      Network  The name of the network in which the cluster is running.    Subnet  The name of the subnet in which the cluster is running.    Cluster User  The name of the cluster user that you created when creating the cluster.    SSH Username  The SSH user which you must use when accessing cluster VMs via SSH. The SSH user is always \"cloudbreak\".    Ambari URL  Link to the Ambari web UI.    Region  The region in which the cluster is running in the cloud provider infrastructure.    Availability Zone  The availability zone within the region in which the cluster is running.    Blueprint  The name of the blueprint selected under \"Cluster Type\" to create this cluster.    Started With  The version of Cloubdreak used to create this cluster.    Ambari Version  The Ambari version which this cluster is currently running.    HDP Version  The HDP version which this cluster is currently running.", 
            "title": "Cluster Information"
        }, 
        {
            "location": "/os-clusters-access/index.html#hardware", 
            "text": "This section includes information about your cluster nodes: instance names, instance IDs (with links to the cloud provider console), and public IPs.", 
            "title": "Hardware"
        }, 
        {
            "location": "/os-clusters-access/index.html#tags", 
            "text": "This section lists user-defined tags, in the same order as you added them.", 
            "title": "Tags"
        }, 
        {
            "location": "/os-clusters-access/index.html#event-history", 
            "text": "The Event History section shows you events logged for the cluster, with the most recent event at the top. For example, after your cluster has been created, the following messages will be written to the log:  \nAmbari cluster built; Ambari ip:34.215.103.66\n10/26/2017, 9:41:58 AM\nBuilding Ambari cluster; Ambari ip:34.215.103.66\n10/26/2017, 9:30:20 AM\nStarting Ambari cluster services\n10/26/2017, 9:27:12 AM\nSetting up infrastructure metadata\n10/26/2017, 9:27:11 AM\nBootstrapping infrastructure cluster\n10/26/2017, 9:26:38 AM\nInfrastructure successfully provisioned\n10/26/2017, 9:26:37 AM\nBilling started, Infrastructure successfully provisioned\n10/26/2017, 9:26:37 AM\nInfrastructure metadata collection finished\n10/26/2017, 9:25:39 AM\nInfrastructure creation took 194 seconds\n10/26/2017, 9:25:37 AM\nCreating infrastructure\n10/26/2017, 9:22:22 AM\nSetting up HDP image\n10/26/2017, 9:22:21 AM", 
            "title": "Event History"
        }, 
        {
            "location": "/os-clusters-access/index.html#access-cluster-via-ssh", 
            "text": "If you plan to access the cluster via the command line clients, SSH into the master node instance in the cluster.    In order to use SSH, you must generate an SSH key pair or use an existing SSH keypair.    You can find the cluster instance public IP addresses on the cluster details page.    When accessing instances via SSH use the  cloudbreak  user.    On Mac OS, you can use the following syntax to SSH to the VM: ssh -i \"privatekey.pem\" cloudbreak@publicIP \nFor example: ssh -i \"dominika-kp.pem\" cloudbreak@p52.25.169.132  On Windows, you can SSH using an SSH client such as PuTTY.", 
            "title": "Access Cluster via SSH"
        }, 
        {
            "location": "/os-clusters-access/index.html#access-ambari", 
            "text": "You can access Ambari web UI by clicking on the links provided in the  Cluster Information     Ambari URL .  Steps    From the cluster dashboard, click on the tile representing your cluster to navigate to cluster details.    Find the Ambairi URL in the  Cluster Information  section. This URL is available once the Ambari cluster creation process has completed.      Click on the  Ambari URL  link.    The first time you access the server, your browser will attempt to confirm that the SSL Certificate is valid. Since Cloudbreak automatically generates a self-signed certificate, your browser will warn you about an Untrusted Connection and ask you to confirm a Security Exception. Depending on your browser, perform the steps below to proceed.     Browser  Steps      Firefox  Click  Advanced    Click  Add Exception...    Click  Confirm Security Exception    Safari  Click  Continue    Chrome  Click  Advanced    Click  Proceed...", 
            "title": "Access Ambari"
        }, 
        {
            "location": "/os-clusters-access/index.html#user-accounts", 
            "text": "The following table describes what credentials to use to access Cloudbreak and Cloudbreak-managed clusters:     Component  Method  Description      Cloudbreak  Web UI, CLI  Access with the username and password provided when launching Cloudbreak on the cloud provider.    Cloudbreak  SSH to VM  Access as the \"cloudbreak\" user with the SSH key provided when launching Cloudbreak on the cloud provider.    Cluster  SSH to VMs  Access as the \"cloudbreak\" user with the SSH key provided during cluster creation.    Cluster  Ambari UI  Access with the credentials provided in the \u201cCluster User\u201d parameter during cluster creation.      Next: Manage and Monitor Clusters", 
            "title": "User Accounts"
        }, 
        {
            "location": "/os-clusters-manage/index.html", 
            "text": "Manage and Monitor Clusters\n\n\nYou can manage monitor your clusters from the Cloudbreak UI. To do that, click on the tile representing the cluster that you want to access. The actions available for your cluster are listed in the top right corner: \n\n\n \n\n\n\n  \nTips\n\n  \n\n  \nTo add or remove nodes from your cluster click \nACTIONS>Resize\n.\n\n  \nTo synchronize your cluster with the cloud provider account click \nACTIONS>Sync\n.\n\n  \nTo temporarily stop your cluster click \nSTOP\n.\n\n  \nTo terminate your cluster click \nTERMINATE\n.\n\n\n\n\n\n\n\n\n\nResize Cluster\n\n\nTo resize a cluster, follow these steps.\n\n\nSteps\n\n\n\n\n\n\nBrowse to the cluster details.\n\n\n\n\n\n\nClick \nActions\n and select \nResize\n. The cluster resize dialog is displayed.\n\n\n\n\n\n\nUsing the +/- controls, adjust how many nodes to add or remove from each host group.  \n\n\n\n\n\n\nClick \nYes\n to confirm the scale-up/scale-down.\n\n\nWhile nodes are being added or removed, cluster status changes to \"Update In Progress\". Once the operation has completed, cluster status changes back to \"Running\". Messages similar to the following are written to the \"Event History\", : \n\n\nAmbari cluster scaled up\n11/6/2017, 12:33:40 PM\nScaling up the Ambari cluster\n11/6/2017, 12:26:59 PM\nStack successfully upscaled\n11/6/2017, 12:26:54 PM\nBootstrapping new nodes\n11/6/2017, 12:26:16 PM\nInfrastructure metadata extension finished\n11/6/2017, 12:26:10 PM\nBilling changed due to upscaling of cluster infrastructure\n11/6/2017, 12:26:10 PM\nAdding 1 new instances to the infrastructure\n11/6/2017, 12:25:23 PM\n\n\n\n\n\n\nSynchronize with Cloud Provider\n\n\nIf you have just made changes on your cloud provider side (for example, deleted cluster VMs) and you would like to synchronize Cloudbreak with the cloud provider, use the \nsync\n option. \n\n\nTo synchronize your cluster with the cloud provider, follow these steps. \n\n\nSteps\n\n\n\n\n\n\nBrowse to the cluster details.\n\n\n\n\n\n\nClick \nActions\n and select \nSync\n.\n\n\n\n\n\n\nClick \nYes\n to confirm.\n\n\nYour cluster infrastructure is synchronized based on changes on the cloud provider. The updates are written to the \"Event History\". \n\n\n\n\n\n\nStop Cluster\n\n\nCloudbreak supports stopping and restarting clusters. To stop and restart a cluster managed by Cloudbreak, use the options available from the Coudbreak UI. \n\n\nSteps\n\n\n\n\n\n\nBrowse to the cluster details.\n\n\n\n\n\n\nClick \nStop\n to stop a currently running cluster.  \n\n\n\n\n\n\nClick \nYes\n to confirm. \n\n\nYour cluster status changes to \"Stopping in progress\" and then to \"Stopped\". You should see the following messages in the \"Event History\":\n\n\nBilling stopped, the cluster and its infrastructure have been terminated\n11/6/2017, 12:46:48 PM\nInfrastructure successfully stopped\n11/6/2017, 12:46:47 PM\nInfrastructure is now stopping\n11/6/2017, 12:43:58 PM\nAmbari cluster stopped\n11/6/2017, 12:43:52 PM\nAmbari services have been stopped.\n11/6/2017, 12:43:49 PM\nStopping Ambari services.\n11/6/2017, 12:42:14 PM\nStopping Ambari cluster\n11/6/2017, 12:42:10 PM\nCluster infrastructure stop requested\n11/6/2017, 12:42:06 PM\n\n\n\n\n\n\nOnce stopping the infrastructure has completed, you will see a \nStart\n option to restart your cluster. \n\n\nRestart Cluster\n\n\nIf your cluster is in the \"Stopped\" state, you can restart the cluster by follow these steps.\n\n\nSteps\n\n\n\n\n\n\nclick \nStart\n. This option is only available when the cluster has been stopped. \n\n\n\n\n\n\nClick \nYes\n to confirm.\n\n\nYour cluster status changes to \"Start in progress\" and then to \"Running\". You should see the following messages in the \"Event History\":\n\n\nAmbari cluster started; Ambari ip:35.203.149.236\n11/6/2017, 1:02:13 PM\nAmbari services have been started.\n11/6/2017, 1:02:08 PM\nStarting Ambari services.\n11/6/2017, 12:54:37 PM\nStarting Ambari cluster\n11/6/2017, 12:52:57 PM\nBilling started, the cluster and its infrastructure have successfully been started\n11/6/2017, 12:52:48 PM\nInfrastructure successfully started\n11/6/2017, 12:52:48 PM\nInfrastructure is now starting\n11/6/2017, 12:52:26 PM\nAmbari cluster start requested\n11/6/2017, 12:52:24 PM\n\n\n\n\n\n\nTerminate Cluster\n\n\nTo terminate a cluster managed by Cloudbreak, use the option available from the Coudbreak UI. \n\n\nSteps\n\n\n\n\n\n\nBrowse to the cluster details.\n\n\n\n\n\n\nClick \nTerminate\n. \n\n\n\n\n\n\nClick \nYes\n to confirm.\n\n\nAll cluster-related resources will be deleted, unless the resources (such as networks and subnets) existed prior to cluster creation or are used by other VMs in which case they will be preserved. \n\n\n\n\n\n\nForce Terminate\n\n\nCluster deletion may fail if Cloudbreak is unable to delete one or more of the cloud resources that were part of your cluster infrastructure. In such as case, you can use the \nTerminate\n \n \nForce terminate\n option to remove the cluster entry from the Cloudbreak web UI, but you must also check your cloud provider account to see if there are any resources that must be deleted manually.\n\n\nSteps\n\n\n\n\n\n\nBrowse to the cluster details.\n\n\n\n\n\n\nClick \nTerminate\n. \n\n\n\n\n\n\nCheck  \nForce terminate\n.\n\n\n\n\n\n\nClick \nYes\n to confirm. \n\n\n\n\n\n\nThis deletes the cluster tile from the UI.  \n\n\n\n\n\n\nLog in to your cloud provider account and \nmanually delete\n any resources that failed to be deleted.\n\n\n\n\n\n\nView Cluster History\n\n\nFrom the navigation menu in the Cloudbreak UI, you can access the History page that allows you to generate a report showing basic information related to the clusters that were running within the specified range of dates.\n\n\nTo generate a report, follow these steps.\n\n\nSteps\n\n\n\n\n\n\nFrom the Cloudbreak UI navigation menu, select \nHistory\n.\n\n\n\n\n\n\nOn the History page, select the range of dates and click \nShow History\n to generate a report for the selected period.\n\n\n\n\n\n\nHistory Report Content\n\n\nEach entry in the report represents one cluster instance group. For each entry, the report includes the following information:\n\n\n\n\nCreated\n - The date when your cluster was created (YYYY-MM-DD).\n\n\nProvider\n - The name of the cloud provider (AWS, Azure, Google, or OpenStack) on which the cluster instances are/were running.\n\n\nCluster Name\n - The name that you selected for the cluster.\n\n\nWorker Count\n - The number of worker nodes in the cluster. This number may be a decimal if a cluster has been resized.\n\n\nInstance Type\n - Provider-specific VM type of the cluster instances.\n\n\nInstance Group\n - The name of the instance group.  \n\n\nRegion\n - The AWS region in which your cluster is running.\n\n\nRunning Time (hours)\n - The sum of the running times for all the nodes in the instance group.\n\n\n\n\nThe \nAGGREGATE RUNNING TIME\n is the sum of the Running Times, adjusted for the selected time range.\n\n\nTo learn about how your cloud provider bills you for the VMs, refer to their documentation:\n\n\n\n\nAWS\n      \n\n\nAzure\n     \n\n\nGCP", 
            "title": "Manage and Monitor Clusters"
        }, 
        {
            "location": "/os-clusters-manage/index.html#manage-and-monitor-clusters", 
            "text": "You can manage monitor your clusters from the Cloudbreak UI. To do that, click on the tile representing the cluster that you want to access. The actions available for your cluster are listed in the top right corner:      \n   Tips \n   \n   To add or remove nodes from your cluster click  ACTIONS>Resize . \n   To synchronize your cluster with the cloud provider account click  ACTIONS>Sync . \n   To temporarily stop your cluster click  STOP . \n   To terminate your cluster click  TERMINATE .", 
            "title": "Manage and Monitor Clusters"
        }, 
        {
            "location": "/os-clusters-manage/index.html#resize-cluster", 
            "text": "To resize a cluster, follow these steps.  Steps    Browse to the cluster details.    Click  Actions  and select  Resize . The cluster resize dialog is displayed.    Using the +/- controls, adjust how many nodes to add or remove from each host group.      Click  Yes  to confirm the scale-up/scale-down.  While nodes are being added or removed, cluster status changes to \"Update In Progress\". Once the operation has completed, cluster status changes back to \"Running\". Messages similar to the following are written to the \"Event History\", :   Ambari cluster scaled up\n11/6/2017, 12:33:40 PM\nScaling up the Ambari cluster\n11/6/2017, 12:26:59 PM\nStack successfully upscaled\n11/6/2017, 12:26:54 PM\nBootstrapping new nodes\n11/6/2017, 12:26:16 PM\nInfrastructure metadata extension finished\n11/6/2017, 12:26:10 PM\nBilling changed due to upscaling of cluster infrastructure\n11/6/2017, 12:26:10 PM\nAdding 1 new instances to the infrastructure\n11/6/2017, 12:25:23 PM", 
            "title": "Resize Cluster"
        }, 
        {
            "location": "/os-clusters-manage/index.html#synchronize-with-cloud-provider", 
            "text": "If you have just made changes on your cloud provider side (for example, deleted cluster VMs) and you would like to synchronize Cloudbreak with the cloud provider, use the  sync  option.   To synchronize your cluster with the cloud provider, follow these steps.   Steps    Browse to the cluster details.    Click  Actions  and select  Sync .    Click  Yes  to confirm.  Your cluster infrastructure is synchronized based on changes on the cloud provider. The updates are written to the \"Event History\".", 
            "title": "Synchronize with Cloud Provider"
        }, 
        {
            "location": "/os-clusters-manage/index.html#stop-cluster", 
            "text": "Cloudbreak supports stopping and restarting clusters. To stop and restart a cluster managed by Cloudbreak, use the options available from the Coudbreak UI.   Steps    Browse to the cluster details.    Click  Stop  to stop a currently running cluster.      Click  Yes  to confirm.   Your cluster status changes to \"Stopping in progress\" and then to \"Stopped\". You should see the following messages in the \"Event History\":  Billing stopped, the cluster and its infrastructure have been terminated\n11/6/2017, 12:46:48 PM\nInfrastructure successfully stopped\n11/6/2017, 12:46:47 PM\nInfrastructure is now stopping\n11/6/2017, 12:43:58 PM\nAmbari cluster stopped\n11/6/2017, 12:43:52 PM\nAmbari services have been stopped.\n11/6/2017, 12:43:49 PM\nStopping Ambari services.\n11/6/2017, 12:42:14 PM\nStopping Ambari cluster\n11/6/2017, 12:42:10 PM\nCluster infrastructure stop requested\n11/6/2017, 12:42:06 PM    Once stopping the infrastructure has completed, you will see a  Start  option to restart your cluster.", 
            "title": "Stop Cluster"
        }, 
        {
            "location": "/os-clusters-manage/index.html#restart-cluster", 
            "text": "If your cluster is in the \"Stopped\" state, you can restart the cluster by follow these steps.  Steps    click  Start . This option is only available when the cluster has been stopped.     Click  Yes  to confirm.  Your cluster status changes to \"Start in progress\" and then to \"Running\". You should see the following messages in the \"Event History\":  Ambari cluster started; Ambari ip:35.203.149.236\n11/6/2017, 1:02:13 PM\nAmbari services have been started.\n11/6/2017, 1:02:08 PM\nStarting Ambari services.\n11/6/2017, 12:54:37 PM\nStarting Ambari cluster\n11/6/2017, 12:52:57 PM\nBilling started, the cluster and its infrastructure have successfully been started\n11/6/2017, 12:52:48 PM\nInfrastructure successfully started\n11/6/2017, 12:52:48 PM\nInfrastructure is now starting\n11/6/2017, 12:52:26 PM\nAmbari cluster start requested\n11/6/2017, 12:52:24 PM", 
            "title": "Restart Cluster"
        }, 
        {
            "location": "/os-clusters-manage/index.html#terminate-cluster", 
            "text": "To terminate a cluster managed by Cloudbreak, use the option available from the Coudbreak UI.   Steps    Browse to the cluster details.    Click  Terminate .     Click  Yes  to confirm.  All cluster-related resources will be deleted, unless the resources (such as networks and subnets) existed prior to cluster creation or are used by other VMs in which case they will be preserved.", 
            "title": "Terminate Cluster"
        }, 
        {
            "location": "/os-clusters-manage/index.html#force-terminate", 
            "text": "Cluster deletion may fail if Cloudbreak is unable to delete one or more of the cloud resources that were part of your cluster infrastructure. In such as case, you can use the  Terminate     Force terminate  option to remove the cluster entry from the Cloudbreak web UI, but you must also check your cloud provider account to see if there are any resources that must be deleted manually.  Steps    Browse to the cluster details.    Click  Terminate .     Check   Force terminate .    Click  Yes  to confirm.     This deletes the cluster tile from the UI.      Log in to your cloud provider account and  manually delete  any resources that failed to be deleted.", 
            "title": "Force Terminate"
        }, 
        {
            "location": "/os-clusters-manage/index.html#view-cluster-history", 
            "text": "From the navigation menu in the Cloudbreak UI, you can access the History page that allows you to generate a report showing basic information related to the clusters that were running within the specified range of dates.  To generate a report, follow these steps.  Steps    From the Cloudbreak UI navigation menu, select  History .    On the History page, select the range of dates and click  Show History  to generate a report for the selected period.", 
            "title": "View Cluster History"
        }, 
        {
            "location": "/os-clusters-manage/index.html#history-report-content", 
            "text": "Each entry in the report represents one cluster instance group. For each entry, the report includes the following information:   Created  - The date when your cluster was created (YYYY-MM-DD).  Provider  - The name of the cloud provider (AWS, Azure, Google, or OpenStack) on which the cluster instances are/were running.  Cluster Name  - The name that you selected for the cluster.  Worker Count  - The number of worker nodes in the cluster. This number may be a decimal if a cluster has been resized.  Instance Type  - Provider-specific VM type of the cluster instances.  Instance Group  - The name of the instance group.    Region  - The AWS region in which your cluster is running.  Running Time (hours)  - The sum of the running times for all the nodes in the instance group.   The  AGGREGATE RUNNING TIME  is the sum of the Running Times, adjusted for the selected time range.  To learn about how your cloud provider bills you for the VMs, refer to their documentation:   AWS         Azure        GCP", 
            "title": "History Report Content"
        }, 
        {
            "location": "/blueprints/index.html", 
            "text": "Blueprints\n\n\nAmbari blueprints\n are your declarative definition of a Hadoop cluster, defining the host groups and which components to install on which host group. Ambari uses them as a base for your clusters. \n\n\nYou have three options concerning using blueprints with Cloudbreak:\n\n\n\n\nUse one of the pre-defined blueprints.    \n\n\nAdd your custom blueprint by uploading a JSON file or pasting the JSON text. \n\n\n\n\nWe recommend that you review the default blueprints to check if they meet your requirements. You can do this by selecting \nBlueprints\n from the navigation pane in the Cloudbreak web UI or by reading the documentation below.\n\n\nUse Default Blueprints\n\n\nTo use one of the default blueprints, simply select them when creating a cluster. The option is available on the \nGeneral\n page. First select the \nStack Version\n and then select your chosen blueprint under \nCluster Type\n. \n\n\nDefault Blueprints\n\n\nCloudbreak includes the following default HDP cluster blueprints:\n\n\nPlatform Version: \nHDP 2.6\n\n\n\n\n\n\n\n\nCluster Type\n\n\nMain Services\n\n\nList of All Services Included\n\n\n\n\n\n\n\n\n\n\nData Science\n\n\n Spark 1.6,\nZeppelin 0.7.0\n\n\nHDFS, YARN, MapReduce2, Tez, Hive 1.2.1, Pig, Sqoop, ZooKeeper, Ambari Metrics, Spark 1.6, Zeppelin 0.7.0\n\n\n\n\n\n\nData Science\n\n\n Spark 2.1,\nZeppelin 0.7.0\n\n\nHDFS, YARN, MapReduce2, Tez, Hive 1.2.1, Pig, Sqoop, ZooKeeper, Ambari Metrics, Spark 2.1, Zeppelin 0.7.0\n\n\n\n\n\n\nEDW - Analytics\n\n\n Hive 2 LLAP\n,\nZeppelin 0.7.0\n\n\nHDFS, YARN, MapReduce2, Tez, Hive 2 LLAP, Druid, Pig, ZooKeeper, Ambari Metrics, Spark 2.1\n\n\n\n\n\n\nEDW - ETL\n\n\n Hive 1.2.1,\nSpark 1.6\n\n\nHDFS, YARN, MapReduce2, Tez, Hive 1.2.1, Pig, Sqoop, ZooKeeper, Ambari Metrics, Spark 1.6\n\n\n\n\n\n\nEDW - ETL\n\n\n Hive 1.2.1,\n Spark 2.1\n\n\nHDFS, YARN, MapReduce2, Tez, Hive 1.2.1, Pig, ZooKeeper, Ambari Metrics, Spark 2.1\n\n\n\n\n\n\nBI\n\n\n Druid 0.9.2\n\n\nHDFS, YARN, MapReduce2, Tez, Druid, Sqoop, ZooKeeper, Ambari Metrics\n\n\n\n\n\n\n\n\n\n    \nChoosing Your Configuration\n\n    \n\nWhen creating a cluster, you can choose a more stable cluster configuration for a predicable experience.\nAlternatively, you can try the latest capabilities by choosing a cluster configuration\nthat is much more experimental. The following configuration classification applies:\n\n\n\n Stable configurations are the best choice if you want to avoid issues and other problems with launching and using clusters.\n\n\n If you want to use a Technical Preview version of a component in a release of HDP, use these configurations.\n\n\n These are the most cutting edge of the configurations, including Technical Preview components in a Technical Preview HDP release.\n\n\n\n\n\n\n\n\n\nAdd Custom Blueprint\n\n\nThis option allows you to save your custom blueprints. For correct blueprint layout and other useful information about Ambari blueprints, refer to the \nAmbari cwiki\n page.\n\n\nCreating a Blueprint\n\n\nAmbari blueprints are specified in the JSON format. After you provide the blueprint to Cloudbreak, the host groups in the JSON will be mapped to a set of instances when starting the cluster, and the specified services and components will be installed on the corresponding nodes. It is not necessary to define a complete configuration in the blueprint. If a configuration is missing, Ambari will fill that with a default value. \nFurthermore, a blueprint can be modified later from the Ambari UI.\n\n\nA blueprint can be exported from a running Ambari cluster and can be reused in Cloudbreak after slight modifications. When a blueprint is exported, some configurations are hardcoded for example domain names, memory configurations, and so on, that won't be applicable to the Cloudbreak cluster. There is no automatic way to modify an exported blueprint and make it instantly usable in Cloudbreak, the modifications have to be done manually.\n\n\nExample Blueprints\n\n\nHere are some \nblueprint examples\n.  \n\n\nUpload a Blueprint\n\n\nOnce you have your blueprint ready, perform these steps.\n\n\nSteps\n\n\n\n\nIn the Cloudbreak UI, select \nBlueprints\n from the navigation pane. \n\n\n\n\nTo add your own blueprint, click \nCreate Blueprint\n and enter the following parameters:\n\n\n\n\n\n\n\n\nParameter\n\n\nValue\n\n\n\n\n\n\n\n\n\n\nName\n\n\nEnter a name for your blueprint.\n\n\n\n\n\n\nDescription\n\n\n(Optional) Enter a description for your blueprint.\n\n\n\n\n\n\nBlueprint Source\n\n\nSelect one of: \nText\n: Paste blueprint in JSON format.\n \nFile\n: Upload a file that contains the blueprint.\n \nURL\n: Specify the URL for your blueprint.\n\n\n\n\n\n\n\n\n \n\n\n\n\n\n\nTo use the uploaded blueprints, select it when creating a cluster. The option is available on the \nGeneral\n page. First select the \nPlatform Version\n and then select your chosen blueprint under \nCluster Type\n. \n\n\n \n\n\n\n\n\n\nView Blueprint Details\n\n\nOnce a blueprint has been registered in Cloudbreak, you can access its details in the Cloudbreak UI.\n\n\nSteps\n\n\n\n\n\n\nIn the Cloudbreak UI, select \nBlueprints\n from the navigation pane. \n\n\n\n\n\n\nClick on an entry to navigate to details. \n\n\nYou can view blueprint details using the \nList View\n and \nRaw View\n:\n\n\n \n\n\n\n\n\n\nDelete Blueprint\n\n\nTo delete a default or custom blueprint, perform these steps.\n\n\nSteps\n\n\n\n\n\n\nIn the Cloudbreak UI, select \nBlueprints\n from the navigation pane. \n\n\n\n\n\n\nClick on an entry to navigate to details. \n\n\n\n\n\n\nClick \nDelete\n. \n\n\n\n\n\n\nConfirm delete. \n\n\n\n\n\n\nModifying Existing Blueprints\n\n\n\n\nYou cannot directly modify default blueprints.\n\n\nYou cannot directly modify the custom blueprints that you have uploaded or pasted. However, If you provided a URL to the location where the blueprint is stored, in order to modify the blueprint simply update it in the location to which the URL is pointing.", 
            "title": "Blueprints"
        }, 
        {
            "location": "/blueprints/index.html#blueprints", 
            "text": "Ambari blueprints  are your declarative definition of a Hadoop cluster, defining the host groups and which components to install on which host group. Ambari uses them as a base for your clusters.   You have three options concerning using blueprints with Cloudbreak:   Use one of the pre-defined blueprints.      Add your custom blueprint by uploading a JSON file or pasting the JSON text.    We recommend that you review the default blueprints to check if they meet your requirements. You can do this by selecting  Blueprints  from the navigation pane in the Cloudbreak web UI or by reading the documentation below.", 
            "title": "Blueprints"
        }, 
        {
            "location": "/blueprints/index.html#use-default-blueprints", 
            "text": "To use one of the default blueprints, simply select them when creating a cluster. The option is available on the  General  page. First select the  Stack Version  and then select your chosen blueprint under  Cluster Type .", 
            "title": "Use Default Blueprints"
        }, 
        {
            "location": "/blueprints/index.html#default-blueprints", 
            "text": "Cloudbreak includes the following default HDP cluster blueprints:  Platform Version:  HDP 2.6     Cluster Type  Main Services  List of All Services Included      Data Science   Spark 1.6, Zeppelin 0.7.0  HDFS, YARN, MapReduce2, Tez, Hive 1.2.1, Pig, Sqoop, ZooKeeper, Ambari Metrics, Spark 1.6, Zeppelin 0.7.0    Data Science   Spark 2.1, Zeppelin 0.7.0  HDFS, YARN, MapReduce2, Tez, Hive 1.2.1, Pig, Sqoop, ZooKeeper, Ambari Metrics, Spark 2.1, Zeppelin 0.7.0    EDW - Analytics   Hive 2 LLAP , Zeppelin 0.7.0  HDFS, YARN, MapReduce2, Tez, Hive 2 LLAP, Druid, Pig, ZooKeeper, Ambari Metrics, Spark 2.1    EDW - ETL   Hive 1.2.1, Spark 1.6  HDFS, YARN, MapReduce2, Tez, Hive 1.2.1, Pig, Sqoop, ZooKeeper, Ambari Metrics, Spark 1.6    EDW - ETL   Hive 1.2.1,  Spark 2.1  HDFS, YARN, MapReduce2, Tez, Hive 1.2.1, Pig, ZooKeeper, Ambari Metrics, Spark 2.1    BI   Druid 0.9.2  HDFS, YARN, MapReduce2, Tez, Druid, Sqoop, ZooKeeper, Ambari Metrics     \n     Choosing Your Configuration \n     \nWhen creating a cluster, you can choose a more stable cluster configuration for a predicable experience.\nAlternatively, you can try the latest capabilities by choosing a cluster configuration\nthat is much more experimental. The following configuration classification applies:   Stable configurations are the best choice if you want to avoid issues and other problems with launching and using clusters.   If you want to use a Technical Preview version of a component in a release of HDP, use these configurations.   These are the most cutting edge of the configurations, including Technical Preview components in a Technical Preview HDP release.", 
            "title": "Default Blueprints"
        }, 
        {
            "location": "/blueprints/index.html#add-custom-blueprint", 
            "text": "This option allows you to save your custom blueprints. For correct blueprint layout and other useful information about Ambari blueprints, refer to the  Ambari cwiki  page.", 
            "title": "Add Custom Blueprint"
        }, 
        {
            "location": "/blueprints/index.html#creating-a-blueprint", 
            "text": "Ambari blueprints are specified in the JSON format. After you provide the blueprint to Cloudbreak, the host groups in the JSON will be mapped to a set of instances when starting the cluster, and the specified services and components will be installed on the corresponding nodes. It is not necessary to define a complete configuration in the blueprint. If a configuration is missing, Ambari will fill that with a default value. \nFurthermore, a blueprint can be modified later from the Ambari UI.  A blueprint can be exported from a running Ambari cluster and can be reused in Cloudbreak after slight modifications. When a blueprint is exported, some configurations are hardcoded for example domain names, memory configurations, and so on, that won't be applicable to the Cloudbreak cluster. There is no automatic way to modify an exported blueprint and make it instantly usable in Cloudbreak, the modifications have to be done manually.", 
            "title": "Creating a Blueprint"
        }, 
        {
            "location": "/blueprints/index.html#example-blueprints", 
            "text": "Here are some  blueprint examples .", 
            "title": "Example Blueprints"
        }, 
        {
            "location": "/blueprints/index.html#upload-a-blueprint", 
            "text": "Once you have your blueprint ready, perform these steps.  Steps   In the Cloudbreak UI, select  Blueprints  from the navigation pane.    To add your own blueprint, click  Create Blueprint  and enter the following parameters:     Parameter  Value      Name  Enter a name for your blueprint.    Description  (Optional) Enter a description for your blueprint.    Blueprint Source  Select one of:  Text : Paste blueprint in JSON format.   File : Upload a file that contains the blueprint.   URL : Specify the URL for your blueprint.          To use the uploaded blueprints, select it when creating a cluster. The option is available on the  General  page. First select the  Platform Version  and then select your chosen blueprint under  Cluster Type .", 
            "title": "Upload a Blueprint"
        }, 
        {
            "location": "/blueprints/index.html#view-blueprint-details", 
            "text": "Once a blueprint has been registered in Cloudbreak, you can access its details in the Cloudbreak UI.  Steps    In the Cloudbreak UI, select  Blueprints  from the navigation pane.     Click on an entry to navigate to details.   You can view blueprint details using the  List View  and  Raw View :", 
            "title": "View Blueprint Details"
        }, 
        {
            "location": "/blueprints/index.html#delete-blueprint", 
            "text": "To delete a default or custom blueprint, perform these steps.  Steps    In the Cloudbreak UI, select  Blueprints  from the navigation pane.     Click on an entry to navigate to details.     Click  Delete .     Confirm delete.", 
            "title": "Delete Blueprint"
        }, 
        {
            "location": "/blueprints/index.html#modifying-existing-blueprints", 
            "text": "You cannot directly modify default blueprints.  You cannot directly modify the custom blueprints that you have uploaded or pasted. However, If you provided a URL to the location where the blueprint is stored, in order to modify the blueprint simply update it in the location to which the URL is pointing.", 
            "title": "Modifying Existing Blueprints"
        }, 
        {
            "location": "/recipes/index.html", 
            "text": "Recipes\n\n\nAlthough Cloudbreak lets you provision HDP clusters in the cloud based on custom Ambari blueprints, Cloudbreak provisioning options don't consider all possible use cases. For that reason, we introduced recipes. \n\n\nA recipe is a script that runs on all nodes of a selected node group before or after the Ambari cluster installation. You can use recipes for tasks such as installing additional software or performing advanced cluster configuration. For example, you can use a recipe to put a JAR file on the Hadoop classpath.\n\n\nWhen creating a cluster, you can optionally upload one or more \"recipes\" (custom scripts) and they will be executed on a specific host group before or after the cluster installation. \n\n\nWriting Recipes\n\n\nWhen using recipes, consider the following:\n\n\n\n\nThe scripts will be executed on the node types you specify (such as \"master\", \"worker\", \"compute\"). If you want to run a a script on all nodes, define the recipe one per node type.  \n\n\nThe script will execute on all of the nodes of that type as root.  \n\n\nIn order to be executed, your script must be in a network location which is accessible from the cloud controller and cluster instances VPC.  \n\n\nMake sure to follow Linux best practices when creating your scripts. For example, don't forget to script \"Yes\" auto-answers where needed.  \n\n\nDo not execute yum update \u2013y since it may update other components on the instances (such as salt), which can create unintended or unstable behavior.   \n\n\nThe scripts will be executed as root. The recipe output is written to \n/var/log/recipes\n on each node on which it was executed.\n\n\n\n\nSample Recipe for Yum Proxy Setting\n\n\n#!/bin/bash\ncat \n /etc/yum.conf \nENDOF\nproxy=http://10.0.0.133:3128\nENDOF\n\n\n\n\nAdding Recipes\n\n\nTo add a recipe, perform these steps.\n\n\nSteps\n\n\n\n\n\n\nPlace your script in a network location accessible from Cloudbreak and cluster instances virtual network. \n\n\n\n\n\n\nSelect \nBlueprints\n from the navigation pane. \n\n\n\n\n\n\nClick on \nCreate Blueprint\n. \n\n\n\n\n\n\nProvide the following:\n\n\n\n\n\n\n\n\nParameter\n\n\nValue\n\n\n\n\n\n\n\n\n\n\nName\n\n\nEnter a name for your recipe.\n\n\n\n\n\n\nDescription\n\n\n(Optional) Enter a description for your recipe.\n\n\n\n\n\n\nExecution Type\n\n\nSelect one of the following options: \nPRE\n: The script will be executed prior to Ambari cluster deployment.\nPOST\n: The script will be executed after Ambari cluster deployment.\n\n\n\n\n\n\nScript\n\n\nSelect one of: \nScript\n: Paste the script.\n \nFile\n: Point to a file on your machine that contains the recipe.\n \nURL\n: Specify the URL for your recipe.\n\n\n\n\n\n\n\n\n \n\n\n\n\n\n\nWhen creating a cluster, you can select previously added recipes in the \nRecipes\n section. \n\n\n \n\n\n\n\n\n\nDeleting Recipes\n\n\nYou can delete previously added items by selecting and item and using the \ndelete\n option. \n\n\nModifying Existing Recipes\n\n\nTo modify previously added recipes:\n\n\n\n\nIf you pasted or uploaded your recipe in the Cloudbreak UI, in order to modify it you must delete the entry and add the recipe again in a new entry.   \n\n\nIf you provided a URL to the location where the recipe is stored, in order to modify the recipe simply update it in the location to which the URL is pointing.", 
            "title": "Recipes"
        }, 
        {
            "location": "/recipes/index.html#recipes", 
            "text": "Although Cloudbreak lets you provision HDP clusters in the cloud based on custom Ambari blueprints, Cloudbreak provisioning options don't consider all possible use cases. For that reason, we introduced recipes.   A recipe is a script that runs on all nodes of a selected node group before or after the Ambari cluster installation. You can use recipes for tasks such as installing additional software or performing advanced cluster configuration. For example, you can use a recipe to put a JAR file on the Hadoop classpath.  When creating a cluster, you can optionally upload one or more \"recipes\" (custom scripts) and they will be executed on a specific host group before or after the cluster installation.", 
            "title": "Recipes"
        }, 
        {
            "location": "/recipes/index.html#writing-recipes", 
            "text": "When using recipes, consider the following:   The scripts will be executed on the node types you specify (such as \"master\", \"worker\", \"compute\"). If you want to run a a script on all nodes, define the recipe one per node type.    The script will execute on all of the nodes of that type as root.    In order to be executed, your script must be in a network location which is accessible from the cloud controller and cluster instances VPC.    Make sure to follow Linux best practices when creating your scripts. For example, don't forget to script \"Yes\" auto-answers where needed.    Do not execute yum update \u2013y since it may update other components on the instances (such as salt), which can create unintended or unstable behavior.     The scripts will be executed as root. The recipe output is written to  /var/log/recipes  on each node on which it was executed.", 
            "title": "Writing Recipes"
        }, 
        {
            "location": "/recipes/index.html#sample-recipe-for-yum-proxy-setting", 
            "text": "#!/bin/bash\ncat   /etc/yum.conf  ENDOF\nproxy=http://10.0.0.133:3128\nENDOF", 
            "title": "Sample Recipe for Yum Proxy Setting"
        }, 
        {
            "location": "/recipes/index.html#adding-recipes", 
            "text": "To add a recipe, perform these steps.  Steps    Place your script in a network location accessible from Cloudbreak and cluster instances virtual network.     Select  Blueprints  from the navigation pane.     Click on  Create Blueprint .     Provide the following:     Parameter  Value      Name  Enter a name for your recipe.    Description  (Optional) Enter a description for your recipe.    Execution Type  Select one of the following options:  PRE : The script will be executed prior to Ambari cluster deployment. POST : The script will be executed after Ambari cluster deployment.    Script  Select one of:  Script : Paste the script.   File : Point to a file on your machine that contains the recipe.   URL : Specify the URL for your recipe.          When creating a cluster, you can select previously added recipes in the  Recipes  section.", 
            "title": "Adding Recipes"
        }, 
        {
            "location": "/recipes/index.html#deleting-recipes", 
            "text": "You can delete previously added items by selecting and item and using the  delete  option.", 
            "title": "Deleting Recipes"
        }, 
        {
            "location": "/recipes/index.html#modifying-existing-recipes", 
            "text": "To modify previously added recipes:   If you pasted or uploaded your recipe in the Cloudbreak UI, in order to modify it you must delete the entry and add the recipe again in a new entry.     If you provided a URL to the location where the recipe is stored, in order to modify the recipe simply update it in the location to which the URL is pointing.", 
            "title": "Modifying Existing Recipes"
        }, 
        {
            "location": "/tags/index.html", 
            "text": "Resource Tagging\n\n\nWhen you manually create resources (such as VMs) in the cloud, you have an option to add custom tags that help you track these resources. Likewise, when creating clusters, you can instruct Cloudbreak to tag the cloud resources that it creates on your behalf.\n\n\nThe tags added during cluster creation will be displayed on your cloud account, allowing you to track your resources. \n\n\nYou can use tags to categorize your cloud resources by purpose, owner, and so on. Tags come in especially handy when you are using a corporate AWS account and you want to quickly identify which resources belong to your cluster(s). In fact, your corporate cloud account admin may require you to tag all the resources that you create, in particular resources, such as VMs, which incur charges.\n\n\nAdd Tags When Creating a Cluster\n\n\nYou can tag the cloud resources used for a cluster by providing custom tag names and values when creating a cluster via UI or CLI. In the Cloudbreak UI, this option is available in the create clusetr wizard, in the advanced \nGeneral Configuration\n \n \nTags\n section:\n\n\n \n\n\nIt is not possible to add tags after your cluster has been created.  \n\n\nCloud Provider Documentation\n\n\nTo learn more about tags and their restrictions, refer to the cloud provider documentation:\n\n\n\n\nTags on AWS\n    \n\n\nTags on Azure\n  \n\n\nLabels on GCP\n  \n\n\nTags on OpenStack", 
            "title": "Resource Tagging"
        }, 
        {
            "location": "/tags/index.html#resource-tagging", 
            "text": "When you manually create resources (such as VMs) in the cloud, you have an option to add custom tags that help you track these resources. Likewise, when creating clusters, you can instruct Cloudbreak to tag the cloud resources that it creates on your behalf.  The tags added during cluster creation will be displayed on your cloud account, allowing you to track your resources.   You can use tags to categorize your cloud resources by purpose, owner, and so on. Tags come in especially handy when you are using a corporate AWS account and you want to quickly identify which resources belong to your cluster(s). In fact, your corporate cloud account admin may require you to tag all the resources that you create, in particular resources, such as VMs, which incur charges.", 
            "title": "Resource Tagging"
        }, 
        {
            "location": "/tags/index.html#add-tags-when-creating-a-cluster", 
            "text": "You can tag the cloud resources used for a cluster by providing custom tag names and values when creating a cluster via UI or CLI. In the Cloudbreak UI, this option is available in the create clusetr wizard, in the advanced  General Configuration     Tags  section:     It is not possible to add tags after your cluster has been created.", 
            "title": "Add Tags When Creating a Cluster"
        }, 
        {
            "location": "/tags/index.html#cloud-provider-documentation", 
            "text": "To learn more about tags and their restrictions, refer to the cloud provider documentation:   Tags on AWS       Tags on Azure     Labels on GCP     Tags on OpenStack", 
            "title": "Cloud Provider Documentation"
        }, 
        {
            "location": "/images/index.html", 
            "text": "Custom Images\n\n\nBy default, Cloudbreak launches clusters from an image that includes default configuration and default tooling for provisioning. These are considered the Standard default images.\n\n\nIn some cases, these default images might not fit the requirements of users: for example when they need custom OS hardening, libraries, tooling, and so on. In such cases, the user would like to start their clusters from their own custom image.\n\n\nBuilding Custom Images\n\n\nRefer to \nCustom Images for Cloudbreak\n. This repository includes instructions and scripts to help you build custom images. Once you have the images, refer to the documentation below for information on how to register and use these images with Cloudbreak.\n\n\nRegistering Custom Images\n\n\nRegister your custom images in Cloudbreak by placing \nyml\n files that declare your custom images in the \n/var/lib/cloudbreak-deployment/etc\n directory on the Cloudbreak host. The \netc\n directory does not exist by default, so you need to create it.\n\n\nThe format of the \nyml\n files is cloud provider specific and described in the following sections.  \n\n\n\n\nIf you register the images after Cloudbreak has been started, you need to restart Cloudbreak after updating the images.\n\n\n\n\nRegister Images for AWS\n\n\nTo override the default images, perform these steps.\n\n\nSteps\n\n\n\n\nNavigate to the \n/var/lib/cloudbreak-deployment/\n directory and create a new directory called \netc\n.\n\n\nNavigate to \n/var/lib/cloudbreak-deployment/etc/\n and create a new file called \naws-images.yml\n. Use the content below as base content for \naws-images.yml\n but replace the images listed with your custom images for each region that you want to use:\n\n\n\n\n\naws:\n  ap-northeast-1: ami-76729917\n  ap-northeast-2: ami-7c1ad112\n  ap-southeast-1: ami-a7ac7fc4\n  ap-southeast-2: ami-acf7decf\n  eu-central-1: ami-71da331e\n  eu-west-1: ami-cba43bb8\n  sa-east-1: ami-f8901a94\n  us-east-1: ami-48ba9d5e\n  us-west-1: ami-b76421d7\n  us-west-2: ami-d541bbb5\n\n\n\n\nRegister Images for Azure\n\n\nTo override the default images, perform these steps.\n\n\nSteps\n\n\n\n\nNavigate to the \n/var/lib/cloudbreak-deployment/\n directory and create a new directory called \netc\n.\n\n\nNavigate to \n/var/lib/cloudbreak-deployment/etc/\n and create a new file called \narm-images.yml\n. Use the content below as base content for \narm-images.yml\n but replace the images listed with your custom images for each region that you want to use:\n\n\n\n\nazure_rm:\n  East Asia: https://sequenceiqeastasia2.blob.core.windows.net/images/cb-2016-06-14-03-27.vhd\n  East US: https://sequenceiqeastus2.blob.core.windows.net/images/cb-2016-06-14-03-27.vhd\n  Central US: https://sequenceiqcentralus2.blob.core.windows.net/images/cb-2016-06-14-03-27.vhd\n  North Europe: https://sequenceiqnortheurope2.blob.core.windows.net/images/cb-2016-06-14-03-27.vhd\n  South Central US: https://sequenceiqouthcentralus2.blob.core.windows.net/images/cb-2016-06-14-03-27.vhd\n  North Central US: https://sequenceiqorthcentralus2.blob.core.windows.net/images/cb-2016-06-14-03-27.vhd\n  East US 2: https://sequenceiqeastus22.blob.core.windows.net/images/cb-2016-06-14-03-27.vhd\n  Japan East: https://sequenceiqjapaneast2.blob.core.windows.net/images/cb-2016-06-14-03-27.vhd\n  Japan West: https://sequenceiqjapanwest2.blob.core.windows.net/images/cb-2016-06-14-03-27.vhd\n  Southeast Asia: https://sequenceiqsoutheastasia2.blob.core.windows.net/images/cb-2016-06-14-03-27.vhd\n  West US: https://sequenceiqwestus2.blob.core.windows.net/images/cb-2016-06-14-03-27.vhd\n  West Europe: https://sequenceiqwesteurope2.blob.core.windows.net/images/cb-2016-06-14-03-27.vhd\n  Brazil South: https://sequenceiqbrazilsouth2.blob.core.windows.net/images/cb-2016-06-14-03-27.vhd\n\n\n\n\nRegister Images for GCP\n\n\nTo override the default images, perform these steps.\n\n\nSteps\n\n\n\n\nNavigate to the \n/var/lib/cloudbreak-deployment/\n directory and create a new directory called \netc\n.\n\n\nNavigate to \n/var/lib/cloudbreak-deployment/etc/\n and create a new file called \ngcp-images.yml\n. Use the content below as base content for \ngcp-images.yml\n but replace the images listed with your custom images for each region that you want to use:\n\n\n\n\n\ngcp:\n  default: sequenceiqimage/cb-2016-06-14-03-27.tar.gz\n\n\n\n\nRegister Images for OpenStack\n\n\nTo override the default images, perform these steps.\n\n\nSteps\n\n\n\n\nNavigate to the \n/var/lib/cloudbreak-deployment/\n directory and create a new directory called \netc\n.\n\n\nNavigate to \n/var/lib/cloudbreak-deployment/etc/\n and create a new file called \nos-images.yml\n. Use the content below as base content for \nos-images.yml\n but replace the images listed with your custom images for each region that you want to use:\n\n\n\n\n\nopenstack:\n  default: cloudbreak-2016-06-14-10-58", 
            "title": "Custom Images"
        }, 
        {
            "location": "/images/index.html#custom-images", 
            "text": "By default, Cloudbreak launches clusters from an image that includes default configuration and default tooling for provisioning. These are considered the Standard default images.  In some cases, these default images might not fit the requirements of users: for example when they need custom OS hardening, libraries, tooling, and so on. In such cases, the user would like to start their clusters from their own custom image.", 
            "title": "Custom Images"
        }, 
        {
            "location": "/images/index.html#building-custom-images", 
            "text": "Refer to  Custom Images for Cloudbreak . This repository includes instructions and scripts to help you build custom images. Once you have the images, refer to the documentation below for information on how to register and use these images with Cloudbreak.", 
            "title": "Building Custom Images"
        }, 
        {
            "location": "/images/index.html#registering-custom-images", 
            "text": "Register your custom images in Cloudbreak by placing  yml  files that declare your custom images in the  /var/lib/cloudbreak-deployment/etc  directory on the Cloudbreak host. The  etc  directory does not exist by default, so you need to create it.  The format of the  yml  files is cloud provider specific and described in the following sections.     If you register the images after Cloudbreak has been started, you need to restart Cloudbreak after updating the images.", 
            "title": "Registering Custom Images"
        }, 
        {
            "location": "/images/index.html#register-images-for-aws", 
            "text": "To override the default images, perform these steps.  Steps   Navigate to the  /var/lib/cloudbreak-deployment/  directory and create a new directory called  etc .  Navigate to  /var/lib/cloudbreak-deployment/etc/  and create a new file called  aws-images.yml . Use the content below as base content for  aws-images.yml  but replace the images listed with your custom images for each region that you want to use:   \naws:\n  ap-northeast-1: ami-76729917\n  ap-northeast-2: ami-7c1ad112\n  ap-southeast-1: ami-a7ac7fc4\n  ap-southeast-2: ami-acf7decf\n  eu-central-1: ami-71da331e\n  eu-west-1: ami-cba43bb8\n  sa-east-1: ami-f8901a94\n  us-east-1: ami-48ba9d5e\n  us-west-1: ami-b76421d7\n  us-west-2: ami-d541bbb5", 
            "title": "Register Images for AWS"
        }, 
        {
            "location": "/images/index.html#register-images-for-azure", 
            "text": "To override the default images, perform these steps.  Steps   Navigate to the  /var/lib/cloudbreak-deployment/  directory and create a new directory called  etc .  Navigate to  /var/lib/cloudbreak-deployment/etc/  and create a new file called  arm-images.yml . Use the content below as base content for  arm-images.yml  but replace the images listed with your custom images for each region that you want to use:   azure_rm:\n  East Asia: https://sequenceiqeastasia2.blob.core.windows.net/images/cb-2016-06-14-03-27.vhd\n  East US: https://sequenceiqeastus2.blob.core.windows.net/images/cb-2016-06-14-03-27.vhd\n  Central US: https://sequenceiqcentralus2.blob.core.windows.net/images/cb-2016-06-14-03-27.vhd\n  North Europe: https://sequenceiqnortheurope2.blob.core.windows.net/images/cb-2016-06-14-03-27.vhd\n  South Central US: https://sequenceiqouthcentralus2.blob.core.windows.net/images/cb-2016-06-14-03-27.vhd\n  North Central US: https://sequenceiqorthcentralus2.blob.core.windows.net/images/cb-2016-06-14-03-27.vhd\n  East US 2: https://sequenceiqeastus22.blob.core.windows.net/images/cb-2016-06-14-03-27.vhd\n  Japan East: https://sequenceiqjapaneast2.blob.core.windows.net/images/cb-2016-06-14-03-27.vhd\n  Japan West: https://sequenceiqjapanwest2.blob.core.windows.net/images/cb-2016-06-14-03-27.vhd\n  Southeast Asia: https://sequenceiqsoutheastasia2.blob.core.windows.net/images/cb-2016-06-14-03-27.vhd\n  West US: https://sequenceiqwestus2.blob.core.windows.net/images/cb-2016-06-14-03-27.vhd\n  West Europe: https://sequenceiqwesteurope2.blob.core.windows.net/images/cb-2016-06-14-03-27.vhd\n  Brazil South: https://sequenceiqbrazilsouth2.blob.core.windows.net/images/cb-2016-06-14-03-27.vhd", 
            "title": "Register Images for Azure"
        }, 
        {
            "location": "/images/index.html#register-images-for-gcp", 
            "text": "To override the default images, perform these steps.  Steps   Navigate to the  /var/lib/cloudbreak-deployment/  directory and create a new directory called  etc .  Navigate to  /var/lib/cloudbreak-deployment/etc/  and create a new file called  gcp-images.yml . Use the content below as base content for  gcp-images.yml  but replace the images listed with your custom images for each region that you want to use:   \ngcp:\n  default: sequenceiqimage/cb-2016-06-14-03-27.tar.gz", 
            "title": "Register Images for GCP"
        }, 
        {
            "location": "/images/index.html#register-images-for-openstack", 
            "text": "To override the default images, perform these steps.  Steps   Navigate to the  /var/lib/cloudbreak-deployment/  directory and create a new directory called  etc .  Navigate to  /var/lib/cloudbreak-deployment/etc/  and create a new file called  os-images.yml . Use the content below as base content for  os-images.yml  but replace the images listed with your custom images for each region that you want to use:   \nopenstack:\n  default: cloudbreak-2016-06-14-10-58", 
            "title": "Register Images for OpenStack"
        }, 
        {
            "location": "/vm-launch/index.html", 
            "text": "Install Cloudbreak in Your Own VM\n\n\nThis is an advanced deployment option. Select this option if you have custom VM requirements. Otherwise, you should use one of the pre-built images and follow these instructions:\n\n\n\n\nLaunch on AWS\n  \n\n\nLaunch on Azure\n  \n\n\nLaunch on GCP\n  \n\n\nLaunch on OpenStack\n   \n\n\n\n\nSystem Requirements\n\n\nTo launch the Cloudbreak deployer and install the Cloudbreak application, your system must meet the following requirements:\n\n\n\n\nMinimum VM requirements: 8GB RAM, 10GB disk, 2 cores\n\n\nSupported operating systems: RHEL, CentOS, and Oracle Linux 7 (64-bit)\n\n\nDocker 1.9.1 must be installed \n\n\n\n\n\n\nYou can install Cloudbreak on Mac OS X for evaluation purposes only. Mac OS X is not supported for a production deployment of Cloudbreak.\n\n\n\n\nPrerequisites\n\n\nTo launch the Cloudbreak deployer and install the Cloudbreak application, you must first meet the following prerequisites:\n\n\nPorts\n\n\nPorts 22 (SSH) and 443 (HTTPS) must be open.\n\n\nRoot Access\n\n\nEvery command must be executed as root. In order to get root privileges execute: \n\n\nsudo -i\n\n\n\nSystem Updates\n\n\nEnsure that your system is up-to-date by executing:\n\n\nyum -y update\n\n\n\nReboot it if necessary.\n\n\nIptables\n\n\nInstall iptables-services:\n\n\nyum -y install iptables-services net-tools\n\n\n\nWithout iptables-services installed the \niptables save\n command will not be available.\n\n\nNext, configure permissive iptables on your machine:\n\n\n\niptables --flush INPUT \n&\n&\n \\\niptables --flush FORWARD \n&\n&\n \\\nservice iptables save\n\n\n\n\nMore\n\n\nAdditionally, review the following prerequisites: \n\n\n\n\nPrerequisites on AWS\n\n\nPrerequisites on Azure\n\n\nPrerequisites on GCP\n\n\nPrerequisites on OpenStack\n \n\n\n\n\nInstall Cloudbreak on Your Own VM\n\n\nInstall Cloudbreak using the following steps.\n\n\nSteps\n\n\n\n\n\n\nInstall the Cloudbreak deployer and unzip the platform-specific single binary to your PATH. For example:\n\n\nyum -y install unzip tar\ncurl -Ls s3.amazonaws.com/public-repo-1.hortonworks.com/HDP/cloudbreak/cloudbreak-deployer_1.16.1_$(uname)_x86_64.tgz | sudo tar -xz -C /bin cbd\ncbd --version\n\n\nOnce the Cloudbreak Deployer is installed, you can set up the Cloudbreak application.\n\n\n\n\n\n\nCreate a Cloudbreak deployment directory and navigate to it:\n\n\nmkdir cloudbreak-deployment\ncd cloudbreak-deployment\n\n\n\n\n\n\nIn the directory, create a file called \nProfile\n with the following content:\n\n\nexport UAA_DEFAULT_SECRET=MY-SECRET\nexport UAA_DEFAULT_USER_PW=MY-PASSWORD\n\n\nFor example:\n\n\nexport UAA_DEFAULT_SECRET=MySecret123\nexport UAA_DEFAULT_USER_PW=MySecurePassword123\n\n\n\n\nYou will need to provide the password when logging in to the Cloudbreak web UI and when using the Cloudbreak Shell. The secret will be used by Cloudbreak for authentication.\n\n\n\n\n\n\n\n\nGenerate configurations by executing:\n\n\nrm *.yml\ncbd generate\n   \n\n\nThe cbd start command includes the cbd generate command which applies the following steps:\n\n\n\n\nCreates the \ndocker-compose.yml\n file, which describes the configuration of all the Docker containers required for the Cloudbreak deployment.  \n\n\nCreates the \nuaa.yml\n file, which holds the configuration of the identity server used to authenticate users with Cloudbreak.   \n\n\n\n\n\n\n\n\nStart the Cloudbreak application by using the following commands:\n\n\ncbd pull\ncbd start\n\n\nThis will start the Docker containers and initialize the application. The first time you start the Coudbreak app, the process will take longer than usual due to the download of all the necessary docker images.\n\n\n\n\n\n\nNext, check Cloudbreak Application logs: \n\n\ncbd logs cloudbreak\n\n\nYou should see a message like this in the log: \nStarted CloudbreakApplication in 36.823 seconds.\n Cloudbreak normally takes less than a minute to start.\n\n\n\n\n\n\nNext Steps\n\n\nLog in to the Cloudbreak web UI and create a credential for Cloubdreak using the following platform-specific instructions:\n\n\n\n\nAccess Cloudbreak UI on AWS\n  \n\n\nAccess Cloudbreak UI on Azure\n  \n\n\nAccess Cloudbreak UI on GCP\n  \n\n\nAccess Cloudbreak UI on OpenStack", 
            "title": "Install on Your Own VM"
        }, 
        {
            "location": "/vm-launch/index.html#install-cloudbreak-in-your-own-vm", 
            "text": "This is an advanced deployment option. Select this option if you have custom VM requirements. Otherwise, you should use one of the pre-built images and follow these instructions:   Launch on AWS     Launch on Azure     Launch on GCP     Launch on OpenStack", 
            "title": "Install Cloudbreak in Your Own VM"
        }, 
        {
            "location": "/vm-launch/index.html#system-requirements", 
            "text": "To launch the Cloudbreak deployer and install the Cloudbreak application, your system must meet the following requirements:   Minimum VM requirements: 8GB RAM, 10GB disk, 2 cores  Supported operating systems: RHEL, CentOS, and Oracle Linux 7 (64-bit)  Docker 1.9.1 must be installed     You can install Cloudbreak on Mac OS X for evaluation purposes only. Mac OS X is not supported for a production deployment of Cloudbreak.", 
            "title": "System Requirements"
        }, 
        {
            "location": "/vm-launch/index.html#prerequisites", 
            "text": "To launch the Cloudbreak deployer and install the Cloudbreak application, you must first meet the following prerequisites:", 
            "title": "Prerequisites"
        }, 
        {
            "location": "/vm-launch/index.html#ports", 
            "text": "Ports 22 (SSH) and 443 (HTTPS) must be open.", 
            "title": "Ports"
        }, 
        {
            "location": "/vm-launch/index.html#root-access", 
            "text": "Every command must be executed as root. In order to get root privileges execute:   sudo -i", 
            "title": "Root Access"
        }, 
        {
            "location": "/vm-launch/index.html#system-updates", 
            "text": "Ensure that your system is up-to-date by executing:  yum -y update  Reboot it if necessary.", 
            "title": "System Updates"
        }, 
        {
            "location": "/vm-launch/index.html#iptables", 
            "text": "Install iptables-services:  yum -y install iptables-services net-tools  Without iptables-services installed the  iptables save  command will not be available.  Next, configure permissive iptables on your machine:  \niptables --flush INPUT  & &  \\\niptables --flush FORWARD  & &  \\\nservice iptables save", 
            "title": "Iptables"
        }, 
        {
            "location": "/vm-launch/index.html#more", 
            "text": "Additionally, review the following prerequisites:    Prerequisites on AWS  Prerequisites on Azure  Prerequisites on GCP  Prerequisites on OpenStack", 
            "title": "More"
        }, 
        {
            "location": "/vm-launch/index.html#install-cloudbreak-on-your-own-vm", 
            "text": "Install Cloudbreak using the following steps.  Steps    Install the Cloudbreak deployer and unzip the platform-specific single binary to your PATH. For example:  yum -y install unzip tar\ncurl -Ls s3.amazonaws.com/public-repo-1.hortonworks.com/HDP/cloudbreak/cloudbreak-deployer_1.16.1_$(uname)_x86_64.tgz | sudo tar -xz -C /bin cbd\ncbd --version  Once the Cloudbreak Deployer is installed, you can set up the Cloudbreak application.    Create a Cloudbreak deployment directory and navigate to it:  mkdir cloudbreak-deployment\ncd cloudbreak-deployment    In the directory, create a file called  Profile  with the following content:  export UAA_DEFAULT_SECRET=MY-SECRET\nexport UAA_DEFAULT_USER_PW=MY-PASSWORD  For example:  export UAA_DEFAULT_SECRET=MySecret123\nexport UAA_DEFAULT_USER_PW=MySecurePassword123   You will need to provide the password when logging in to the Cloudbreak web UI and when using the Cloudbreak Shell. The secret will be used by Cloudbreak for authentication.     Generate configurations by executing:  rm *.yml\ncbd generate      The cbd start command includes the cbd generate command which applies the following steps:   Creates the  docker-compose.yml  file, which describes the configuration of all the Docker containers required for the Cloudbreak deployment.    Creates the  uaa.yml  file, which holds the configuration of the identity server used to authenticate users with Cloudbreak.        Start the Cloudbreak application by using the following commands:  cbd pull\ncbd start  This will start the Docker containers and initialize the application. The first time you start the Coudbreak app, the process will take longer than usual due to the download of all the necessary docker images.    Next, check Cloudbreak Application logs:   cbd logs cloudbreak  You should see a message like this in the log:  Started CloudbreakApplication in 36.823 seconds.  Cloudbreak normally takes less than a minute to start.", 
            "title": "Install Cloudbreak on Your Own VM"
        }, 
        {
            "location": "/vm-launch/index.html#next-steps", 
            "text": "Log in to the Cloudbreak web UI and create a credential for Cloubdreak using the following platform-specific instructions:   Access Cloudbreak UI on AWS     Access Cloudbreak UI on Azure     Access Cloudbreak UI on GCP     Access Cloudbreak UI on OpenStack", 
            "title": "Next Steps"
        }, 
        {
            "location": "/upgrade/index.html", 
            "text": "Upgrade Cloudbreak\n\n\nUpdate Cloudbreak Deployer\n\n\nTo upgrade Cloudbreak to the newest version, perform the following steps.\n\n\nSteps\n\n\n\n\n\n\nOn the VM where Cloudbreak is running, navigate to the directory where your Profile file is located:\n\n\ncd /var/lib/cloudbreak-deployment/\n\n\n\n\n\n\nStop all of the running Cloudbreak components:\n\n\ncbd kill\n\n\n\n\n\n\nUpdate Cloudbreak deployer:\n\n\ncbd update\n\n\n\n\n\n\nUpdate the \ndocker-compose.yml\n file with new Docker containers needed for the cbd:\n\n\ncbd regenerate\n\n\n\n\n\n\nIf there are no other Cloudbreak instances that still use old Cloudbreak versions, remove the obsolete containers:\n\n\ncbd util cleanup\n\n\n\n\n\n\nCheck the health and version of the updated cbd:\n\n\ncbd doctor\n\n\n\n\n\n\nStart the new version of the cbd:\n\n\ncbd start\n\n\nCloudbreak needs to download updated docker images for the new version, so this step may take a while.\n\n\n\n\n\n\nIn addition, if you have any clusters running, you must update them using the folloing steps. \n\n\nUpdate Existing Clusters\n\n\nUpgrading from version 1.4.0 to the newest version does not require any manual modification from the users.\n\n\nUpgrading from version 1.3.0 to the newest version requires that you update existing clusters. To update existing clusters, run the following commands on the \ncbgateway\n node of the cluster.\n\n\nSteps\n\n\n\n\n\n\nUpdate the version of the Salt-Bootsrap tool on the nodes:\n    \nsalt '*' cmd.run 'curl -Ls https://github.com/sequenceiq/salt-bootstrap/releases/download/v0.1.2/salt-bootstrap_0.1.2_Linux_x86_64.tgz | tar -zx -C /usr/sbin/ salt-bootstrap'\n\n\n\n\n\n\nTrigger restart of the tool on the nodes:\n\n\nsalt '*' service.dead salt-bootstrap\n\n\n\n\nTo check the version of the Salt-Bootsrap on the nodes, use \nsalt '*' cmd.run 'salt-bootstrap --version'", 
            "title": "Upgrade Clodbreak"
        }, 
        {
            "location": "/upgrade/index.html#upgrade-cloudbreak", 
            "text": "", 
            "title": "Upgrade Cloudbreak"
        }, 
        {
            "location": "/upgrade/index.html#update-cloudbreak-deployer", 
            "text": "To upgrade Cloudbreak to the newest version, perform the following steps.  Steps    On the VM where Cloudbreak is running, navigate to the directory where your Profile file is located:  cd /var/lib/cloudbreak-deployment/    Stop all of the running Cloudbreak components:  cbd kill    Update Cloudbreak deployer:  cbd update    Update the  docker-compose.yml  file with new Docker containers needed for the cbd:  cbd regenerate    If there are no other Cloudbreak instances that still use old Cloudbreak versions, remove the obsolete containers:  cbd util cleanup    Check the health and version of the updated cbd:  cbd doctor    Start the new version of the cbd:  cbd start  Cloudbreak needs to download updated docker images for the new version, so this step may take a while.    In addition, if you have any clusters running, you must update them using the folloing steps.", 
            "title": "Update Cloudbreak Deployer"
        }, 
        {
            "location": "/upgrade/index.html#update-existing-clusters", 
            "text": "Upgrading from version 1.4.0 to the newest version does not require any manual modification from the users.  Upgrading from version 1.3.0 to the newest version requires that you update existing clusters. To update existing clusters, run the following commands on the  cbgateway  node of the cluster.  Steps    Update the version of the Salt-Bootsrap tool on the nodes:\n     salt '*' cmd.run 'curl -Ls https://github.com/sequenceiq/salt-bootstrap/releases/download/v0.1.2/salt-bootstrap_0.1.2_Linux_x86_64.tgz | tar -zx -C /usr/sbin/ salt-bootstrap'    Trigger restart of the tool on the nodes:  salt '*' service.dead salt-bootstrap   To check the version of the Salt-Bootsrap on the nodes, use  salt '*' cmd.run 'salt-bootstrap --version'", 
            "title": "Update Existing Clusters"
        }, 
        {
            "location": "/delete/index.html", 
            "text": "Delete Resources\n\n\nYou must terminate all clusters associated with a Cloudbreak before you can terminate the Cloudbreak instance. In general, you should delete clusters from the Cloudbreak UI. If needed, you can also delete the cluster resources manually via the cloud provider tools. \n\n\nDelete Clusters\n\n\nThe proper way to delete clusters is to use the the \nTerminate\n option available in the Cloudbreak UI. If the terminate process fails, try the \nTerminate\n \n \nForce terminate\n option.\n\n\nIf the force termination does not delete all cluster resources, delete the resources manually:\n\n\n\n\nTo find the VMs, click on the links available in the cluster details. \n\n\nTo find the network and subnet, see the \nCluster Information\n in the cluster details. \n\n\nOn Azure, you can delete the cluster manually by deleting the whole resource group created when the cluster was deployed. The name of the resource group, under which the cluster-related resources are organized always includes the name of the cluster, so you should be able to find the group by searching for that name in the \nResource groups\n.\n\n\n\n\nDelete Cloudbreak on AWS\n\n\nIf you want to delete the Cloudbreak instance, you can do so by deleting the EC2 instance on which it is running.\n\n\nSteps\n\n\n\n\n\n\nLog in to the AWS Management Console.\n\n\n\n\n\n\nBrowse to the EC2 Management Console.\n\n\n\n\n\n\nnavigate to \nInstances\n.\n\n\n\n\n\n\nSelect the instance that you want to delete and then select \nActions\n \n \nInstance State\n \n \nTerminate\n.\n\n\n\n\n\n\nClick \nYes, Terminate\n to confirm.\n\n\n  \n\n\n\n\n\n\nDelete Cloudbreak on Azure\n\n\nYou can delete Cloudbreak instance from your Azure account by deleting related resoucres. To delete a Cloudbreak instance:\n\n\n\n\n\n\nIf you deployed Cloudbreak in a new resource group: to delete Cloudbreak, delete the whole related resource group.\n\n\n\n\n\n\nIf you deployed Cloudbreak in an existing resource group: navigate to the group and delete only Cloubdreak-related resources such as the VM.\n\n\n\n\n\n\nSteps\n\n\n\n\n\n\nFrom the Microsoft Azure Portal dashboard, select \nResource groups\n.\n\n\n\n\n\n\nFind the resource group that you want to delete.\n\n\n\n\n\n\nIf you deployed Cloudbreak in a new resource group, you can delete the whole resource group. Click on \n...\n and select \nDelete\n:\n\n\n  \n\n\nNext, type the name of the resource group to delete and click \nDelete\n.\n\n\n\n\n\n\nIf you deployed Cloudbreak in an existing resource group, navigate to the details of the resource group and delete only Cloudbreak-related resources such as the VM.    \n\n\n\n\n\n\nDelete Cloudbreak on GCP\n\n\nYou can delete Cloudbreak instance from your Google Cloud account. \n\n\nSteps\n\n\n\n\n\n\nNavigate to your Google Cloud account.\n\n\n\n\n\n\nNavigate to \nCompute Engine\n \n \nVM instances\n.\n\n\n\n\n\n\nSelect the instance that to delete.\n\n\n\n\n\n\nClick on the delete icon and then confirm delete. \n\n\n\n\n\n\nDeleting Cloudbreak on OpenStack\n\n\nYou can delete Cloudbreak instance from your OpeenStack console. \n\n\nSteps\n\n\n\n\n\n\nNavigate to your OpenStack account.\n\n\n\n\n\n\nNavigate to \nInstances\n.\n\n\n\n\n\n\nSelect the instance to delete, click \nTerminate Instances\n, and confirm.", 
            "title": "Delete Cloudbreak"
        }, 
        {
            "location": "/delete/index.html#delete-resources", 
            "text": "You must terminate all clusters associated with a Cloudbreak before you can terminate the Cloudbreak instance. In general, you should delete clusters from the Cloudbreak UI. If needed, you can also delete the cluster resources manually via the cloud provider tools.", 
            "title": "Delete Resources"
        }, 
        {
            "location": "/delete/index.html#delete-clusters", 
            "text": "The proper way to delete clusters is to use the the  Terminate  option available in the Cloudbreak UI. If the terminate process fails, try the  Terminate     Force terminate  option.  If the force termination does not delete all cluster resources, delete the resources manually:   To find the VMs, click on the links available in the cluster details.   To find the network and subnet, see the  Cluster Information  in the cluster details.   On Azure, you can delete the cluster manually by deleting the whole resource group created when the cluster was deployed. The name of the resource group, under which the cluster-related resources are organized always includes the name of the cluster, so you should be able to find the group by searching for that name in the  Resource groups .", 
            "title": "Delete Clusters"
        }, 
        {
            "location": "/delete/index.html#delete-cloudbreak-on-aws", 
            "text": "If you want to delete the Cloudbreak instance, you can do so by deleting the EC2 instance on which it is running.  Steps    Log in to the AWS Management Console.    Browse to the EC2 Management Console.    navigate to  Instances .    Select the instance that you want to delete and then select  Actions     Instance State     Terminate .    Click  Yes, Terminate  to confirm.", 
            "title": "Delete Cloudbreak on AWS"
        }, 
        {
            "location": "/delete/index.html#delete-cloudbreak-on-azure", 
            "text": "You can delete Cloudbreak instance from your Azure account by deleting related resoucres. To delete a Cloudbreak instance:    If you deployed Cloudbreak in a new resource group: to delete Cloudbreak, delete the whole related resource group.    If you deployed Cloudbreak in an existing resource group: navigate to the group and delete only Cloubdreak-related resources such as the VM.    Steps    From the Microsoft Azure Portal dashboard, select  Resource groups .    Find the resource group that you want to delete.    If you deployed Cloudbreak in a new resource group, you can delete the whole resource group. Click on  ...  and select  Delete :      Next, type the name of the resource group to delete and click  Delete .    If you deployed Cloudbreak in an existing resource group, navigate to the details of the resource group and delete only Cloudbreak-related resources such as the VM.", 
            "title": "Delete Cloudbreak on Azure"
        }, 
        {
            "location": "/delete/index.html#delete-cloudbreak-on-gcp", 
            "text": "You can delete Cloudbreak instance from your Google Cloud account.   Steps    Navigate to your Google Cloud account.    Navigate to  Compute Engine     VM instances .    Select the instance that to delete.    Click on the delete icon and then confirm delete.", 
            "title": "Delete Cloudbreak on GCP"
        }, 
        {
            "location": "/delete/index.html#deleting-cloudbreak-on-openstack", 
            "text": "You can delete Cloudbreak instance from your OpeenStack console.   Steps    Navigate to your OpenStack account.    Navigate to  Instances .    Select the instance to delete, click  Terminate Instances , and confirm.", 
            "title": "Deleting Cloudbreak on OpenStack"
        }, 
        {
            "location": "/credentials/index.html", 
            "text": "Manage Cloudbreak Credentials\n\n\nYou can view and manage Cloudbreak credentials in the \nCredentials\n tab by clicking \nCreate credential\n and providing required parameters. You must create at least one credential in order to be able to create a cluster. \n\n\n \n\n\nCreate Cloudbreak Credental\n\n\nFor steps, refer to:\n\n\n\n\nCreate Credential on AWS\n  \n\n\nCreate Credential on Azure\n  \n\n\nCreate Credential on GCP\n \n\n\nCreate Credential on OpenStack\n\n\n\n\nView Credential Details\n\n\nTo view credential details, follow these steps.\n\n\nSteps\n\n\n\n\nIn the Cloudbreak UI, select \nCredentials\n from the navigation pane.  \n\n\nClick on the name of a credential. \n\n\n\n\nSet a Default Credential\n\n\nIf using multiple Cloudbreak credentials, you can select one credential and use it as default for creating clusters. This default credential will be pre-selected in the create cluster wizard.\n\n\nSteps\n\n\n\n\nIn the Cloudbreak UI, select \nCredentials\n from the navigation pane.  \n\n\nClick \nSet as default\n next to the credential that you would like to set as default.  \n\n\nClick \nYes\n to confirm. \n\n\n\n\nDelete a Credential\n\n\nTo delete a credential, follow these steps.\n\n\nSteps\n\n\n\n\nIn the Cloudbreak UI, select \nCredentials\n from the navigation pane.  \n\n\nSelect one or more credentials by checking their corresponding checkboxes.\n\n\nClick \nDelete\n. \n\n\nClick \nYes\n to confirm.", 
            "title": "Manage Cloudbreak Credentials"
        }, 
        {
            "location": "/credentials/index.html#manage-cloudbreak-credentials", 
            "text": "You can view and manage Cloudbreak credentials in the  Credentials  tab by clicking  Create credential  and providing required parameters. You must create at least one credential in order to be able to create a cluster.", 
            "title": "Manage Cloudbreak Credentials"
        }, 
        {
            "location": "/credentials/index.html#create-cloudbreak-credental", 
            "text": "For steps, refer to:   Create Credential on AWS     Create Credential on Azure     Create Credential on GCP    Create Credential on OpenStack", 
            "title": "Create Cloudbreak Credental"
        }, 
        {
            "location": "/credentials/index.html#view-credential-details", 
            "text": "To view credential details, follow these steps.  Steps   In the Cloudbreak UI, select  Credentials  from the navigation pane.    Click on the name of a credential.", 
            "title": "View Credential Details"
        }, 
        {
            "location": "/credentials/index.html#set-a-default-credential", 
            "text": "If using multiple Cloudbreak credentials, you can select one credential and use it as default for creating clusters. This default credential will be pre-selected in the create cluster wizard.  Steps   In the Cloudbreak UI, select  Credentials  from the navigation pane.    Click  Set as default  next to the credential that you would like to set as default.    Click  Yes  to confirm.", 
            "title": "Set a Default Credential"
        }, 
        {
            "location": "/credentials/index.html#delete-a-credential", 
            "text": "To delete a credential, follow these steps.  Steps   In the Cloudbreak UI, select  Credentials  from the navigation pane.    Select one or more credentials by checking their corresponding checkboxes.  Click  Delete .   Click  Yes  to confirm.", 
            "title": "Delete a Credential"
        }, 
        {
            "location": "/profile/index.html", 
            "text": "Configure Cloudbreak via Profile Variables\n\n\nCloudbreak deployer configuration is based on environment variables.  \n\n\nDuring startup, Cloudbreak deployer tries to determine the underlying infrastructure and then sets required environment variables with appropriate default values. If these environment variables are not sufficient for your use case, you can set additional environment variables in your \nProfile\n file. \n\n\nCloudbreak deployer always opens a new bash subprocess without inheriting environment variables. Only the following environment variables are inherited:\n\n\n\n\nHOME\n  \n\n\nDEBUG\n  \n\n\nTRACE\n  \n\n\nCBD_DEFAULT_PROFILE\n  \n\n\nall \nDOCKER_XXX\n \n\n\n\n\nSet Profile Variables\n\n\nTo set environment variables relevant for Cloudbreak Deployer, add them to a file called \nProfile\n located in the Cloudbreak deployment directory (typically \n/var/lib/cloudbreak-deployment\n).\n\n\nThe \nProfile\n file is sourced, so you can use the usual syntax to set configuration values:\n\n\nexport MY_VAR=some_value\nexport MY_OTHER_VAR=another_value \n\n\n\n\nCheck Available Profile Variables\n\n\nTo see all available environment variables with their default values, use:\n\n\ncbd env show\n\n\n\n\nCreate Environment Specific Profiles\n\n\nIf you would like to use a different versions of Cloudbreak for prod and qa profile, you must create two environment specific configurations that can be sourced. For example:\n\n\n\n\nProfile.prod  \n\n\nProfile.qa   \n\n\n\n\nFor example, to create and use a prod profile, you need to:\n\n\n\n\nCreate a file called \nProfile.prod\n  \n\n\nWrite the environment-specific \nexport DOCKER_TAG_CLOUDBREAK=0.3.99\n into \nProfile.prod\n to specify Docker image.  \n\n\nSet the environment variable: \nCBD_DEFAULT_PROFILE=prod\n  \n\n\n\n\nTo use the prod specific profile once, set:  \n\n\nCBD_DEFAULT_PROFILE=prod cbd some_commands\n\n\n\nTo permanently use the prod profile, set \nexport CBD_DEFAULT_PROFILE=prod\n in your \n.bash_profile\n.\n\n\nProfile Variables\n\n\nCloudbreak Variables\n\n\nRefer to this list for available environment variables. The variables are listed with their default values. If default is unset, no value is listed. \n\n\n\n\n\n\n\n\nVariable Name\n\n\nDefault Value\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nADDRESS_RESOLVING_TIMEOUT\n\n\n120000\n\n\nDNS lookup timeout for internal service discovery\n\n\n\n\n\n\nCAPTURE_CRON_EXPRESSION\n\n\n\n\nSmartSense bundle generation time interval in cron format\n\n\n\n\n\n\nCBD_CERT_ROOT_PATH\n\n\n\"${PWD}/certs\"\n\n\nPath where deployer stores Cloudbreak certificates. ${PWD} refers to the Cloudbreak deployment directory\n\n\n\n\n\n\nCBD_LOG_NAME\n\n\ncbreak\n\n\nName of the Cloudbreak log file\n\n\n\n\n\n\nCBD_TRAEFIK_TLS\n\n\n\"/certs/traefik/client.pem,/certs/traefik/client-key.pem\"\n\n\nPath inside of the Traefik container where TLS files are located\n\n\n\n\n\n\nCB_BLUEPRINT_DEFAULTS\n\n\n\"hdp-small-default;hdp-spark-cluster;hdp-streaming-cluster\"\n\n\nComma separated list of the default blueprints that Cloudbreak initializes in its database\n\n\n\n\n\n\nCB_BYOS_DFS_DATA_DIR\n\n\n\"/hadoop/hdfs/data\"\n\n\n(Deprecated) Default data directory for BYOS orchestrators\n\n\n\n\n\n\nCB_COMPONENT_CLUSTER_ID\n\n\n\n\nSmartSense component cluster ID\n\n\n\n\n\n\nCB_COMPONENT_ID\n\n\n\n\nSmartSense component ID\n\n\n\n\n\n\nCB_COMPOSE_PROJECT\n\n\ncbreak\n\n\nName of the Docker Compose project; it will appear in container names\n\n\n\n\n\n\nCB_DB_ENV_DB\n\n\n\"cbdb\"\n\n\nName of the Cloudbreak database\n\n\n\n\n\n\nCB_DB_ENV_PASS\n\n\n\"\"\n\n\nPassword for the Cloudbreak database authentication\n\n\n\n\n\n\nCB_DB_ENV_SCHEMA\n\n\n\"public\"\n\n\nSchema used in the Cloudbreak database\n\n\n\n\n\n\nCB_DB_ENV_USER\n\n\n\"postgres\"\n\n\nUser for the Cloudbreak database authentication\n\n\n\n\n\n\nCB_DB_ROOT_PATH\n\n\n\"/var/lib/cloudbreak\"\n\n\n(Deprecated) Location of the database volume on Cloudbreak host\n\n\n\n\n\n\nCB_DEFAULT_SUBSCRIPTION_ADDRESS\n\n\nhttp://uluwatu.service.consul:3000/notifications\n\n\nURL of the default subscription for Cloudbreak notifications\n\n\n\n\n\n\nCB_ENABLEDPLATFORMS\n\n\n\n\nSet this to disable Cloudbreak resource called Platform\n\n\n\n\n\n\nCB_ENABLE_CUSTOM_IMAGE\n\n\n\"false\"\n\n\nSet to \"true\" to enable custom cloud images\n\n\n\n\n\n\nCBD_FORCE_START\n\n\n\n\nSet this to disable docker-compose.yml and uaa.yml validation\n\n\n\n\n\n\nCB_HBM2DDL_STRATEGY\n\n\n\"validate\"\n\n\nConfigures hibernate.hbm2ddl.auto in Cloudbreak\n\n\n\n\n\n\nCB_HOST_DISCOVERY_CUSTOM_DOMAIN\n\n\n\"\"\n\n\nCustom domain of the provisioned cluster\n\n\n\n\n\n\nCB_HTTPS_PROXY\n\n\n\"\"\n\n\nHTTPS proxy URL\n\n\n\n\n\n\nCB_HTTP_PROXY\n\n\n\"\"\n\n\nHTTP proxy URL\n\n\n\n\n\n\nCB_IMAGE_CATALOG_URL\n\n\n\"https://s3-eu-west-1.amazonaws.com/cloudbreak-info/cb-image-catalog.json\"\n\n\nImage catalog URL\n\n\n\n\n\n\nCB_INSTANCE_NODE_ID\n\n\n\n\nUnique identifier of the Cloudbreak node\n\n\n\n\n\n\nCB_INSTANCE_PROVIDER\n\n\n\n\nCloud provider of the Cloudbreak instance\n\n\n\n\n\n\nCB_INSTANCE_REGION\n\n\n\n\nCloud region of the Cloudbreak instance\n\n\n\n\n\n\nCB_INSTANCE_UUID\n\n\n\n\nUnique identifier of Cloudbreak deployment\n\n\n\n\n\n\nCB_JAVA_OPTS\n\n\n\"\"\n\n\nExtra Java options for Autoscale and Cloudbreak\n\n\n\n\n\n\nCB_LOG_LEVEL\n\n\n\"INFO\"\n\n\nLog level of the Cloudbreak service\n\n\n\n\n\n\nCB_MAX_SALT_NEW_SERVICE_RETRY\n\n\n90\n\n\nSalt orchestrator max retry count\n\n\n\n\n\n\nCB_MAX_SALT_RECIPE_EXECUTION_RETRY\n\n\n90\n\n\nSalt orchestrator max retry count for recipes\n\n\n\n\n\n\nCB_PLATFORM_DEFAULT_REGIONS\n\n\n\n\nComma separated list of default regions by platform. For example: \nAWS:eu-west-1\n.\n\n\n\n\n\n\nCB_PRODUCT_ID\n\n\n\n\nSmartSense product ID\n\n\n\n\n\n\nCB_SCHEMA_MIGRATION_AUTO\n\n\ntrue\n\n\nWhen set to true, enables Cloudbreak automatic database schema update\n\n\n\n\n\n\nCB_SMARTSENSE_CONFIGURE\n\n\n\"false\"\n\n\nSet to \u201ctrue\u201d to install and configure SmartSense on cluster nodes\n\n\n\n\n\n\nCB_SMARTSENSE_CLUSTER_NAME_PREFIX\n\n\n\n\nSmartSense Cloudbreak cluster name prefix\n\n\n\n\n\n\nCB_SMARTSENSE_ID\n\n\n\"\"\n\n\nSmartSense subscription ID\n\n\n\n\n\n\nCB_TEMPLATE_DEFAULTS\n\n\n\"minviable-gcp,minviable-azure,minviable-aws\"\n\n\nComma separated list of the default templates that Cloudbreak initializes in its database\n\n\n\n\n\n\nCB_UI_MAX_WAIT\n\n\n400\n\n\nWait timeout for \ncbd start-wait\n command\n\n\n\n\n\n\nCERT_VALIDATION\n\n\n\"true\"\n\n\nWhen set to \"true\", enables cert validation in Cloudbreak and Autoscale\n\n\n\n\n\n\nCLOUDBREAK_SMTP_AUTH\n\n\n\"true\"\n\n\nWhen set to \"true\", configures mail.smtp.auth in Cloudbreak\n\n\n\n\n\n\nCLOUDBREAK_SMTP_SENDER_FROM\n\n\n\"noreply@hortonworks.com\"\n\n\nEmail address of the sender\n\n\n\n\n\n\nCLOUDBREAK_SMTP_SENDER_HOST\n\n\n\"smtp.service.consul\"\n\n\nSMTP server address of the hostname\n\n\n\n\n\n\nCLOUDBREAK_SMTP_SENDER_PASSWORD\n\n\n\"$LOCAL_SMTP_PASSWORD\"\n\n\nSMTP server password\n\n\n\n\n\n\nCLOUDBREAK_SMTP_SENDER_PORT\n\n\n25\n\n\nPort of the SMTP server\n\n\n\n\n\n\nCLOUDBREAK_SMTP_SENDER_USERNAME\n\n\n\"admin\"\n\n\nUsername for SMTP authentication\n\n\n\n\n\n\nCLOUDBREAK_SMTP_STARTTLS_ENABLE\n\n\n\"false\"\n\n\nSet to \"true\" to configure mail.smtp.starttls.enable in Cloudbreak\n\n\n\n\n\n\nCLOUDBREAK_SMTP_TYPE\n\n\n\"smtp\"\n\n\nDefines mail.transport.protocol in CLoudbreak\n\n\n\n\n\n\nCOMMON_DB\n\n\ncommondb\n\n\nName of the database container\n\n\n\n\n\n\nCOMMON_DB_VOL\n\n\ncommon\n\n\nName of the database volume\n\n\n\n\n\n\nCOMPOSE_HTTP_TIMEOUT\n\n\n120\n\n\nDocker Compose execution timeout\n\n\n\n\n\n\nDB_DUMP_VOLUME\n\n\ncbreak_dump\n\n\nName of the database dump volume\n\n\n\n\n\n\nDB_MIGRATION_LOG\n\n\n\"db_migration.log\"\n\n\nDatabase migration log file\n\n\n\n\n\n\nDEFAULT_INBOUND_ACCESS_IP\n\n\n\"\"\n\n\nOpens default ports on AWS instances for Cloudbreak\n\n\n\n\n\n\nDOCKER_CONSUL_OPTIONS\n\n\n\"\"\n\n\nExtra options for Consul\n\n\n\n\n\n\nDOCKER_IMAGE_CBD_SMARTSENSE\n\n\nhortonworks/cbd-smartsense\n\n\nSmartSense Docker image name\n\n\n\n\n\n\nDOCKER_IMAGE_CLOUDBREAK\n\n\nhortonworks/cloudbreak\n\n\nCloudbreak Docker image name\n\n\n\n\n\n\nDOCKER_IMAGE_CLOUDBREAK_AUTH\n\n\nhortonworks/cloudbreak-auth\n\n\nAuthentication service Docker image name\n\n\n\n\n\n\nDOCKER_IMAGE_CLOUDBREAK_PERISCOPE\n\n\nhortonworks/cloudbreak-autoscale\n\n\nAutoscale Docker image name\n\n\n\n\n\n\nDOCKER_IMAGE_CLOUDBREAK_SHELL\n\n\nhortonworks/cloudbreak-shell\n\n\nCloudbreak Shell Docker image name\n\n\n\n\n\n\nDOCKER_IMAGE_CLOUDBREAK_WEB\n\n\nhortonworks/cloudbreak-web\n\n\nWeb UI Docker image name\n\n\n\n\n\n\nDOCKER_TAG_ALPINE\n\n\n3.1\n\n\nAlpine container version\n\n\n\n\n\n\nDOCKER_TAG_CBD_SMARTSENSE\n\n\n0.10.0\n\n\nSmartSense container version\n\n\n\n\n\n\nDOCKER_TAG_CERT_TOOL\n\n\n0.2.0\n\n\nCert tool container version\n\n\n\n\n\n\nDOCKER_TAG_CLOUDBREAK\n\n\n2.1.0-dev.70\n\n\nCloudbreak container version\n\n\n\n\n\n\nDOCKER_TAG_CLOUDBREAK_SHELL\n\n\n2.1.0-dev.70\n\n\nCloudbreak Shell container version\n\n\n\n\n\n\nDOCKER_TAG_CONSUL\n\n\n0.5\n\n\nConsul container version\n\n\n\n\n\n\nDOCKER_TAG_HAVEGED\n\n\n1.1.0\n\n\nHaveged container version\n\n\n\n\n\n\nDOCKER_TAG_LOGROTATE\n\n\n1.0.0\n\n\nLogrotate container version\n\n\n\n\n\n\nDOCKER_TAG_MIGRATION\n\n\n1.0.0\n\n\nMigration container version\n\n\n\n\n\n\nDOCKER_TAG_PERISCOPE\n\n\n2.1.0-dev.70\n\n\nAutoscale container version\n\n\n\n\n\n\nDOCKER_TAG_POSTFIX\n\n\nlatest\n\n\nPostfix container version\n\n\n\n\n\n\nDOCKER_TAG_POSTGRES\n\n\n9.6.1-alpine\n\n\nPostgresql container version\n\n\n\n\n\n\nDOCKER_TAG_REGISTRATOR\n\n\nv5\n\n\nRegistrator container version\n\n\n\n\n\n\nDOCKER_TAG_SULTANS\n\n\n2.1.0-dev.70\n\n\nAuthentication service container version\n\n\n\n\n\n\nDOCKER_TAG_TRAEFIK\n\n\nv1.2.0\n\n\nTraefik container version\n\n\n\n\n\n\nDOCKER_TAG_UAA\n\n\n3.6.5\n\n\nIdentity container version\n\n\n\n\n\n\nDOCKER_TAG_ULUWATU\n\n\n2.1.0-dev.70\n\n\nWeb UI container version\n\n\n\n\n\n\nIDENTITY_DB_NAME\n\n\n\"uaadb\"\n\n\nName of the Identity database\n\n\n\n\n\n\nIDENTITY_DB_PASS\n\n\n\"\"\n\n\nPassword for the Identity database authentication\n\n\n\n\n\n\nIDENTITY_DB_URL\n\n\n\"${COMMON_DB}.service.consul:5432\"\n\n\nURL for the Identity database connection, including the port number\n\n\n\n\n\n\nIDENTITY_DB_USER\n\n\n\"postgres\"\n\n\nUser for the Identity database authentication\n\n\n\n\n\n\nLOCAL_SMTP_PASSWORD\n\n\n\"$UAA_DEFAULT_USER_PW\"\n\n\nDefault password for the internal mail server\n\n\n\n\n\n\nPERISCOPE_DB_HBM2DDL_STRATEGY\n\n\n\"validate\"\n\n\nConfigures hibernate.hbm2ddl.auto in Autoscale\n\n\n\n\n\n\nPERISCOPE_DB_NAME\n\n\n\"periscopedb\"\n\n\nName of the Autoscale database\n\n\n\n\n\n\nPERISCOPE_DB_PASS\n\n\n\"\"\n\n\nPassword for the Autoscale database authentication\n\n\n\n\n\n\nPERISCOPE_DB_SCHEMA_NAME\n\n\n\"public\"\n\n\nSchema used in the Autoscale database\n\n\n\n\n\n\nPERISCOPE_DB_USER\n\n\n\"postgres\"\n\n\nUser for the Autoscale database authentication\n\n\n\n\n\n\nPERISCOPE_DB_TCP_ADDR\n\n\n\n\nAddress of the Autoscale database\n\n\n\n\n\n\nPERISCOPE_DB_TCP_PORT\n\n\n\n\nPort number of the Autoscale database\n\n\n\n\n\n\nPERISCOPE_LOG_LEVEL\n\n\n\"INFO\"\n\n\nLog level of the Autoscale service\n\n\n\n\n\n\nPERISCOPE_SCHEMA_MIGRATION_AUTO\n\n\ntrue\n\n\nWhen set to \"true\", enables Autoscale automatic database schema update\n\n\n\n\n\n\nPUBLIC_IP\n\n\n\n\nIP address or hostname of the public interface\n\n\n\n\n\n\nREST_DEBUG\n\n\n\"false\"\n\n\nSet to \"true\" to enable REST call debug level in Cloudbreak and Autoscale\n\n\n\n\n\n\nSL_ADDRESS_RESOLVING_TIMEOUT\n\n\n\n\nDNS lookup timeout of Authentication service for internal service discovery\n\n\n\n\n\n\nSL_NODE_TLS_REJECT_UNAUTHORIZED\n\n\n\"0\"\n\n\nWhen set to \"0\", enables self-signed certifications in Authentication service\n\n\n\n\n\n\nSULTANS_CONTAINER_PATH\n\n\n/sultans\n\n\nDefault project location in Authentication service container\n\n\n\n\n\n\nTRAEFIK_MAX_IDLE_CONNECTION\n\n\n100\n\n\nSets --maxidleconnsperhost for Traefik to the value entered\n\n\n\n\n\n\nUAA_CLOUDBREAK_ID\n\n\ncloudbreak\n\n\nIdentity of the Cloudbreak scope in Identity\n\n\n\n\n\n\nUAA_CLOUDBREAK_SECRET\n\n\n$UAA_DEFAULT_SECRET\n\n\nSecret of the Cloudbreak scope in Identity\n\n\n\n\n\n\nUAA_CLOUDBREAK_SHELL_ID\n\n\ncloudbreak_shell\n\n\nIdentity of the Cloudbreak Shell scope in Identity\n\n\n\n\n\n\nUAA_DEFAULT_ACCOUNT\n\n\n\"seq1234567.SequenceIQ\"\n\n\nDefault account for users as an Identity group\n\n\n\n\n\n\nUAA_DEFAULT_SECRET\n\n\n\n\nDefault secret for all the scopes and encryptions\n\n\n\n\n\n\nUAA_DEFAULT_USER_EMAIL\n\n\nadmin@example.com\n\n\nEmail address of default admin user\n\n\n\n\n\n\nUAA_DEFAULT_USER_FIRSTNAME\n\n\nJoe\n\n\nFirst name of default admin user\n\n\n\n\n\n\nUAA_DEFAULT_USER_GROUPS\n\n\nSee \nhere\n\n\nDefault user groups of the users\n\n\n\n\n\n\nUAA_DEFAULT_USER_LASTNAME\n\n\nAdmin\n\n\nLast name of default admin user\n\n\n\n\n\n\nUAA_DEFAULT_USER_PW\n\n\n\n\nPassword of default admin user\n\n\n\n\n\n\nUAA_FLEX_USAGE_CLIENT_ID\n\n\nflex_usage_client\n\n\nIdentity of the Flex usage generator scope in Identity\n\n\n\n\n\n\nUAA_FLEX_USAGE_CLIENT_SECRET\n\n\n$UAA_DEFAULT_SECRET\n\n\nSecret of the Flex usage generator scope in Identity\n\n\n\n\n\n\nUAA_PERISCOPE_ID\n\n\nperiscope\n\n\nIdentity of the Autoscale scope in Identity\n\n\n\n\n\n\nUAA_PERISCOPE_SECRET\n\n\n$UAA_DEFAULT_SECRET\n\n\nSecret of the Autoscale scope in Identity\n\n\n\n\n\n\nUAA_PORT\n\n\n8089\n\n\nIdentity service public port\n\n\n\n\n\n\nUAA_SULTANS_ID\n\n\nsultans\n\n\nIdentity of the Authentication service scope in Identity\n\n\n\n\n\n\nUAA_SULTANS_SECRET\n\n\n$UAA_DEFAULT_SECRET\n\n\nSecret of the Authentication service scope in Identity\n\n\n\n\n\n\nUAA_ULUWATU_ID\n\n\nuluwatu\n\n\nIdentity of the Web UI scope in Identity\n\n\n\n\n\n\nUAA_ULUWATU_SECRET\n\n\n$UAA_DEFAULT_SECRET\n\n\nSecret of the Web UI scope in Identity\n\n\n\n\n\n\nUAA_ZONE_DOMAIN\n\n\nexample.com\n\n\nExternal domain name for zone in Identity\n\n\n\n\n\n\nULUWATU_CONTAINER_PATH\n\n\n/uluwatu\n\n\nDefault project location in the Web UI container\n\n\n\n\n\n\nULU_DEFAULT_SSH_KEY\n\n\n\"\"\n\n\nDefault SSH key for the credentials in Cloudbreak\n\n\n\n\n\n\nULU_HOST_ADDRESS\n\n\n\"https://$PUBLIC_IP\"\n\n\nURL for the Web UI host\n\n\n\n\n\n\nULU_NODE_TLS_REJECT_UNAUTHORIZED\n\n\n\"0\"\n\n\nWhen set to \"0\", enables self-signed certifications in Web UI\n\n\n\n\n\n\nULU_OAUTH_REDIRECT_URI\n\n\n\"$ULU_HOST_ADDRESS/authorize\"\n\n\nAuthorization page on Web UI\n\n\n\n\n\n\nULU_SUBSCRIBE_TO_NOTIFICATIONS\n\n\n\"false\"\n\n\nSet to \u201ctrue\u201d to enable email notifications for Cloudbreak events\n\n\n\n\n\n\nULU_SULTANS_ADDRESS\n\n\n\"https://$PUBLIC_IP/sl\"\n\n\nAuthentication service URL\n\n\n\n\n\n\nVERBOSE_MIGRATION\n\n\nfalse\n\n\nWhen set to true, enables verbose database migration\n\n\n\n\n\n\n\n\nAWS Variables\n\n\n\n\n\n\n\n\nVariable Name\n\n\nDefault Value\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nAWS_ACCESS_KEY_ID\n\n\n\"\"\n\n\nAccess key of the AWS account\n\n\n\n\n\n\nAWS_ROLE_NAME\n\n\ncbreak-deployer\n\n\nName of the AWS role for the \ncbd aws [generate-rol, show role]\n commands\n\n\n\n\n\n\nAWS_SECRET_ACCESS_KEY\n\n\n\"\"\n\n\nSecret access key of the AWS account\n\n\n\n\n\n\nCB_AWS_CUSTOM_CF_TAGS\n\n\n\"\"\n\n\nComma separated list of AWS CloudFormation stack tags\n\n\n\n\n\n\nCB_AWS_DEFAULT_CF_TAG\n\n\n\"\"\n\n\nDefault tag for AWS CloudFormation stack\n\n\n\n\n\n\nCB_AWS_DEFAULT_INBOUND_SECURITY_GROUP\n\n\n\"\"\n\n\nDefault inbound policy name for AWS CloudFormation stack\n\n\n\n\n\n\nCB_AWS_EXTERNAL_ID\n\n\nprovision-ambari\n\n\nExternal ID of the assume role policy\n\n\n\n\n\n\nCB_AWS_HOSTKEY_VERIFY\n\n\n\"false\"\n\n\nEnables host fingerprint verification on AWS\n\n\n\n\n\n\nCB_AWS_VPC\n\n\n\"\"\n\n\nConfigures the VPC ID on AWS. Set this variable if you are provisioning cluster to the same VPC where Cloudbreak is deployed on AWS.\n\n\n\n\n\n\nCERTS_BUCKET\n\n\n\"\"\n\n\nS3 bucket name for backup and restore certificates via \ncbd aws [certs-restore-s3  certs-upload-s3]\n commands\n\n\n\n\n\n\n\n\nAzure Variables\n\n\n\n\n\n\n\n\nVariable Name\n\n\nDefault Value\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nAZURE_SUBSCRIPTION_ID\n\n\n\n\nAzure subscription ID for interactive login in the web UI\n\n\n\n\n\n\nAZURE_TENANT_ID\n\n\n\n\nAzure tenant ID for interactive login in the web UI\n\n\n\n\n\n\n\n\nGCP Variables\n\n\n\n\n\n\n\n\nVariable Name\n\n\nDefault Value\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nCB_GCP_HOSTKEY_VERIFY\n\n\n\"false\"\n\n\nWhen set to \"true\", enables host fingerprint verification on GCP\n\n\n\n\n\n\n\n\nLocal Development Variables\n\n\n\n\n\n\n\n\nVariable Name\n\n\nDefault Value\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nCB_LOCAL_DEV_BIND_ADDR\n\n\n\"192.168.64.1\"\n\n\nAmbassador external address for local development of Cloudbreak and Autoscale\n\n\n\n\n\n\nCB_SCHEMA_SCRIPTS_LOCATION\n\n\n\"container\"\n\n\nLocation of Cloudbreak schema update files\n\n\n\n\n\n\nDOCKER_TAG_AMBASSADOR\n\n\n0.5.0\n\n\nAmbassador container version for local development\n\n\n\n\n\n\nPERISCOPE_SCHEMA_SCRIPTS_LOCATION\n\n\n\"container\"\n\n\nLocation of Cloudbreak schema update files\n\n\n\n\n\n\nPRIVATE_IP\n\n\n$BRIDGE_IP\n\n\nIP address or hostname of the private interface\n\n\n\n\n\n\nREMOVE_CONTAINER\n\n\n\"--rm\"\n\n\nWhen set to \"--rm\" (default), removes side effect containers for debug purpose. Set to \" \" to keep side effect containers for debug purpose\n\n\n\n\n\n\nSULTANS_VOLUME_HOST\n\n\n/dev/null\n\n\nLocation of the locally developed Authentication service project\n\n\n\n\n\n\nUAA_SCHEMA_SCRIPTS_LOCATION\n\n\n\"container\"\n\n\nLocation of Identity schema update files\n\n\n\n\n\n\nULUWATU_VOLUME_HOST\n\n\n/dev/null\n\n\nLocation of the locally developed web UI project\n\n\n\n\n\n\n\n\nMacOS Variables\n\n\n\n\n\n\n\n\nVariable Name\n\n\nDefault Value\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nDOCKER_MACHINE\n\n\n\"\"\n\n\nName of the Docker Machine where Cloudbreak runs\n\n\n\n\n\n\nDOCKER_PROFILE\n\n\nProfile\n\n\nProfile file for environment variables related to Docker Machine\n\n\n\n\n\n\nMACHINE_CPU\n\n\n2\n\n\nNumber of the CPU cores on the Docker Machine instance\n\n\n\n\n\n\nMACHINE_MEM\n\n\n4096\n\n\nAmount of RAM on the Docker Machine instance\n\n\n\n\n\n\nMACHINE_NAME\n\n\ncbd\n\n\nName of the Docker Machine instance\n\n\n\n\n\n\nMACHINE_OPTS\n\n\n\"--xhyve-virtio-9p\"\n\n\nExtra options for Docker Machine instance\n\n\n\n\n\n\nMACHINE_STORAGE_PATH\n\n\n$HOME/.docker/machine\n\n\nDocker Machine storage path\n\n\n\n\n\n\n\n\nUAA_DEFAULT_USER_GROUPS\n\n\nDefault value fro \nUAA_DEFAULT_USER_GROUPS\n is:\n\n\n\"openid,cloudbreak.networks,cloudbreak.securitygroups,cloudbreak.templates,cloudbreak.blueprints,cloudbreak.credentials,cloudbreak.stacks,sequenceiq.cloudbreak.admin,sequenceiq.cloudbreak.user,sequenceiq.account.${UAA_DEFAULT_ACCOUNT},cloudbreak.events,cloudbreak.usages.global,cloudbreak.usages.account,cloudbreak.usages.user,periscope.cluster,cloudbreak.recipes,cloudbreak.blueprints.read,cloudbreak.templates.read,cloudbreak.credentials.read,cloudbreak.recipes.read,cloudbreak.networks.read,cloudbreak.securitygroups.read,cloudbreak.stacks.read,cloudbreak.sssdconfigs,cloudbreak.sssdconfigs.read,cloudbreak.platforms,cloudbreak.platforms.read\"\n\n\n\nChange SMTP Parameters\n\n\nIf you want to change SMTP parameters, add them your \nProfile\n.\n\n\nThe default values of the SMTP parameters are:\n\n\nexport CLOUDBREAK_SMTP_SENDER_USERNAME=\nexport CLOUDBREAK_SMTP_SENDER_PASSWORD=\nexport CLOUDBREAK_SMTP_SENDER_HOST=\nexport CLOUDBREAK_SMTP_SENDER_PORT=25\nexport CLOUDBREAK_SMTP_SENDER_FROM=\nexport CLOUDBREAK_SMTP_AUTH=true\nexport CLOUDBREAK_SMTP_STARTTLS_ENABLE=true\nexport CLOUDBREAK_SMTP_TYPE=smtp\n\n\n\n\nIf your SMTP server uses SMTPS, you must set the protocol in your \nProfile\n to smtps:\n\n\nexport CLOUDBREAK_SMTP_TYPE=smtps\n\n\n\n\nConfigure Consul\n\n\nCloudbreak uses \nConsul\n for DNS resolution. All Cloudbreak related services are registered as someservice.service.consul.\n\n\nConsul\u2019s built-in DNS server is able to fallback on another DNS server. This option is called \n-recursor\n. Clodbreak Deployer first tries to discover the DNS settings of the host by looking for nameserver entry in the \n/etc/resolv.conf\n file. If it finds one, consul will use it as a recursor. Otherwise, it will use \n8.8.8.8\n.\n\n\nFor a full list of available consul config options, refer to \nConsul documentation\n.\n\n\nTo pass any additional Consul configuration, define the \nDOCKER_CONSUL_OPTIONS\n variable in the Profile file.\n\n\nAdd Tags in Profile (AWS)\n\n\nIn order to differentiate launched instances, you can optionally define custom tags for your AWS resources deployed by Cloudbreak. \n\n\n\n\n\n\nIf you want just one custom tag for your Cloudformation resources, set this variable in the \nProfile\n:\n\n\nexport CB_AWS_DEFAULT_CF_TAG=mytagcontent\n\n\nIn this example, the name of the tag will be \nCloudbreakId\n and the value will be \nmytagcontent\n.\n\n\n\n\n\n\nIf you prefer to customize the tag name, set this variable:\n\n\nexport CB_AWS_CUSTOM_CF_TAGS=mytagname:mytagvalue\n\n\nIn this example the name of the tag will be \nmytagname\n and the value will be \nmytagvalue\n. \n\n\n\n\n\n\nYou can specify a list of tags with a comma separated list: \n\n\nexport CB_AWS_CUSTOM_CF_TAGS=tag1:value1,tag2:value2,tag3:value3", 
            "title": "Configure Cloudbreak"
        }, 
        {
            "location": "/profile/index.html#configure-cloudbreak-via-profile-variables", 
            "text": "Cloudbreak deployer configuration is based on environment variables.    During startup, Cloudbreak deployer tries to determine the underlying infrastructure and then sets required environment variables with appropriate default values. If these environment variables are not sufficient for your use case, you can set additional environment variables in your  Profile  file.   Cloudbreak deployer always opens a new bash subprocess without inheriting environment variables. Only the following environment variables are inherited:   HOME     DEBUG     TRACE     CBD_DEFAULT_PROFILE     all  DOCKER_XXX", 
            "title": "Configure Cloudbreak via Profile Variables"
        }, 
        {
            "location": "/profile/index.html#set-profile-variables", 
            "text": "To set environment variables relevant for Cloudbreak Deployer, add them to a file called  Profile  located in the Cloudbreak deployment directory (typically  /var/lib/cloudbreak-deployment ).  The  Profile  file is sourced, so you can use the usual syntax to set configuration values:  export MY_VAR=some_value\nexport MY_OTHER_VAR=another_value", 
            "title": "Set Profile Variables"
        }, 
        {
            "location": "/profile/index.html#check-available-profile-variables", 
            "text": "To see all available environment variables with their default values, use:  cbd env show", 
            "title": "Check Available Profile Variables"
        }, 
        {
            "location": "/profile/index.html#create-environment-specific-profiles", 
            "text": "If you would like to use a different versions of Cloudbreak for prod and qa profile, you must create two environment specific configurations that can be sourced. For example:   Profile.prod    Profile.qa      For example, to create and use a prod profile, you need to:   Create a file called  Profile.prod     Write the environment-specific  export DOCKER_TAG_CLOUDBREAK=0.3.99  into  Profile.prod  to specify Docker image.    Set the environment variable:  CBD_DEFAULT_PROFILE=prod      To use the prod specific profile once, set:    CBD_DEFAULT_PROFILE=prod cbd some_commands  To permanently use the prod profile, set  export CBD_DEFAULT_PROFILE=prod  in your  .bash_profile .", 
            "title": "Create Environment Specific Profiles"
        }, 
        {
            "location": "/profile/index.html#profile-variables", 
            "text": "", 
            "title": "Profile Variables"
        }, 
        {
            "location": "/profile/index.html#cloudbreak-variables", 
            "text": "Refer to this list for available environment variables. The variables are listed with their default values. If default is unset, no value is listed.      Variable Name  Default Value  Description      ADDRESS_RESOLVING_TIMEOUT  120000  DNS lookup timeout for internal service discovery    CAPTURE_CRON_EXPRESSION   SmartSense bundle generation time interval in cron format    CBD_CERT_ROOT_PATH  \"${PWD}/certs\"  Path where deployer stores Cloudbreak certificates. ${PWD} refers to the Cloudbreak deployment directory    CBD_LOG_NAME  cbreak  Name of the Cloudbreak log file    CBD_TRAEFIK_TLS  \"/certs/traefik/client.pem,/certs/traefik/client-key.pem\"  Path inside of the Traefik container where TLS files are located    CB_BLUEPRINT_DEFAULTS  \"hdp-small-default;hdp-spark-cluster;hdp-streaming-cluster\"  Comma separated list of the default blueprints that Cloudbreak initializes in its database    CB_BYOS_DFS_DATA_DIR  \"/hadoop/hdfs/data\"  (Deprecated) Default data directory for BYOS orchestrators    CB_COMPONENT_CLUSTER_ID   SmartSense component cluster ID    CB_COMPONENT_ID   SmartSense component ID    CB_COMPOSE_PROJECT  cbreak  Name of the Docker Compose project; it will appear in container names    CB_DB_ENV_DB  \"cbdb\"  Name of the Cloudbreak database    CB_DB_ENV_PASS  \"\"  Password for the Cloudbreak database authentication    CB_DB_ENV_SCHEMA  \"public\"  Schema used in the Cloudbreak database    CB_DB_ENV_USER  \"postgres\"  User for the Cloudbreak database authentication    CB_DB_ROOT_PATH  \"/var/lib/cloudbreak\"  (Deprecated) Location of the database volume on Cloudbreak host    CB_DEFAULT_SUBSCRIPTION_ADDRESS  http://uluwatu.service.consul:3000/notifications  URL of the default subscription for Cloudbreak notifications    CB_ENABLEDPLATFORMS   Set this to disable Cloudbreak resource called Platform    CB_ENABLE_CUSTOM_IMAGE  \"false\"  Set to \"true\" to enable custom cloud images    CBD_FORCE_START   Set this to disable docker-compose.yml and uaa.yml validation    CB_HBM2DDL_STRATEGY  \"validate\"  Configures hibernate.hbm2ddl.auto in Cloudbreak    CB_HOST_DISCOVERY_CUSTOM_DOMAIN  \"\"  Custom domain of the provisioned cluster    CB_HTTPS_PROXY  \"\"  HTTPS proxy URL    CB_HTTP_PROXY  \"\"  HTTP proxy URL    CB_IMAGE_CATALOG_URL  \"https://s3-eu-west-1.amazonaws.com/cloudbreak-info/cb-image-catalog.json\"  Image catalog URL    CB_INSTANCE_NODE_ID   Unique identifier of the Cloudbreak node    CB_INSTANCE_PROVIDER   Cloud provider of the Cloudbreak instance    CB_INSTANCE_REGION   Cloud region of the Cloudbreak instance    CB_INSTANCE_UUID   Unique identifier of Cloudbreak deployment    CB_JAVA_OPTS  \"\"  Extra Java options for Autoscale and Cloudbreak    CB_LOG_LEVEL  \"INFO\"  Log level of the Cloudbreak service    CB_MAX_SALT_NEW_SERVICE_RETRY  90  Salt orchestrator max retry count    CB_MAX_SALT_RECIPE_EXECUTION_RETRY  90  Salt orchestrator max retry count for recipes    CB_PLATFORM_DEFAULT_REGIONS   Comma separated list of default regions by platform. For example:  AWS:eu-west-1 .    CB_PRODUCT_ID   SmartSense product ID    CB_SCHEMA_MIGRATION_AUTO  true  When set to true, enables Cloudbreak automatic database schema update    CB_SMARTSENSE_CONFIGURE  \"false\"  Set to \u201ctrue\u201d to install and configure SmartSense on cluster nodes    CB_SMARTSENSE_CLUSTER_NAME_PREFIX   SmartSense Cloudbreak cluster name prefix    CB_SMARTSENSE_ID  \"\"  SmartSense subscription ID    CB_TEMPLATE_DEFAULTS  \"minviable-gcp,minviable-azure,minviable-aws\"  Comma separated list of the default templates that Cloudbreak initializes in its database    CB_UI_MAX_WAIT  400  Wait timeout for  cbd start-wait  command    CERT_VALIDATION  \"true\"  When set to \"true\", enables cert validation in Cloudbreak and Autoscale    CLOUDBREAK_SMTP_AUTH  \"true\"  When set to \"true\", configures mail.smtp.auth in Cloudbreak    CLOUDBREAK_SMTP_SENDER_FROM  \"noreply@hortonworks.com\"  Email address of the sender    CLOUDBREAK_SMTP_SENDER_HOST  \"smtp.service.consul\"  SMTP server address of the hostname    CLOUDBREAK_SMTP_SENDER_PASSWORD  \"$LOCAL_SMTP_PASSWORD\"  SMTP server password    CLOUDBREAK_SMTP_SENDER_PORT  25  Port of the SMTP server    CLOUDBREAK_SMTP_SENDER_USERNAME  \"admin\"  Username for SMTP authentication    CLOUDBREAK_SMTP_STARTTLS_ENABLE  \"false\"  Set to \"true\" to configure mail.smtp.starttls.enable in Cloudbreak    CLOUDBREAK_SMTP_TYPE  \"smtp\"  Defines mail.transport.protocol in CLoudbreak    COMMON_DB  commondb  Name of the database container    COMMON_DB_VOL  common  Name of the database volume    COMPOSE_HTTP_TIMEOUT  120  Docker Compose execution timeout    DB_DUMP_VOLUME  cbreak_dump  Name of the database dump volume    DB_MIGRATION_LOG  \"db_migration.log\"  Database migration log file    DEFAULT_INBOUND_ACCESS_IP  \"\"  Opens default ports on AWS instances for Cloudbreak    DOCKER_CONSUL_OPTIONS  \"\"  Extra options for Consul    DOCKER_IMAGE_CBD_SMARTSENSE  hortonworks/cbd-smartsense  SmartSense Docker image name    DOCKER_IMAGE_CLOUDBREAK  hortonworks/cloudbreak  Cloudbreak Docker image name    DOCKER_IMAGE_CLOUDBREAK_AUTH  hortonworks/cloudbreak-auth  Authentication service Docker image name    DOCKER_IMAGE_CLOUDBREAK_PERISCOPE  hortonworks/cloudbreak-autoscale  Autoscale Docker image name    DOCKER_IMAGE_CLOUDBREAK_SHELL  hortonworks/cloudbreak-shell  Cloudbreak Shell Docker image name    DOCKER_IMAGE_CLOUDBREAK_WEB  hortonworks/cloudbreak-web  Web UI Docker image name    DOCKER_TAG_ALPINE  3.1  Alpine container version    DOCKER_TAG_CBD_SMARTSENSE  0.10.0  SmartSense container version    DOCKER_TAG_CERT_TOOL  0.2.0  Cert tool container version    DOCKER_TAG_CLOUDBREAK  2.1.0-dev.70  Cloudbreak container version    DOCKER_TAG_CLOUDBREAK_SHELL  2.1.0-dev.70  Cloudbreak Shell container version    DOCKER_TAG_CONSUL  0.5  Consul container version    DOCKER_TAG_HAVEGED  1.1.0  Haveged container version    DOCKER_TAG_LOGROTATE  1.0.0  Logrotate container version    DOCKER_TAG_MIGRATION  1.0.0  Migration container version    DOCKER_TAG_PERISCOPE  2.1.0-dev.70  Autoscale container version    DOCKER_TAG_POSTFIX  latest  Postfix container version    DOCKER_TAG_POSTGRES  9.6.1-alpine  Postgresql container version    DOCKER_TAG_REGISTRATOR  v5  Registrator container version    DOCKER_TAG_SULTANS  2.1.0-dev.70  Authentication service container version    DOCKER_TAG_TRAEFIK  v1.2.0  Traefik container version    DOCKER_TAG_UAA  3.6.5  Identity container version    DOCKER_TAG_ULUWATU  2.1.0-dev.70  Web UI container version    IDENTITY_DB_NAME  \"uaadb\"  Name of the Identity database    IDENTITY_DB_PASS  \"\"  Password for the Identity database authentication    IDENTITY_DB_URL  \"${COMMON_DB}.service.consul:5432\"  URL for the Identity database connection, including the port number    IDENTITY_DB_USER  \"postgres\"  User for the Identity database authentication    LOCAL_SMTP_PASSWORD  \"$UAA_DEFAULT_USER_PW\"  Default password for the internal mail server    PERISCOPE_DB_HBM2DDL_STRATEGY  \"validate\"  Configures hibernate.hbm2ddl.auto in Autoscale    PERISCOPE_DB_NAME  \"periscopedb\"  Name of the Autoscale database    PERISCOPE_DB_PASS  \"\"  Password for the Autoscale database authentication    PERISCOPE_DB_SCHEMA_NAME  \"public\"  Schema used in the Autoscale database    PERISCOPE_DB_USER  \"postgres\"  User for the Autoscale database authentication    PERISCOPE_DB_TCP_ADDR   Address of the Autoscale database    PERISCOPE_DB_TCP_PORT   Port number of the Autoscale database    PERISCOPE_LOG_LEVEL  \"INFO\"  Log level of the Autoscale service    PERISCOPE_SCHEMA_MIGRATION_AUTO  true  When set to \"true\", enables Autoscale automatic database schema update    PUBLIC_IP   IP address or hostname of the public interface    REST_DEBUG  \"false\"  Set to \"true\" to enable REST call debug level in Cloudbreak and Autoscale    SL_ADDRESS_RESOLVING_TIMEOUT   DNS lookup timeout of Authentication service for internal service discovery    SL_NODE_TLS_REJECT_UNAUTHORIZED  \"0\"  When set to \"0\", enables self-signed certifications in Authentication service    SULTANS_CONTAINER_PATH  /sultans  Default project location in Authentication service container    TRAEFIK_MAX_IDLE_CONNECTION  100  Sets --maxidleconnsperhost for Traefik to the value entered    UAA_CLOUDBREAK_ID  cloudbreak  Identity of the Cloudbreak scope in Identity    UAA_CLOUDBREAK_SECRET  $UAA_DEFAULT_SECRET  Secret of the Cloudbreak scope in Identity    UAA_CLOUDBREAK_SHELL_ID  cloudbreak_shell  Identity of the Cloudbreak Shell scope in Identity    UAA_DEFAULT_ACCOUNT  \"seq1234567.SequenceIQ\"  Default account for users as an Identity group    UAA_DEFAULT_SECRET   Default secret for all the scopes and encryptions    UAA_DEFAULT_USER_EMAIL  admin@example.com  Email address of default admin user    UAA_DEFAULT_USER_FIRSTNAME  Joe  First name of default admin user    UAA_DEFAULT_USER_GROUPS  See  here  Default user groups of the users    UAA_DEFAULT_USER_LASTNAME  Admin  Last name of default admin user    UAA_DEFAULT_USER_PW   Password of default admin user    UAA_FLEX_USAGE_CLIENT_ID  flex_usage_client  Identity of the Flex usage generator scope in Identity    UAA_FLEX_USAGE_CLIENT_SECRET  $UAA_DEFAULT_SECRET  Secret of the Flex usage generator scope in Identity    UAA_PERISCOPE_ID  periscope  Identity of the Autoscale scope in Identity    UAA_PERISCOPE_SECRET  $UAA_DEFAULT_SECRET  Secret of the Autoscale scope in Identity    UAA_PORT  8089  Identity service public port    UAA_SULTANS_ID  sultans  Identity of the Authentication service scope in Identity    UAA_SULTANS_SECRET  $UAA_DEFAULT_SECRET  Secret of the Authentication service scope in Identity    UAA_ULUWATU_ID  uluwatu  Identity of the Web UI scope in Identity    UAA_ULUWATU_SECRET  $UAA_DEFAULT_SECRET  Secret of the Web UI scope in Identity    UAA_ZONE_DOMAIN  example.com  External domain name for zone in Identity    ULUWATU_CONTAINER_PATH  /uluwatu  Default project location in the Web UI container    ULU_DEFAULT_SSH_KEY  \"\"  Default SSH key for the credentials in Cloudbreak    ULU_HOST_ADDRESS  \"https://$PUBLIC_IP\"  URL for the Web UI host    ULU_NODE_TLS_REJECT_UNAUTHORIZED  \"0\"  When set to \"0\", enables self-signed certifications in Web UI    ULU_OAUTH_REDIRECT_URI  \"$ULU_HOST_ADDRESS/authorize\"  Authorization page on Web UI    ULU_SUBSCRIBE_TO_NOTIFICATIONS  \"false\"  Set to \u201ctrue\u201d to enable email notifications for Cloudbreak events    ULU_SULTANS_ADDRESS  \"https://$PUBLIC_IP/sl\"  Authentication service URL    VERBOSE_MIGRATION  false  When set to true, enables verbose database migration", 
            "title": "Cloudbreak Variables"
        }, 
        {
            "location": "/profile/index.html#aws-variables", 
            "text": "Variable Name  Default Value  Description      AWS_ACCESS_KEY_ID  \"\"  Access key of the AWS account    AWS_ROLE_NAME  cbreak-deployer  Name of the AWS role for the  cbd aws [generate-rol, show role]  commands    AWS_SECRET_ACCESS_KEY  \"\"  Secret access key of the AWS account    CB_AWS_CUSTOM_CF_TAGS  \"\"  Comma separated list of AWS CloudFormation stack tags    CB_AWS_DEFAULT_CF_TAG  \"\"  Default tag for AWS CloudFormation stack    CB_AWS_DEFAULT_INBOUND_SECURITY_GROUP  \"\"  Default inbound policy name for AWS CloudFormation stack    CB_AWS_EXTERNAL_ID  provision-ambari  External ID of the assume role policy    CB_AWS_HOSTKEY_VERIFY  \"false\"  Enables host fingerprint verification on AWS    CB_AWS_VPC  \"\"  Configures the VPC ID on AWS. Set this variable if you are provisioning cluster to the same VPC where Cloudbreak is deployed on AWS.    CERTS_BUCKET  \"\"  S3 bucket name for backup and restore certificates via  cbd aws [certs-restore-s3  certs-upload-s3]  commands", 
            "title": "AWS Variables"
        }, 
        {
            "location": "/profile/index.html#azure-variables", 
            "text": "Variable Name  Default Value  Description      AZURE_SUBSCRIPTION_ID   Azure subscription ID for interactive login in the web UI    AZURE_TENANT_ID   Azure tenant ID for interactive login in the web UI", 
            "title": "Azure Variables"
        }, 
        {
            "location": "/profile/index.html#gcp-variables", 
            "text": "Variable Name  Default Value  Description      CB_GCP_HOSTKEY_VERIFY  \"false\"  When set to \"true\", enables host fingerprint verification on GCP", 
            "title": "GCP Variables"
        }, 
        {
            "location": "/profile/index.html#local-development-variables", 
            "text": "Variable Name  Default Value  Description      CB_LOCAL_DEV_BIND_ADDR  \"192.168.64.1\"  Ambassador external address for local development of Cloudbreak and Autoscale    CB_SCHEMA_SCRIPTS_LOCATION  \"container\"  Location of Cloudbreak schema update files    DOCKER_TAG_AMBASSADOR  0.5.0  Ambassador container version for local development    PERISCOPE_SCHEMA_SCRIPTS_LOCATION  \"container\"  Location of Cloudbreak schema update files    PRIVATE_IP  $BRIDGE_IP  IP address or hostname of the private interface    REMOVE_CONTAINER  \"--rm\"  When set to \"--rm\" (default), removes side effect containers for debug purpose. Set to \" \" to keep side effect containers for debug purpose    SULTANS_VOLUME_HOST  /dev/null  Location of the locally developed Authentication service project    UAA_SCHEMA_SCRIPTS_LOCATION  \"container\"  Location of Identity schema update files    ULUWATU_VOLUME_HOST  /dev/null  Location of the locally developed web UI project", 
            "title": "Local Development Variables"
        }, 
        {
            "location": "/profile/index.html#macos-variables", 
            "text": "Variable Name  Default Value  Description      DOCKER_MACHINE  \"\"  Name of the Docker Machine where Cloudbreak runs    DOCKER_PROFILE  Profile  Profile file for environment variables related to Docker Machine    MACHINE_CPU  2  Number of the CPU cores on the Docker Machine instance    MACHINE_MEM  4096  Amount of RAM on the Docker Machine instance    MACHINE_NAME  cbd  Name of the Docker Machine instance    MACHINE_OPTS  \"--xhyve-virtio-9p\"  Extra options for Docker Machine instance    MACHINE_STORAGE_PATH  $HOME/.docker/machine  Docker Machine storage path", 
            "title": "MacOS Variables"
        }, 
        {
            "location": "/profile/index.html#uaa_default_user_groups", 
            "text": "Default value fro  UAA_DEFAULT_USER_GROUPS  is:  \"openid,cloudbreak.networks,cloudbreak.securitygroups,cloudbreak.templates,cloudbreak.blueprints,cloudbreak.credentials,cloudbreak.stacks,sequenceiq.cloudbreak.admin,sequenceiq.cloudbreak.user,sequenceiq.account.${UAA_DEFAULT_ACCOUNT},cloudbreak.events,cloudbreak.usages.global,cloudbreak.usages.account,cloudbreak.usages.user,periscope.cluster,cloudbreak.recipes,cloudbreak.blueprints.read,cloudbreak.templates.read,cloudbreak.credentials.read,cloudbreak.recipes.read,cloudbreak.networks.read,cloudbreak.securitygroups.read,cloudbreak.stacks.read,cloudbreak.sssdconfigs,cloudbreak.sssdconfigs.read,cloudbreak.platforms,cloudbreak.platforms.read\"", 
            "title": "UAA_DEFAULT_USER_GROUPS"
        }, 
        {
            "location": "/profile/index.html#change-smtp-parameters", 
            "text": "If you want to change SMTP parameters, add them your  Profile .  The default values of the SMTP parameters are:  export CLOUDBREAK_SMTP_SENDER_USERNAME=\nexport CLOUDBREAK_SMTP_SENDER_PASSWORD=\nexport CLOUDBREAK_SMTP_SENDER_HOST=\nexport CLOUDBREAK_SMTP_SENDER_PORT=25\nexport CLOUDBREAK_SMTP_SENDER_FROM=\nexport CLOUDBREAK_SMTP_AUTH=true\nexport CLOUDBREAK_SMTP_STARTTLS_ENABLE=true\nexport CLOUDBREAK_SMTP_TYPE=smtp  If your SMTP server uses SMTPS, you must set the protocol in your  Profile  to smtps:  export CLOUDBREAK_SMTP_TYPE=smtps", 
            "title": "Change SMTP Parameters"
        }, 
        {
            "location": "/profile/index.html#configure-consul", 
            "text": "Cloudbreak uses  Consul  for DNS resolution. All Cloudbreak related services are registered as someservice.service.consul.  Consul\u2019s built-in DNS server is able to fallback on another DNS server. This option is called  -recursor . Clodbreak Deployer first tries to discover the DNS settings of the host by looking for nameserver entry in the  /etc/resolv.conf  file. If it finds one, consul will use it as a recursor. Otherwise, it will use  8.8.8.8 .  For a full list of available consul config options, refer to  Consul documentation .  To pass any additional Consul configuration, define the  DOCKER_CONSUL_OPTIONS  variable in the Profile file.", 
            "title": "Configure Consul"
        }, 
        {
            "location": "/profile/index.html#add-tags-in-profile-aws", 
            "text": "In order to differentiate launched instances, you can optionally define custom tags for your AWS resources deployed by Cloudbreak.     If you want just one custom tag for your Cloudformation resources, set this variable in the  Profile :  export CB_AWS_DEFAULT_CF_TAG=mytagcontent  In this example, the name of the tag will be  CloudbreakId  and the value will be  mytagcontent .    If you prefer to customize the tag name, set this variable:  export CB_AWS_CUSTOM_CF_TAGS=mytagname:mytagvalue  In this example the name of the tag will be  mytagname  and the value will be  mytagvalue .     You can specify a list of tags with a comma separated list:   export CB_AWS_CUSTOM_CF_TAGS=tag1:value1,tag2:value2,tag3:value3", 
            "title": "Add Tags in Profile (AWS)"
        }, 
        {
            "location": "/cb-db/index.html", 
            "text": "Manage Cloudbreak Database\n\n\nBy default, Cloudbreak uses a built-in PostgreSQL database to persist data. For production environments, we suggest that you use an external database, an RDS served by your cloud provider. However, if you choose to use the default database, you should know that Cloudbreak deployer includes features for dumping and restoring built-in databases.\n\n\nDump and Restore Database\n\n\nCloudbreak deployer uses Docker for the underlying infrastructure and uses Docker volume for storing data. There are two separate volumes: \n\n\n\n\na volume called \ncommon\n for storing live data  \n\n\na volume called \ncbreak_dump\n for database dumps \n\n\n\n\nYou can override default live data volume any time by extending your \nProfile\n with the following variable:\n\n\nexport COMMON_DB_VOL=\nmy-live-data-volume\n\n\n\n\n\nTo create database dumps, execute the following commands:\n\n\ncbd db dump common cbdb\ncbd db dump common uaadb\ncbd db dump common periscopedb\n\n\n\n\nThe dump command has an optional third parameter, the \nname\n of the dump. If you give your dump a name, Cloudbreak deployer will create a symbolic link which points to the SQL dump. For example: \n\n\ncbd db dump common cbdb name-of-the-dump\n\n\n\n\nTo list existing dumps, execute the \ncbd db list-dumps\n command. Each kind of database dump (cbdb, uaadb, periscopedb) has a link to the latest dump on the \ncbreak_dump\n volume. During the restore process, Cloudbreak deployer restores from latest dump. To check which dump is the latest, execute:\n\n\ndocker run --rm -v cbreak_dump:/dump -it alpine ls -lsa /dump/cbdb/latest\n\n\n\n\nTo set any of the existing dumps as latest use the \nset-dump\n command. You can set both regular or named dumps. For example: \n\n\ncbd db set-dump cbdb 20170628_1805\n\n\n\n\nor\n\n\ncbd db set-dump cbdb name-of-the-dump\n\n\n\n\nTo remove the existing \ncommon\n volume, stop all the related Cloudbreak containers with \ncbd kill\n command, and then remove the volume:\n\n\ndocker volume rm common\n\n\n\n\nTo restore databases from dumps, execute:\n\n\ncbd db restore-volume-from-dump common cbdb\ncbd db restore-volume-from-dump common uaadb\ncbd db restore-volume-from-dump common periscopedb\n\n\n\n\nTo save your dumps to the host machine, execute:\n\n\ndocker run --rm -v cbreak_dump:/dump -it alpine cat /dump/cbdb/latest/dump.sql \n cbdb.sql\ndocker run --rm -v cbreak_dump:/dump -it alpine cat /dump/uaadb/latest/dump.sql \n uaadb.sql\ndocker run --rm -v cbreak_dump:/dump -it alpine cat /dump/periscopedb/latest/dump.sql \n periscopedb.sql", 
            "title": "Manage Cloudbreak Database"
        }, 
        {
            "location": "/cb-db/index.html#manage-cloudbreak-database", 
            "text": "By default, Cloudbreak uses a built-in PostgreSQL database to persist data. For production environments, we suggest that you use an external database, an RDS served by your cloud provider. However, if you choose to use the default database, you should know that Cloudbreak deployer includes features for dumping and restoring built-in databases.", 
            "title": "Manage Cloudbreak Database"
        }, 
        {
            "location": "/cb-db/index.html#dump-and-restore-database", 
            "text": "Cloudbreak deployer uses Docker for the underlying infrastructure and uses Docker volume for storing data. There are two separate volumes:    a volume called  common  for storing live data    a volume called  cbreak_dump  for database dumps    You can override default live data volume any time by extending your  Profile  with the following variable:  export COMMON_DB_VOL= my-live-data-volume   To create database dumps, execute the following commands:  cbd db dump common cbdb\ncbd db dump common uaadb\ncbd db dump common periscopedb  The dump command has an optional third parameter, the  name  of the dump. If you give your dump a name, Cloudbreak deployer will create a symbolic link which points to the SQL dump. For example:   cbd db dump common cbdb name-of-the-dump  To list existing dumps, execute the  cbd db list-dumps  command. Each kind of database dump (cbdb, uaadb, periscopedb) has a link to the latest dump on the  cbreak_dump  volume. During the restore process, Cloudbreak deployer restores from latest dump. To check which dump is the latest, execute:  docker run --rm -v cbreak_dump:/dump -it alpine ls -lsa /dump/cbdb/latest  To set any of the existing dumps as latest use the  set-dump  command. You can set both regular or named dumps. For example:   cbd db set-dump cbdb 20170628_1805  or  cbd db set-dump cbdb name-of-the-dump  To remove the existing  common  volume, stop all the related Cloudbreak containers with  cbd kill  command, and then remove the volume:  docker volume rm common  To restore databases from dumps, execute:  cbd db restore-volume-from-dump common cbdb\ncbd db restore-volume-from-dump common uaadb\ncbd db restore-volume-from-dump common periscopedb  To save your dumps to the host machine, execute:  docker run --rm -v cbreak_dump:/dump -it alpine cat /dump/cbdb/latest/dump.sql   cbdb.sql\ndocker run --rm -v cbreak_dump:/dump -it alpine cat /dump/uaadb/latest/dump.sql   uaadb.sql\ndocker run --rm -v cbreak_dump:/dump -it alpine cat /dump/periscopedb/latest/dump.sql   periscopedb.sql", 
            "title": "Dump and Restore Database"
        }, 
        {
            "location": "/security/index.html", 
            "text": "Security Overview\n\n\nCloudbreak utilizes cloud provider security resources such as virtual networks, security groups, and identity and access management:\n\n\n\n\nNetwork isolation\n is achieved via user-configured virtual networks and subnets.\n\n    Read more about \nVirtual Networks\n.  \n\n\nNetwork security\n is achieved via out-of-the-box security group settings.\n\n    Read more about \nNetwork Security\n.   \n\n\nControlled use of cloud resources\n using IAM roles (AWS, GCP) or Active Directory (in case of Azure). \n    Read more about \nIdentity Management\n.    \n\n\n\n\nVirtual Networks\n\n\nCloud providers use virtual networks which resemble traditional networks. Depending on the options that you selected during deployment, your Cloudbreak instance and clusters are launched into new or existing cloud provider networking infrastructure (virtual networks and subnets). For more information about virtual networks, refer to the cloud-provider documentation:\n\n\n\n\n\n\n\n\nCloud Provider\n\n\nDocumentation Link\n\n\n\n\n\n\n\n\n\n\nAWS\n\n\nAmazon Virtual Private Cloud (Amazon VPC)\n\n\n\n\n\n\nAzure\n\n\nMicrosoft Azure Virtual Network\n\n\n\n\n\n\nGoogle Cloud Platform\n\n\nVirtual Private Cloud (VPC) network\n\n\n\n\n\n\nOpenStack\n\n\nNetwork\n\n\n\n\n\n\n\n\nNetwork Security\n\n\nSecurity groups are set up to control network traffic to the instances in the system. By default, the system is configured to restrict inbound network traffic to the minimal set of ports. You can add or modify rules to each security group that allow traffic to or from its associated instances: you can do this either when creating your cluster or when the cluster is already running.  \n\n\nThe following table lists the minimum security group port configuration required for the Cloudbreak instance:\n\n\n\n\n\n\n\n\nInbound Port\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\n22\n\n\nSSH access to the Cloudbreak VM.\n\n\n\n\n\n\n80\n\n\nHTTP access to the Cloudbreak UI.\n\n\n\n\n\n\n\n\nIdentity Management\n\n\nTo securely control access to cloud resources, cloud providers use identity management services such as IAM roles (AWS and GCP) and Active Directory (Azure). \n\n\n\n\n\n\n\n\nCloud Provider\n\n\nDocumentation Link\n\n\n\n\n\n\n\n\n\n\nAWS\n\n\nAWS Identity and Access Management (IAM)\n\n\n\n\n\n\nAzure\n\n\nAzure Active Directory ((Azure AD))\n\n\n\n\n\n\nGoogle\n\n\nGoogle Cloud Identity and Access Management (IAM)\n\n\n\n\n\n\nOpenStack\n\n\nKeystone\n\n\n\n\n\n\n\n\nCloudbreak utilizes cloud provider\u2019s identity management services via Cloudbreak credential. After launching Cloudbreak on your chosen cloud provider, you must create a Cloudbreak credential, which allows Cloudbreak to authenticate with your cloud provider identity management service. Only after you have completed this step, Cloudbreak can create resources on your behalf. \n\n\nAuthentication with AWS\n\n\nWhen launching Cloudbreak on AWS, you must select a way for Cloudbreak to authenticate with your AWS account and create resources on your behalf. While key-based authentication uses your AWS access key and secret key, role-based authentication uses IAM roles.\n\n\nIf you are using role-based authentication for Cloudbreak on AWS, you must create two IAM roles: one to grant Cloudbreak access to allow Cloudbreak to assume AWS roles (using the \"AssumeRole\" policy) and the second one to provide Cloudbreak with the capabilities required for cluster creation (using the \"cb-policy\" policy).\n\n\nThe following table provides contextual information about the two roles required: \n\n\n\n\n\n\n\n\nRole\n\n\nPurpose\n\n\nOverview of Steps\n\n\nConfiguration\n\n\n\n\n\n\n\n\n\n\nCloudbreakRole\n\n\nAllows Cloudbreak to assume other IAM roles - specifically the CredentialRole.\n\n\nCreate a role called \"CloudbreakRole\" and attach the \"AssumeRole\" policy. The \"AssumeRole\" policy definition and steps for creating the CloudbreakRole are provided below.\n\n\nWhen launching your Cloudbreak VM, during \nStep 3: Configure Instance Details\n \n \nIAM\n, you will attach the \"CloudbreakRole\" IAM role to the VM.\n\n\n\n\n\n\nCredentialRole\n\n\nAllows Cloudbreak to create AWS resources required for clusters.\n\n\nCreate a new IAM role called \"CredentialRole\" and attach the \"cb-policy\" policy to it. The \"cb-policy\" policy definition and steps for creating the CredentialRole are provided below.\n When creating this role using the AWS Console, make sure that that it is a role for cross-account access and that the trust-relation is set up as follows: 'Account ID' is your own 12-digit AWS account ID and 'External ID' is \u201cprovision-ambari\u201d. See steps below.\n\n\nOnce you log in to the Cloudbreak UI and are ready to create clusters, you will use this role to create the Cloudbreak credential.\n\n\n\n\n\n\n\n\nRelated Links\n\n\nMeet the Prerequisites: Authentication\n  \n\n\nAuthentication with Azure\n\n\nAfter launching Cloudbread on Azure, you are required to create a Cloudbreak credential, which allows Cloudbreak to authenticate with your Azure Active Directory. \n\n\nYou have two options:\n\n\n\n\n\n\nInteractive: The app and service principal creation and role assignment are fully automated, so the only input that you need to provide to Cloudbreak is your Subscription ID and Directory ID. \n\n\n\n\n\n\nApp-based: The app and service principal creation and role assignment are not automated You must create an Azure Active Directory application registration and then provide its parameters to Cloudbreak, in addition to providing your Subscription ID and Directory ID. \n\n\n\n\n\n\nRelated Links\n\n\nCreate Cloudbreak Credential\n  \n\n\nAuthentication with GCP\n\n\nAfter launching Cloudbreak on GCP, you are required to register a service account in Cloudbrak via creating a Cloudbreak credential. Cloudbreak uses this account to authenticate with the GCP identity management service.\n\n\nRelated Links\n\n\nMeet the Prerequisites: Service Account\n  \n\n\nAuthentication with OpenStack\n\n\nAfter launching Cloudbread on OpenStack, you are required to create a Cloudbreak credential, which allows Cloudbreak to authenticate with keystone. \n\n\nRelated Links\n\n\nCreate Cloudbreak Credential", 
            "title": "Security Overview"
        }, 
        {
            "location": "/security/index.html#security-overview", 
            "text": "Cloudbreak utilizes cloud provider security resources such as virtual networks, security groups, and identity and access management:   Network isolation  is achieved via user-configured virtual networks and subnets. \n    Read more about  Virtual Networks .    Network security  is achieved via out-of-the-box security group settings. \n    Read more about  Network Security .     Controlled use of cloud resources  using IAM roles (AWS, GCP) or Active Directory (in case of Azure). \n    Read more about  Identity Management .", 
            "title": "Security Overview"
        }, 
        {
            "location": "/security/index.html#virtual-networks", 
            "text": "Cloud providers use virtual networks which resemble traditional networks. Depending on the options that you selected during deployment, your Cloudbreak instance and clusters are launched into new or existing cloud provider networking infrastructure (virtual networks and subnets). For more information about virtual networks, refer to the cloud-provider documentation:     Cloud Provider  Documentation Link      AWS  Amazon Virtual Private Cloud (Amazon VPC)    Azure  Microsoft Azure Virtual Network    Google Cloud Platform  Virtual Private Cloud (VPC) network    OpenStack  Network", 
            "title": "Virtual Networks"
        }, 
        {
            "location": "/security/index.html#network-security", 
            "text": "Security groups are set up to control network traffic to the instances in the system. By default, the system is configured to restrict inbound network traffic to the minimal set of ports. You can add or modify rules to each security group that allow traffic to or from its associated instances: you can do this either when creating your cluster or when the cluster is already running.    The following table lists the minimum security group port configuration required for the Cloudbreak instance:     Inbound Port  Description      22  SSH access to the Cloudbreak VM.    80  HTTP access to the Cloudbreak UI.", 
            "title": "Network Security"
        }, 
        {
            "location": "/security/index.html#identity-management", 
            "text": "To securely control access to cloud resources, cloud providers use identity management services such as IAM roles (AWS and GCP) and Active Directory (Azure).      Cloud Provider  Documentation Link      AWS  AWS Identity and Access Management (IAM)    Azure  Azure Active Directory ((Azure AD))    Google  Google Cloud Identity and Access Management (IAM)    OpenStack  Keystone     Cloudbreak utilizes cloud provider\u2019s identity management services via Cloudbreak credential. After launching Cloudbreak on your chosen cloud provider, you must create a Cloudbreak credential, which allows Cloudbreak to authenticate with your cloud provider identity management service. Only after you have completed this step, Cloudbreak can create resources on your behalf.", 
            "title": "Identity Management"
        }, 
        {
            "location": "/security/index.html#authentication-with-aws", 
            "text": "When launching Cloudbreak on AWS, you must select a way for Cloudbreak to authenticate with your AWS account and create resources on your behalf. While key-based authentication uses your AWS access key and secret key, role-based authentication uses IAM roles.  If you are using role-based authentication for Cloudbreak on AWS, you must create two IAM roles: one to grant Cloudbreak access to allow Cloudbreak to assume AWS roles (using the \"AssumeRole\" policy) and the second one to provide Cloudbreak with the capabilities required for cluster creation (using the \"cb-policy\" policy).  The following table provides contextual information about the two roles required:      Role  Purpose  Overview of Steps  Configuration      CloudbreakRole  Allows Cloudbreak to assume other IAM roles - specifically the CredentialRole.  Create a role called \"CloudbreakRole\" and attach the \"AssumeRole\" policy. The \"AssumeRole\" policy definition and steps for creating the CloudbreakRole are provided below.  When launching your Cloudbreak VM, during  Step 3: Configure Instance Details     IAM , you will attach the \"CloudbreakRole\" IAM role to the VM.    CredentialRole  Allows Cloudbreak to create AWS resources required for clusters.  Create a new IAM role called \"CredentialRole\" and attach the \"cb-policy\" policy to it. The \"cb-policy\" policy definition and steps for creating the CredentialRole are provided below.  When creating this role using the AWS Console, make sure that that it is a role for cross-account access and that the trust-relation is set up as follows: 'Account ID' is your own 12-digit AWS account ID and 'External ID' is \u201cprovision-ambari\u201d. See steps below.  Once you log in to the Cloudbreak UI and are ready to create clusters, you will use this role to create the Cloudbreak credential.     Related Links  Meet the Prerequisites: Authentication", 
            "title": "Authentication with AWS"
        }, 
        {
            "location": "/security/index.html#authentication-with-azure", 
            "text": "After launching Cloudbread on Azure, you are required to create a Cloudbreak credential, which allows Cloudbreak to authenticate with your Azure Active Directory.   You have two options:    Interactive: The app and service principal creation and role assignment are fully automated, so the only input that you need to provide to Cloudbreak is your Subscription ID and Directory ID.     App-based: The app and service principal creation and role assignment are not automated You must create an Azure Active Directory application registration and then provide its parameters to Cloudbreak, in addition to providing your Subscription ID and Directory ID.     Related Links  Create Cloudbreak Credential", 
            "title": "Authentication with Azure"
        }, 
        {
            "location": "/security/index.html#authentication-with-gcp", 
            "text": "After launching Cloudbreak on GCP, you are required to register a service account in Cloudbrak via creating a Cloudbreak credential. Cloudbreak uses this account to authenticate with the GCP identity management service.  Related Links  Meet the Prerequisites: Service Account", 
            "title": "Authentication with GCP"
        }, 
        {
            "location": "/security/index.html#authentication-with-openstack", 
            "text": "After launching Cloudbread on OpenStack, you are required to create a Cloudbreak credential, which allows Cloudbreak to authenticate with keystone.   Related Links  Create Cloudbreak Credential", 
            "title": "Authentication with OpenStack"
        }, 
        {
            "location": "/security-cb/index.html", 
            "text": "Securing Cloudbreak After Launch\n\n\nCloudbreak comes with default settings designed for easy first experience rather than strict security. To secure Cloudbreak, follow these recommendations. \n\n\nRestricting Inbound Access\n\n\nWe recommend that you block all communication ports except 443 on the firewall or security group (depending on the provider). \n\n\nIf you have to log in to the Cloudbreak host remotely, use the SSH port (usually 22).\n\n\nSecure the Profile\n\n\nBefore starting Cloudbreak for the first time, configure the Profile file as directed below. Changes are applied during startup so a restart (\ncbd restart\n) is required after each change.\n\n\n\n\n\n\nExecute the following command in the directory where you want to store Cloudbreak-related files:\n\n\n\necho export PUBLIC_IP=[the ip or hostname to bind] \n Profile\n\n\n\ncomment\n: \n (TO-DO: Do you mean that this needs to be executed in the deployment directory? Or?)\n\n\n\n\n\n\nAfter you have a base Profile file, add the following custom properties to it:\n\n\n\nexport UAA_DEFAULT_SECRET='[custom secret]'\nexport UAA_DEFAULT_USER_EMAIL='[default admin email address]'\nexport UAA_DEFAULT_USER_PW='[default admin password]'\nexport UAA_DEFAULT_USER_FIRSTNAME='[default admin first name]'\nexport UAA_DEFAULT_USER_LASTNAME='[default admin last name]'\n\n\n\nCloudbreak has additional secrets which by default inherit their values from \nUAA_DEFAULT_SECRET\n. Instead of using the default, you can define different values in the Profile for each of these service clients:\n\n\n\nexport UAA_CLOUDBREAK_SECRET='[cloudbreak secret]'\nexport UAA_PERISCOPE_SECRET='[auto scaling secret]'\nexport UAA_ULUWATU_SECRET='[web ui secret]'\nexport UAA_SULTANS_SECRET='[authenticator secret]'\n\n\n\nYou can change these secrets at any time, except \nUAA_CLOUDBREAK_SECRET\n which is used to encrypt sensitive information at database level. \n\n\ncomment\n: \n (TO-DO: The info below is explained in a way that is confusing. Can you rephrase?) \n\n\nUAA_DEFAULT_USER_PW\n is stored in plain text format, but if \nUAA_DEFAULT_USER_PW\n is missing from the Profile, it gets a default value. Because default password is not an option, if you set an empty password explicitly in the Profile Cloudbreak deployer will ask for password all the time when it is needed for the operation.\n\n\n\nexport UAA_DEFAULT_USER_PW=''\n\n\n\nIn this case, Cloudbreak deployer wouldn't be able to add the default user, so you have to do it manually by executing the following command:\n\n\n\ncbd util add-default-user\n\n\n\n\n\n\n\nFor more information about setting environment variables in Profile, refer to \nConfigure Profile Variables\n.\n\n\nAdd SSL Certificate for Cloudbreak UI\n\n\nBy default Cloudbreak has been configured with a self-signed certificate for access via HTTPS. This is sufficient for many deployments such as trials, development, testing, or staging. However, for production deployments, a trusted certificate is preferred and can be configured in the controller. Follow these steps to configure the cloud controller to use your own trusted certificate. \n\n\nPrerequisites\n\n\nTo use your own certificate, you must have:\n\n\n\n\nA resolvable fully qualified domain name (FQDN) for the controller host IP address. For example, this can be set up in \nAmazon Route 53\n.  \n\n\nA valid SSL certificate for this fully qualified domain name. The certificate can be obtained from a number of certificate providers.  \n\n\n\n\nSteps\n\n\n\n\n\n\nSSH to the Cloudbreak host instance:\n\n\nssh -i mykeypair.pem cloudbreak@[CONTROLLER-IP-ADDRESS]\n\n\n\n\n\n\nMake sure that the target fully qualified domain name (FQDN) which you plan to use for Cloudbreak is resolvable:\n\n\nnslookup [TARGET-CONTROLLER-FQDN]\n\n\nFor example:\n\n\nnslookup hdcloud.example.com\n\n\n\n\n\n\nBrowse to the Cloudbreak deployment directory and edit the \nProfile\n file:\n\n\nvi /var/lib/cloudbreak-deployment/Profile\n\n\n\n\n\n\nReplace the value of the \nPUBLIC_IP\n variable with the \nTARGET-CONTROLLER-FQDN\n value:\n\n\nPUBLIC_IP=[TARGET-CONTROLLER-FQDN]\n\n\n\n\n\n\nCopy your private key and certificate files for the FQDN onto the Cloudbreak host. These files must be placed under \n/var/lib/cloudbreak-deployment/certs/traefik/\n directory.\n\n\n\n\nFile permissions for the private key and certificate files can be set to 600.\n\n\n\n\n\n\n\n\n\n\nFile\n\n\nExample\n\n\n\n\n\n\n\n\n\n\nPRIV-KEY-LOCATION\n\n\n/var/lib/cloudbreak-deployment/certs/traefik/hdcloud.example.com.key\n\n\n\n\n\n\nCERT-LOCATION\n\n\n/var/lib/cloudbreak-deployment/certs/traefik/hdcloud.example.com.crt\n\n\n\n\n\n\n\n\n\n\n\n\nConfigure TLS details in your \nProfile\n by adding the following line at the end of the file.\n\n\n\n\nNotice that \nCERT-LOCATION\n and \nPRIV-KEY-LOCATION\n are file locations from Step 5, starting at the \n/certs/...\n path.\n\n\n\n\nexport CBD_TRAEFIK_TLS=\u201d[CERT-LOCATION],[PRIV-KEY-LOCATION]\u201d\n\n\nFor example:\n\n\nexport CBD_TRAEFIK_TLS=\"/certs/traefik/hdcloud.example.com.crt,/certs/traefik/hdcloud.example.com.key\"\n\n\n\n\n\n\nRestart Cloudbreak deployer:\n\n\ncbd restart\n\n\n\n\n\n\nUsing your web browser, access to the Cloudbreak UI using the new resolvable fully qualified domain name.\n\n\n\n\n\n\nConfirm that the connection is SSL-protected and that the certificate used is the certificate that you provided to Cloudbreak.\n\n\n\n\n\n\nConfigure Access from Custom Domains\n\n\nCloudbreak Deployer, which uses UAA as an identity provider, supports multitenancy. In UAA, multitenancy is managed through identity zones. An identity zone is accessed through a unique subdomain. For example, if the standard UAA responds to \nhttps://uaa.10.244.0.34.xip.io\n, a zone on this UAA can be accessed through a unique subdomain \nhttps://testzone1.uaa.10.244.0.34.xip.io\n.\n\n\nIf you want to use a custom domain for your identity or deployment, add the \nUAA_ZONE_DOMAIN\n line to your \nProfile\n:\n\n\nexport UAA_ZONE_DOMAIN=my-subdomain.example.com\n\n\n\nThis variable is necessary for UAA to identify which zone provider should handle the requests that arrive to that domain.", 
            "title": "Configure Basic Security"
        }, 
        {
            "location": "/security-cb/index.html#securing-cloudbreak-after-launch", 
            "text": "Cloudbreak comes with default settings designed for easy first experience rather than strict security. To secure Cloudbreak, follow these recommendations.", 
            "title": "Securing Cloudbreak After Launch"
        }, 
        {
            "location": "/security-cb/index.html#restricting-inbound-access", 
            "text": "We recommend that you block all communication ports except 443 on the firewall or security group (depending on the provider).   If you have to log in to the Cloudbreak host remotely, use the SSH port (usually 22).", 
            "title": "Restricting Inbound Access"
        }, 
        {
            "location": "/security-cb/index.html#secure-the-profile", 
            "text": "Before starting Cloudbreak for the first time, configure the Profile file as directed below. Changes are applied during startup so a restart ( cbd restart ) is required after each change.    Execute the following command in the directory where you want to store Cloudbreak-related files:  \necho export PUBLIC_IP=[the ip or hostname to bind]   Profile  comment :   (TO-DO: Do you mean that this needs to be executed in the deployment directory? Or?)    After you have a base Profile file, add the following custom properties to it:  \nexport UAA_DEFAULT_SECRET='[custom secret]'\nexport UAA_DEFAULT_USER_EMAIL='[default admin email address]'\nexport UAA_DEFAULT_USER_PW='[default admin password]'\nexport UAA_DEFAULT_USER_FIRSTNAME='[default admin first name]'\nexport UAA_DEFAULT_USER_LASTNAME='[default admin last name]'  Cloudbreak has additional secrets which by default inherit their values from  UAA_DEFAULT_SECRET . Instead of using the default, you can define different values in the Profile for each of these service clients:  \nexport UAA_CLOUDBREAK_SECRET='[cloudbreak secret]'\nexport UAA_PERISCOPE_SECRET='[auto scaling secret]'\nexport UAA_ULUWATU_SECRET='[web ui secret]'\nexport UAA_SULTANS_SECRET='[authenticator secret]'  You can change these secrets at any time, except  UAA_CLOUDBREAK_SECRET  which is used to encrypt sensitive information at database level.   comment :   (TO-DO: The info below is explained in a way that is confusing. Can you rephrase?)   UAA_DEFAULT_USER_PW  is stored in plain text format, but if  UAA_DEFAULT_USER_PW  is missing from the Profile, it gets a default value. Because default password is not an option, if you set an empty password explicitly in the Profile Cloudbreak deployer will ask for password all the time when it is needed for the operation.  \nexport UAA_DEFAULT_USER_PW=''  In this case, Cloudbreak deployer wouldn't be able to add the default user, so you have to do it manually by executing the following command:  \ncbd util add-default-user    For more information about setting environment variables in Profile, refer to  Configure Profile Variables .", 
            "title": "Secure the Profile"
        }, 
        {
            "location": "/security-cb/index.html#add-ssl-certificate-for-cloudbreak-ui", 
            "text": "By default Cloudbreak has been configured with a self-signed certificate for access via HTTPS. This is sufficient for many deployments such as trials, development, testing, or staging. However, for production deployments, a trusted certificate is preferred and can be configured in the controller. Follow these steps to configure the cloud controller to use your own trusted certificate.   Prerequisites  To use your own certificate, you must have:   A resolvable fully qualified domain name (FQDN) for the controller host IP address. For example, this can be set up in  Amazon Route 53 .    A valid SSL certificate for this fully qualified domain name. The certificate can be obtained from a number of certificate providers.     Steps    SSH to the Cloudbreak host instance:  ssh -i mykeypair.pem cloudbreak@[CONTROLLER-IP-ADDRESS]    Make sure that the target fully qualified domain name (FQDN) which you plan to use for Cloudbreak is resolvable:  nslookup [TARGET-CONTROLLER-FQDN]  For example:  nslookup hdcloud.example.com    Browse to the Cloudbreak deployment directory and edit the  Profile  file:  vi /var/lib/cloudbreak-deployment/Profile    Replace the value of the  PUBLIC_IP  variable with the  TARGET-CONTROLLER-FQDN  value:  PUBLIC_IP=[TARGET-CONTROLLER-FQDN]    Copy your private key and certificate files for the FQDN onto the Cloudbreak host. These files must be placed under  /var/lib/cloudbreak-deployment/certs/traefik/  directory.   File permissions for the private key and certificate files can be set to 600.      File  Example      PRIV-KEY-LOCATION  /var/lib/cloudbreak-deployment/certs/traefik/hdcloud.example.com.key    CERT-LOCATION  /var/lib/cloudbreak-deployment/certs/traefik/hdcloud.example.com.crt       Configure TLS details in your  Profile  by adding the following line at the end of the file.   Notice that  CERT-LOCATION  and  PRIV-KEY-LOCATION  are file locations from Step 5, starting at the  /certs/...  path.   export CBD_TRAEFIK_TLS=\u201d[CERT-LOCATION],[PRIV-KEY-LOCATION]\u201d  For example:  export CBD_TRAEFIK_TLS=\"/certs/traefik/hdcloud.example.com.crt,/certs/traefik/hdcloud.example.com.key\"    Restart Cloudbreak deployer:  cbd restart    Using your web browser, access to the Cloudbreak UI using the new resolvable fully qualified domain name.    Confirm that the connection is SSL-protected and that the certificate used is the certificate that you provided to Cloudbreak.", 
            "title": "Add SSL Certificate for Cloudbreak UI"
        }, 
        {
            "location": "/security-cb/index.html#configure-access-from-custom-domains", 
            "text": "Cloudbreak Deployer, which uses UAA as an identity provider, supports multitenancy. In UAA, multitenancy is managed through identity zones. An identity zone is accessed through a unique subdomain. For example, if the standard UAA responds to  https://uaa.10.244.0.34.xip.io , a zone on this UAA can be accessed through a unique subdomain  https://testzone1.uaa.10.244.0.34.xip.io .  If you want to use a custom domain for your identity or deployment, add the  UAA_ZONE_DOMAIN  line to your  Profile :  export UAA_ZONE_DOMAIN=my-subdomain.example.com  This variable is necessary for UAA to identify which zone provider should handle the requests that arrive to that domain.", 
            "title": "Configure Access from Custom Domains"
        }, 
        {
            "location": "/security-kerberos/index.html", 
            "text": "Configure Kerberos\n\n\nCloudbreak supports using Kerberos security for its clusters. It supports three ways of provisioning Kerberos-enabled clusters:\n\n\n\n\nCreate new MIT Kerberos at cluster provisioning time  \n\n\nUse your existing MIT Kerberos server with a Cloudbreak provisioned cluster  \n\n\nUse your existing Active Directory with a Cloudbreak provisioned cluster  \n\n\n\n\nCreate New MIT Kerberos at Cluster Provisioning\n\n\nWhen creating a cluster, you can provide your Kerberos configuration details and Cloudbreak will install an MIT KDC and enable Kerberos on the cluster.\n\n\nTo enable Kerberos on a cluster, follow these steps when creating your cluster via Cloudbreak web UI.\n\n\nSteps\n\n\n\n\n\n\nIn the \nCreate cluster\n wizard, in the \nSetup Network and Security\n tab, check the \nEnable security\n option and select \nCreate New MIT Kerberos\n.\n\n\n\n\n\n\nProvide the following information for the KDC:\n\n\n\n\n\n\n\n\n\n\n\n\nField\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nKerberos master key\n\n\nThe master key to use for the KDC.\n\n\n\n\n\n\nKerberos admin\n\n\nThe KDC admin username to use for the KDC.\n\n\n\n\n\n\nKerberos password\n\n\nThe KDC admin password to use for the KDC.\n\n\n\n\n\n\n\n\nTesting Kerberos\n\n\nTo run a job on the cluster, you can use one of the default Hadoop users, like \nambari-qa\n.\n\n\nOnce kerberos is enabled, you need a \nticket\n to execute any job on the cluster. Here's an example of how to get a ticket:\n\n\nkinit -V -kt /etc/security/keytabs/smokeuser.headless.keytab ambari-qa-sparktest-rec@NODE.DC1.CONSUL\n\n\n\n\nHere is an example job:\n\n\nexport HADOOP_LIBS=/usr/hdp/current/hadoop-mapreduce-client\nexport JAR_EXAMPLES=$HADOOP_LIBS/hadoop-mapreduce-examples.jar\nexport JAR_JOBCLIENT=$HADOOP_LIBS/hadoop-mapreduce-client-jobclient.jar\n\nhadoop jar $JAR_EXAMPLES teragen 10000000 /user/ambari-qa/terasort-input\n\nhadoop jar $JAR_JOBCLIENT mrbench -baseDir /user/ambari-qa/smallJobsBenchmark -numRuns 5 -maps 10 -reduces 5 -inputLines 10 -inputType ascending\n\n\n\n\nUse Existing MIT Kerberos Server with a Cloudbreak Provisioned Cluster\n\n\nCloudbreak supports using Kerberos security on the cluster with an existing MIT Kerberos. When creating a cluster, provide your Kerberos configuration details, and Cloudbreak will automatically extend your blueprint configuration with the defined properties. \nSetup an exiting MIT KDC\n\n\nTo enable Kerberos on a cluster, perform the following steps when creating your cluster via Cloudbreak web UI.\n\n\nSteps\n\n\n\n\nIn the \nCreate cluster\n wizard, in the \nSetup Network and Security\n tab, check the \nEnable security\n option and select \nUse Existing MIT Kerberos\n.\n\n\nFill in the following fields:\n\n\n\n\n\n\n\n\n\n\nField\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nKerberos Password\n\n\nThe KDC admin password to use for the KDC.\n\n\n\n\n\n\nExisting Kerberos Principal\n\n\nThe KDC principal in your existing MIT KDC.\n\n\n\n\n\n\nExisting Kerberos URL\n\n\nThe location of your existing MIT KDC.\n\n\n\n\n\n\nUse Tcp Connection\n\n\nThe connection type for your existing MIT KDC (default is \nUDP\n).\n\n\n\n\n\n\nExisting Kerberos Realm\n\n\nThe realm in your existing MIT KDC.\n\n\n\n\n\n\n\n\nUse Existing Active Directory with a Cloudbreak Provisioned Cluster\n\n\nCloudbreak supports using Kerberos security on the cluster with an existing Active Directory. When creating a cluster, provide your Kerberos configuration details, and Cloudbreak will automatically extend your blueprint configuration with the defined properties. \nSetup an Active Directory for Kerberos\n\n\nTo enable Kerberos on a cluster, perform these steps when creating your cluster via Cloudbreak UI.\n\n\nSteps\n\n\n\n\nIn the \nCreate cluster\n wizard, in the \nSetup Network and Security\n tab, check the \nEnable security\n option and select \nUse Existing Active Directory\n.\n\n\nProvide the following informstion:\n\n\n\n\n\n\n\n\n\n\nField\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nKerberos Password\n\n\nThe KDC admin password to use for the KDC.\n\n\n\n\n\n\nExisting Kerberos Principal\n\n\nThe KDC principal in your existing MIT KDC.\n\n\n\n\n\n\nExisting Kerberos URL\n\n\nThe location of your existing MIT KDC.\n\n\n\n\n\n\nUse Tcp Connection\n\n\nThe connection type for your existing MIT KDC (default is \nUDP\n).\n\n\n\n\n\n\nExisting Kerberos Realm\n\n\nThe realm in your existing MIT KDC.\n\n\n\n\n\n\nExisting Kerberos Ldap AD Url\n\n\nThe url of the existing secure ldap (eg. ldaps://10.1.1.5).\n\n\n\n\n\n\nExisting Kerberos AD Container DN\n\n\nActive Directory User container for principals. For example \"OU=Hadoop,OU=People,dc=apache,dc=org\".\n\n\n\n\n\n\n\n\nCreate Hadoop Users\n\n\nTo create Hadoop users, follow these steps.\n\n\nSteps\n\n\n\n\n\n\nLog in via SSH to the node where the Ambari Server is (IP address is the same as the Ambari UI) and run:\n\n\n\nkadmin -p [admin_user]/[admin_user]@NODE.DC1.CONSUL (type admin password)\naddprinc custom-user (type user password twice)\n\n\n\n\n\n\n\nLog in via SSH to all other nodes and, on each node, run:\n\n\n\nuseradd custom-user\n\n\n\n\n\n\n\nLog in via SSH to one of the nodes and run:\n\n\n\nsu custom-user\nkinit -p custom-user (type user password)\nhdfs dfs -mkdir input\nhdfs dfs -put /tmp/wait-for-host-number.sh input\nyarn jar $(find /usr/hdp -name hadoop-mapreduce-examples.jar) wordcount input output\nhdfs dfs -cat output/*", 
            "title": "Configure Kerberos"
        }, 
        {
            "location": "/security-kerberos/index.html#configure-kerberos", 
            "text": "Cloudbreak supports using Kerberos security for its clusters. It supports three ways of provisioning Kerberos-enabled clusters:   Create new MIT Kerberos at cluster provisioning time    Use your existing MIT Kerberos server with a Cloudbreak provisioned cluster    Use your existing Active Directory with a Cloudbreak provisioned cluster", 
            "title": "Configure Kerberos"
        }, 
        {
            "location": "/security-kerberos/index.html#create-new-mit-kerberos-at-cluster-provisioning", 
            "text": "When creating a cluster, you can provide your Kerberos configuration details and Cloudbreak will install an MIT KDC and enable Kerberos on the cluster.  To enable Kerberos on a cluster, follow these steps when creating your cluster via Cloudbreak web UI.  Steps    In the  Create cluster  wizard, in the  Setup Network and Security  tab, check the  Enable security  option and select  Create New MIT Kerberos .    Provide the following information for the KDC:       Field  Description      Kerberos master key  The master key to use for the KDC.    Kerberos admin  The KDC admin username to use for the KDC.    Kerberos password  The KDC admin password to use for the KDC.", 
            "title": "Create New MIT Kerberos at Cluster Provisioning"
        }, 
        {
            "location": "/security-kerberos/index.html#testing-kerberos", 
            "text": "To run a job on the cluster, you can use one of the default Hadoop users, like  ambari-qa .  Once kerberos is enabled, you need a  ticket  to execute any job on the cluster. Here's an example of how to get a ticket:  kinit -V -kt /etc/security/keytabs/smokeuser.headless.keytab ambari-qa-sparktest-rec@NODE.DC1.CONSUL  Here is an example job:  export HADOOP_LIBS=/usr/hdp/current/hadoop-mapreduce-client\nexport JAR_EXAMPLES=$HADOOP_LIBS/hadoop-mapreduce-examples.jar\nexport JAR_JOBCLIENT=$HADOOP_LIBS/hadoop-mapreduce-client-jobclient.jar\n\nhadoop jar $JAR_EXAMPLES teragen 10000000 /user/ambari-qa/terasort-input\n\nhadoop jar $JAR_JOBCLIENT mrbench -baseDir /user/ambari-qa/smallJobsBenchmark -numRuns 5 -maps 10 -reduces 5 -inputLines 10 -inputType ascending", 
            "title": "Testing Kerberos"
        }, 
        {
            "location": "/security-kerberos/index.html#use-existing-mit-kerberos-server-with-a-cloudbreak-provisioned-cluster", 
            "text": "Cloudbreak supports using Kerberos security on the cluster with an existing MIT Kerberos. When creating a cluster, provide your Kerberos configuration details, and Cloudbreak will automatically extend your blueprint configuration with the defined properties.  Setup an exiting MIT KDC  To enable Kerberos on a cluster, perform the following steps when creating your cluster via Cloudbreak web UI.  Steps   In the  Create cluster  wizard, in the  Setup Network and Security  tab, check the  Enable security  option and select  Use Existing MIT Kerberos .  Fill in the following fields:      Field  Description      Kerberos Password  The KDC admin password to use for the KDC.    Existing Kerberos Principal  The KDC principal in your existing MIT KDC.    Existing Kerberos URL  The location of your existing MIT KDC.    Use Tcp Connection  The connection type for your existing MIT KDC (default is  UDP ).    Existing Kerberos Realm  The realm in your existing MIT KDC.", 
            "title": "Use Existing MIT Kerberos Server with a Cloudbreak Provisioned Cluster"
        }, 
        {
            "location": "/security-kerberos/index.html#use-existing-active-directory-with-a-cloudbreak-provisioned-cluster", 
            "text": "Cloudbreak supports using Kerberos security on the cluster with an existing Active Directory. When creating a cluster, provide your Kerberos configuration details, and Cloudbreak will automatically extend your blueprint configuration with the defined properties.  Setup an Active Directory for Kerberos  To enable Kerberos on a cluster, perform these steps when creating your cluster via Cloudbreak UI.  Steps   In the  Create cluster  wizard, in the  Setup Network and Security  tab, check the  Enable security  option and select  Use Existing Active Directory .  Provide the following informstion:      Field  Description      Kerberos Password  The KDC admin password to use for the KDC.    Existing Kerberos Principal  The KDC principal in your existing MIT KDC.    Existing Kerberos URL  The location of your existing MIT KDC.    Use Tcp Connection  The connection type for your existing MIT KDC (default is  UDP ).    Existing Kerberos Realm  The realm in your existing MIT KDC.    Existing Kerberos Ldap AD Url  The url of the existing secure ldap (eg. ldaps://10.1.1.5).    Existing Kerberos AD Container DN  Active Directory User container for principals. For example \"OU=Hadoop,OU=People,dc=apache,dc=org\".", 
            "title": "Use Existing Active Directory with a Cloudbreak Provisioned Cluster"
        }, 
        {
            "location": "/security-kerberos/index.html#create-hadoop-users", 
            "text": "To create Hadoop users, follow these steps.  Steps    Log in via SSH to the node where the Ambari Server is (IP address is the same as the Ambari UI) and run:  \nkadmin -p [admin_user]/[admin_user]@NODE.DC1.CONSUL (type admin password)\naddprinc custom-user (type user password twice)    Log in via SSH to all other nodes and, on each node, run:  \nuseradd custom-user    Log in via SSH to one of the nodes and run:  \nsu custom-user\nkinit -p custom-user (type user password)\nhdfs dfs -mkdir input\nhdfs dfs -put /tmp/wait-for-host-number.sh input\nyarn jar $(find /usr/hdp -name hadoop-mapreduce-examples.jar) wordcount input output\nhdfs dfs -cat output/*", 
            "title": "Create Hadoop Users"
        }, 
        {
            "location": "/cli-install/index.html", 
            "text": "Install Cloudbreak CLI\n\n\nThe Cloudbreak Command Line Interface (CLI) is a tool to help you manage your Cloudbreak cluster instances. This tool can be used to interact with Cloudbreak for automating cluster creation, management, monitoring, and termination. \n\n\nThe CLI is available for Linux, Mac OS X, and Windows. \n\n\nInstall the CLI\n\n\nAfter you have launched Cloudbreak, the CLI is available for download from that Cloudbreak instance.\n\n\nSteps\n\n\n\n\nBrowse to your Cloudbreak instance and log in to the Cloubdreak web UI.  \n\n\nSelect \nDownload CLI\n from the navigation pane. \n\n\nSelect your operating system. The CLI is available for Linux, Mac OS X, and Windows:\n\n    \n  \n\n\nDownload the selected bundle to your local machine.  \n\n\nExtract the bundle.  \n\n\nYou can optionally add \ncb\n to your system path.\n\n\n\n\nRun the executable to verify the CLI: \n\n\ncb --version\n\n\n\n\n\n\nConfigure the CLI\n\n\nOnce you have installed the CLI, you need to configure the CLI to work with Cloudbreak.\n\n\nSteps\n\n\n\n\n\n\nUse the \ncb configure\n command to set up the CLI configuration file. The configuration options are:  \n\n\n\n\n--server\n server address [$CB_SERVER_ADDRESS]  \n\n\n--username\n user name (e-mail address) [$CB_USER_NAME]  \n\n\n--password\n password [$CB_PASSWORD]  \n\n\n\n\nThe password configuration is optional. If you do not provide the password, no password is stored in the CLI configuration file. Therefore, you will need to provide the password with each command you execute or via an environment variable.\n\n\nFor example:\n\n\ncb configure --server https://ec2-11-111-111-11.compute-1.amazonaws.com --username admin@hortonworks.com\n\n\n\n\n\n\nThe CLI configuration file will be saved at \n~/.cb/config\n. The content will look similar to the following:\n\n\ndefault:\n  username: admin@hortonworks.com\n  server: https://ec2-11-111-111-11.compute-1.amazonaws.com\n\n\n\n\n\n\nRun any command to verify that you can connect to the Cloudbreak instance via CLI. For example:\n\n\ncb list-clusters\n  \n\n\n\n\n\n\n\n    \nConfiguration Precedence\n\n    \n\n    The CLI can look for configuration options from different locations. You can optionally\n    pass the configuration options on each command or from environment variables. The following\n    order is used for the CLI to look for configuration options: \nCommand Line\n, \nEnvironment Variables\n\n    and the \nConfiguration File\n.\n    \n\n\n\n\n\nAdd Multiple Configurations\n\n\nIf you are using multiple profiles for multiple environments, you can configure them using the \ncb configure\n command and passing the name of your environment-specific profile file using the \n--profile\n parameter. After running the command, the configuration will be added as a new entry to the \nconfig\n file. For example, running the following command \ncb configure --server https://192.167.65.4 --username test@hortonworks.com --profile staging\n will add the \"staging\" entry:\n\n\ndefault\n  username: admin@hortonworks.com\n  server: https://192.167.65.4\nstaging\n  username: test@hortonworks.com\n  server: https://192.167.65.4  \n\n\n\n\nFor example:\n\n\n#cb configure --server https://192.167.65.4 --username test@hortonworks.com --profile staging\nINFO:  [writeConfigToFile] dir already exists: /Users/rkovacs/.cb\nINFO:  [writeConfigToFile] writing credentials to file: /Users/rkovacs/.cb/config\n# cat /Users/rkovacs/.cb/config\ndefault:\n  username: admin@example.com\n  server: https://192.167.65.4\n  output: table\nstaging:\n  username: test@hortonworks.com\n  server: https://192.167.65.4\n\n\n\nConfigure Default Output\n\n\nBy default, JSON format is used in command output. For example, if you run \ncb list-clusters\n without specifying output type, the output will be JSON. If you would like to change default output, add it to the config file. For example:\n\n\ndefault\n  username: admin@hortonworks.com\n  server: https://192.167.65.4\n  output: table\n\n\n\nGet Started with the CLI\n\n\nAfter \ninstalling\n and \nconfiguring\n the CLI, you can use it to perform the same tasks as are available in the Cloudbreak UI: create and manage clusters, credentials, blueprints, and recipes.\n\n\nSteps\n\n\n\n\n\n\nThe first step is to create at least one Cloudbreak credential using the \ncreate-credential\n command.   \n\n\n\n\n\n\nIf you are just getting started, you may want to try one of the default blueprints. You can use \nlist-blueprints\n and \ndescribe-blueprint\n to learn about those default blueprints.  \n\n\nIn case you would like to use your own blueprint, you can add it using \ncrate-blueprint\n command.\n\n\n\n\n\n\n(Optional) You can optionally create \nrecipes\n (custom scrpts) and add them using the \ncreate-recipe\n command.  \n\n\n\n\n\n\nTo create a cluster, first generate a JSON skeleton using the \ngenerate-cluster-template\n command. You can copy it to a text editor and save it in JSON format.\n\n\n\n\n\n\nEdit the JSON file by providing your parameters.  \n\n\n\n\n\n\nOnce your JSON file is ready, you can use it to create a cluster via the \ncreate-cluster\n command.\n\n\n\n\n\n\nOnce your cluster is running, use can use the CLI to manage and monitor your cluster:\n\n\n\n\nlist-clusters\n  \n\n\ndescribe-cluster\n  \n\n\nrepair-cluster\n  \n\n\nscale-cluster\n  \n\n\nstart-cluster\n  \n\n\nstop-cluster\n  \n\n\nsync-cluster\n  \n\n\nchange-ambari-passowrd\n  \n\n\n\n\n\n\n\n\nFor a full list of commands, refer to \nCLI Reference\n.    \n\n\nGet Help\n\n\nTo get CLI help, you can add help to the end of a command. The following will list help for the CLI at the top-level:\n\n\ncb help\n\n\n\nor \n\n\ncb h\n\n\n\nThe following will list help for the create-cluster command, including its command options and global options:\n\n\ncb create-cluster help\n\n\n\nor\n\n\ncb create-cluster h\n\n\n\n\n\nNext: CLI Reference", 
            "title": "Install the CLI"
        }, 
        {
            "location": "/cli-install/index.html#install-cloudbreak-cli", 
            "text": "The Cloudbreak Command Line Interface (CLI) is a tool to help you manage your Cloudbreak cluster instances. This tool can be used to interact with Cloudbreak for automating cluster creation, management, monitoring, and termination.   The CLI is available for Linux, Mac OS X, and Windows.", 
            "title": "Install Cloudbreak CLI"
        }, 
        {
            "location": "/cli-install/index.html#install-the-cli", 
            "text": "After you have launched Cloudbreak, the CLI is available for download from that Cloudbreak instance.  Steps   Browse to your Cloudbreak instance and log in to the Cloubdreak web UI.    Select  Download CLI  from the navigation pane.   Select your operating system. The CLI is available for Linux, Mac OS X, and Windows: \n         Download the selected bundle to your local machine.    Extract the bundle.    You can optionally add  cb  to your system path.   Run the executable to verify the CLI:   cb --version", 
            "title": "Install the CLI"
        }, 
        {
            "location": "/cli-install/index.html#configure-the-cli", 
            "text": "Once you have installed the CLI, you need to configure the CLI to work with Cloudbreak.  Steps    Use the  cb configure  command to set up the CLI configuration file. The configuration options are:     --server  server address [$CB_SERVER_ADDRESS]    --username  user name (e-mail address) [$CB_USER_NAME]    --password  password [$CB_PASSWORD]     The password configuration is optional. If you do not provide the password, no password is stored in the CLI configuration file. Therefore, you will need to provide the password with each command you execute or via an environment variable.  For example:  cb configure --server https://ec2-11-111-111-11.compute-1.amazonaws.com --username admin@hortonworks.com    The CLI configuration file will be saved at  ~/.cb/config . The content will look similar to the following:  default:\n  username: admin@hortonworks.com\n  server: https://ec2-11-111-111-11.compute-1.amazonaws.com    Run any command to verify that you can connect to the Cloudbreak instance via CLI. For example:  cb list-clusters       \n     Configuration Precedence \n     \n    The CLI can look for configuration options from different locations. You can optionally\n    pass the configuration options on each command or from environment variables. The following\n    order is used for the CLI to look for configuration options:  Command Line ,  Environment Variables \n    and the  Configuration File .", 
            "title": "Configure the CLI"
        }, 
        {
            "location": "/cli-install/index.html#add-multiple-configurations", 
            "text": "If you are using multiple profiles for multiple environments, you can configure them using the  cb configure  command and passing the name of your environment-specific profile file using the  --profile  parameter. After running the command, the configuration will be added as a new entry to the  config  file. For example, running the following command  cb configure --server https://192.167.65.4 --username test@hortonworks.com --profile staging  will add the \"staging\" entry:  default\n  username: admin@hortonworks.com\n  server: https://192.167.65.4\nstaging\n  username: test@hortonworks.com\n  server: https://192.167.65.4    For example:  #cb configure --server https://192.167.65.4 --username test@hortonworks.com --profile staging\nINFO:  [writeConfigToFile] dir already exists: /Users/rkovacs/.cb\nINFO:  [writeConfigToFile] writing credentials to file: /Users/rkovacs/.cb/config\n# cat /Users/rkovacs/.cb/config\ndefault:\n  username: admin@example.com\n  server: https://192.167.65.4\n  output: table\nstaging:\n  username: test@hortonworks.com\n  server: https://192.167.65.4", 
            "title": "Add Multiple Configurations"
        }, 
        {
            "location": "/cli-install/index.html#configure-default-output", 
            "text": "By default, JSON format is used in command output. For example, if you run  cb list-clusters  without specifying output type, the output will be JSON. If you would like to change default output, add it to the config file. For example:  default\n  username: admin@hortonworks.com\n  server: https://192.167.65.4\n  output: table", 
            "title": "Configure Default Output"
        }, 
        {
            "location": "/cli-install/index.html#get-started-with-the-cli", 
            "text": "After  installing  and  configuring  the CLI, you can use it to perform the same tasks as are available in the Cloudbreak UI: create and manage clusters, credentials, blueprints, and recipes.  Steps    The first step is to create at least one Cloudbreak credential using the  create-credential  command.       If you are just getting started, you may want to try one of the default blueprints. You can use  list-blueprints  and  describe-blueprint  to learn about those default blueprints.    In case you would like to use your own blueprint, you can add it using  crate-blueprint  command.    (Optional) You can optionally create  recipes  (custom scrpts) and add them using the  create-recipe  command.      To create a cluster, first generate a JSON skeleton using the  generate-cluster-template  command. You can copy it to a text editor and save it in JSON format.    Edit the JSON file by providing your parameters.      Once your JSON file is ready, you can use it to create a cluster via the  create-cluster  command.    Once your cluster is running, use can use the CLI to manage and monitor your cluster:   list-clusters     describe-cluster     repair-cluster     scale-cluster     start-cluster     stop-cluster     sync-cluster     change-ambari-passowrd        For a full list of commands, refer to  CLI Reference .", 
            "title": "Get Started with the CLI"
        }, 
        {
            "location": "/cli-install/index.html#get-help", 
            "text": "To get CLI help, you can add help to the end of a command. The following will list help for the CLI at the top-level:  cb help  or   cb h  The following will list help for the create-cluster command, including its command options and global options:  cb create-cluster help  or  cb create-cluster h   Next: CLI Reference", 
            "title": "Get Help"
        }, 
        {
            "location": "/cli-reference/index.html", 
            "text": "Cloudbreak CLI Reference\n\n\nThis section will help you get started with the Cloudbreak CLI after you have \ninstalled and configured it\n.\n\n\nCommand Structure\n\n\nThe CLI command can contain multiple parts. The first part is a set of global options. The next part is the command. The next part is a set of command options and arguments which could include sub-commands.\n\n\ncb [global options] command [command options] [arguments...]\n\n\n\nCommand Output\n\n\nYou can control the output from the CLI using the --output argument. The possible output formats include:\n\n\n\n\nJSON (\njson\n)\n\n\nYAML (\nyaml\n)\n\n\nFormatted table (\ntable\n)\n\n\n\n\nFor example:\n\n\ncb list-clusters --output json\n\n\n\ncb list-clusters --output yaml\n\n\n\ncb list-clusters --output table\n\n\n\nCommands\n\n\nGeneral:  \n\n\n\n\nconfigure\n  \n\n\n\n\nCredentials:  \n\n\n\n\ncreate-credential\n         \n\n\ndescribe-credential\n        \n\n\nlist-credentials\n         \n\n\ndelete-credential\n   \n\n\n\n\nBlueprints:   \n\n\n\n\ncreate-blueprint\n           \n\n\ndescribe-blueprint\n         \n\n\nlist-blueprints\n           \n\n\ndelete-blueprint\n   \n\n\n\n\nClusters: \n\n\n\n\ngenerate-cluster-template\n  \n\n\ncreate-cluster\n             \n\n\ndescribe-cluster\n           \n\n\nscale-cluster\n      \n\n\nstart-cluster\n        \n\n\nstop-cluster\n              \n\n\nsync-cluster\n             \n\n\nrepair-cluster\n            \n\n\nchange-ambari-password\n     \n\n\nlist-clusters\n             \n\n\ndelete-cluster\n  \n\n\n\n\nRecipes:  \n\n\n\n\ncreate-recipe\n              \n\n\ndescribe-recipe\n            \n\n\nlist-recipes\n              \n\n\ndelete-recipe\n               \n\n\n\n\n\n\nchange-ambari-password\n\n\nChanges Ambari password.\n\n\nRequired Options\n\n\n--name \nvalue\n  Cluster name\n\n\n--old-password \nvalue\n Old Ambari password \n\n\n--new-password \nvalue\n  New Ambari password  \n\n\n--ambari-user\n  Ambari user   \n\n\nOptions\n\n\n--server \nvalue\n  Server address [$CB_SERVER_ADDRESS]\n\n\n--username \nvalue\n  User name (e-mail address) [$CB_USER_NAME]\n\n\n--password \nvalue\n Password [$CB_PASSWORD]\n\n\n--profile \nvalue\n  Selects a config profile to use [$CB_PROFILE]\n\n\n--output \nvalue\n  Supported formats: json, yaml, table (default: \"json\") [$CB_OUT_FORMAT]\n\n\nExamples\n\n\nChanges password for Ambari user called \"admin\" for a cluster called \"test1234\":\n\n\ncb change-ambari-password --name test1234 --old-password 123456 --new-password Ambari123456 --ambari-user admin\n\n\n\n\n\nconfigure\n\n\nConfigures the Cloudbreak server address and credentials used to communicate with this server.\n\n\nRequired Options\n\n\n--server value\n  Server address [$CB_SERVER_ADDRESS]\n\n\n--username value\n   User name (e-mail address) [$CB_USER_NAME]  \n\n\nOptions\n\n\n--password value\n  Password [$CB_PASSWORD]\n\n\n--profile value\n  Select a config profile to use [$CB_PROFILE] \n\n\n--output value\n  Supported formats: json, yaml, table (default: \"json\") [$CB_OUT_FORMAT]  \n\n\nExamples\n\n\nThis example configures the server address with username and password:\n\n\ncb configure --server https://ec2-11-111-111-11.compute-1.amazonaws.com --username admin@hortonworks.com --password MySecurePassword123\n\n\n\nThis example configures the server address with username but without a password:\n\n\ncb configure --server https://ec2-11-111-111-11.compute-1.amazonaws.com --username admin@hortonworks.com\n\n\n\nRelated Links\n\n\nConfigure CLI\n\n\n\n\ncreate-blueprint\n\n\nAdds a new blueprint from a file or from a URL.\n\n\nCommands\n\n\nfrom-url\n Creates a blueprint by downloading it from a URL location\n\n\nfrom-file\n Creates a blueprint by reading it from a local file\n\n\nRequired Options\n\n\nfrom-url\n \n\n\n--name \nvalue\n Name of the blueprint\n\n\n--url \nvalue\n URL location of the Ambari blueprint JSON file\n\n\nfrom-file\n \n\n\n--name \nvalue\n Name of the blueprint\n\n\n--file \nvalue\n Location of the Ambari blueprint JSON file on the local machine\n\n\nOptions\n\n\n--description \nvalue\n  Description of the resource\n\n\n--public\n  Public in account\n\n\n--server \nvalue\n Server address [$CB_SERVER_ADDRESS]\n\n\n--username \nvalue\n User name (e-mail address) [$CB_USER_NAME]\n\n\n--password \nvalue\n Password [$CB_PASSWORD]\n\n\n--profile \nvalue\n Selects a config profile to use [$CB_PROFILE] \n\n\nExamples\n\n\nAdds a blueprint from a URL:\n\n\ncb create-blueprint from-url --url https://someurl.com/test.bp --name test1\n\n\n\nAdds a blueprint from a local file:\n\n\ncb create-blueprint from-file --file /Users/test/Documents/blueprints/test.bp --name test2\n\n\n\nRelated Links\n\n\nBlueprints\n\n\n\n\ncreate-cluster\n\n\nCreates a new cluster based on a JSON template.\n\n\nRequired Options\n\n\n--name \nvalue\n  Name for the cluster\n\n\n--cli-input-json \nvalue\n  User provided file in JSON format  \n\n\nOptions\n\n\n--description \nvalue\n  Description of resource \n\n\n--public\n  Public in account \n\n\n--input-json-param-password \nvalue\n  Password for the cluster and Ambari \n\n\n--wait\n  Wait for the operation to finish. No argument is required \n\n\n--server \nvalue\n   Server address   [$CB_SERVER_ADDRESS] \n\n\n--username \nvalue\n  User name (e-mail address) [$CB_USER_NAME]\n\n\n--password \nvalue\n  Password [$CB_PASSWORD]\n\n\n--profile \nvalue\n  Selects a config profile to use [$CB_PROFILE]  \n\n\nExamples\n\n\nCreates a cluster called \"testcluster\" based on a local JSON file called \"mytemplate.json\" located in the /Users/test/Documents directory:   \n\n\ncb create-cluster --name testcluster --cli-input-json /Users/test/Documents/mytemplate.json\n\n\n\nRelated Commands\n\n\ngenerate-cluster-template\n\n\n\n\ncreate-credential\n\n\nCreates a new Cloudbreak credential.\n\n\nCommands\n\n\naws role-based\n  Creates a new AWS credential\n\n\naws key-based\n  Creates a new AWS credential\n\n\nazure app-based\n Creates a new app-based Azure credential\n\n\ngcp\n Creates a new gcp credential\n\n\nopenstack keystone-v2\n Creates a new OpenStack credential\n\n\nopenstack keystone-v3\n Creates a new OpenStack credential \n\n\nRequired Options\n\n\naws role-based\n \n\n\n--name \nvalue\n  Credential name\n\n\n--role-arn \nvalue\n IAM Role ARN of the role used for Cloudbreak credential  \n\n\naws key-based\n \n\n\n--name \nvalue\n  Credential name\n\n\n--access-key \nvalue\n  AWS Access Key\n\n\n--secret-key \nvalue\n  AWS Secret Key  \n\n\nazure app-based\n \n\n\n--name \nvalue\n  Credential name\n\n\n--subscription-id \nvalue\n  Subscription ID from your Azure Subscriptions\n\n\n--tenant-id \nvalue\n  Directory ID from your Azure Active Directory \n Properties    \n\n\n--app-id \nvalue\n  Application ID of your app from your Azure Active Directory \n App Registrations          \n\n\n--app-password \nvalue\n  Your application key from app registration's Settings \n Keys  \n\n\ngcp\n \n\n\n--name \nvalue\n  Credential name \n\n\n--project-id \nvalue\n  Project ID from your GCP account                      \n\n\n--service-account-id \nvalue\n  Your GCP Service account ID from IAM \n Admin \n Service accounts               \n\n\n--service-account-private-key-file \nvalue\n  P12 key from your GCP service account  \n\n\nopenstack keystone-v2\n  \n\n\n--name \nvalue\n  Credential name\n\n\n--tenant-user \nvalue\n  OpenStack user name   \n\n\n--tenant-password \nvalue\n  OpenStack password\n\n\n--tenant-name \nvalue\n  OpenStack tenant name    \n\n\n--endpoint \nvalue\n   OpenStack endpoint   \n\n\nopenstack keystone-v3\n \n\n\n--name \nvalue\n  Credential name\n\n\n--tenant-user \nvalue\n  OpenStack user name   \n\n\n--tenant-password \nvalue\n  OpenStack password\n\n\n--user-domain \nvalue\n  OpenStack user domain    \n\n\n--endpoint \nvalue\n   OpenStack endpoint  \n\n\nOptions\n\n\n--description \nvalue\n  Description of the resource\n\n\n--public\n  Public in account\n\n\n--server \nvalue\n Server address [$CB_SERVER_ADDRESS]\n\n\n--username \nvalue\n User name (e-mail address) [$CB_USER_NAME]\n\n\n--password \nvalue\n Password [$CB_PASSWORD]\n\n\n--profile \nvalue\n Selects a config profile to use [$CB_PROFILE]  \n\n\nAdditionally, the following option is available for OpenStack Keystone2 and Keystone3:\n\n\n--facing \nvalue\n API facing. One of: public, admin, internal\n\n\nAdditionally, the following option is available for OpenStack Keystone3:\n\n\n--keystone-scope \nvalue\n OpenStack keystone scope. One of: default, domain, project \n\n\nExamples\n\n\nCreates a role-based credential on AWS:\n\n\ncb create-credential aws role-based --name my-credential1 --role-arn arn:aws:iam::517127065441:role/CredentialRole\n\n\n\nCreates a key-based credential on AWS:\n\n\ncb create-credential aws key-based --name my-credential2 --access-key ABDVIRDFV3K4HLJ45SKA --secret-key D89L5pOPM+426Rtj3curKzJEJL3lYoNcP8GvguBV\n\n\n\nCreates an app-based credential on Azure:\n\n\ncb create-credential azure app-based --name my-credential3 --subscription-id b8e7379e-568g-55d3-na82-45b8d421e998 --tenant-id  c79n5399-3231-65ba-8dgg-2g4e2a40085e --app-id 6d147d89-48d2-5de2-eef8-b89775bbfcg1 --app-password 4a8hBgfI52s/C8R5Sea2YHGnBFrD3fRONfdG8w7F2Ua=\n\n\n\nCreates a credential on Google Cloud:\n\n\ncb create-credential gcp --name my-credential4 --project-id test-proj --service-account-id test@test-proj.iam.gserviceaccount.com --service-account-private-key-file /Users/test/3fff57a6f68e.p12\n\n\n\nCreates a role-based credential on OpenStack with Keystone-v2:\n\n\ncb create-credential openstack keystone-v2 --name my-credential5 --tenant-user test --tenant-password MySecurePass123 --tenant-name test --endpoint http://openstack.test.organization.com:5000/v2.0\n\n\n\nRelated Links\n\n\nCreate Credential on AWS\n\n\nCreate Credential on Azure\n\n\nCreate Credential on GCP\n\n\nCreate Credential on OpenStack\n  \n\n\n\n\ndelete-blueprint\n\n\nDeletes an existing blueprint.\n\n\nRequired Options\n\n\n--name \nvalue\n Name of the blueprint \n\n\nOptions\n\n\n--server \nvalue\n Server address [$CB_SERVER_ADDRESS]\n\n\n--username \nvalue\n User name (e-mail address) [$CB_USER_NAME]\n\n\n--password \nvalue\n Password [$CB_PASSWORD]\n\n\n--profile \nvalue\n Selects a config profile to use [$CB_PROFILE] \n\n\nExamples\n\n\ncb delete-blueprint --name \"testbp\"\n\n\n\nRelated Commands\n\n\nlist-blueprints\n\n\n\n\ncreate-recipe\n\n\nAdds a new recipe from a file or from a URL.\n\n\nCommands\n\n\nfrom-url\n  Creates a recipe by downloading it from a URL location\n\n\nfrom-file\n  Creates a recipe by reading it from a local file  \n\n\nRequired Options\n\n\nfrom-url\n  \n\n\n--name \nvalue\n  Name for the recipe \n\n\n--execution-type \nvalue\n  Type of execution [pre, post]\n\n\n--url \nvalue\n  URL location of the Ambari blueprint JSON file  \n\n\nfrom-file\n \n\n\n--name \nvalue\n  Name for the recipe\n\n\n--execution-type \nvalue\n  Type of execution [pre, post]\n\n\n--file \nvalue\n  Location of the Ambari blueprint JSON file  \n\n\nOptions\n\n\n--description \nvalue\n  Description for the recipe \n\n\n--public\n   Public in account\n\n\n--server \nvalue\n  Server address [$CB_SERVER_ADDRESS]\n\n\n--username \nvalue\n  User name (e-mail address) [$CB_USER_NAME]\n\n\n--password \nvalue\n  Password [$CB_PASSWORD]\n\n\n--profile \nvalue\n  Selects a config profile to use [$CB_PROFILE]  \n\n\nExamples\n\n\nAdds a new recipe called \"test1\" from a URL:\n\n\ncb create-recipe from-url --name \"test1\" --execution post --url http://some-site.com/test.sh\n\n\n\nAdds a new recipe called \"test2\" from a file:\n\n\ncb create-recipe from-url --name \"test2\" --execution post --file /Users/test/Documents/test.sh\n\n\n\nRelated Links\n\n\nRecipes\n\n\n\n\ndelete-cluster\n\n\nDeletes an existing cluster. \n\n\nRequired Options\n\n\n--name \nvalue\n  Cluster name\n\n\nOptions\n\n\n--force\n  Force the operation\n\n\n--wait\n  Wait for the operation to finish. No argument is required\n\n\n--server \nvalue\n  Server address [$CB_SERVER_ADDRESS]\n\n\n--username \nvalue\n  User name (e-mail address) [$CB_USER_NAME]\n\n\n--password \nvalue\n Password [$CB_PASSWORD]\n\n\n--profile \nvalue\n  Selects a config profile to use [$CB_PROFILE]   \n\n\nExamples\n\n\ncb delete-cluster --name test1234\n\n\n\nRelated Commands\n\n\nlist-clusters\n\n\n\n\ndelete-credential\n\n\nDeletes an existing Cloudbreak credential.\n\n\nRequired Options\n\n\n--name \nvalue\n  Name of the credential \n\n\nOptions\n\n\n--server \nvalue\n Server address [$CB_SERVER_ADDRESS]\n\n\n--username \nvalue\n User name (e-mail address) [$CB_USER_NAME]\n\n\n--password \nvalue\n Password [$CB_PASSWORD]\n\n\n--profile \nvalue\n Selects a config profile to use [$CB_PROFILE] \n\n\nExamples\n\n\ncb delete-credential --name test-cred\n\n\n\nRelated Commands\n\n\nlist-credentials\n\n\n\n\ndelete-recipe\n\n\nDeletes an existing recipe.\n\n\nRequired Options\n\n\n--name \nvalue\n  Name for the recipe  \n\n\nOptions\n\n\n--server \nvalue\n  Server address [$CB_SERVER_ADDRESS]\n\n\n--username \nvalue\n  User name (e-mail address) [$CB_USER_NAME]\n\n\n--password \nvalue\n  Password [$CB_PASSWORD]\n\n\n--profile \nvalue\n  Selects a config profile to use [$CB_PROFILE]\n\n\n--output \nvalue\n  Supported formats: json, yaml, table (default: \"json\") [$CB_OUT_FORMAT]\n\n\nExamples\n\n\ncb delete-recipe --name test\n\n\n\nRelated Commands\n\n\nlist-recipes\n\n\n\n\ndescribe-blueprint\n\n\nDescribes an existing blueprint.\n\n\nRequired Options\n\n\n--name \nvalue\n Name of the blueprint \n\n\nOptions\n\n\n--server \nvalue\n Server address [$CB_SERVER_ADDRESS]\n\n\n--username \nvalue\n User name (e-mail address) [$CB_USER_NAME]\n\n\n--password \nvalue\n Password [$CB_PASSWORD]\n\n\n--profile \nvalue\n Selects a config profile to use [$CB_PROFILE]\n\n\n--output \nvalue\n  Supported formats: json, yaml, table (default: \"json\") [$CB_OUT_FORMAT]\n\n\nExamples\n\n\ncb describe-blueprint --name \"Data Science: Apache Spark 2.1, Apache Zeppelin 0.7.0\"\n{\n  \"Name\": \"Data Science: Apache Spark 2.1, Apache Zeppelin 0.7.0\",\n  \"Description\": \"Data Science: Apache Spark 2.1, Apache Zeppelin 0.7.0\",\n  \"HDPVersion\": \"2.6\",\n  \"HostgroupCount\": \"3\",\n  \"Tags\": \"DEFAULT\"\n}\n\n\n\nRelated Commands\n\n\nlist-blueprints\n\n\n\n\ndescribe-cluster\n\n\nDescribes an existing cluster.\n\n\nRequired Options\n\n\n--name \nvalue\n  Cluster name\n\n\nOptions\n\n\n--server \nvalue\n  Server address [$CB_SERVER_ADDRESS]\n\n\n--username \nvalue\n  User name (e-mail address) [$CB_USER_NAME]\n\n\n--password \nvalue\n  Password [$CB_PASSWORD]\n\n\n--profile \nvalue\n  Selects a config profile to use [$CB_PROFILE]\n\n\n--output \nvalue\n  Supported formats: json, yaml, table (default: \"json\") [$CB_OUT_FORMAT]  \n\n\nExamples\n\n\nReturns a JSON file describing an existing cluster called \"test1234\":\n\n\n./cb describe-cluster --name test1234\n\n\n\nThe command returns JSON output which due to space limitation was not captured in the example.\n\n\nRelated Commands\n\n\nlist-clusters\n\n\n\n\ndescribe-credential\n\n\nDescribes an existing credential.\n\n\nRequired Options\n\n\n--name \nvalue\n  Name of the credential \n\n\nOptions\n\n\n--server \nvalue\n Server address [$CB_SERVER_ADDRESS]\n\n\n--username \nvalue\n User name (e-mail address) [$CB_USER_NAME]\n\n\n--password \nvalue\n Password [$CB_PASSWORD]\n\n\n--profile \nvalue\n Selects a config profile to use [$CB_PROFILE]\n\n\n--output \nvalue\n  Supported formats: json, yaml, table (default: \"json\") [$CB_OUT_FORMAT]\n\n\nExamples\n\n\ncb describe-credential --name testcred\n{\n  \"Name\": \"testcred\",\n  \"Description\": \"\",\n  \"CloudPlatform\": \"AZURE\"\n}\n\n\n\nRelated Commands\n\n\nlist-credentials\n\n\n\n\ndescribe-recipe\n\n\nDescribes an existing recipe.\n\n\nRequired Options\n\n\n--name \nvalue\n  Name for the recipe    \n\n\nOptions\n\n\n--server \nvalue\n  Server address [$CB_SERVER_ADDRESS]\n\n\n--username \nvalue\n  User name (e-mail address) [$CB_USER_NAME]\n\n\n--password \nvalue\n  Password [$CB_PASSWORD]\n\n\n--profile \nvalue\n  Selects a config profile to use [$CB_PROFILE]\n\n\n--output \nvalue\n  Supported formats: json, yaml, table (default: \"json\") [$CB_OUT_FORMAT]\n\n\nExamples\n\n\nDescribes a recipe called \"test\":\n\n\ncb describe-recipe --name test\n{\n  \"Name\": \"test\",\n  \"Description\": \"\",\n  \"ExecutionType\": \"POST\"\n}\n\n\n\nDescribes a recipe called \"test\", with output presented in a table format:\n\n\ncb describe-recipe --name test --output table\n+------+-------------+----------------+\n| NAME | DESCRIPTION | EXECUTION TYPE |\n+------+-------------+----------------+\n| test |             | POST           |\n+------+-------------+----------------+\n\n\n\nRelated Commands\n\n\nlist-recipes\n\n\n\n\ngenerate-cluster-template\n\n\nGenerates a cluster template in JSON format.\n\n\nCommands\n\n\naws new-network\n Generates an AWS cluster JSON template with new network\n\n\naws existing-network\n Generates an AWS cluster JSON template with existing network\n\n\naws existing-subnet\n Generates an AWS cluster JSON template with existing network and subnet  \n\n\nazure new-network\n Generates an Azure cluster JSON template with new network\n\n\nazure existing-subnet\n Generates an Azure cluster JSON template with existing network and subnet  \n\n\ngcp new-network\n Generates an GCP cluster JSON template with new network\n\n\ngcp existing-network\n Generates an GCP cluster JSON template with existing network \n\n\ngcp existing-subnet\n Generates an GCP cluster JSON template with existing network and subnet\n\n\ngcp legacy-network\n Generates an GCP cluster JSON template with legacy network without subnets    \n\n\nopenstack new-network\n Generates an OS cluster JSON template with new network \n\n\nopenstack existing-network\n Generates an OS cluster JSON template with existing network \n\n\nopenstack existing-subnet\n Generates an OS cluster JSON template with existing network and subnet   \n\n\nRelated Commands\n\n\ncreate-cluster\n\n\n\n\nlist-blueprints\n\n\nLists available blueprints.\n\n\nRequired Options\n\n\nNome\n\n\nOptions\n\n\n--server \nvalue\n Server address [$CB_SERVER_ADDRESS]\n\n\n--username \nvalue\n User name (e-mail address) [$CB_USER_NAME]\n\n\n--password \nvalue\n Password [$CB_PASSWORD]\n\n\n--profile \nvalue\n Selects a config profile to use [$CB_PROFILE]\n\n\n--output \nvalue\n Supported formats: json, yaml, table (default: \"json\") [$CB_OUT_FORMAT]  \n\n\nExamples\n\n\ncb list-blueprints\n[\n  {\n    \"Name\": \"Data Science: Apache Spark 2.1, Apache Zeppelin 0.7.0\",\n    \"Description\": \"Data Science: Apache Spark 2.1, Apache Zeppelin 0.7.0\",\n    \"HDPVersion\": \"2.6\",\n    \"HostgroupCount\": \"3\",\n    \"Tags\": \"DEFAULT\"\n  },\n  {\n    \"Name\": \"EDW-ETL: Apache Hive 1.2.1, Apache Spark 2.1\",\n    \"Description\": \"EDW-ETL: Apache Hive 1.2.1, Apache Spark 2.1\",\n    \"HDPVersion\": \"2.6\",\n    \"HostgroupCount\": \"3\",\n    \"Tags\": \"DEFAULT\"\n  },\n  {\n    \"Name\": \"26EDW-ETL: Apache Hive 1.2.1, Apache Spark 1.6\",\n    \"Description\": \"26EDW-ETL: Apache Hive 1.2.1, Apache Spark 1.6\",\n    \"HDPVersion\": \"2.6\",\n    \"HostgroupCount\": \"3\",\n    \"Tags\": \"DEFAULT\"\n  },\n  {\n    \"Name\": \"Data Science: Apache Spark 1.6, Apache Zeppelin 0.7.0\",\n    \"Description\": \"Data Science: Apache Spark 1.6, Apache Zeppelin 0.7.0\",\n    \"HDPVersion\": \"2.6\",\n    \"HostgroupCount\": \"3\",\n    \"Tags\": \"DEFAULT\"\n  },\n  {\n    \"Name\": \"BI: Druid 0.9.2 (Technical Preview)\",\n    \"Description\": \"BI: Druid 0.9.2 (Technical Preview)\",\n    \"HDPVersion\": \"2.6\",\n    \"HostgroupCount\": \"3\",\n    \"Tags\": \"DEFAULT\"\n  },\n  {\n    \"Name\": \"EDW-Analytics: Apache Hive 2 LLAP, Apache Zeppelin 0.7.0\",\n    \"Description\": \"EDW-Analytics: Apache Hive 2 LLAP, Apache Zeppelin 0.7.0\",\n    \"HDPVersion\": \"2.6\",\n    \"HostgroupCount\": \"3\",\n    \"Tags\": \"DEFAULT\"\n  },\n  {\n    \"Name\": \"EDW-ETL: Apache Hive 1.2.1, Apache Spark 1.6\",\n    \"Description\": \"EDW-ETL: Apache Hive 1.2.1, Apache Spark 1.6\",\n    \"HDPVersion\": \"2.6\",\n    \"HostgroupCount\": \"3\",\n    \"Tags\": \"DEFAULT\"\n  }\n]\n\n\n\nRelated Commands\n\n\ndescribe-blueprint\n\n\n\n\nlist-clusters\n\n\nLists all clusters which are currently associated with the Cloudbreak instance.\n\n\nRequired Options\n\n\nNone\n\n\nOptions\n\n\n--server \nvalue\n  Server address [$CB_SERVER_ADDRESS]\n\n\n--username \nvalue\n  User name (e-mail address) [$CB_USER_NAME]\n\n\n--password \nvalue\n Password [$CB_PASSWORD]\n\n\n--profile \nvalue\n  Selects a config profile to use [$CB_PROFILE]\n\n\n--output \nvalue\n  Supported formats: json, yaml, table (default: \"json\") [$CB_OUT_FORMAT]\n\n\nExamples\n\n\nLists available clusters: \n\n\ncb list-clusters\n[\n  {\n    \"Name\": \"test1234\",\n    \"Description\": \"\",\n    \"CloudPlatform\": \"AZURE\",\n    \"StackStatus\": \"UPDATE_IN_PROGRESS\",\n    \"ClusterStatus\": \"REQUESTED\"\n  }\n]\n\n\n\nLists available clusters, with output in a table format:\n\n\ncb list-clusters --output table\n+----------+-------------+---------------+--------------------+---------------+\n|   NAME   | DESCRIPTION | CLOUDPLATFORM |    STACKSTATUS     | CLUSTERSTATUS |\n+----------+-------------+---------------+--------------------+---------------+\n| test1234 |             | AZURE         | UPDATE_IN_PROGRESS | REQUESTED     |\n+----------+-------------+---------------+--------------------+---------------+\n\n\n\nRelated Commands\n\n\ndescribe-cluster\n\n\n\n\nlist-credentials\n\n\nLists existing Cloudbreak credentials.\n\n\nRequired Options\n\n\nNone\n\n\nOptions\n\n\n--server \nvalue\n Server address [$CB_SERVER_ADDRESS]\n\n\n--username \nvalue\n User name (e-mail address) [$CB_USER_NAME]\n\n\n--password \nvalue\n Password [$CB_PASSWORD]\n\n\n--profile \nvalue\n Selects a config profile to use [$CB_PROFILE]\n\n\n--output \nvalue\n  Supported formats: json, yaml, table (default: \"json\") [$CB_OUT_FORMAT]\n\n\nExamples\n\n\nLists credentials:\n\n\ncb list-credentials\n[\n  {\n    \"Name\": \"testcred\",\n    \"Description\": \"\",\n    \"CloudPlatform\": \"AZURE\"\n  }\n]\n\n\n\n\nLists credentials, with output formatted in a table format:\n\n\ncb list-credentials --output table\n+---------+-------------+---------------+\n|  NAME   | DESCRIPTION | CLOUDPLATFORM |\n+---------+-------------+---------------+\n| armcred |             | AZURE         |\n+---------+-------------+---------------+\n\n\n\nRelated Commands\n\n\ndescribe-credential\n\n\n\n\nlist-recipes\n\n\nLists all available recipes.\n\n\nRequired Options\n\n\nNone\n\n\nOptions\n\n\n--server \nvalue\n  Server address [$CB_SERVER_ADDRESS]\n\n\n--username \nvalue\n  User name (e-mail address) [$CB_USER_NAME]\n\n\n--password \nvalue\n  Password [$CB_PASSWORD]\n\n\n--profile \nvalue\n  Selects a config profile to use [$CB_PROFILE]\n\n\n--output \nvalue\n  Supported formats: json, yaml, table (default: \"json\") [$CB_OUT_FORMAT]\n\n\nExamples\n\n\nLists existing recipes:\n\n\ncb list-recipes\n[\n  {\n    \"Name\": \"test\",\n    \"Description\": \"\",\n    \"ExecutionType\": \"POST\"\n  }\n]\n\n\n\nLists existing recipes, with output presented in a table format:\n\n\ncb list-recipes --output table\n+------+-------------+----------------+\n| NAME | DESCRIPTION | EXECUTION TYPE |\n+------+-------------+----------------+\n| test |             | POST           |\n+------+-------------+----------------+\n\n\n\n\nRelated Commands\n\n\ndescribe-recipe\n\n\n\n\nrepair-cluster\n\n\nRepairs a cluster if cluster installation failed.\n\n\nRequired Options\n\n\n--name \nvalue\n  Cluster name\n\n\nOptions\n\n\n--wait\n  Wait for the operation to finish. No argument is required\n\n\n--server \nvalue\n  Server address [$CB_SERVER_ADDRESS]\n\n\n--username \nvalue\n  User name (e-mail address) [$CB_USER_NAME]\n\n\n--password \nvalue\n Password [$CB_PASSWORD]\n\n\n--profile \nvalue\n  Selects a config profile to use [$CB_PROFILE]\n\n\n--output \nvalue\n  Supported formats: json, yaml, table (default: \"json\") [$CB_OUT_FORMAT]\n\n\nExamples\n\n\ncb repair-cluster --name test1234\n\n\n\nRelated Commands\n\n\nlist-clusters\n\n\n\n\nscale-cluster\n\n\nScales a cluster by adding or removing nodes.\n\n\nRequired Options\n\n\n--name \nvalue\n  Name of the cluster\n\n--group-name \nvalue\n  Name of the group to scale\n\n--desired-node-count \nvalue\n  Desired number of nodes\n\n\nOptions\n\n\n--wait\n  Wait for the operation to finish. No argument is required\n\n\n--server \nvalue\n  Server address [$CB_SERVER_ADDRESS]\n\n\n--username \nvalue\n  User name (e-mail address) [$CB_USER_NAME]\n\n\n--password \nvalue\n Password [$CB_PASSWORD]\n\n\n--profile \nvalue\n  Selects a config profile to use [$CB_PROFILE]\n\n\n--output \nvalue\n  Supported formats: json, yaml, table (default: \"json\") [$CB_OUT_FORMAT]  \n\n\nExamples\n\n\ncb scale-cluster --name test1234 --group-name worker --desired node-count 3\n\n\n\nRelated Commands\n\n\nlist-clusters\n\n\n\n\nstart-cluster\n\n\nStarts a cluster which has previously been stopped.\n\n\nRequired Options\n\n\n--name \nvalue\n  Cluster name\n\n\nOptions\n\n\n--wait\n  Wait for the operation to finish. No argument is required\n\n\n--server \nvalue\n  Server address [$CB_SERVER_ADDRESS]\n\n\n--username \nvalue\n  User name (e-mail address) [$CB_USER_NAME]\n\n\n--password \nvalue\n Password [$CB_PASSWORD]\n\n\n--profile \nvalue\n  Selects a config profile to use [$CB_PROFILE]\n\n\n--output \nvalue\n  Supported formats: json, yaml, table (default: \"json\") [$CB_OUT_FORMAT]\n\n\nExamples\n\n\ncb start-cluster --name test1234\n\n\n\nRelated Commands\n\n\nlist-clusters\n\n\n\n\nstop-cluster\n\n\nStops a cluster.\n\n\nRequired Options\n\n\n--name \nvalue\n  Cluster name\n\n\nOptions\n\n\n--wait\n  Wait for the operation to finish. No argument is required\n\n\n--server \nvalue\n  Server address [$CB_SERVER_ADDRESS]\n\n\n--username \nvalue\n  User name (e-mail address) [$CB_USER_NAME]\n\n\n--password \nvalue\n Password [$CB_PASSWORD]\n\n\n--profile \nvalue\n  Selects a config profile to use [$CB_PROFILE]\n\n\n--output \nvalue\n  Supported formats: json, yaml, table (default: \"json\") [$CB_OUT_FORMAT]\n\n\nExamples\n\n\ncb stop-cluster --name test1234\n\n\n\nRelated Commands\n\n\nlist-clusters\n\n\n\n\nsync-cluster\n\n\nSynchronizes a cluster with the cloud provider.\n\n\nRequired Options\n\n\n--name \nvalue\n  Cluster name\n\n\nOptions\n\n\n--server \nvalue\n  Server address [$CB_SERVER_ADDRESS]\n\n\n--username \nvalue\n  User name (e-mail address) [$CB_USER_NAME]\n\n\n--password \nvalue\n Password [$CB_PASSWORD]\n\n\n--profile \nvalue\n  Selects a config profile to use [$CB_PROFILE]\n\n\n--output \nvalue\n  Supported formats: json, yaml, table (default: \"json\") [$CB_OUT_FORMAT]\n\n\nExamples\n\n\ncb sync-cluster --name test1234\n\n\n\nDebugging\n\n\nTo use debugging mode, pass the \n--debug\n option. \n\n\nChecking CLI Version\n\n\nTo check CLI version, use \ncb --version\n.", 
            "title": "CLI Reference"
        }, 
        {
            "location": "/cli-reference/index.html#cloudbreak-cli-reference", 
            "text": "This section will help you get started with the Cloudbreak CLI after you have  installed and configured it .", 
            "title": "Cloudbreak CLI Reference"
        }, 
        {
            "location": "/cli-reference/index.html#command-structure", 
            "text": "The CLI command can contain multiple parts. The first part is a set of global options. The next part is the command. The next part is a set of command options and arguments which could include sub-commands.  cb [global options] command [command options] [arguments...]", 
            "title": "Command Structure"
        }, 
        {
            "location": "/cli-reference/index.html#command-output", 
            "text": "You can control the output from the CLI using the --output argument. The possible output formats include:   JSON ( json )  YAML ( yaml )  Formatted table ( table )   For example:  cb list-clusters --output json  cb list-clusters --output yaml  cb list-clusters --output table", 
            "title": "Command Output"
        }, 
        {
            "location": "/cli-reference/index.html#commands", 
            "text": "General:     configure      Credentials:     create-credential            describe-credential           list-credentials            delete-credential       Blueprints:      create-blueprint              describe-blueprint            list-blueprints              delete-blueprint       Clusters:    generate-cluster-template     create-cluster                describe-cluster              scale-cluster         start-cluster           stop-cluster                 sync-cluster                repair-cluster               change-ambari-password        list-clusters                delete-cluster      Recipes:     create-recipe                 describe-recipe               list-recipes                 delete-recipe", 
            "title": "Commands"
        }, 
        {
            "location": "/cli-reference/index.html#change-ambari-password", 
            "text": "Changes Ambari password.  Required Options  --name  value   Cluster name  --old-password  value  Old Ambari password   --new-password  value   New Ambari password    --ambari-user   Ambari user     Options  --server  value   Server address [$CB_SERVER_ADDRESS]  --username  value   User name (e-mail address) [$CB_USER_NAME]  --password  value  Password [$CB_PASSWORD]  --profile  value   Selects a config profile to use [$CB_PROFILE]  --output  value   Supported formats: json, yaml, table (default: \"json\") [$CB_OUT_FORMAT]  Examples  Changes password for Ambari user called \"admin\" for a cluster called \"test1234\":  cb change-ambari-password --name test1234 --old-password 123456 --new-password Ambari123456 --ambari-user admin", 
            "title": "change-ambari-password"
        }, 
        {
            "location": "/cli-reference/index.html#configure", 
            "text": "Configures the Cloudbreak server address and credentials used to communicate with this server.  Required Options  --server value   Server address [$CB_SERVER_ADDRESS]  --username value    User name (e-mail address) [$CB_USER_NAME]    Options  --password value   Password [$CB_PASSWORD]  --profile value   Select a config profile to use [$CB_PROFILE]   --output value   Supported formats: json, yaml, table (default: \"json\") [$CB_OUT_FORMAT]    Examples  This example configures the server address with username and password:  cb configure --server https://ec2-11-111-111-11.compute-1.amazonaws.com --username admin@hortonworks.com --password MySecurePassword123  This example configures the server address with username but without a password:  cb configure --server https://ec2-11-111-111-11.compute-1.amazonaws.com --username admin@hortonworks.com  Related Links  Configure CLI", 
            "title": "configure"
        }, 
        {
            "location": "/cli-reference/index.html#create-blueprint", 
            "text": "Adds a new blueprint from a file or from a URL.  Commands  from-url  Creates a blueprint by downloading it from a URL location  from-file  Creates a blueprint by reading it from a local file  Required Options  from-url    --name  value  Name of the blueprint  --url  value  URL location of the Ambari blueprint JSON file  from-file    --name  value  Name of the blueprint  --file  value  Location of the Ambari blueprint JSON file on the local machine  Options  --description  value   Description of the resource  --public   Public in account  --server  value  Server address [$CB_SERVER_ADDRESS]  --username  value  User name (e-mail address) [$CB_USER_NAME]  --password  value  Password [$CB_PASSWORD]  --profile  value  Selects a config profile to use [$CB_PROFILE]   Examples  Adds a blueprint from a URL:  cb create-blueprint from-url --url https://someurl.com/test.bp --name test1  Adds a blueprint from a local file:  cb create-blueprint from-file --file /Users/test/Documents/blueprints/test.bp --name test2  Related Links  Blueprints", 
            "title": "create-blueprint"
        }, 
        {
            "location": "/cli-reference/index.html#create-cluster", 
            "text": "Creates a new cluster based on a JSON template.  Required Options  --name  value   Name for the cluster  --cli-input-json  value   User provided file in JSON format    Options  --description  value   Description of resource   --public   Public in account   --input-json-param-password  value   Password for the cluster and Ambari   --wait   Wait for the operation to finish. No argument is required   --server  value    Server address   [$CB_SERVER_ADDRESS]   --username  value   User name (e-mail address) [$CB_USER_NAME]  --password  value   Password [$CB_PASSWORD]  --profile  value   Selects a config profile to use [$CB_PROFILE]    Examples  Creates a cluster called \"testcluster\" based on a local JSON file called \"mytemplate.json\" located in the /Users/test/Documents directory:     cb create-cluster --name testcluster --cli-input-json /Users/test/Documents/mytemplate.json  Related Commands  generate-cluster-template", 
            "title": "create-cluster"
        }, 
        {
            "location": "/cli-reference/index.html#create-credential", 
            "text": "Creates a new Cloudbreak credential.  Commands  aws role-based   Creates a new AWS credential  aws key-based   Creates a new AWS credential  azure app-based  Creates a new app-based Azure credential  gcp  Creates a new gcp credential  openstack keystone-v2  Creates a new OpenStack credential  openstack keystone-v3  Creates a new OpenStack credential   Required Options  aws role-based    --name  value   Credential name  --role-arn  value  IAM Role ARN of the role used for Cloudbreak credential    aws key-based    --name  value   Credential name  --access-key  value   AWS Access Key  --secret-key  value   AWS Secret Key    azure app-based    --name  value   Credential name  --subscription-id  value   Subscription ID from your Azure Subscriptions  --tenant-id  value   Directory ID from your Azure Active Directory   Properties      --app-id  value   Application ID of your app from your Azure Active Directory   App Registrations            --app-password  value   Your application key from app registration's Settings   Keys    gcp    --name  value   Credential name   --project-id  value   Project ID from your GCP account                        --service-account-id  value   Your GCP Service account ID from IAM   Admin   Service accounts                 --service-account-private-key-file  value   P12 key from your GCP service account    openstack keystone-v2     --name  value   Credential name  --tenant-user  value   OpenStack user name     --tenant-password  value   OpenStack password  --tenant-name  value   OpenStack tenant name      --endpoint  value    OpenStack endpoint     openstack keystone-v3    --name  value   Credential name  --tenant-user  value   OpenStack user name     --tenant-password  value   OpenStack password  --user-domain  value   OpenStack user domain      --endpoint  value    OpenStack endpoint    Options  --description  value   Description of the resource  --public   Public in account  --server  value  Server address [$CB_SERVER_ADDRESS]  --username  value  User name (e-mail address) [$CB_USER_NAME]  --password  value  Password [$CB_PASSWORD]  --profile  value  Selects a config profile to use [$CB_PROFILE]    Additionally, the following option is available for OpenStack Keystone2 and Keystone3:  --facing  value  API facing. One of: public, admin, internal  Additionally, the following option is available for OpenStack Keystone3:  --keystone-scope  value  OpenStack keystone scope. One of: default, domain, project   Examples  Creates a role-based credential on AWS:  cb create-credential aws role-based --name my-credential1 --role-arn arn:aws:iam::517127065441:role/CredentialRole  Creates a key-based credential on AWS:  cb create-credential aws key-based --name my-credential2 --access-key ABDVIRDFV3K4HLJ45SKA --secret-key D89L5pOPM+426Rtj3curKzJEJL3lYoNcP8GvguBV  Creates an app-based credential on Azure:  cb create-credential azure app-based --name my-credential3 --subscription-id b8e7379e-568g-55d3-na82-45b8d421e998 --tenant-id  c79n5399-3231-65ba-8dgg-2g4e2a40085e --app-id 6d147d89-48d2-5de2-eef8-b89775bbfcg1 --app-password 4a8hBgfI52s/C8R5Sea2YHGnBFrD3fRONfdG8w7F2Ua=  Creates a credential on Google Cloud:  cb create-credential gcp --name my-credential4 --project-id test-proj --service-account-id test@test-proj.iam.gserviceaccount.com --service-account-private-key-file /Users/test/3fff57a6f68e.p12  Creates a role-based credential on OpenStack with Keystone-v2:  cb create-credential openstack keystone-v2 --name my-credential5 --tenant-user test --tenant-password MySecurePass123 --tenant-name test --endpoint http://openstack.test.organization.com:5000/v2.0  Related Links  Create Credential on AWS  Create Credential on Azure  Create Credential on GCP  Create Credential on OpenStack", 
            "title": "create-credential"
        }, 
        {
            "location": "/cli-reference/index.html#delete-blueprint", 
            "text": "Deletes an existing blueprint.  Required Options  --name  value  Name of the blueprint   Options  --server  value  Server address [$CB_SERVER_ADDRESS]  --username  value  User name (e-mail address) [$CB_USER_NAME]  --password  value  Password [$CB_PASSWORD]  --profile  value  Selects a config profile to use [$CB_PROFILE]   Examples  cb delete-blueprint --name \"testbp\"  Related Commands  list-blueprints", 
            "title": "delete-blueprint"
        }, 
        {
            "location": "/cli-reference/index.html#create-recipe", 
            "text": "Adds a new recipe from a file or from a URL.  Commands  from-url   Creates a recipe by downloading it from a URL location  from-file   Creates a recipe by reading it from a local file    Required Options  from-url     --name  value   Name for the recipe   --execution-type  value   Type of execution [pre, post]  --url  value   URL location of the Ambari blueprint JSON file    from-file    --name  value   Name for the recipe  --execution-type  value   Type of execution [pre, post]  --file  value   Location of the Ambari blueprint JSON file    Options  --description  value   Description for the recipe   --public    Public in account  --server  value   Server address [$CB_SERVER_ADDRESS]  --username  value   User name (e-mail address) [$CB_USER_NAME]  --password  value   Password [$CB_PASSWORD]  --profile  value   Selects a config profile to use [$CB_PROFILE]    Examples  Adds a new recipe called \"test1\" from a URL:  cb create-recipe from-url --name \"test1\" --execution post --url http://some-site.com/test.sh  Adds a new recipe called \"test2\" from a file:  cb create-recipe from-url --name \"test2\" --execution post --file /Users/test/Documents/test.sh  Related Links  Recipes", 
            "title": "create-recipe"
        }, 
        {
            "location": "/cli-reference/index.html#delete-cluster", 
            "text": "Deletes an existing cluster.   Required Options  --name  value   Cluster name  Options  --force   Force the operation  --wait   Wait for the operation to finish. No argument is required  --server  value   Server address [$CB_SERVER_ADDRESS]  --username  value   User name (e-mail address) [$CB_USER_NAME]  --password  value  Password [$CB_PASSWORD]  --profile  value   Selects a config profile to use [$CB_PROFILE]     Examples  cb delete-cluster --name test1234  Related Commands  list-clusters", 
            "title": "delete-cluster"
        }, 
        {
            "location": "/cli-reference/index.html#delete-credential", 
            "text": "Deletes an existing Cloudbreak credential.  Required Options  --name  value   Name of the credential   Options  --server  value  Server address [$CB_SERVER_ADDRESS]  --username  value  User name (e-mail address) [$CB_USER_NAME]  --password  value  Password [$CB_PASSWORD]  --profile  value  Selects a config profile to use [$CB_PROFILE]   Examples  cb delete-credential --name test-cred  Related Commands  list-credentials", 
            "title": "delete-credential"
        }, 
        {
            "location": "/cli-reference/index.html#delete-recipe", 
            "text": "Deletes an existing recipe.  Required Options  --name  value   Name for the recipe    Options  --server  value   Server address [$CB_SERVER_ADDRESS]  --username  value   User name (e-mail address) [$CB_USER_NAME]  --password  value   Password [$CB_PASSWORD]  --profile  value   Selects a config profile to use [$CB_PROFILE]  --output  value   Supported formats: json, yaml, table (default: \"json\") [$CB_OUT_FORMAT]  Examples  cb delete-recipe --name test  Related Commands  list-recipes", 
            "title": "delete-recipe"
        }, 
        {
            "location": "/cli-reference/index.html#describe-blueprint", 
            "text": "Describes an existing blueprint.  Required Options  --name  value  Name of the blueprint   Options  --server  value  Server address [$CB_SERVER_ADDRESS]  --username  value  User name (e-mail address) [$CB_USER_NAME]  --password  value  Password [$CB_PASSWORD]  --profile  value  Selects a config profile to use [$CB_PROFILE]  --output  value   Supported formats: json, yaml, table (default: \"json\") [$CB_OUT_FORMAT]  Examples  cb describe-blueprint --name \"Data Science: Apache Spark 2.1, Apache Zeppelin 0.7.0\"\n{\n  \"Name\": \"Data Science: Apache Spark 2.1, Apache Zeppelin 0.7.0\",\n  \"Description\": \"Data Science: Apache Spark 2.1, Apache Zeppelin 0.7.0\",\n  \"HDPVersion\": \"2.6\",\n  \"HostgroupCount\": \"3\",\n  \"Tags\": \"DEFAULT\"\n}  Related Commands  list-blueprints", 
            "title": "describe-blueprint"
        }, 
        {
            "location": "/cli-reference/index.html#describe-cluster", 
            "text": "Describes an existing cluster.  Required Options  --name  value   Cluster name  Options  --server  value   Server address [$CB_SERVER_ADDRESS]  --username  value   User name (e-mail address) [$CB_USER_NAME]  --password  value   Password [$CB_PASSWORD]  --profile  value   Selects a config profile to use [$CB_PROFILE]  --output  value   Supported formats: json, yaml, table (default: \"json\") [$CB_OUT_FORMAT]    Examples  Returns a JSON file describing an existing cluster called \"test1234\":  ./cb describe-cluster --name test1234  The command returns JSON output which due to space limitation was not captured in the example.  Related Commands  list-clusters", 
            "title": "describe-cluster"
        }, 
        {
            "location": "/cli-reference/index.html#describe-credential", 
            "text": "Describes an existing credential.  Required Options  --name  value   Name of the credential   Options  --server  value  Server address [$CB_SERVER_ADDRESS]  --username  value  User name (e-mail address) [$CB_USER_NAME]  --password  value  Password [$CB_PASSWORD]  --profile  value  Selects a config profile to use [$CB_PROFILE]  --output  value   Supported formats: json, yaml, table (default: \"json\") [$CB_OUT_FORMAT]  Examples  cb describe-credential --name testcred\n{\n  \"Name\": \"testcred\",\n  \"Description\": \"\",\n  \"CloudPlatform\": \"AZURE\"\n}  Related Commands  list-credentials", 
            "title": "describe-credential"
        }, 
        {
            "location": "/cli-reference/index.html#describe-recipe", 
            "text": "Describes an existing recipe.  Required Options  --name  value   Name for the recipe      Options  --server  value   Server address [$CB_SERVER_ADDRESS]  --username  value   User name (e-mail address) [$CB_USER_NAME]  --password  value   Password [$CB_PASSWORD]  --profile  value   Selects a config profile to use [$CB_PROFILE]  --output  value   Supported formats: json, yaml, table (default: \"json\") [$CB_OUT_FORMAT]  Examples  Describes a recipe called \"test\":  cb describe-recipe --name test\n{\n  \"Name\": \"test\",\n  \"Description\": \"\",\n  \"ExecutionType\": \"POST\"\n}  Describes a recipe called \"test\", with output presented in a table format:  cb describe-recipe --name test --output table\n+------+-------------+----------------+\n| NAME | DESCRIPTION | EXECUTION TYPE |\n+------+-------------+----------------+\n| test |             | POST           |\n+------+-------------+----------------+  Related Commands  list-recipes", 
            "title": "describe-recipe"
        }, 
        {
            "location": "/cli-reference/index.html#generate-cluster-template", 
            "text": "Generates a cluster template in JSON format.  Commands  aws new-network  Generates an AWS cluster JSON template with new network  aws existing-network  Generates an AWS cluster JSON template with existing network  aws existing-subnet  Generates an AWS cluster JSON template with existing network and subnet    azure new-network  Generates an Azure cluster JSON template with new network  azure existing-subnet  Generates an Azure cluster JSON template with existing network and subnet    gcp new-network  Generates an GCP cluster JSON template with new network  gcp existing-network  Generates an GCP cluster JSON template with existing network   gcp existing-subnet  Generates an GCP cluster JSON template with existing network and subnet  gcp legacy-network  Generates an GCP cluster JSON template with legacy network without subnets      openstack new-network  Generates an OS cluster JSON template with new network   openstack existing-network  Generates an OS cluster JSON template with existing network   openstack existing-subnet  Generates an OS cluster JSON template with existing network and subnet     Related Commands  create-cluster", 
            "title": "generate-cluster-template"
        }, 
        {
            "location": "/cli-reference/index.html#list-blueprints", 
            "text": "Lists available blueprints.  Required Options  Nome  Options  --server  value  Server address [$CB_SERVER_ADDRESS]  --username  value  User name (e-mail address) [$CB_USER_NAME]  --password  value  Password [$CB_PASSWORD]  --profile  value  Selects a config profile to use [$CB_PROFILE]  --output  value  Supported formats: json, yaml, table (default: \"json\") [$CB_OUT_FORMAT]    Examples  cb list-blueprints\n[\n  {\n    \"Name\": \"Data Science: Apache Spark 2.1, Apache Zeppelin 0.7.0\",\n    \"Description\": \"Data Science: Apache Spark 2.1, Apache Zeppelin 0.7.0\",\n    \"HDPVersion\": \"2.6\",\n    \"HostgroupCount\": \"3\",\n    \"Tags\": \"DEFAULT\"\n  },\n  {\n    \"Name\": \"EDW-ETL: Apache Hive 1.2.1, Apache Spark 2.1\",\n    \"Description\": \"EDW-ETL: Apache Hive 1.2.1, Apache Spark 2.1\",\n    \"HDPVersion\": \"2.6\",\n    \"HostgroupCount\": \"3\",\n    \"Tags\": \"DEFAULT\"\n  },\n  {\n    \"Name\": \"26EDW-ETL: Apache Hive 1.2.1, Apache Spark 1.6\",\n    \"Description\": \"26EDW-ETL: Apache Hive 1.2.1, Apache Spark 1.6\",\n    \"HDPVersion\": \"2.6\",\n    \"HostgroupCount\": \"3\",\n    \"Tags\": \"DEFAULT\"\n  },\n  {\n    \"Name\": \"Data Science: Apache Spark 1.6, Apache Zeppelin 0.7.0\",\n    \"Description\": \"Data Science: Apache Spark 1.6, Apache Zeppelin 0.7.0\",\n    \"HDPVersion\": \"2.6\",\n    \"HostgroupCount\": \"3\",\n    \"Tags\": \"DEFAULT\"\n  },\n  {\n    \"Name\": \"BI: Druid 0.9.2 (Technical Preview)\",\n    \"Description\": \"BI: Druid 0.9.2 (Technical Preview)\",\n    \"HDPVersion\": \"2.6\",\n    \"HostgroupCount\": \"3\",\n    \"Tags\": \"DEFAULT\"\n  },\n  {\n    \"Name\": \"EDW-Analytics: Apache Hive 2 LLAP, Apache Zeppelin 0.7.0\",\n    \"Description\": \"EDW-Analytics: Apache Hive 2 LLAP, Apache Zeppelin 0.7.0\",\n    \"HDPVersion\": \"2.6\",\n    \"HostgroupCount\": \"3\",\n    \"Tags\": \"DEFAULT\"\n  },\n  {\n    \"Name\": \"EDW-ETL: Apache Hive 1.2.1, Apache Spark 1.6\",\n    \"Description\": \"EDW-ETL: Apache Hive 1.2.1, Apache Spark 1.6\",\n    \"HDPVersion\": \"2.6\",\n    \"HostgroupCount\": \"3\",\n    \"Tags\": \"DEFAULT\"\n  }\n]  Related Commands  describe-blueprint", 
            "title": "list-blueprints"
        }, 
        {
            "location": "/cli-reference/index.html#list-clusters", 
            "text": "Lists all clusters which are currently associated with the Cloudbreak instance.  Required Options  None  Options  --server  value   Server address [$CB_SERVER_ADDRESS]  --username  value   User name (e-mail address) [$CB_USER_NAME]  --password  value  Password [$CB_PASSWORD]  --profile  value   Selects a config profile to use [$CB_PROFILE]  --output  value   Supported formats: json, yaml, table (default: \"json\") [$CB_OUT_FORMAT]  Examples  Lists available clusters:   cb list-clusters\n[\n  {\n    \"Name\": \"test1234\",\n    \"Description\": \"\",\n    \"CloudPlatform\": \"AZURE\",\n    \"StackStatus\": \"UPDATE_IN_PROGRESS\",\n    \"ClusterStatus\": \"REQUESTED\"\n  }\n]  Lists available clusters, with output in a table format:  cb list-clusters --output table\n+----------+-------------+---------------+--------------------+---------------+\n|   NAME   | DESCRIPTION | CLOUDPLATFORM |    STACKSTATUS     | CLUSTERSTATUS |\n+----------+-------------+---------------+--------------------+---------------+\n| test1234 |             | AZURE         | UPDATE_IN_PROGRESS | REQUESTED     |\n+----------+-------------+---------------+--------------------+---------------+  Related Commands  describe-cluster", 
            "title": "list-clusters"
        }, 
        {
            "location": "/cli-reference/index.html#list-credentials", 
            "text": "Lists existing Cloudbreak credentials.  Required Options  None  Options  --server  value  Server address [$CB_SERVER_ADDRESS]  --username  value  User name (e-mail address) [$CB_USER_NAME]  --password  value  Password [$CB_PASSWORD]  --profile  value  Selects a config profile to use [$CB_PROFILE]  --output  value   Supported formats: json, yaml, table (default: \"json\") [$CB_OUT_FORMAT]  Examples  Lists credentials:  cb list-credentials\n[\n  {\n    \"Name\": \"testcred\",\n    \"Description\": \"\",\n    \"CloudPlatform\": \"AZURE\"\n  }\n]  Lists credentials, with output formatted in a table format:  cb list-credentials --output table\n+---------+-------------+---------------+\n|  NAME   | DESCRIPTION | CLOUDPLATFORM |\n+---------+-------------+---------------+\n| armcred |             | AZURE         |\n+---------+-------------+---------------+  Related Commands  describe-credential", 
            "title": "list-credentials"
        }, 
        {
            "location": "/cli-reference/index.html#list-recipes", 
            "text": "Lists all available recipes.  Required Options  None  Options  --server  value   Server address [$CB_SERVER_ADDRESS]  --username  value   User name (e-mail address) [$CB_USER_NAME]  --password  value   Password [$CB_PASSWORD]  --profile  value   Selects a config profile to use [$CB_PROFILE]  --output  value   Supported formats: json, yaml, table (default: \"json\") [$CB_OUT_FORMAT]  Examples  Lists existing recipes:  cb list-recipes\n[\n  {\n    \"Name\": \"test\",\n    \"Description\": \"\",\n    \"ExecutionType\": \"POST\"\n  }\n]  Lists existing recipes, with output presented in a table format:  cb list-recipes --output table\n+------+-------------+----------------+\n| NAME | DESCRIPTION | EXECUTION TYPE |\n+------+-------------+----------------+\n| test |             | POST           |\n+------+-------------+----------------+  Related Commands  describe-recipe", 
            "title": "list-recipes"
        }, 
        {
            "location": "/cli-reference/index.html#repair-cluster", 
            "text": "Repairs a cluster if cluster installation failed.  Required Options  --name  value   Cluster name  Options  --wait   Wait for the operation to finish. No argument is required  --server  value   Server address [$CB_SERVER_ADDRESS]  --username  value   User name (e-mail address) [$CB_USER_NAME]  --password  value  Password [$CB_PASSWORD]  --profile  value   Selects a config profile to use [$CB_PROFILE]  --output  value   Supported formats: json, yaml, table (default: \"json\") [$CB_OUT_FORMAT]  Examples  cb repair-cluster --name test1234  Related Commands  list-clusters", 
            "title": "repair-cluster"
        }, 
        {
            "location": "/cli-reference/index.html#scale-cluster", 
            "text": "Scales a cluster by adding or removing nodes.  Required Options  --name  value   Name of the cluster --group-name  value   Name of the group to scale --desired-node-count  value   Desired number of nodes  Options  --wait   Wait for the operation to finish. No argument is required  --server  value   Server address [$CB_SERVER_ADDRESS]  --username  value   User name (e-mail address) [$CB_USER_NAME]  --password  value  Password [$CB_PASSWORD]  --profile  value   Selects a config profile to use [$CB_PROFILE]  --output  value   Supported formats: json, yaml, table (default: \"json\") [$CB_OUT_FORMAT]    Examples  cb scale-cluster --name test1234 --group-name worker --desired node-count 3  Related Commands  list-clusters", 
            "title": "scale-cluster"
        }, 
        {
            "location": "/cli-reference/index.html#start-cluster", 
            "text": "Starts a cluster which has previously been stopped.  Required Options  --name  value   Cluster name  Options  --wait   Wait for the operation to finish. No argument is required  --server  value   Server address [$CB_SERVER_ADDRESS]  --username  value   User name (e-mail address) [$CB_USER_NAME]  --password  value  Password [$CB_PASSWORD]  --profile  value   Selects a config profile to use [$CB_PROFILE]  --output  value   Supported formats: json, yaml, table (default: \"json\") [$CB_OUT_FORMAT]  Examples  cb start-cluster --name test1234  Related Commands  list-clusters", 
            "title": "start-cluster"
        }, 
        {
            "location": "/cli-reference/index.html#stop-cluster", 
            "text": "Stops a cluster.  Required Options  --name  value   Cluster name  Options  --wait   Wait for the operation to finish. No argument is required  --server  value   Server address [$CB_SERVER_ADDRESS]  --username  value   User name (e-mail address) [$CB_USER_NAME]  --password  value  Password [$CB_PASSWORD]  --profile  value   Selects a config profile to use [$CB_PROFILE]  --output  value   Supported formats: json, yaml, table (default: \"json\") [$CB_OUT_FORMAT]  Examples  cb stop-cluster --name test1234  Related Commands  list-clusters", 
            "title": "stop-cluster"
        }, 
        {
            "location": "/cli-reference/index.html#sync-cluster", 
            "text": "Synchronizes a cluster with the cloud provider.  Required Options  --name  value   Cluster name  Options  --server  value   Server address [$CB_SERVER_ADDRESS]  --username  value   User name (e-mail address) [$CB_USER_NAME]  --password  value  Password [$CB_PASSWORD]  --profile  value   Selects a config profile to use [$CB_PROFILE]  --output  value   Supported formats: json, yaml, table (default: \"json\") [$CB_OUT_FORMAT]  Examples  cb sync-cluster --name test1234", 
            "title": "sync-cluster"
        }, 
        {
            "location": "/cli-reference/index.html#debugging", 
            "text": "To use debugging mode, pass the  --debug  option.", 
            "title": "Debugging"
        }, 
        {
            "location": "/cli-reference/index.html#checking-cli-version", 
            "text": "To check CLI version, use  cb --version .", 
            "title": "Checking CLI Version"
        }, 
        {
            "location": "/trouble-cb/index.html", 
            "text": "Troubleshooting Cloudbreak\n\n\nChecking the Logs\n\n\nWhen troubleshooting, you can access the following Cloudbreak logs.\n\n\nCloudbreak Logs\n\n\nWhen installing Cloudbreak using a pre-built cloud image, the  Cloudbreak Deployer location and the cbd root folder is \n/var/lib/cloudbreak-deployment\n. You must execute all cbd actions from the cbd root folder as a cloudbreak user. \n\n\n\n\nYour cbd root directory may be different if you installed Cloudbreak on your own VM. \n\n\n\n\nCloudbreak consists of multiple microservices deployed into Docker containers. \n\n\nAggregated Logs\n\n\nTo check aggregated service logs, use the following commands:\n\n\ncbd logs\n shows all service logs.\n\n\ncbd logs | tee cloudbreak.log\n allows you to redirect the input into a file for sharing these logs.\n\n\nIndividual Service Logs\n\n\nTo check individual service logs, use the following commands:\n\n\ncbd logs cloudbreak\n shows Cloudbreak logs. This service is the backend service that handles all deployments.\n\n\ncbd logs uluwatu\n shows Cloudbreak UI logs. Uluwatu is the UI component of Cloudbreak.\n\n\ncbd logs identity\n shows Identity logs. Identity is responsible for authentication and authorization.\n\n\ncbd logs periscope\n shows Periscope logs. Periscope is responsible for triggering autoscaling rules.\n\n\nDocker Logs\n\n\nThe same logs can be accessed via Docker commands:\n\n\ndocker logs cbreak_cloudbreak_1\n shows the same logs as \ncbd logs cloudbreak\n.\n\n\nCloudbreak logs are rotated and can be accessed later from the Cloudbreak deployment folder. Each time you restart the application via cbd restart a new log file is created with a timestamp in the name (for example, cbreak-20170821-105900.log). \n\n\n\n\nThere is a symlink called \ncbreak.log\n which points to the latest log file. Sharing this symlink does not share the log itself.\n\n\n\n\nSaltstack Logs\n\n\nCloudbreak uses Saltstack to install Ambari and the necessary packages for the HDP provisioning. Salt Master always runs alongside the Ambari Server node. Each instance in the cluster runs a Salt Minion, which connects to the Salt Master. There can be multiple Salt Masters if the cluster is configured to run in HA (High Availability) mode and in this case each Salt Minion connects to each Salt Master.\n\n\nCloudbreak also uses SaltStack to execute user-provided customization scripts called \"recipes\". \n\n\nSalt Master and Salt Minion logs can be found at the following location: \n/var/log/salt\n\n\nAmbari Logs\n\n\nCloudbreak uses Ambari to orchestrate the installation of the different HDP components. Each instance in the cluster runs an Ambari agent which connects to the Ambari server. Ambari server is declared by the user during the cluster installation wizard. \n\n\nAmbari Server Logs\n\n\nAmbari server logs can be found on the nodes where Ambari server is installed in the following locations:\n\n\n/var/log/ambari-server/ambari-server.log\n\n\n/var/log/ambari-server/ambari-server.out\n\n\nBoth files contain important information about the root cause of a certain issue so it is advised to check both.\n\n\nAmbari Agent Logs\n\n\nAmbari agent logs can be found on the nodes where Ambari agent is installed in the following locations:\n\n\n/var/log/ambari-agent/ambari-agent.log\n\n\nRecipe Logs\n\n\nCloudbreak supports \"recipes\" - user-provided customization scripts that can be run prior to or after cluster installtion. It is the user\u2019s responsibility to provide an idempotent well tested script. If the execution fails, the recipe logs can be found at \n/var/log/recipes\n on the nodes on which the recipes were executed.\n\n\nIt is advised, but not required to have an advanced logging mechanism in the script, as Cloudbreak always logs every script that are run. Recipes are often the sources of installation failures as users might try to remove necessary packages or reconfigure services.\n\n\nCommon Errors\n\n\nQuota Limitations\n\n\nEach cloud provider has quota limitations on various cloud resources, and these quotas can usually be increased on request. If there is an error message in Cloudbreak saying that there are no more available EIPs (Elastic IP Address) or VPCs, you need to request more of these resources. \n\n\nTo see the limitations visit the cloud provider\u2019s site:\n\n\n\n\nAWS Service Limits\n \n\n\nAzure subscription and service limits, quotas, and constraints\n\n\nGCP Resource Quotas\n \n\n\n\n\nConnection Timeout: Ports Not Open\n\n\nIn the cluster installation wizard, you must specify on which node you want to run the Ambari server. Cloudbreak communicates with this node to orchestrate the installation.\n\n\nA common reason for connection timeout is security group misconfiguration. Cloudbreak allows configuring different security groups for the different instance groups; however, there are certain requirements for the Ambari server node. Specifically, the following ports must be open in order to communicate with that node:\n\n\n\n\n22 (SSH)  \n\n\n9443 (two-way-ssl through nginx) \n\n\n\n\nBlueprints: Invalid Services and Configurations\n\n\nAmbari blueprints are a declarative definition of a cluster. With a blueprint, you specify a stack, the component layout, and the configurations to materialize a Hadoop cluster instance via a REST API without having to use the Ambari cluster install wizard. \n\n\nCloudbreak supports any type of blueprints, which is a common source of errors. These errors are only visible once the core infrastructure is up and running and Cloudbreak tries to initiate the cluster installation through Ambari. Ambari validates the blueprint and  rejects it if it's invalid. \n\n\nFor example, if there are configurations for a certain service like Hive but Hive as a service is not mapped to any host group, the blueprint is invalid.\n\n\nTo fix these type of issues, edit your blueprint and then reinstall your cluster. Cloudbreak UI has support for this so the infrastructure does not have to be terminated.\n\n\nThere are some cases when Ambari cannot validate your blueprint beforehand. In these cases, the issues are only visible in the Ambari server logs. To trubleshoot, check Ambari server logs.\n\n\nBlueprints: High Availability\n\n\nCloudbreak always tries to validate that a blueprint not to include multiple master services into different host groups. However, this exact setup is required for HA clusters. To overcome this, you can disable blueprint validation in the UI (using an advanced option in the Create Cluster wizard \n Choose Blueprint), but you must include the necessary configurations.\n\n\nBlueprints: Wrong HDP Version\n\n\nIn the blueprint, only the major and minor HDP version should be defined (for example, \"2.6\"). If wrong version number is provided, the following error can be found in the logs:\n\n\n5/15/2017 12:23:19 PM testcluster26 - create failed: Cannot use the specified Ambari stack: HDPRepo\n{stack='null'; utils='null'}\n. Error: org.apache.ambari.server.controller.spi.NoSuchResourceException: The specified resource doesn't exist: Stack data, Stack HDP 2.6.0.3 is not found in Ambari metainfo\n\n\n\n\nFor correct blueprint layout, refer to the \nAmbari cwiki\n page.\n\n\nRecipes: Recipe Execution Times Out\n\n\nIf the scripts are taking too much time to execute, the processes will time out, as the threshold for all recipes is set to 15 minutes. To change this threshold, you must override the default value by adding the following to the cbd Profile file:\n\n\nexport CB_JAVA_OPTS=\u201d -Dcb.max.salt.recipe.execution.retry=90\u201d\n\n\n\n\nThis property indicates the number of tries for checking if the scripts have finished with a sleep time (i.e. the wait time between two polling attempts) of 10 seconds. The default value is 90. To increase the threshold, provide a number greater than 90. You must restart Cloudbreak after changing properties in the Profile file.\n\n\nRecipes: Recipe Execution Fails\n\n\nIt often happens that a script cannot be executed successfully because there are typos or errors in the script. To verify this you can check the recipe logs at\n\n/var/log/recipes\n. For each script, there will be a separate log file with the name of the script that you provided on the Cloudbreak UI.\n\n\nInvalid PUBLIC_IP in CBD Profile\n\n\nThe \nPUBLIC_IP\n property must be set in the cbd Profile file or else you won\u2019t be able to log in on the Cloudbreak UI. \n\n\nIf you are migrating your instance, make sure that after the start the IP remains valid. If you need to edit the \nPUBLIC_IP\n property in Profile, make sure to restart Cloudbreak using \ncbd restart\n.\n\n\nCbd Cannot Get VM's Public IP\n\n\nBy default the \ncbd\n tool tries to get the VM's public IP to bind Cloudbreak UI to it. But if \ncbd\n cannot get the IP address during the initialization, you must set it manually. Check your \nProfile\n and if \nPUBLIC_IP\n is not set, add the \nPUBLIC_IP\n variable and set it to the public IP of the VM. For example: \n\n\nexport PUBLIC_IP=192.134.23.10\n\n\n\nPermission or Connection Problems\n\n\nIf you face permission or connection issues, disable SELinux:\n\n\n\n\nSet \nSELINUX=disabled\n in \n/etc/selinux/config\n.  \n\n\nReboot the machine.  \n\n\n\n\nEnsure the SELinux is not turned on afterwards:\n\n\n\n\n\n\n\n\nChanging Properties in the Cloudbreak Profile File\n\n\nThere are many properties that can be changed in the Cloudbreak application. These values must be changed in the Cloudbreak \nProfil\ne file. To see all possible options, use the following command:\n\ncbd env show\n.\n\n\nAfter changing a property, you must regenerate the config file and restart the application. There are two ways to do this:\n\n\nIn version 1.4.0 and newer of the cbd command line, you can regenerate the config file and restart the application with a single command:\n\n\ncbd restart\n - same as cbd regenerate/kill/start.\n\n\nIn versions earlier than 1.4.0, you must run the following three commands:\n\n\ncbd regenerate\n regenerates the Docker compose file\n\ncbd kill\n removes all Docker containers (there is no stop command for this).\n\ncbd start\n starts the application with the new compose file.", 
            "title": "General Troubleshooting"
        }, 
        {
            "location": "/trouble-cb/index.html#troubleshooting-cloudbreak", 
            "text": "", 
            "title": "Troubleshooting Cloudbreak"
        }, 
        {
            "location": "/trouble-cb/index.html#checking-the-logs", 
            "text": "When troubleshooting, you can access the following Cloudbreak logs.", 
            "title": "Checking the Logs"
        }, 
        {
            "location": "/trouble-cb/index.html#cloudbreak-logs", 
            "text": "When installing Cloudbreak using a pre-built cloud image, the  Cloudbreak Deployer location and the cbd root folder is  /var/lib/cloudbreak-deployment . You must execute all cbd actions from the cbd root folder as a cloudbreak user.    Your cbd root directory may be different if you installed Cloudbreak on your own VM.    Cloudbreak consists of multiple microservices deployed into Docker containers.   Aggregated Logs  To check aggregated service logs, use the following commands:  cbd logs  shows all service logs.  cbd logs | tee cloudbreak.log  allows you to redirect the input into a file for sharing these logs.  Individual Service Logs  To check individual service logs, use the following commands:  cbd logs cloudbreak  shows Cloudbreak logs. This service is the backend service that handles all deployments.  cbd logs uluwatu  shows Cloudbreak UI logs. Uluwatu is the UI component of Cloudbreak.  cbd logs identity  shows Identity logs. Identity is responsible for authentication and authorization.  cbd logs periscope  shows Periscope logs. Periscope is responsible for triggering autoscaling rules.  Docker Logs  The same logs can be accessed via Docker commands:  docker logs cbreak_cloudbreak_1  shows the same logs as  cbd logs cloudbreak .  Cloudbreak logs are rotated and can be accessed later from the Cloudbreak deployment folder. Each time you restart the application via cbd restart a new log file is created with a timestamp in the name (for example, cbreak-20170821-105900.log).    There is a symlink called  cbreak.log  which points to the latest log file. Sharing this symlink does not share the log itself.", 
            "title": "Cloudbreak Logs"
        }, 
        {
            "location": "/trouble-cb/index.html#saltstack-logs", 
            "text": "Cloudbreak uses Saltstack to install Ambari and the necessary packages for the HDP provisioning. Salt Master always runs alongside the Ambari Server node. Each instance in the cluster runs a Salt Minion, which connects to the Salt Master. There can be multiple Salt Masters if the cluster is configured to run in HA (High Availability) mode and in this case each Salt Minion connects to each Salt Master.  Cloudbreak also uses SaltStack to execute user-provided customization scripts called \"recipes\".   Salt Master and Salt Minion logs can be found at the following location:  /var/log/salt", 
            "title": "Saltstack Logs"
        }, 
        {
            "location": "/trouble-cb/index.html#ambari-logs", 
            "text": "Cloudbreak uses Ambari to orchestrate the installation of the different HDP components. Each instance in the cluster runs an Ambari agent which connects to the Ambari server. Ambari server is declared by the user during the cluster installation wizard.   Ambari Server Logs  Ambari server logs can be found on the nodes where Ambari server is installed in the following locations:  /var/log/ambari-server/ambari-server.log  /var/log/ambari-server/ambari-server.out  Both files contain important information about the root cause of a certain issue so it is advised to check both.  Ambari Agent Logs  Ambari agent logs can be found on the nodes where Ambari agent is installed in the following locations:  /var/log/ambari-agent/ambari-agent.log", 
            "title": "Ambari Logs"
        }, 
        {
            "location": "/trouble-cb/index.html#recipe-logs", 
            "text": "Cloudbreak supports \"recipes\" - user-provided customization scripts that can be run prior to or after cluster installtion. It is the user\u2019s responsibility to provide an idempotent well tested script. If the execution fails, the recipe logs can be found at  /var/log/recipes  on the nodes on which the recipes were executed.  It is advised, but not required to have an advanced logging mechanism in the script, as Cloudbreak always logs every script that are run. Recipes are often the sources of installation failures as users might try to remove necessary packages or reconfigure services.", 
            "title": "Recipe Logs"
        }, 
        {
            "location": "/trouble-cb/index.html#common-errors", 
            "text": "", 
            "title": "Common Errors"
        }, 
        {
            "location": "/trouble-cb/index.html#quota-limitations", 
            "text": "Each cloud provider has quota limitations on various cloud resources, and these quotas can usually be increased on request. If there is an error message in Cloudbreak saying that there are no more available EIPs (Elastic IP Address) or VPCs, you need to request more of these resources.   To see the limitations visit the cloud provider\u2019s site:   AWS Service Limits    Azure subscription and service limits, quotas, and constraints  GCP Resource Quotas", 
            "title": "Quota Limitations"
        }, 
        {
            "location": "/trouble-cb/index.html#connection-timeout-ports-not-open", 
            "text": "In the cluster installation wizard, you must specify on which node you want to run the Ambari server. Cloudbreak communicates with this node to orchestrate the installation.  A common reason for connection timeout is security group misconfiguration. Cloudbreak allows configuring different security groups for the different instance groups; however, there are certain requirements for the Ambari server node. Specifically, the following ports must be open in order to communicate with that node:   22 (SSH)    9443 (two-way-ssl through nginx)", 
            "title": "Connection Timeout: Ports Not Open"
        }, 
        {
            "location": "/trouble-cb/index.html#blueprints-invalid-services-and-configurations", 
            "text": "Ambari blueprints are a declarative definition of a cluster. With a blueprint, you specify a stack, the component layout, and the configurations to materialize a Hadoop cluster instance via a REST API without having to use the Ambari cluster install wizard.   Cloudbreak supports any type of blueprints, which is a common source of errors. These errors are only visible once the core infrastructure is up and running and Cloudbreak tries to initiate the cluster installation through Ambari. Ambari validates the blueprint and  rejects it if it's invalid.   For example, if there are configurations for a certain service like Hive but Hive as a service is not mapped to any host group, the blueprint is invalid.  To fix these type of issues, edit your blueprint and then reinstall your cluster. Cloudbreak UI has support for this so the infrastructure does not have to be terminated.  There are some cases when Ambari cannot validate your blueprint beforehand. In these cases, the issues are only visible in the Ambari server logs. To trubleshoot, check Ambari server logs.", 
            "title": "Blueprints: Invalid Services and Configurations"
        }, 
        {
            "location": "/trouble-cb/index.html#blueprints-high-availability", 
            "text": "Cloudbreak always tries to validate that a blueprint not to include multiple master services into different host groups. However, this exact setup is required for HA clusters. To overcome this, you can disable blueprint validation in the UI (using an advanced option in the Create Cluster wizard   Choose Blueprint), but you must include the necessary configurations.", 
            "title": "Blueprints: High Availability"
        }, 
        {
            "location": "/trouble-cb/index.html#blueprints-wrong-hdp-version", 
            "text": "In the blueprint, only the major and minor HDP version should be defined (for example, \"2.6\"). If wrong version number is provided, the following error can be found in the logs:  5/15/2017 12:23:19 PM testcluster26 - create failed: Cannot use the specified Ambari stack: HDPRepo\n{stack='null'; utils='null'}\n. Error: org.apache.ambari.server.controller.spi.NoSuchResourceException: The specified resource doesn't exist: Stack data, Stack HDP 2.6.0.3 is not found in Ambari metainfo  For correct blueprint layout, refer to the  Ambari cwiki  page.", 
            "title": "Blueprints: Wrong HDP Version"
        }, 
        {
            "location": "/trouble-cb/index.html#recipes-recipe-execution-times-out", 
            "text": "If the scripts are taking too much time to execute, the processes will time out, as the threshold for all recipes is set to 15 minutes. To change this threshold, you must override the default value by adding the following to the cbd Profile file:  export CB_JAVA_OPTS=\u201d -Dcb.max.salt.recipe.execution.retry=90\u201d  This property indicates the number of tries for checking if the scripts have finished with a sleep time (i.e. the wait time between two polling attempts) of 10 seconds. The default value is 90. To increase the threshold, provide a number greater than 90. You must restart Cloudbreak after changing properties in the Profile file.", 
            "title": "Recipes: Recipe Execution Times Out"
        }, 
        {
            "location": "/trouble-cb/index.html#recipes-recipe-execution-fails", 
            "text": "It often happens that a script cannot be executed successfully because there are typos or errors in the script. To verify this you can check the recipe logs at /var/log/recipes . For each script, there will be a separate log file with the name of the script that you provided on the Cloudbreak UI.", 
            "title": "Recipes: Recipe Execution Fails"
        }, 
        {
            "location": "/trouble-cb/index.html#invalid-public_ip-in-cbd-profile", 
            "text": "The  PUBLIC_IP  property must be set in the cbd Profile file or else you won\u2019t be able to log in on the Cloudbreak UI.   If you are migrating your instance, make sure that after the start the IP remains valid. If you need to edit the  PUBLIC_IP  property in Profile, make sure to restart Cloudbreak using  cbd restart .", 
            "title": "Invalid PUBLIC_IP in CBD Profile"
        }, 
        {
            "location": "/trouble-cb/index.html#cbd-cannot-get-vms-public-ip", 
            "text": "By default the  cbd  tool tries to get the VM's public IP to bind Cloudbreak UI to it. But if  cbd  cannot get the IP address during the initialization, you must set it manually. Check your  Profile  and if  PUBLIC_IP  is not set, add the  PUBLIC_IP  variable and set it to the public IP of the VM. For example:   export PUBLIC_IP=192.134.23.10", 
            "title": "Cbd Cannot Get VM's Public IP"
        }, 
        {
            "location": "/trouble-cb/index.html#permission-or-connection-problems", 
            "text": "If you face permission or connection issues, disable SELinux:   Set  SELINUX=disabled  in  /etc/selinux/config .    Reboot the machine.     Ensure the SELinux is not turned on afterwards:", 
            "title": "Permission or Connection Problems"
        }, 
        {
            "location": "/trouble-cb/index.html#changing-properties-in-the-cloudbreak-profile-file", 
            "text": "There are many properties that can be changed in the Cloudbreak application. These values must be changed in the Cloudbreak  Profil e file. To see all possible options, use the following command: cbd env show .  After changing a property, you must regenerate the config file and restart the application. There are two ways to do this:  In version 1.4.0 and newer of the cbd command line, you can regenerate the config file and restart the application with a single command:  cbd restart  - same as cbd regenerate/kill/start.  In versions earlier than 1.4.0, you must run the following three commands:  cbd regenerate  regenerates the Docker compose file cbd kill  removes all Docker containers (there is no stop command for this). cbd start  starts the application with the new compose file.", 
            "title": "Changing Properties in the Cloudbreak Profile File"
        }, 
        {
            "location": "/trouble-aws/index.html", 
            "text": "Troubleshooting Cloudbreak on AWS\n\n\nComing soon! \n\n\nMeanwhile, check out \nHDCloud\n troubleshooting docs.", 
            "title": "Troubleshooting AWS"
        }, 
        {
            "location": "/trouble-aws/index.html#troubleshooting-cloudbreak-on-aws", 
            "text": "Coming soon!   Meanwhile, check out  HDCloud  troubleshooting docs.", 
            "title": "Troubleshooting Cloudbreak on AWS"
        }, 
        {
            "location": "/trouble-azure/index.html", 
            "text": "Troubleshooting Cloudbreak on Azure\n\n\nCredential Creation Errors\n\n\nRole already exists\n\n\nExample error message: \nRole already exists in Azure with the name: CloudbreakCustom50\n\n\nSymptom\n: You specified that you want to create a new role for Cloudbreak credential, but an existing role with the same name already exists in Azure. \n\n\nSolution\n: You should either rename the role during credential creation or select the \nReuse existing custom role\n option. \n\n\nRole does not exist\n\n\nExample error message: \nRole does not exist in Azure with the name: CloudbreakCustom60\n\n\nSymptom\n: You specified that you want to reuse an existing role for your Cloudbreak credential, but that particular role does not exist in Azure.\n\n\nSolution\n: You should either rename the new role during the credential creation to match the existing role's name or select the \nLet Cloudbreak create a custom role\n option. \n\n\nRole does not have enough privileges\n\n\nExample error message: \nCloudbreakCustom 50 role does not have enough privileges to be used by Cloudbreak!\n\n\nPlease contact the documentaion for more information!\n\n\nSymptom\n: You specified that you want to reuse an  existing role for your Cloudbreak credential, but that particular role does not have the necessary privileges for Cloudbreak cluster management.\n\n\nSolution\n: You should either select an existing role with enough privileges or select the \nLet Cloudbreak create a custom role\n option.\n\n\nThe necessary action set for Cloudbreak to be able to manage the clusters includes:\n        \n\"Microsoft.Compute/*\",\n        \"Microsoft.Network/*\",\n        \"Microsoft.Storage/*\",\n        \"Microsoft.Resources/*\"\n\n\nCloud not validate publickey certificate\n\n\nExample error message: \nCould not validate publickey certificate [certificate: 'fdfdsf'], detailed message: Corrupt or unknown public key file format\n\n\nSymptom\n: The syntax of your SSH public key is incorrect.\n\n\nSolution\n: You must correct the syntax of your SSH key. For information about the correct syntax, refer to \nthis\n page.", 
            "title": "Troubleshooting Azure"
        }, 
        {
            "location": "/trouble-azure/index.html#troubleshooting-cloudbreak-on-azure", 
            "text": "", 
            "title": "Troubleshooting Cloudbreak on Azure"
        }, 
        {
            "location": "/trouble-azure/index.html#credential-creation-errors", 
            "text": "", 
            "title": "Credential Creation Errors"
        }, 
        {
            "location": "/trouble-azure/index.html#role-already-exists", 
            "text": "Example error message:  Role already exists in Azure with the name: CloudbreakCustom50  Symptom : You specified that you want to create a new role for Cloudbreak credential, but an existing role with the same name already exists in Azure.   Solution : You should either rename the role during credential creation or select the  Reuse existing custom role  option.", 
            "title": "Role already exists"
        }, 
        {
            "location": "/trouble-azure/index.html#role-does-not-exist", 
            "text": "Example error message:  Role does not exist in Azure with the name: CloudbreakCustom60  Symptom : You specified that you want to reuse an existing role for your Cloudbreak credential, but that particular role does not exist in Azure.  Solution : You should either rename the new role during the credential creation to match the existing role's name or select the  Let Cloudbreak create a custom role  option.", 
            "title": "Role does not exist"
        }, 
        {
            "location": "/trouble-azure/index.html#role-does-not-have-enough-privileges", 
            "text": "Example error message:  CloudbreakCustom 50 role does not have enough privileges to be used by Cloudbreak!  Please contact the documentaion for more information!  Symptom : You specified that you want to reuse an  existing role for your Cloudbreak credential, but that particular role does not have the necessary privileges for Cloudbreak cluster management.  Solution : You should either select an existing role with enough privileges or select the  Let Cloudbreak create a custom role  option.  The necessary action set for Cloudbreak to be able to manage the clusters includes:\n         \"Microsoft.Compute/*\",\n        \"Microsoft.Network/*\",\n        \"Microsoft.Storage/*\",\n        \"Microsoft.Resources/*\"", 
            "title": "Role does not have enough privileges"
        }, 
        {
            "location": "/trouble-azure/index.html#cloud-not-validate-publickey-certificate", 
            "text": "Example error message:  Could not validate publickey certificate [certificate: 'fdfdsf'], detailed message: Corrupt or unknown public key file format  Symptom : The syntax of your SSH public key is incorrect.  Solution : You must correct the syntax of your SSH key. For information about the correct syntax, refer to  this  page.", 
            "title": "Cloud not validate publickey certificate"
        }, 
        {
            "location": "/trouble-gcp/index.html", 
            "text": "Troubleshooting Cloudbreak on GCP\n\n\nComing soon!", 
            "title": "Troubleshooting GCP"
        }, 
        {
            "location": "/trouble-gcp/index.html#troubleshooting-cloudbreak-on-gcp", 
            "text": "Coming soon!", 
            "title": "Troubleshooting Cloudbreak on GCP"
        }, 
        {
            "location": "/trouble-os/index.html", 
            "text": "Troubleshooting Cloudbreak on OpenStack", 
            "title": "Troubleshooting OpenStack"
        }, 
        {
            "location": "/trouble-os/index.html#troubleshooting-cloudbreak-on-openstack", 
            "text": "", 
            "title": "Troubleshooting Cloudbreak on OpenStack"
        }, 
        {
            "location": "/dev/index.html", 
            "text": "Developer Documentation\n\n\nThe following table includes links to Cloudbreak developer documentation: \n\n\n\n\n\n\n\n\nDocumentation Link\n\n\nDescripriton\n\n\n\n\n\n\n\n\n\n\nSet Up Local Development\n\n\nThis documentation will help you set up your local development environment.\n\n\n\n\n\n\nSPI Reference\n\n\nThis is Cloudbreak SPI reference documentation.\n\n\n\n\n\n\nAPI Reference\n\n\nThis is Cloudbreak API reference documentation.\n\n\n\n\n\n\nFlow Diagrams\n\n\nThis is Cloudbreak flow diagrams reference documentation.", 
            "title": "Developer Docs"
        }, 
        {
            "location": "/dev/index.html#developer-documentation", 
            "text": "The following table includes links to Cloudbreak developer documentation:      Documentation Link  Descripriton      Set Up Local Development  This documentation will help you set up your local development environment.    SPI Reference  This is Cloudbreak SPI reference documentation.    API Reference  This is Cloudbreak API reference documentation.    Flow Diagrams  This is Cloudbreak flow diagrams reference documentation.", 
            "title": "Developer Documentation"
        }, 
        {
            "location": "/dev-spi/index.html", 
            "text": "Cloudbreak Service Provider Interface (SPI)\n\n\nIn addition to supporting multiple cloud platforms, Cloudbreak provides an easy way to integrate a new provider trough its \nService Provider Interface (SPI)\n, a plugin mechanism that enables seamless integration with any cloud provider. \n\n\nThis SPI plugin mechanism has been used to integrate all currently supported providers with Cloudbreak. The following links point to the Cloudbreak SPI implementations for AWS, Azure, Google Cloud, and OpenStack. You can use these implementations as a reference:\n\n\n\n\nThe \ncloud-aws\n module integrates Amazon Web Services\n\n\nThe \ncloud-arm\n module integrates Microsoft Azure\n\n\nThe \ncloud-gcp\n module integrates Google Cloud Platform  \n\n\nThe \ncloud-openstack\n module integrates OpenStack\n\n\n\n\nThe Cloubdreak SPI interface is event-based, scalable, and decoupled from Cloudbreak. The core of Cloudbreak uses \nEventBus\n to communicate with the providers, but the complexity of event handling is hidden from the provider implementation.\n\n\nSupported Resource Management Methods\n\n\nCloud providers support two kinds of deployment and resource management methods:\n\n\n\n\nTemplate based deployments\n\n\nIndividual resource based deployments\n\n\n\n\nCloudbreak's SPI supports both of these methods. It provides a well-defined interface, abstract classes, and helper classes, scheduling and polling of resources to aid the integration and to avoid any boilerplate code in the module of cloud provider.\n\n\nTemplate Based Deployments\n\n\nProviders with template-based deployments such as \nAWS CloudFormation\n, \nAzure ARM\n or \nOpenStack Heat\n have the ability to create and manage a collection of related cloud resources, provisioning and updating them in an orderly and predictable fashion. \n\n\nWhen working with providers that use template-based deployments, Cloudbreak needs to be provided with a reference to the template, because every change in the infrastructure (for example, creating a new instance or deleting one) is managed through this templating mechanism.\n\n\nIf a provider has templating support, then the provider's \ngradle\n module depends on the \ncloud-api\n module:\n\n\napply plugin: 'java'\n\nsourceCompatibility = 1.7\n\nrepositories {\n    mavenCentral()\n}\n\njar {\n    baseName = 'cloud-new-provider'\n}\n\ndependencies {\n\n    compile project(':cloud-api')\n\n}\n\n\n\n\nThe entry point for the provider is the  \nCloudConnector\n interface and every interface that needs to be implemented is reachable trough this interface.\n\n\nIndividual Resource Based Deployments\n\n\nThere are providers such as GCP that do not support a templating mechanism, and customizable providers such as OpenStack where the Heat Orchestration (templating) component is optional and individual resources need to be handled separately. \n\n\nWhen working with such providers, resources such as networks, discs, and compute instances need to be created and managed with an ordered sequence of API calls, and Cloudbreak needs to provide a solution to manage the collection of related cloud resources as a whole.\n\n\nIf the provider has no templating support, then the provider's \ngradle\n module typically depends on the \ncloud-template\n module, which includes Cloudbreak-defined abstract template. This template is a set of abstract and utility classes to support provisioning and updating related resources in an orderly and predictable manner trough ordered sequences of cloud API calls:\n\n\napply plugin: 'java'\n\nsourceCompatibility = 1.7\n\nrepositories {\n    mavenCentral()\n}\n\njar {\n    baseName = 'cloud-new-provider'\n}\n\ndependencies {\n\n    compile project(':cloud-template')\n\n}\n\n\n\n\nSupport for Modularity\n\n\nCloudbreak uses \nvariants\n to deal with highly modular providers such as OpenStack, which allows you to install different components (for volume storage, networking, and so on) and to entirely exclude certain components. For example, Nova or Neutron can be used for networking in OpenStack and some components such as Heat may not installed at all in some deployment scenarios. \n\n\nCloudbreak SPI interface uses variants to support this flexibility: if some part of the cloud provider uses a different component, you don't need to re-implement the complete stack, but just use a different variant and re-implement the part that is different.\n\n\nAn example implementation for this feature can be found in the \ncloud-openstack\n module which supports a HEAT and NATIVE variants. While the HEAT variant utilizes the Heat templating to launch a stack, the NATIVE variant starts the cluster by using a sequence of API calls without Heat to achieve the same result. Both of them use the same authentication and credential management.", 
            "title": "Cloudbreak SPI"
        }, 
        {
            "location": "/dev-spi/index.html#cloudbreak-service-provider-interface-spi", 
            "text": "In addition to supporting multiple cloud platforms, Cloudbreak provides an easy way to integrate a new provider trough its  Service Provider Interface (SPI) , a plugin mechanism that enables seamless integration with any cloud provider.   This SPI plugin mechanism has been used to integrate all currently supported providers with Cloudbreak. The following links point to the Cloudbreak SPI implementations for AWS, Azure, Google Cloud, and OpenStack. You can use these implementations as a reference:   The  cloud-aws  module integrates Amazon Web Services  The  cloud-arm  module integrates Microsoft Azure  The  cloud-gcp  module integrates Google Cloud Platform    The  cloud-openstack  module integrates OpenStack   The Cloubdreak SPI interface is event-based, scalable, and decoupled from Cloudbreak. The core of Cloudbreak uses  EventBus  to communicate with the providers, but the complexity of event handling is hidden from the provider implementation.", 
            "title": "Cloudbreak Service Provider Interface (SPI)"
        }, 
        {
            "location": "/dev-spi/index.html#supported-resource-management-methods", 
            "text": "Cloud providers support two kinds of deployment and resource management methods:   Template based deployments  Individual resource based deployments   Cloudbreak's SPI supports both of these methods. It provides a well-defined interface, abstract classes, and helper classes, scheduling and polling of resources to aid the integration and to avoid any boilerplate code in the module of cloud provider.", 
            "title": "Supported Resource Management Methods"
        }, 
        {
            "location": "/dev-spi/index.html#template-based-deployments", 
            "text": "Providers with template-based deployments such as  AWS CloudFormation ,  Azure ARM  or  OpenStack Heat  have the ability to create and manage a collection of related cloud resources, provisioning and updating them in an orderly and predictable fashion.   When working with providers that use template-based deployments, Cloudbreak needs to be provided with a reference to the template, because every change in the infrastructure (for example, creating a new instance or deleting one) is managed through this templating mechanism.  If a provider has templating support, then the provider's  gradle  module depends on the  cloud-api  module:  apply plugin: 'java'\n\nsourceCompatibility = 1.7\n\nrepositories {\n    mavenCentral()\n}\n\njar {\n    baseName = 'cloud-new-provider'\n}\n\ndependencies {\n\n    compile project(':cloud-api')\n\n}  The entry point for the provider is the   CloudConnector  interface and every interface that needs to be implemented is reachable trough this interface.", 
            "title": "Template Based Deployments"
        }, 
        {
            "location": "/dev-spi/index.html#individual-resource-based-deployments", 
            "text": "There are providers such as GCP that do not support a templating mechanism, and customizable providers such as OpenStack where the Heat Orchestration (templating) component is optional and individual resources need to be handled separately.   When working with such providers, resources such as networks, discs, and compute instances need to be created and managed with an ordered sequence of API calls, and Cloudbreak needs to provide a solution to manage the collection of related cloud resources as a whole.  If the provider has no templating support, then the provider's  gradle  module typically depends on the  cloud-template  module, which includes Cloudbreak-defined abstract template. This template is a set of abstract and utility classes to support provisioning and updating related resources in an orderly and predictable manner trough ordered sequences of cloud API calls:  apply plugin: 'java'\n\nsourceCompatibility = 1.7\n\nrepositories {\n    mavenCentral()\n}\n\njar {\n    baseName = 'cloud-new-provider'\n}\n\ndependencies {\n\n    compile project(':cloud-template')\n\n}", 
            "title": "Individual Resource Based Deployments"
        }, 
        {
            "location": "/dev-spi/index.html#support-for-modularity", 
            "text": "Cloudbreak uses  variants  to deal with highly modular providers such as OpenStack, which allows you to install different components (for volume storage, networking, and so on) and to entirely exclude certain components. For example, Nova or Neutron can be used for networking in OpenStack and some components such as Heat may not installed at all in some deployment scenarios.   Cloudbreak SPI interface uses variants to support this flexibility: if some part of the cloud provider uses a different component, you don't need to re-implement the complete stack, but just use a different variant and re-implement the part that is different.  An example implementation for this feature can be found in the  cloud-openstack  module which supports a HEAT and NATIVE variants. While the HEAT variant utilizes the Heat templating to launch a stack, the NATIVE variant starts the cluster by using a sequence of API calls without Heat to achieve the same result. Both of them use the same authentication and credential management.", 
            "title": "Support for Modularity"
        }, 
        {
            "location": "/dev-api/index.html", 
            "text": "Cloudbreak APIs\n\n\nCloudbreak is a RESTful application development platform whose goal is to help developers deploy HDP clusters in various cloud environments. Once Cloudbreak is deployed in your favorite servlet container, it exposes REST APIs, allowing you to spin up Hadoop clusters of any size with your chosen cloud provider.\n\n\nAPI Documentation\n\n\nThe Cloudbreak API documentation is available \nhere\n. \n\n\n\n\nThis documentation was generated from the code using \nSwagger\n.", 
            "title": "Cloudbreak APIs"
        }, 
        {
            "location": "/dev-api/index.html#cloudbreak-apis", 
            "text": "Cloudbreak is a RESTful application development platform whose goal is to help developers deploy HDP clusters in various cloud environments. Once Cloudbreak is deployed in your favorite servlet container, it exposes REST APIs, allowing you to spin up Hadoop clusters of any size with your chosen cloud provider.", 
            "title": "Cloudbreak APIs"
        }, 
        {
            "location": "/dev-api/index.html#api-documentation", 
            "text": "The Cloudbreak API documentation is available  here .    This documentation was generated from the code using  Swagger .", 
            "title": "API Documentation"
        }, 
        {
            "location": "/dev-flows/index.html", 
            "text": "Flow Diagrams\n\n\nRefer to \nhttp://hortonworks.github.io/cloudbreak-docs/release-1.16.4/flow/\n.", 
            "title": "Flow Diagrams"
        }, 
        {
            "location": "/dev-flows/index.html#flow-diagrams", 
            "text": "Refer to  http://hortonworks.github.io/cloudbreak-docs/release-1.16.4/flow/ .", 
            "title": "Flow Diagrams"
        }, 
        {
            "location": "/releasenotes/index.html", 
            "text": "Release Notes\n\n\n2.1.0 TP\n\n\nThis release is technical preview: it is not suitable for production environments.\n\n\nNew Features\n\n\nNew UI/UX\n\n\nCloudbreak 2.1.0 TP introduces a new user interface.\n\n\nNew CLI\n\n\nCloudbreak 2.1.0 TP introduces a new CLI tool. For more information, refer to the \nInstall CLI\n and \nCLI Reference\n documentation. \n\n\nBehavioral Changes\n\n\nCreating Custom Images\n\n\nThe functionality which enables you to create custom images was changed and improved. Refer to  \nCustom Images\n.\n\n\nRemoval of Cloudbreak Shell\n\n\nCloudbreak Shell is no longer available in Cloudbreak 2.1.0 TP and later. It was replaced by the \nCloudbreak CLI\n.\n\n\nRemoval of Platforms\n\n\nThe \nplatforms\n feature was removed. \n\n\nRemoval of Mesos\n\n\nCloudbreak 2.1.0 TP does not support Mesos cloud provider.\n\n\nRemoval of Templates\n\n\nEarlier versions of Cloudbreak allowed you to save infrastructure, network, and security group templates. This feature was removed. Instead, you can define VMs, storage, networks, and security groups as part of the create cluster wizard. \n\n\nKnown Issues\n\n\nAuto-scaling Is Not Available\n\n\nAuto-scaling functionality is not available in Cloudbreak 2.1.0 TP.", 
            "title": "Release Notes"
        }, 
        {
            "location": "/releasenotes/index.html#release-notes", 
            "text": "", 
            "title": "Release Notes"
        }, 
        {
            "location": "/releasenotes/index.html#210-tp", 
            "text": "This release is technical preview: it is not suitable for production environments.", 
            "title": "2.1.0 TP"
        }, 
        {
            "location": "/releasenotes/index.html#new-features", 
            "text": "", 
            "title": "New Features"
        }, 
        {
            "location": "/releasenotes/index.html#new-uiux", 
            "text": "Cloudbreak 2.1.0 TP introduces a new user interface.", 
            "title": "New UI/UX"
        }, 
        {
            "location": "/releasenotes/index.html#new-cli", 
            "text": "Cloudbreak 2.1.0 TP introduces a new CLI tool. For more information, refer to the  Install CLI  and  CLI Reference  documentation.", 
            "title": "New CLI"
        }, 
        {
            "location": "/releasenotes/index.html#behavioral-changes", 
            "text": "", 
            "title": "Behavioral Changes"
        }, 
        {
            "location": "/releasenotes/index.html#creating-custom-images", 
            "text": "The functionality which enables you to create custom images was changed and improved. Refer to   Custom Images .", 
            "title": "Creating Custom Images"
        }, 
        {
            "location": "/releasenotes/index.html#removal-of-cloudbreak-shell", 
            "text": "Cloudbreak Shell is no longer available in Cloudbreak 2.1.0 TP and later. It was replaced by the  Cloudbreak CLI .", 
            "title": "Removal of Cloudbreak Shell"
        }, 
        {
            "location": "/releasenotes/index.html#removal-of-platforms", 
            "text": "The  platforms  feature was removed.", 
            "title": "Removal of Platforms"
        }, 
        {
            "location": "/releasenotes/index.html#removal-of-mesos", 
            "text": "Cloudbreak 2.1.0 TP does not support Mesos cloud provider.", 
            "title": "Removal of Mesos"
        }, 
        {
            "location": "/releasenotes/index.html#removal-of-templates", 
            "text": "Earlier versions of Cloudbreak allowed you to save infrastructure, network, and security group templates. This feature was removed. Instead, you can define VMs, storage, networks, and security groups as part of the create cluster wizard.", 
            "title": "Removal of Templates"
        }, 
        {
            "location": "/releasenotes/index.html#known-issues", 
            "text": "", 
            "title": "Known Issues"
        }, 
        {
            "location": "/releasenotes/index.html#auto-scaling-is-not-available", 
            "text": "Auto-scaling functionality is not available in Cloudbreak 2.1.0 TP.", 
            "title": "Auto-scaling Is Not Available"
        }, 
        {
            "location": "/faq/index.html", 
            "text": "FAQs\n\n\nHow to...\n\n\nGenerate SSH Key Pair\n\n\nAll the instances created by Cloudbreak are configured to allow key-based SSH, so you'll need to provide an SSH public key that can be used later to SSH onto the instances in the clusters you'll create with Cloudbreak. You can use one of your existing keys or you can generate a new one.\n\n\nTo generate a new SSH key pair, execute:\n\n\nssh-keygen -t rsa -b 4096 -C \"your_email@example.com\"\n# Creates a new ssh key, using the provided email as a label\n# Generating public/private rsa key pair.\n# Enter file in which to save the key (/Users/you/.ssh/id_rsa): [Press enter]\n\n\n\nYou'll be asked to enter a passphrase, but you can leave it empty:\n\n\n# Enter passphrase (empty for no passphrase): [Type a passphrase]\n# Enter same passphrase again: [Type passphrase again]\n\n\n\nAfter you enter (or not) a passphrase, the key pair is generated. The output should look similar to:\n\n\n# Your identification has been saved in /Users/you/.ssh/id_rsa.\n# Your public key has been saved in /Users/you/.ssh/id_rsa.pub.\n# The key fingerprint is:\n# 01:0f:f4:3b:ca:85:sd:17:sd:7d:sd:68:9d:sd:a2:sd your_email@example.com\n\n\n\nLater you'll need to pass the content of the \n.pub\n file to Cloudbreak and use the private key file to SSH to the instances. \n\n\nRecover Public SSH Key\n\n\nThe \n-y\n option of \nssh-keygen\n outputs the public key. For example:\n\n\nssh-keygen -y -f ~/.ssh/id_rsa > ~/.ssh/id_rsa.pub\n\n\n\nSSH to the Hosts\n\n\nTo connect to a running VM through SSH, you need to know its public IP address and have your private key available. \n\n\nThe private key that you must use to access the VM is the counterpart of the public key that you specified when creating a Cloudbreak credential.\n\n\nYou can find the IP addresses of all the running VMs in the Cloudbreak UI, on the cluster details page. Only key-based authentication is supported. \n\n\nCloudbreak creates a cloudbreak user which can be used to ssh into the box. This user has passwordless sudo rights.\n\n\nFor example:\n\n\nssh -i ~/.ssh/your-private-key.pem cloudbreak@\npublic-ip\n\n\n\n\n\nCheck Cloudbreak Version\n\n\nTo check Cloudbreak version, navigate to the Cloudbreak home directory and execute the following command:\n\n\ncbd doctor\n\n\n\n\nCheck Available Environment Variables\n\n\nTo see all available environment variables with their default values, use:\n\n\ncbd env show\n\n\n\n\nAccess Cloudbreak Logs\n\n\nRefer to \nTroubleshooting\n.\n\n\nDebug in Cloudbreak Shell\n\n\nTo get more detailed command prompt output, set the DEBUG environment variable to non-zero:\n\n\nDEBUG=1 cbd \nsome_command\n\n\n\n\n\nConfigure and Test Proxy Settings\n\n\nFor cbd\n\n\nTo configure proxy settings for Cloudbreak Deployer, add the following configs to your Profile:\n\n\nexport http_proxy=\nhttp://YOUR_PROXY_ADDRESS:YOUR_PROXY_PORT/\n\nexport https_proxy=\nhttp(s)://YOUR_PROXY_ADDRESS:YOUR_PROXY_PORT/\n\nexport CB_HTTP_PROXY=\nhttp://YOUR_PROXY_ADDRESS:YOUR_PROXY_PORT/\n\nexport CB_HTTPS_PROXY=\nhttp(s)://YOUR_PROXY_ADDRESS:YOUR_PROXY_PORT/\n\nexport CB_JAVA_OPTS=\n-Dhttp.proxyHost=YOUR_PROXY_ADDRESS -Dhttp.proxyPort=YOUR_PROXY_PORT -Dhttps.proxyHost=YOUR_PROXY_ADDRESS -Dhttps.proxyPort=YOUR_PROXY_PORT -Dhttp.nonProxyHosts=172.17.0.1|*.service.consul|*.node.dc1.consul\n\n\n\n\n\nFor Docker\n\n\nTo download newer Docker images from the official repository, you need to configure proxy settings for the Docker service. You can do this by configuring the 'HTTP_PROXY' variable in your environment. Next, restart the docker service. For more information, refer to \nDocker documentation\n.\n\n\nFor Provisioned Clusters\n\n\nFor a cluster to be provisioned to a (virtual) network that is behind a proxy, the yum on the provisioned machines needs to be configured to use that proxy. This is important because the Ambari install needs access to public repositories. You can configure yum proxy settings by using the recipe functionality of Cloudbreak. Use the following bash script to create a 'pre' recipe that will run on all of the nodes before the Ambari install:\n\n\n#!/bin/bash\ncat \n /etc/yum.conf \nENDOF\n\nproxy=http://YOUR_PROXY_ADDRESS:YOUR_PROXY_PORT\n\nENDOF\n\n\n\n\nTest Your Proxy Settings\n\n\nYou can use the following CURL command to test your proxy settings:\n\n\nhttps_proxy=\nYOUR_PROXY_ADDRESS:YOUR_PROXY_PORT\n curl -X GET -I --insecure https://cloudbreak-api.sequenceiq.com/info\n\n\n\n\nIts output should start with:\n\n\nHTTP/1.1 200 OK", 
            "title": "FAQs"
        }, 
        {
            "location": "/faq/index.html#faqs", 
            "text": "How to...", 
            "title": "FAQs"
        }, 
        {
            "location": "/faq/index.html#generate-ssh-key-pair", 
            "text": "All the instances created by Cloudbreak are configured to allow key-based SSH, so you'll need to provide an SSH public key that can be used later to SSH onto the instances in the clusters you'll create with Cloudbreak. You can use one of your existing keys or you can generate a new one.  To generate a new SSH key pair, execute:  ssh-keygen -t rsa -b 4096 -C \"your_email@example.com\"\n# Creates a new ssh key, using the provided email as a label\n# Generating public/private rsa key pair.\n# Enter file in which to save the key (/Users/you/.ssh/id_rsa): [Press enter]  You'll be asked to enter a passphrase, but you can leave it empty:  # Enter passphrase (empty for no passphrase): [Type a passphrase]\n# Enter same passphrase again: [Type passphrase again]  After you enter (or not) a passphrase, the key pair is generated. The output should look similar to:  # Your identification has been saved in /Users/you/.ssh/id_rsa.\n# Your public key has been saved in /Users/you/.ssh/id_rsa.pub.\n# The key fingerprint is:\n# 01:0f:f4:3b:ca:85:sd:17:sd:7d:sd:68:9d:sd:a2:sd your_email@example.com  Later you'll need to pass the content of the  .pub  file to Cloudbreak and use the private key file to SSH to the instances.", 
            "title": "Generate SSH Key Pair"
        }, 
        {
            "location": "/faq/index.html#recover-public-ssh-key", 
            "text": "The  -y  option of  ssh-keygen  outputs the public key. For example:  ssh-keygen -y -f ~/.ssh/id_rsa > ~/.ssh/id_rsa.pub", 
            "title": "Recover Public SSH Key"
        }, 
        {
            "location": "/faq/index.html#ssh-to-the-hosts", 
            "text": "To connect to a running VM through SSH, you need to know its public IP address and have your private key available.   The private key that you must use to access the VM is the counterpart of the public key that you specified when creating a Cloudbreak credential.  You can find the IP addresses of all the running VMs in the Cloudbreak UI, on the cluster details page. Only key-based authentication is supported.   Cloudbreak creates a cloudbreak user which can be used to ssh into the box. This user has passwordless sudo rights.  For example:  ssh -i ~/.ssh/your-private-key.pem cloudbreak@ public-ip", 
            "title": "SSH to the Hosts"
        }, 
        {
            "location": "/faq/index.html#check-cloudbreak-version", 
            "text": "To check Cloudbreak version, navigate to the Cloudbreak home directory and execute the following command:  cbd doctor", 
            "title": "Check Cloudbreak Version"
        }, 
        {
            "location": "/faq/index.html#check-available-environment-variables", 
            "text": "To see all available environment variables with their default values, use:  cbd env show", 
            "title": "Check Available Environment Variables"
        }, 
        {
            "location": "/faq/index.html#access-cloudbreak-logs", 
            "text": "Refer to  Troubleshooting .", 
            "title": "Access Cloudbreak Logs"
        }, 
        {
            "location": "/faq/index.html#debug-in-cloudbreak-shell", 
            "text": "To get more detailed command prompt output, set the DEBUG environment variable to non-zero:  DEBUG=1 cbd  some_command", 
            "title": "Debug in Cloudbreak Shell"
        }, 
        {
            "location": "/faq/index.html#configure-and-test-proxy-settings", 
            "text": "For cbd  To configure proxy settings for Cloudbreak Deployer, add the following configs to your Profile:  export http_proxy= http://YOUR_PROXY_ADDRESS:YOUR_PROXY_PORT/ \nexport https_proxy= http(s)://YOUR_PROXY_ADDRESS:YOUR_PROXY_PORT/ \nexport CB_HTTP_PROXY= http://YOUR_PROXY_ADDRESS:YOUR_PROXY_PORT/ \nexport CB_HTTPS_PROXY= http(s)://YOUR_PROXY_ADDRESS:YOUR_PROXY_PORT/ \nexport CB_JAVA_OPTS= -Dhttp.proxyHost=YOUR_PROXY_ADDRESS -Dhttp.proxyPort=YOUR_PROXY_PORT -Dhttps.proxyHost=YOUR_PROXY_ADDRESS -Dhttps.proxyPort=YOUR_PROXY_PORT -Dhttp.nonProxyHosts=172.17.0.1|*.service.consul|*.node.dc1.consul   For Docker  To download newer Docker images from the official repository, you need to configure proxy settings for the Docker service. You can do this by configuring the 'HTTP_PROXY' variable in your environment. Next, restart the docker service. For more information, refer to  Docker documentation .  For Provisioned Clusters  For a cluster to be provisioned to a (virtual) network that is behind a proxy, the yum on the provisioned machines needs to be configured to use that proxy. This is important because the Ambari install needs access to public repositories. You can configure yum proxy settings by using the recipe functionality of Cloudbreak. Use the following bash script to create a 'pre' recipe that will run on all of the nodes before the Ambari install:  #!/bin/bash\ncat   /etc/yum.conf  ENDOF\n\nproxy=http://YOUR_PROXY_ADDRESS:YOUR_PROXY_PORT\n\nENDOF  Test Your Proxy Settings  You can use the following CURL command to test your proxy settings:  https_proxy= YOUR_PROXY_ADDRESS:YOUR_PROXY_PORT  curl -X GET -I --insecure https://cloudbreak-api.sequenceiq.com/info  Its output should start with:  HTTP/1.1 200 OK", 
            "title": "Configure and Test Proxy Settings"
        }, 
        {
            "location": "/get-help/index.html", 
            "text": "Get Help\n\n\nIf you need help with Cloudbreak, you have two options:\n\n\n\n\n\n\n\n\nOption\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nHortonworks Community Connection\n\n\nThis is free optional support via Hortonworks Community Connection (HCC).\n\n\n\n\n\n\nHortonworks Flex Support Subscription\n\n\nThis is paid Hortonworks enterprise support.\n\n\n\n\n\n\n\n\nHCC\n\n\nYou can optionally register for optional free community support at \nHortonworks Community Connection\n where you can browse articles and previously answered questions, and ask questions of your own. When posting questions related to Cloudbreak, make sure to use the \"Cloudbreak\" tag.\n\n\nFlex Subscription\n\n\nYou can optionally use your existing Hortonworks \nFlex subscription(s)\n to cover the Cloudbreak node and all clusters created. \n\n\nPrerequisites\n: You must have an existing SmartSense ID and a Flex subscription. For general information about the Hortonworks Flex Support Subscription, visit the Hortonworks Support page at \nhttps://hortonworks.com/services/support/enterprise/\n.\n\n\nThe general steps are:\n\n\n\n\nConfigure Smart Sense in your \nProfile\n file.   \n\n\nRegister your Flex subscription in the Cloudbreak web UI in the the \nmanage flex subscriptions\n pane. You can register and manage multiple Flex subscriptions.   \n\n\n\n\n\n\nAlternatively, you can perform these steps using the Cloudbreak Shell. \n\n\n\n\nConfiguring SmartSense\n\n\nTo configure SmartSense in Cloudbreak, enable SmartSense and add your SmartSense ID to the \nProfile\n by adding the following variables:\n\n\nexport CB_SMARTSENSE_CONFIGURE=true\nexport CB_SMARTSENSE_ID=YOUR-SMARTSENSE-ID\n\n\n\nFor example:\n\n\nexport CB_SMARTSENSE_CONFIGURE=true\nexport CB_SMARTSENSE_ID=A-00000000-C-00000000\n\n\n\nYou can do this in one of the two ways:\n\n\n\n\nWhen initiating Cloudbreak Deployer  \n\n\nAfter you've already initiated Cloudbreak Deployer. If you choose this option, you must restart Cloudbreak using \ncbd restart\n.\n\n\n\n\n\n\nSmartSense ID defined in the \nProfile\n file always overrides the ID registered via Cloudbreak Shell.\n\n\n\n\nManaging Flex Subscriptions\n\n\nOnce you log in to the Cloudbreak web UI, you can manage your Flex subscriptions from the \nmanage flex subscriptions\n pane. You can:\n\n\n\n\nRegister a new Flex subscription.  \n\n\nSet a default Flex subscription.  \n\n\nSelect a Flex subscription to be used for cloud controller.  \n\n\nDelete a Flex subscription.  \n\n\nCheck which clusters are connected to a specific subscription.  \n\n\n\n\nWhen creating a cluster using the advanced options, in the \nCONFIGURE CLUSTER\n \n \nFlex Subscriptions\n, you can select the Flex subscription that you want to use.\n\n\nMore Resources\n\n\nCheck out the following documentation to learn more:\n\n\n\n\n Resource \nDescription\n\n\nHortonworks documentation \n\n\nDuring cluster create process, Hortonworks Data Cloud automatically installs Ambari and sets up a cluster for you. After this deployment is complete, refer to the \nAmbari documentation\n and \nHDP documentation\n for help.\n\n\n\n\n\n\nHortonworks tutorials\n\n\n\n\nUse Hortonworks tutorials to get started with Apache Spark, Apache Hive, Apache Zeppelin, and more.\n\n\nApache documentation\n\n\n\n\n In addition to Hortonworks documentation, refer to the Apache Software Foundation documentation to get information on specific Hadoop services. \n\n\n\n\n\nAmbari Blueprints\nLearn about Ambari Bleuprints. Ambari Blueprints are a declarative definition of a Hadoop cluster that Ambari can use to create Hadoop clusters.\n\n\nCloudbreak Project\nVisit the Hortonworks website to see Cloudbreak-related news and updates.\n\n\nApache Ambari Project\nLearn about the Apache Ambari Project. Apache Ambari is an operational platform for provisioning, managing, and monitoring Apache Hadoop clusters. Ambari exposes a robust set of REST APIs and a rich web interface for cluster management.", 
            "title": "Getting Help"
        }, 
        {
            "location": "/get-help/index.html#get-help", 
            "text": "If you need help with Cloudbreak, you have two options:     Option  Description      Hortonworks Community Connection  This is free optional support via Hortonworks Community Connection (HCC).    Hortonworks Flex Support Subscription  This is paid Hortonworks enterprise support.", 
            "title": "Get Help"
        }, 
        {
            "location": "/get-help/index.html#hcc", 
            "text": "You can optionally register for optional free community support at  Hortonworks Community Connection  where you can browse articles and previously answered questions, and ask questions of your own. When posting questions related to Cloudbreak, make sure to use the \"Cloudbreak\" tag.", 
            "title": "HCC"
        }, 
        {
            "location": "/get-help/index.html#flex-subscription", 
            "text": "You can optionally use your existing Hortonworks  Flex subscription(s)  to cover the Cloudbreak node and all clusters created.   Prerequisites : You must have an existing SmartSense ID and a Flex subscription. For general information about the Hortonworks Flex Support Subscription, visit the Hortonworks Support page at  https://hortonworks.com/services/support/enterprise/ .  The general steps are:   Configure Smart Sense in your  Profile  file.     Register your Flex subscription in the Cloudbreak web UI in the the  manage flex subscriptions  pane. You can register and manage multiple Flex subscriptions.       Alternatively, you can perform these steps using the Cloudbreak Shell.", 
            "title": "Flex Subscription"
        }, 
        {
            "location": "/get-help/index.html#configuring-smartsense", 
            "text": "To configure SmartSense in Cloudbreak, enable SmartSense and add your SmartSense ID to the  Profile  by adding the following variables:  export CB_SMARTSENSE_CONFIGURE=true\nexport CB_SMARTSENSE_ID=YOUR-SMARTSENSE-ID  For example:  export CB_SMARTSENSE_CONFIGURE=true\nexport CB_SMARTSENSE_ID=A-00000000-C-00000000  You can do this in one of the two ways:   When initiating Cloudbreak Deployer    After you've already initiated Cloudbreak Deployer. If you choose this option, you must restart Cloudbreak using  cbd restart .    SmartSense ID defined in the  Profile  file always overrides the ID registered via Cloudbreak Shell.", 
            "title": "Configuring SmartSense"
        }, 
        {
            "location": "/get-help/index.html#managing-flex-subscriptions", 
            "text": "Once you log in to the Cloudbreak web UI, you can manage your Flex subscriptions from the  manage flex subscriptions  pane. You can:   Register a new Flex subscription.    Set a default Flex subscription.    Select a Flex subscription to be used for cloud controller.    Delete a Flex subscription.    Check which clusters are connected to a specific subscription.     When creating a cluster using the advanced options, in the  CONFIGURE CLUSTER     Flex Subscriptions , you can select the Flex subscription that you want to use.", 
            "title": "Managing Flex Subscriptions"
        }, 
        {
            "location": "/get-help/index.html#more-resources", 
            "text": "Check out the following documentation to learn more:    Resource  Description  Hortonworks documentation   During cluster create process, Hortonworks Data Cloud automatically installs Ambari and sets up a cluster for you. After this deployment is complete, refer to the  Ambari documentation  and  HDP documentation  for help.    Hortonworks tutorials   Use Hortonworks tutorials to get started with Apache Spark, Apache Hive, Apache Zeppelin, and more.  Apache documentation    In addition to Hortonworks documentation, refer to the Apache Software Foundation documentation to get information on specific Hadoop services.    Ambari Blueprints Learn about Ambari Bleuprints. Ambari Blueprints are a declarative definition of a Hadoop cluster that Ambari can use to create Hadoop clusters.  Cloudbreak Project Visit the Hortonworks website to see Cloudbreak-related news and updates.  Apache Ambari Project Learn about the Apache Ambari Project. Apache Ambari is an operational platform for provisioning, managing, and monitoring Apache Hadoop clusters. Ambari exposes a robust set of REST APIs and a rich web interface for cluster management.", 
            "title": "More Resources"
        }, 
        {
            "location": "/smartsense/index.html", 
            "text": "SmartSense Telemetry\n\n\nHelp us make a better product by opt'ing in to automatically send information to Hortonworks. This includes enabling\nHortonworks SmartSense and sending performance and usage info. As you use the product,\nSmartSense measures and collects information and then sends these information bundles to Hortonworks.\n\n\nHow to Disable\n\n\nDisable Bundle Upload for the Cloud Controller and New Clusters\n\n\n\n    \nImportant\n\n    \n\n    Do not perform these steps when you have clusters currently in the process of being deployed.\n    Wait for all clusters to be deployed.\n\n\n\n\n\n\n\n\n\nSSH into the cloud controller host.\n\n\n\n\n\n\nEdit \n/var/lib/cloudbreak-deployment/Profile\n.\n\n\n\n\n\n\nChange \nCB_SMARTSENSE_CONFIGURE\n to \nfalse\n:\n\n    \nexport CB_SMARTSENSE_CONFIGURE=false\n\n\n\n\n\n\nRestart the cloud controller:\n\n    \ncd /var/lib/cloudbreak-deployment\ncbd restart\n\n\n\n\n\n\nDisable Bundle Upload for an Existing Cluster\n\n\n\n\n\n\nSSH into the Master node for the cluster.\n\n\n\n\n\n\nEdit \n/etc/hst/conf/hst-server.ini\n.\n\n\n\n\n\n\nChange \n[gateway]\n configuration to \nfalse\n:\n\n    \n[gateway]\nenabled=false\n\n\n\n\n\n\nRestart the SmartSense Server:\n    \nhst restart\n\n\n\n\n\n\n(Optional) Disable SmartSense daily bundle capture:\n\n\n\n\nSmartSense is scheduled to capture a telemetry bundle daily. With the bundle upload disabled, the bundle will still\nbe captured but just saved locally (i.e. not uploaded).\n\n\nTo disable the bundle capture, execute the following:\n\nhst capture-schedule -a pause\n\n\n\n\n\n\n\n\nRepeat on all existing clusters.", 
            "title": "SmartSense"
        }, 
        {
            "location": "/smartsense/index.html#smartsense-telemetry", 
            "text": "Help us make a better product by opt'ing in to automatically send information to Hortonworks. This includes enabling\nHortonworks SmartSense and sending performance and usage info. As you use the product,\nSmartSense measures and collects information and then sends these information bundles to Hortonworks.", 
            "title": "SmartSense Telemetry"
        }, 
        {
            "location": "/smartsense/index.html#how-to-disable", 
            "text": "", 
            "title": "How to Disable"
        }, 
        {
            "location": "/smartsense/index.html#disable-bundle-upload-for-the-cloud-controller-and-new-clusters", 
            "text": "Important \n     \n    Do not perform these steps when you have clusters currently in the process of being deployed.\n    Wait for all clusters to be deployed.     SSH into the cloud controller host.    Edit  /var/lib/cloudbreak-deployment/Profile .    Change  CB_SMARTSENSE_CONFIGURE  to  false : \n     export CB_SMARTSENSE_CONFIGURE=false    Restart the cloud controller: \n     cd /var/lib/cloudbreak-deployment\ncbd restart", 
            "title": "Disable Bundle Upload for the Cloud Controller and New Clusters"
        }, 
        {
            "location": "/smartsense/index.html#disable-bundle-upload-for-an-existing-cluster", 
            "text": "SSH into the Master node for the cluster.    Edit  /etc/hst/conf/hst-server.ini .    Change  [gateway]  configuration to  false : \n     [gateway]\nenabled=false    Restart the SmartSense Server:\n     hst restart    (Optional) Disable SmartSense daily bundle capture:   SmartSense is scheduled to capture a telemetry bundle daily. With the bundle upload disabled, the bundle will still\nbe captured but just saved locally (i.e. not uploaded).  To disable the bundle capture, execute the following: hst capture-schedule -a pause     Repeat on all existing clusters.", 
            "title": "Disable Bundle Upload for an Existing Cluster"
        }
    ]
}